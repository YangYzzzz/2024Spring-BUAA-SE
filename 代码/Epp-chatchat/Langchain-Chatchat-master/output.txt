这里的输出会到文件中
Number of GPUs available: 3
1, 2  2
1, 2
[UploadFile(filename='南大笔试复习.pdf', size=264799, headers=Headers({'content-disposition': 'form-data; name="files"; filename="å\x8d\x97å¤§ç¬\x94è¯\x95å¤\x8dä¹\xa0.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpyv8udjyw, tmpyv8udjyw
File: 南大笔试复习.pdf, msg: 成功上传文件 南大笔试复习.pdf, docs: [Document(page_content='计算机网络', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='1. A类地址1-126，B类地址128-191，C类地址192-223，127是自回环 2. TCP当前序号为x，发送y长度数据，对方回复的Ack=x+y 3. 网段地址（或称为网络地址）是通过将 IP 地址与子网掩码进行逻辑 AND 操作得到的 4. 数据报校验的协议位于哪个层次：传输层，网络层，链路层（循环冗余校验与帧校验序列） 5. 拥塞控制 6. 高优先级包优先发送：帧间隔发送时间 7. 以太网，传输速率100Mbps，信号传播速度200000km/s，如果最小数据帧的长度增大90bits，则 1. 电缆的最大长度能够增加多少？2. 信道利用率是上升还是下降？', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='2. 信道利用率是上升还是下降？ 1. （最小帧长/数据传输速率）* 信号传播速度=2*最大传输距离；即此处电缆的最大长度能够增 加90km； 2. 先根据传输速率算出传输时间 = 2 * 单向时延（即在一个数据帧传输的时间内，上一个数据帧 应该至少走过了两倍的全程），算出单向时延，通过信号传播速度算出最长长度。 8. 以太网速度快的原因：串行通信简单，非常适合长距离传输；串行通信由于控制简单、抗干扰能力 强、成本低等特点，得到了广泛的应用。 数据结构', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='1. m叉树的节点计算：按边算（会省略根节点），按点算（会省略叶子节点）\n2. 图的邻接矩阵：i\n>j，adj(i,j)=1\n3. 拓扑排序（有向无环图）：\n1. 将入度为0的点入栈\n2. 如果栈非空，出栈，将该点的出度邻接点入度\n1，回到步骤1\n4. 各种排序：\n1. 冒泡排序（Bubble Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：不稳定（可以通过适当的调整实现稳定性）\n特点：不受输入数据的影响，但是性能较差\n3. 插入排序（Insertion Sort）\n时间复杂度：平均 O(n^2)，最好情况 O(n^2)（已经排序的输入数据）\n稳定性：稳定\n特点：在接近排序状态的数据上表现良好，适合小规模数据排序\n4. 希尔排序（Shell Sort）\n时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定\n特点：改进版的插入排序，适用于中等大小的数据集\n5. 归并排序（Merge Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：稳定\n特点：非常适合大数据量排序，需要额外的内存开销\n6. 快速排序（Quick Sort）\n时间复杂度：平均 O(nlog n)O(nlogn)，最坏 O(n2)O(n2)\n稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：不稳定\n构造大根堆，将根节点（最大）和末尾节点互换，调整大根堆，迭代\n特点：利用堆数据结构，适合于大数据量排序，无需额外空间\n8. 计数排序（Counting Sort）\n时间复杂度：O(n+k)O(n+k)（其中 kk 是数据的范围）\n稳定性：稳定\n特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）\n时间复杂度：平均 O(n+k)O(n+k)，最坏 O(n2)O(n2)\n稳定性：稳定\n特点：适用于数据分布均匀的情况，需要额外空间\n10. 基数排序（Radix Sort）\n时间复杂度：O(nk)O(nk)（其中 kk 是键长）\n稳定性：稳定\n特点：非比较排序，适用于位数较少的整数或字符串排序，需要较大的空间开销\n5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序\n6. 希尔排序流程：从length/2组开始，对每组分别排序，变为局部有序。后续每次划分组都除以2，\n直至组数为1，排序完毕。\n计组01\n1. 流水线结构中，branch指令发生跳转，需要插入多少个空操作（bubble）或者阻塞多少个周期？\n在F\nD级处理阻塞的话，最多需要空两个操作。\n2. 中断与异常：\n1. 中断：软件中断，syscall调用内核函数；硬件中断，IO交互，时钟中断\n2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止\n3. 中断又分为内中断和外中断：\n1. 内中断（也称异常、例外、陷入）信号来源是CPU内部，与当前执行的指令有关；\n2. 外中断（狭义的中断）信号的来源是CPU外部，与当前执行的指令无关。\n操作系统', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='SIGBUS / SIGEMT/ SIGIOT 指示一个实现定义的硬件故障,非法地址，包括内存地址对齐出错。 SIGFPE( floating-point exception)，此信号表示一个算术运算异常，例如除以0，浮点溢出等。 SIGILL(illegal instruction ), 此信号指示进程已执行一条非法硬件指令。 SIGHUP：hang up 挂断 SIGTRAP：由断点指令或其他trap指令产生。 SIGPIPE：管道破裂。 SIGSEGV：试图访问未分配给自己的内存, 或试图往没有写权限的内存地址写数据.。 SIGPROF 当setitimer( 2 )函数设置的梗概统计间隔时间已经超过时产生此信号 SIGQUIT 当用户在终端上按退出键（一般采用Ctrl - \\）时，产生此信号，并送至前台进程组中的所有进 程（见图9 - 8）。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='此信号不仅终止前台进程组（如 SIGINT所做的那样），同时产生一个core文件。 SIGINFO 这是一种4 . 3 + BSD信号，当用户按状态键（一般采用Ctrl - T）时，终端驱动程序产生此信号 并送至前台进程组中的每一个进程。此信号通常造成在终端上显示前台进程组中各进程的状态信息。 SIGINT 当用户按中断键（一般采用DELETE或Ctrl - C）时，终端驱动程序产生此信号并送至前台进程组 中的每一个进程。当一个进程在运行时失控，特别是它正在屏幕上产生大量不需要的输出时，常用此信 号终止它。 1. 进程：是进程实体运行的过程，它是系统进行资源分配和调度的一个独立单位。 线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。 2. 磁盘调度算法 FCFS先来先服务 First Come First Service SSTF最短寻道时间      Shortest Seek Time First选择寻找时间最短的访问者调度【会产生"饥饿现 象"】 电梯调度算法（SCAN） 磁道编号是从外到里的，即由内到外磁道号越来越小。 从当前所在磁道号，从外向里运动，再从里 向外运动，或反之。这样就避免了饥饿现象，由于这种移臂调度规律颇似电梯的运动，因而称为电梯算 法。 循环扫描算法（CSCAN） 为了减少这种延迟，规定磁头单向读/写运动 写运动 (如只能由内向外)，完成读写后 ，立即返到最小/大 磁道号的位置 (构成循环 )，再进行扫描。即 CSCAN算法。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='即 CSCAN算法。 3. 页面置换算法 1. FIFO[first in first out]先进先出页面置换算法 2. 最优算法 3. LRU 4. 第二机会/时钟算法 4. 死锁必要条件：互斥；占有并等待；非抢占；循环等待 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90cfe640d0> 111
cuda:2
[Document(page_content='计算机网络', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='1. A类地址1-126，B类地址128-191，C类地址192-223，127是自回环 2. TCP当前序号为x，发送y长度数据，对方回复的Ack=x+y 3. 网段地址（或称为网络地址）是通过将 IP 地址与子网掩码进行逻辑 AND 操作得到的 4. 数据报校验的协议位于哪个层次：传输层，网络层，链路层（循环冗余校验与帧校验序列） 5. 拥塞控制 6. 高优先级包优先发送：帧间隔发送时间 7. 以太网，传输速率100Mbps，信号传播速度200000km/s，如果最小数据帧的长度增大90bits，则 1. 电缆的最大长度能够增加多少？2. 信道利用率是上升还是下降？', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='2. 信道利用率是上升还是下降？ 1. （最小帧长/数据传输速率）* 信号传播速度=2*最大传输距离；即此处电缆的最大长度能够增 加90km； 2. 先根据传输速率算出传输时间 = 2 * 单向时延（即在一个数据帧传输的时间内，上一个数据帧 应该至少走过了两倍的全程），算出单向时延，通过信号传播速度算出最长长度。 8. 以太网速度快的原因：串行通信简单，非常适合长距离传输；串行通信由于控制简单、抗干扰能力 强、成本低等特点，得到了广泛的应用。 数据结构', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='1. m叉树的节点计算：按边算（会省略根节点），按点算（会省略叶子节点）\n2. 图的邻接矩阵：i\n>j，adj(i,j)=1\n3. 拓扑排序（有向无环图）：\n1. 将入度为0的点入栈\n2. 如果栈非空，出栈，将该点的出度邻接点入度\n1，回到步骤1\n4. 各种排序：\n1. 冒泡排序（Bubble Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：不稳定（可以通过适当的调整实现稳定性）\n特点：不受输入数据的影响，但是性能较差\n3. 插入排序（Insertion Sort）\n时间复杂度：平均 O(n^2)，最好情况 O(n^2)（已经排序的输入数据）\n稳定性：稳定\n特点：在接近排序状态的数据上表现良好，适合小规模数据排序\n4. 希尔排序（Shell Sort）\n时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定\n特点：改进版的插入排序，适用于中等大小的数据集\n5. 归并排序（Merge Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：稳定\n特点：非常适合大数据量排序，需要额外的内存开销\n6. 快速排序（Quick Sort）\n时间复杂度：平均 O(nlog n)O(nlogn)，最坏 O(n2)O(n2)\n稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：不稳定\n构造大根堆，将根节点（最大）和末尾节点互换，调整大根堆，迭代\n特点：利用堆数据结构，适合于大数据量排序，无需额外空间\n8. 计数排序（Counting Sort）\n时间复杂度：O(n+k)O(n+k)（其中 kk 是数据的范围）\n稳定性：稳定\n特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）\n时间复杂度：平均 O(n+k)O(n+k)，最坏 O(n2)O(n2)\n稳定性：稳定\n特点：适用于数据分布均匀的情况，需要额外空间\n10. 基数排序（Radix Sort）\n时间复杂度：O(nk)O(nk)（其中 kk 是键长）\n稳定性：稳定\n特点：非比较排序，适用于位数较少的整数或字符串排序，需要较大的空间开销\n5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序\n6. 希尔排序流程：从length/2组开始，对每组分别排序，变为局部有序。后续每次划分组都除以2，\n直至组数为1，排序完毕。\n计组01\n1. 流水线结构中，branch指令发生跳转，需要插入多少个空操作（bubble）或者阻塞多少个周期？\n在F\nD级处理阻塞的话，最多需要空两个操作。\n2. 中断与异常：\n1. 中断：软件中断，syscall调用内核函数；硬件中断，IO交互，时钟中断\n2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止\n3. 中断又分为内中断和外中断：\n1. 内中断（也称异常、例外、陷入）信号来源是CPU内部，与当前执行的指令有关；\n2. 外中断（狭义的中断）信号的来源是CPU外部，与当前执行的指令无关。\n操作系统', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='SIGBUS / SIGEMT/ SIGIOT 指示一个实现定义的硬件故障,非法地址，包括内存地址对齐出错。 SIGFPE( floating-point exception)，此信号表示一个算术运算异常，例如除以0，浮点溢出等。 SIGILL(illegal instruction ), 此信号指示进程已执行一条非法硬件指令。 SIGHUP：hang up 挂断 SIGTRAP：由断点指令或其他trap指令产生。 SIGPIPE：管道破裂。 SIGSEGV：试图访问未分配给自己的内存, 或试图往没有写权限的内存地址写数据.。 SIGPROF 当setitimer( 2 )函数设置的梗概统计间隔时间已经超过时产生此信号 SIGQUIT 当用户在终端上按退出键（一般采用Ctrl - \\）时，产生此信号，并送至前台进程组中的所有进 程（见图9 - 8）。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='此信号不仅终止前台进程组（如 SIGINT所做的那样），同时产生一个core文件。 SIGINFO 这是一种4 . 3 + BSD信号，当用户按状态键（一般采用Ctrl - T）时，终端驱动程序产生此信号 并送至前台进程组中的每一个进程。此信号通常造成在终端上显示前台进程组中各进程的状态信息。 SIGINT 当用户按中断键（一般采用DELETE或Ctrl - C）时，终端驱动程序产生此信号并送至前台进程组 中的每一个进程。当一个进程在运行时失控，特别是它正在屏幕上产生大量不需要的输出时，常用此信 号终止它。 1. 进程：是进程实体运行的过程，它是系统进行资源分配和调度的一个独立单位。 线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。 2. 磁盘调度算法 FCFS先来先服务 First Come First Service SSTF最短寻道时间      Shortest Seek Time First选择寻找时间最短的访问者调度【会产生"饥饿现 象"】 电梯调度算法（SCAN） 磁道编号是从外到里的，即由内到外磁道号越来越小。 从当前所在磁道号，从外向里运动，再从里 向外运动，或反之。这样就避免了饥饿现象，由于这种移臂调度规律颇似电梯的运动，因而称为电梯算 法。 循环扫描算法（CSCAN） 为了减少这种延迟，规定磁头单向读/写运动 写运动 (如只能由内向外)，完成读写后 ，立即返到最小/大 磁道号的位置 (构成循环 )，再进行扫描。即 CSCAN算法。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'}), Document(page_content='即 CSCAN算法。 3. 页面置换算法 1. FIFO[first in first out]先进先出页面置换算法 2. 最优算法 3. LRU 4. 第二机会/时钟算法 4. 死锁必要条件：互斥；占有并等待；非抢占；循环等待 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyv8udjyw/南大笔试复习.pdf'})]
cuda:2
你好 tmpyv8udjyw
[]
cuda:2
[1.3147614, 1.3498826, 1.3708022, 1.3885332, 1.3972014, 1.4554722]
你好 tmpyv8udjyw
[]
cuda:2
[1.3147614, 1.3498826, 1.3708022, 1.3885332, 1.3972014, 1.4554722]
计算机网络中ip地址是如何划分的 tmpyv8udjyw
[History(role='user', content='你好'), History(role='assistant', content='根据已知信息，无法回答“你好”的问题。这个问题与操作系统、线程、计算机网络等相关知识无关。')]
cuda:2
[0.64289385, 0.9302034, 0.9629041, 0.9807931, 1.0982863, 1.1108195]
计算机网络中ip地址是如何划分的 tmpyv8udjyw
[History(role='user', content='你好'), History(role='assistant', content='根据已知信息，无法回答“你好”的问题。这个问题与操作系统、线程、计算机网络等相关知识无关。')]
cuda:2
[0.64289385, 0.9302034, 0.9629041, 0.9807931, 1.0982863, 1.1108195]
操作系统中进程管理是如何工作的？ tmpyv8udjyw
[History(role='user', content='你好'), History(role='assistant', content='根据已知信息，无法回答“你好”的问题。这个问题与操作系统、线程、计算机网络等相关知识无关。'), History(role='user', content='计算机网络中ip地址是如何划分的'), History(role='assistant', content='根据已知信息，计算机网络中IP地址的划分分为几个类别：A类地址、B类地址和C类地址。\n\n1. A类地址：范围是1-126，共126个地址。A类地址的IP地址前缀为1，例如：1.1.1.1。\n2. B类地址：范围是128-191，共163个地址。B类地址的IP地址前缀为2，例如：2.1.1.1。\n3. C类地址：范围是192-223，共32个地址。C类地址的IP地址前缀为3，例如：3.1.1.1。\n\nIP地址划分的主要依据是网络接口的特性。不同的IP地址类别具有不同的网络地址长度和地址数。A类地址网络地址长度为32位，共有126个地址；B类地址网络地址长度为32位，共有163个地址；C类地址网络地址长度为24位，共有32个地址。根据网络接口的特性选择合适的IP地址类别。')]
cuda:2
[0.86397946, 0.89990014, 0.9128349, 1.043302, 1.0966227, 1.2153946]
操作系统中进程管理是如何工作的？ tmpyv8udjyw
[History(role='user', content='你好'), History(role='assistant', content='根据已知信息，无法回答“你好”的问题。这个问题与操作系统、线程、计算机网络等相关知识无关。'), History(role='user', content='计算机网络中ip地址是如何划分的'), History(role='assistant', content='根据已知信息，计算机网络中IP地址的划分分为几个类别：A类地址、B类地址和C类地址。\n\n1. A类地址：范围是1-126，共126个地址。A类地址的IP地址前缀为1，例如：1.1.1.1。\n2. B类地址：范围是128-191，共163个地址。B类地址的IP地址前缀为2，例如：2.1.1.1。\n3. C类地址：范围是192-223，共32个地址。C类地址的IP地址前缀为3，例如：3.1.1.1。\n\nIP地址划分的主要依据是网络接口的特性。不同的IP地址类别具有不同的网络地址长度和地址数。A类地址网络地址长度为32位，共有126个地址；B类地址网络地址长度为32位，共有163个地址；C类地址网络地址长度为24位，共有32个地址。根据网络接口的特性选择合适的IP地址类别。')]
cuda:2
[0.86397946, 0.89990014, 0.9128349, 1.043302, 1.0966227, 1.2153946]
[UploadFile(filename='南大笔试复习.pdf', size=264799, headers=Headers({'content-disposition': 'form-data; name="files"; filename="å\x8d\x97å¤§ç¬\x94è¯\x95å¤\x8dä¹\xa0.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp2s7l_3ps, tmp2s7l_3ps
File: 南大笔试复习.pdf, msg: 成功上传文件 南大笔试复习.pdf, docs: [Document(page_content='计算机网络', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='1. A类地址1-126，B类地址128-191，C类地址192-223，127是自回环 2. TCP当前序号为x，发送y长度数据，对方回复的Ack=x+y 3. 网段地址（或称为网络地址）是通过将 IP 地址与子网掩码进行逻辑 AND 操作得到的 4. 数据报校验的协议位于哪个层次：传输层，网络层，链路层（循环冗余校验与帧校验序列） 5. 拥塞控制 6. 高优先级包优先发送：帧间隔发送时间 7. 以太网，传输速率100Mbps，信号传播速度200000km/s，如果最小数据帧的长度增大90bits，则 1. 电缆的最大长度能够增加多少？2. 信道利用率是上升还是下降？', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='2. 信道利用率是上升还是下降？ 1. （最小帧长/数据传输速率）* 信号传播速度=2*最大传输距离；即此处电缆的最大长度能够增 加90km； 2. 先根据传输速率算出传输时间 = 2 * 单向时延（即在一个数据帧传输的时间内，上一个数据帧 应该至少走过了两倍的全程），算出单向时延，通过信号传播速度算出最长长度。 8. 以太网速度快的原因：串行通信简单，非常适合长距离传输；串行通信由于控制简单、抗干扰能力 强、成本低等特点，得到了广泛的应用。 数据结构', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='1. m叉树的节点计算：按边算（会省略根节点），按点算（会省略叶子节点）\n2. 图的邻接矩阵：i\n>j，adj(i,j)=1\n3. 拓扑排序（有向无环图）：\n1. 将入度为0的点入栈\n2. 如果栈非空，出栈，将该点的出度邻接点入度\n1，回到步骤1\n4. 各种排序：\n1. 冒泡排序（Bubble Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：不稳定（可以通过适当的调整实现稳定性）\n特点：不受输入数据的影响，但是性能较差\n3. 插入排序（Insertion Sort）\n时间复杂度：平均 O(n^2)，最好情况 O(n^2)（已经排序的输入数据）\n稳定性：稳定\n特点：在接近排序状态的数据上表现良好，适合小规模数据排序\n4. 希尔排序（Shell Sort）\n时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定\n特点：改进版的插入排序，适用于中等大小的数据集\n5. 归并排序（Merge Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：稳定\n特点：非常适合大数据量排序，需要额外的内存开销\n6. 快速排序（Quick Sort）\n时间复杂度：平均 O(nlog n)O(nlogn)，最坏 O(n2)O(n2)\n稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：不稳定\n构造大根堆，将根节点（最大）和末尾节点互换，调整大根堆，迭代\n特点：利用堆数据结构，适合于大数据量排序，无需额外空间\n8. 计数排序（Counting Sort）\n时间复杂度：O(n+k)O(n+k)（其中 kk 是数据的范围）\n稳定性：稳定\n特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）\n时间复杂度：平均 O(n+k)O(n+k)，最坏 O(n2)O(n2)\n稳定性：稳定\n特点：适用于数据分布均匀的情况，需要额外空间\n10. 基数排序（Radix Sort）\n时间复杂度：O(nk)O(nk)（其中 kk 是键长）\n稳定性：稳定\n特点：非比较排序，适用于位数较少的整数或字符串排序，需要较大的空间开销\n5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序\n6. 希尔排序流程：从length/2组开始，对每组分别排序，变为局部有序。后续每次划分组都除以2，\n直至组数为1，排序完毕。\n计组01\n1. 流水线结构中，branch指令发生跳转，需要插入多少个空操作（bubble）或者阻塞多少个周期？\n在F\nD级处理阻塞的话，最多需要空两个操作。\n2. 中断与异常：\n1. 中断：软件中断，syscall调用内核函数；硬件中断，IO交互，时钟中断\n2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止\n3. 中断又分为内中断和外中断：\n1. 内中断（也称异常、例外、陷入）信号来源是CPU内部，与当前执行的指令有关；\n2. 外中断（狭义的中断）信号的来源是CPU外部，与当前执行的指令无关。\n操作系统', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='SIGBUS / SIGEMT/ SIGIOT 指示一个实现定义的硬件故障,非法地址，包括内存地址对齐出错。 SIGFPE( floating-point exception)，此信号表示一个算术运算异常，例如除以0，浮点溢出等。 SIGILL(illegal instruction ), 此信号指示进程已执行一条非法硬件指令。 SIGHUP：hang up 挂断 SIGTRAP：由断点指令或其他trap指令产生。 SIGPIPE：管道破裂。 SIGSEGV：试图访问未分配给自己的内存, 或试图往没有写权限的内存地址写数据.。 SIGPROF 当setitimer( 2 )函数设置的梗概统计间隔时间已经超过时产生此信号 SIGQUIT 当用户在终端上按退出键（一般采用Ctrl - \\）时，产生此信号，并送至前台进程组中的所有进 程（见图9 - 8）。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='此信号不仅终止前台进程组（如 SIGINT所做的那样），同时产生一个core文件。 SIGINFO 这是一种4 . 3 + BSD信号，当用户按状态键（一般采用Ctrl - T）时，终端驱动程序产生此信号 并送至前台进程组中的每一个进程。此信号通常造成在终端上显示前台进程组中各进程的状态信息。 SIGINT 当用户按中断键（一般采用DELETE或Ctrl - C）时，终端驱动程序产生此信号并送至前台进程组 中的每一个进程。当一个进程在运行时失控，特别是它正在屏幕上产生大量不需要的输出时，常用此信 号终止它。 1. 进程：是进程实体运行的过程，它是系统进行资源分配和调度的一个独立单位。 线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。 2. 磁盘调度算法 FCFS先来先服务 First Come First Service SSTF最短寻道时间      Shortest Seek Time First选择寻找时间最短的访问者调度【会产生"饥饿现 象"】 电梯调度算法（SCAN） 磁道编号是从外到里的，即由内到外磁道号越来越小。 从当前所在磁道号，从外向里运动，再从里 向外运动，或反之。这样就避免了饥饿现象，由于这种移臂调度规律颇似电梯的运动，因而称为电梯算 法。 循环扫描算法（CSCAN） 为了减少这种延迟，规定磁头单向读/写运动 写运动 (如只能由内向外)，完成读写后 ，立即返到最小/大 磁道号的位置 (构成循环 )，再进行扫描。即 CSCAN算法。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='即 CSCAN算法。 3. 页面置换算法 1. FIFO[first in first out]先进先出页面置换算法 2. 最优算法 3. LRU 4. 第二机会/时钟算法 4. 死锁必要条件：互斥；占有并等待；非抢占；循环等待 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90f17259d0> 111
cuda:2
[Document(page_content='计算机网络', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='1. A类地址1-126，B类地址128-191，C类地址192-223，127是自回环 2. TCP当前序号为x，发送y长度数据，对方回复的Ack=x+y 3. 网段地址（或称为网络地址）是通过将 IP 地址与子网掩码进行逻辑 AND 操作得到的 4. 数据报校验的协议位于哪个层次：传输层，网络层，链路层（循环冗余校验与帧校验序列） 5. 拥塞控制 6. 高优先级包优先发送：帧间隔发送时间 7. 以太网，传输速率100Mbps，信号传播速度200000km/s，如果最小数据帧的长度增大90bits，则 1. 电缆的最大长度能够增加多少？2. 信道利用率是上升还是下降？', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='2. 信道利用率是上升还是下降？ 1. （最小帧长/数据传输速率）* 信号传播速度=2*最大传输距离；即此处电缆的最大长度能够增 加90km； 2. 先根据传输速率算出传输时间 = 2 * 单向时延（即在一个数据帧传输的时间内，上一个数据帧 应该至少走过了两倍的全程），算出单向时延，通过信号传播速度算出最长长度。 8. 以太网速度快的原因：串行通信简单，非常适合长距离传输；串行通信由于控制简单、抗干扰能力 强、成本低等特点，得到了广泛的应用。 数据结构', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='1. m叉树的节点计算：按边算（会省略根节点），按点算（会省略叶子节点）\n2. 图的邻接矩阵：i\n>j，adj(i,j)=1\n3. 拓扑排序（有向无环图）：\n1. 将入度为0的点入栈\n2. 如果栈非空，出栈，将该点的出度邻接点入度\n1，回到步骤1\n4. 各种排序：\n1. 冒泡排序（Bubble Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：不稳定（可以通过适当的调整实现稳定性）\n特点：不受输入数据的影响，但是性能较差\n3. 插入排序（Insertion Sort）\n时间复杂度：平均 O(n^2)，最好情况 O(n^2)（已经排序的输入数据）\n稳定性：稳定\n特点：在接近排序状态的数据上表现良好，适合小规模数据排序\n4. 希尔排序（Shell Sort）\n时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定\n特点：改进版的插入排序，适用于中等大小的数据集\n5. 归并排序（Merge Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：稳定\n特点：非常适合大数据量排序，需要额外的内存开销\n6. 快速排序（Quick Sort）\n时间复杂度：平均 O(nlog n)O(nlogn)，最坏 O(n2)O(n2)\n稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：不稳定\n构造大根堆，将根节点（最大）和末尾节点互换，调整大根堆，迭代\n特点：利用堆数据结构，适合于大数据量排序，无需额外空间\n8. 计数排序（Counting Sort）\n时间复杂度：O(n+k)O(n+k)（其中 kk 是数据的范围）\n稳定性：稳定\n特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）\n时间复杂度：平均 O(n+k)O(n+k)，最坏 O(n2)O(n2)\n稳定性：稳定\n特点：适用于数据分布均匀的情况，需要额外空间\n10. 基数排序（Radix Sort）\n时间复杂度：O(nk)O(nk)（其中 kk 是键长）\n稳定性：稳定\n特点：非比较排序，适用于位数较少的整数或字符串排序，需要较大的空间开销\n5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序\n6. 希尔排序流程：从length/2组开始，对每组分别排序，变为局部有序。后续每次划分组都除以2，\n直至组数为1，排序完毕。\n计组01\n1. 流水线结构中，branch指令发生跳转，需要插入多少个空操作（bubble）或者阻塞多少个周期？\n在F\nD级处理阻塞的话，最多需要空两个操作。\n2. 中断与异常：\n1. 中断：软件中断，syscall调用内核函数；硬件中断，IO交互，时钟中断\n2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止\n3. 中断又分为内中断和外中断：\n1. 内中断（也称异常、例外、陷入）信号来源是CPU内部，与当前执行的指令有关；\n2. 外中断（狭义的中断）信号的来源是CPU外部，与当前执行的指令无关。\n操作系统', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='SIGBUS / SIGEMT/ SIGIOT 指示一个实现定义的硬件故障,非法地址，包括内存地址对齐出错。 SIGFPE( floating-point exception)，此信号表示一个算术运算异常，例如除以0，浮点溢出等。 SIGILL(illegal instruction ), 此信号指示进程已执行一条非法硬件指令。 SIGHUP：hang up 挂断 SIGTRAP：由断点指令或其他trap指令产生。 SIGPIPE：管道破裂。 SIGSEGV：试图访问未分配给自己的内存, 或试图往没有写权限的内存地址写数据.。 SIGPROF 当setitimer( 2 )函数设置的梗概统计间隔时间已经超过时产生此信号 SIGQUIT 当用户在终端上按退出键（一般采用Ctrl - \\）时，产生此信号，并送至前台进程组中的所有进 程（见图9 - 8）。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='此信号不仅终止前台进程组（如 SIGINT所做的那样），同时产生一个core文件。 SIGINFO 这是一种4 . 3 + BSD信号，当用户按状态键（一般采用Ctrl - T）时，终端驱动程序产生此信号 并送至前台进程组中的每一个进程。此信号通常造成在终端上显示前台进程组中各进程的状态信息。 SIGINT 当用户按中断键（一般采用DELETE或Ctrl - C）时，终端驱动程序产生此信号并送至前台进程组 中的每一个进程。当一个进程在运行时失控，特别是它正在屏幕上产生大量不需要的输出时，常用此信 号终止它。 1. 进程：是进程实体运行的过程，它是系统进行资源分配和调度的一个独立单位。 线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。 2. 磁盘调度算法 FCFS先来先服务 First Come First Service SSTF最短寻道时间      Shortest Seek Time First选择寻找时间最短的访问者调度【会产生"饥饿现 象"】 电梯调度算法（SCAN） 磁道编号是从外到里的，即由内到外磁道号越来越小。 从当前所在磁道号，从外向里运动，再从里 向外运动，或反之。这样就避免了饥饿现象，由于这种移臂调度规律颇似电梯的运动，因而称为电梯算 法。 循环扫描算法（CSCAN） 为了减少这种延迟，规定磁头单向读/写运动 写运动 (如只能由内向外)，完成读写后 ，立即返到最小/大 磁道号的位置 (构成循环 )，再进行扫描。即 CSCAN算法。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'}), Document(page_content='即 CSCAN算法。 3. 页面置换算法 1. FIFO[first in first out]先进先出页面置换算法 2. 最优算法 3. LRU 4. 第二机会/时钟算法 4. 死锁必要条件：互斥；占有并等待；非抢占；循环等待 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2s7l_3ps/南大笔试复习.pdf'})]
cuda:2
你认识王怡萱么 tmp2s7l_3ps
[]
cuda:2
[]
你认识王怡萱么 tmp2s7l_3ps
[]
cuda:2
[]
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpalnw1yg3, tmpalnw1yg3
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f9059114510> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpalnw1yg3/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmptf72umj7, tmptf72umj7
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f9059148d90> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptf72umj7/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp_3ejag83, tmp_3ejag83
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90591e24d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_3ejag83/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp9_624f8e, tmp9_624f8e
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90591dbe90> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9_624f8e/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp217wh65x, tmp217wh65x
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90591d91d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp217wh65x/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='05473f62-adf9-4adf-9196-f589d9c96780.txt', size=5283, headers=Headers({'content-disposition': 'form-data; name="files"; filename="05473f62-adf9-4adf-9196-f589d9c96780.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpu2y5uxbe, tmpu2y5uxbe
File: 05473f62-adf9-4adf-9196-f589d9c96780.txt, msg: 成功上传文件 05473f62-adf9-4adf-9196-f589d9c96780.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd599950> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu2y5uxbe/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
cuda:2
[UploadFile(filename='杨博文为何是北航校草.pdf', size=151520, headers=Headers({'content-disposition': 'form-data; name="files"; filename="æ\x9d¨å\x8d\x9aæ\x96\x87ä¸ºä½\x95æ\x98¯å\x8c\x97è\x88ªæ\xa0¡è\x8d\x89.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp3ayf375w, tmp3ayf375w
File: 杨博文为何是北航校草.pdf, msg: 成功上传文件 杨博文为何是北航校草.pdf, docs: [Document(page_content='杨博文为什么是北航校草\n引言\n在北京航空航天大学计算机学院，杨博文以其出众的外貌和优异的学术成绩广受关注。在一个竞争激烈\n的学术环境中，杨博文不仅在学术上取得了显著的成就，而且在课外活动和社会交往中也表现出色，自\n然而然地成为了北航的校草。本文将探讨杨博文获此殊荣的几个关键因素。\n主体\n首先，杨博文的外表无疑是他成为校草的一个重要因素。他的容貌俊朗，总是给人留下良好的第一印\n象。然而，仅凭外表的吸引力是不足以让他在北航这样以工程和技术见长的学校中脱颖而出的。他的个\n人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp3ayf375w/杨博文为何是北航校草.pdf'}), Document(page_content='人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理\n论和技术概念的深刻理解。他的能力不仅限于理论学习，更在实践中得到了验证。杨博文参与并带领了\n多个工程项目，包括开发软件应用和参与硬件设计，这些项目不仅成功实施，还在学术和技术会议上获\n得了认可。\n例如，他曾领导一个团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n此外，他在操作系统的课程设计中，独立完成了一个微型操作系统的设计，该系统能有效管理硬件资源\n并支持基本的多任务处理，这一成就显示了他在软件和硬件接口方面的高水平能力。\n除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp3ayf375w/杨博文为何是北航校草.pdf'}), Document(page_content='除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与\n各种校园活动，展现了他的领导能力和社交技巧。此外，他还参与志愿服务工作，帮助社区中的弱势群\n体，体现了他的社会责任感。\n结论\n综上所述，杨博文之所以被誉为北航校草，不仅仅是因为他俊朗的外表，更重要的是他在学术、领导力\n和社会责任方面的全面表现。他在计算机科学的各个方面的深厚造诣，尤其是在计算机组成和操作系统\n等专业课程中的杰出表现，以及他在实际工程项目中的实际应用能力，使他在北航获得了师生的广泛认\n可和尊敬。杨博文的故事激励着每一个北航学子，不仅要追求学术上的卓越，更要在个人成长和社会贡\n献上不断努力。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp3ayf375w/杨博文为何是北航校草.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd58c7d0> 111
cuda:2
[Document(page_content='杨博文为什么是北航校草\n引言\n在北京航空航天大学计算机学院，杨博文以其出众的外貌和优异的学术成绩广受关注。在一个竞争激烈\n的学术环境中，杨博文不仅在学术上取得了显著的成就，而且在课外活动和社会交往中也表现出色，自\n然而然地成为了北航的校草。本文将探讨杨博文获此殊荣的几个关键因素。\n主体\n首先，杨博文的外表无疑是他成为校草的一个重要因素。他的容貌俊朗，总是给人留下良好的第一印\n象。然而，仅凭外表的吸引力是不足以让他在北航这样以工程和技术见长的学校中脱颖而出的。他的个\n人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp3ayf375w/杨博文为何是北航校草.pdf'}), Document(page_content='人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理\n论和技术概念的深刻理解。他的能力不仅限于理论学习，更在实践中得到了验证。杨博文参与并带领了\n多个工程项目，包括开发软件应用和参与硬件设计，这些项目不仅成功实施，还在学术和技术会议上获\n得了认可。\n例如，他曾领导一个团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n此外，他在操作系统的课程设计中，独立完成了一个微型操作系统的设计，该系统能有效管理硬件资源\n并支持基本的多任务处理，这一成就显示了他在软件和硬件接口方面的高水平能力。\n除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp3ayf375w/杨博文为何是北航校草.pdf'}), Document(page_content='除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与\n各种校园活动，展现了他的领导能力和社交技巧。此外，他还参与志愿服务工作，帮助社区中的弱势群\n体，体现了他的社会责任感。\n结论\n综上所述，杨博文之所以被誉为北航校草，不仅仅是因为他俊朗的外表，更重要的是他在学术、领导力\n和社会责任方面的全面表现。他在计算机科学的各个方面的深厚造诣，尤其是在计算机组成和操作系统\n等专业课程中的杰出表现，以及他在实际工程项目中的实际应用能力，使他在北航获得了师生的广泛认\n可和尊敬。杨博文的故事激励着每一个北航学子，不仅要追求学术上的卓越，更要在个人成长和社会贡\n献上不断努力。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp3ayf375w/杨博文为何是北航校草.pdf'})]
cuda:2
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmplz546gzj, tmplz546gzj
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90590f6a90> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplz546gzj/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
[UploadFile(filename='Efficient Flow-Guided Multi-frame De-fencing.pdf', size=21637096, headers=Headers({'content-disposition': 'form-data; name="files"; filename="Efficient Flow-Guided Multi-frame De-fencing.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpwqlm1x6o, tmpwqlm1x6o
File: Efficient Flow-Guided Multi-frame De-fencing.pdf, msg: 成功上传文件 Efficient Flow-Guided Multi-frame De-fencing.pdf, docs: [Document(page_content='Efficient Flow-Guided Multi-frame De-fencing\nStavros Tsogkas\nFengjia Zhang\nAllan Jepson\nAlex Levinshtein\nSamsung AI Center Toronto\n101 College St., Toronto, ON, Canada, M5G 1L7\n{stavros.t, f.zhang2, allan.jepson, alex.lev}@samsung.com\nAbstract\nTaking photographs “in-the-wild” is often hindered by\nfence obstructions that stand between the camera user and\nthe scene of interest, and which are hard or impossible\nto avoid. De-fencing is the algorithmic process of auto-\nmatically removing such obstructions from images, reveal-\ning the invisible parts of the scene.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='to avoid. De-fencing is the algorithmic process of auto-\nmatically removing such obstructions from images, reveal-\ning the invisible parts of the scene.\nWhile this problem\ncan be formulated as a combination of fence segmentation\nand image inpainting, this often leads to implausible hal-\nlucinations of the occluded regions. Existing multi-frame\napproaches rely on propagating information to a selected\nkeyframe from its temporal neighbors, but they are often in-\nefficient and struggle with alignment of severely obstructed\nimages. In this work we draw inspiration from the video\ncompletion literature, and develop a simplified framework\nfor multi-frame de-fencing that computes high quality flow\nmaps directly from obstructed frames, and uses them to ac-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='for multi-frame de-fencing that computes high quality flow\nmaps directly from obstructed frames, and uses them to ac-\ncurately align frames. Our primary focus is efficiency and\npracticality in a real world setting: the input to our algo-\nrithm is a short image burst (5 frames) – a data modality\ncommonly available in modern smartphones– and the out-\nput is a single reconstructed keyframe, with the fence re-\nmoved. Our approach leverages simple yet effective CNN\nmodules, trained on carefully generated synthetic data, and\noutperforms more complicated alternatives real bursts, both\nquantitatively and qualitatively, while running real-time.\n1. Introduction\nRapid improvements in both the camera hardware and\nimage processing software have turned modern cell phones', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='1. Introduction\nRapid improvements in both the camera hardware and\nimage processing software have turned modern cell phones\ninto powerful yet portable image and video recording de-\nvices.\nThis has enabled and encouraged casual users to\nshoot photos without any time for special preparation,\nsetup, or framing of the shot. On the flip side, photos and\nvideos taken under these conditions rarely contain just the\nobject(s) of interest, and are hindered by various obstruc-\ntions that stand between the subject and the user.\nOne type of obstruction that is of special interest because\nFigure 1: We train a simple and efficient model for de-\nfencing: removing fence obstructions from images, reveal-\ning the underlying scene of interest. Our method is fast and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fencing: removing fence obstructions from images, reveal-\ning the underlying scene of interest. Our method is fast and\ncan accurately remove fences of varying size and appear-\nance from real bursts, without online finetuning.\nof its commonness is fences. Imagine, for instance, taking\na photo of an animal through a zoo fence, or people play-\ning basketball in a fenced-out outdoor court; these are just\na few everyday scenes that are obstructed by fence struc-\ntures that are either inconvenient or completely impossible\nto avoid. De-fencing uses computer vision algorithms to\nautomatically remove such fence obstructions from images,\nas shown in Figure 1. De-fencing is a harder problem than\nit may initially seem. Fences have varying structure and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='as shown in Figure 1. De-fencing is a harder problem than\nit may initially seem. Fences have varying structure and\nappearance patterns, and come in different thicknesses and\nsizes. Furthermore, reconstruction of the background scene\ncan become challenging because of low lighting and noise,\nor motion blur, caused by rapidly moving objects.\nThe first works that tackled de-fencing in a principled\nmanner were by Liu et al. [32, 19]. They formulate the prob-\nlem as the segmentation of a repeating foreground pattern\nthat exhibits approximate translational symmetry, followed\nby inpainting to recover the occluded image regions. The\napproach of [32] is mainly limited by the fact that it uses a\nsingle image as input. Because of the opaque nature of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='approach of [32] is mainly limited by the fact that it uses a\nsingle image as input. Because of the opaque nature of the\nfence obstruction, the occluded parts of the scene must be\nhallucinated by the inpainting algorithm; [19] partially ad-\ndresses this by using a photo taken from a different view, to\nreduce the number of pixels that must be hallucinated.\nDe-fencing can also be viewed as a special case of the\nmore general problem of layer separation [21, 1, 5, 14],\nwhich models an image as a composition of individual lay-\narXiv:2301.10759v1  [cs.CV]  25 Jan 2023\ners, e.g., a foreground layer containing the obstruction and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ers, e.g., a foreground layer containing the obstruction and\na background layer containing the scene of interest. Xue\net al. [31] formulate generic obstruction removal as a layer\nseparation problem, driven by motion parallax. Although\ntheir solution is generic and works well, it involves multi-\nple time-consuming, hand-tuned optimization steps, and the\nuse of hand-crafted motion and image priors. SOLD [14] is\na deep learning re-incarnation of [31] that achieved state-\nof-the-art results on obstruction removal, and can also be\nadapted to remove fences from a multi-frame input burst.\nUnfortunately, SOLD depends on computationally expen-\nsive networks for flow computation and frame reconstruc-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Unfortunately, SOLD depends on computationally expen-\nsive networks for flow computation and frame reconstruc-\ntion, making it impractical for use in low-powered devices.\nIt also often requires an online optimization step that takes\n∼ 3 minutes to produce acceptable results on real bursts,\nand even without this input-specific finetuning, it cannot be\nrun in real time. Finally, since the background frames are\nthe output of a reconstruction CNN module, they occasion-\nally contain inconsistencies or artifacts.\nFlow-guided completion methods [7, 29, 29, 13, 35] re-\nduce such artifacts by computing inpainted flow maps be-\ntween pairs of obstructed frames and using them to explic-\nitly transfer pixel values to a reference frame from its tem-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tween pairs of obstructed frames and using them to explic-\nitly transfer pixel values to a reference frame from its tem-\nporal neighbors. Inpainting flows in the occluded area is\neasier than directly inpainting pixel values, so a less pow-\nerful network can be used, and the results tend to look\nmore plausible because the pixel values are taken from real\nframes instead of being hallucinated by a generative net-\nwork. These works do not make any assumptions regard-\ning the shape and type of the occlusion, other the fact that\nit is completely opaque. However, the mask marking the\noccluded area is considered to be known, which is an unre-\nalistic requirement for our purposes.\nHere our goal is to develop a de-fencing algorithm that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='alistic requirement for our purposes.\nHere our goal is to develop a de-fencing algorithm that\nprioritizes efficiency and practicality. We develop a frame-\nwork that enjoys the realism and modularity of flow-based\nvideo completion approaches, while being significantly\nsimpler to train and deploy. Instead of videos, the input to\nour algorithm is shorter bursts of K = 5 frames, which are a\nvery common photo modality in modern smartphones. The\ntype of occlusion is known (fences), but we do not make\nany assumptions regarding its spatial extent or location; in-\nstead, we train a class-specific segmentation model to au-\ntomatically detect fences in images. Computing flow maps\nof scenes occluded by fences, presents us with a new chal-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tomatically detect fences in images. Computing flow maps\nof scenes occluded by fences, presents us with a new chal-\nlenge, as standard optical flow networks fail under the pres-\nence of repeated patterns [9, 14]. To solve this problem,\nwe train a segmentation-aware SPyNet [23] that can simul-\ntaneously compute and inpaint flow maps corresponding to\nthe occluded background scene, ignoring foreground occlu-\nsions. Finally, to quantitatively evaluate the performance of\nour approach on real data, we collect a dataset of multi-\nframe sequences and corresponding “pseudo-groundtruth”\nfor the reference frame using a alignment procedure, akin\nto [3]. To summarize:\nWe design a CNN pipeline for multi-frame de-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='for the reference frame using a alignment procedure, akin\nto [3]. To summarize:\nWe design a CNN pipeline for multi-frame de-fencing\nthat is simple, modular, efficient, and easy to train.\nUnlike flow-based works, which assume the occlusion\nis known, we estimate it automatically from the input.\nWe train a segmentation-aware optical flow model that\ncan reliably estimate flows corresponding to the back-\nground scene despite severe fence obstructions.\nOur method achieves state-of-the-art results on syn-\nthetic and real bursts, without requiring sequence-\nspecific finetuning.\nAs a result, it has significantly\nlower runtimes compared to alternatives.\n2. Related Work', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='specific finetuning.\nAs a result, it has significantly\nlower runtimes compared to alternatives.\n2. Related Work\n2.1. Image and Video De-fencing\nLiu et al. [32] is perhaps the first work to formally intro-\nduce de-fencing in a computer vision context, as symmetry-\ndriven automatic fence segmentation, followed by inpaint-\ning. [19] improves on this work by using online learning\nto aid lattice detection and segmentation, and by leverag-\ning a second viewpoint to improve inpainting.\nJonna et\nal. [11, 10] also improve fence segmentation, by comple-\nmenting RGB with depth data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Jonna et\nal. [11, 10] also improve fence segmentation, by comple-\nmenting RGB with depth data.\n[16, 34] extend de-fencing to video sequences of arbi-\ntrary number of frames. Mu et al [16] rely on motion par-\nallax to separate foreground fence obstructions from back-\nground (although the definition of “fence” is quite loose),\nwhile Yi et al. [34] describe a bottom-up approach for video\nde-fencing that groups pixels in each frame using color and\nmotion cues. Both methods rely on optimization techniques\nto refine their optical flow or frame inpainting results.\nMore recently, deep learning has been adopted for video', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='motion cues. Both methods rely on optimization techniques\nto refine their optical flow or frame inpainting results.\nMore recently, deep learning has been adopted for video\ndefencing [9, 4]. Jonna et al. [9] use a pretrained classifi-\ncation CNN as a feature extractor and train a SVM classi-\nfier that distinguishes fence from non-fence patches. The\nauthors reformulate an existing optical flow algorithm to\nmake it occlusion-aware and recover the de-fenced image\nusing FISTA optimization [2].\nDu et al. [4] replace the\nCNN-SVM combination with a fully convolutional network\n(FCN) [15] and apply temporal refinement to the extracted\nsegmentations by aggregating information from neighbor-\ning frames.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(FCN) [15] and apply temporal refinement to the extracted\nsegmentations by aggregating information from neighbor-\ning frames.\nOur approach shares a similar pipeline but\nsimplifies both the segmentation extraction and occlusion-\naware flow computation steps, while being considerably\nfaster, as we do not perform test-time optimization.\n2.2. Layer Separation\nA more generic formulation of the fence removal prob-\nlem views an image as a composition of layers, each with\nits own alpha map (which can be semi transparent), and\nthe goal is to separate the layers. In [5] the foreground-\nbackground layers are the output of two convolutional net-\nwork, trained per image in an unsupervised fashion, and are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='background layers are the output of two convolutional net-\nwork, trained per image in an unsupervised fashion, and are\nrecovered using a deep image prior [28]. A similar idea\nis used by Alayrac et al. [1] for video decomposition, but\nwith supervised training. Other approaches for video de-\ncomposition use explicit motion information [31, 14]. Xue\net al. [31] describe a computational method for decompos-\ning a scene into an foreground obstruction layer and a back-\nground scene from multi-scale motion cues of a multi-frame\nsequence, in an unsupervised way. They solve an optimiza-\ntion problem that alternatingly finds the constituent layers\nand the respective motion fields that, when used to align', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tion problem that alternatingly finds the constituent layers\nand the respective motion fields that, when used to align\nthe burst to a reference frame, can reconstruct the original\nframes with low error. A modern reincarnation of this ap-\nproach is proposed by Liu et al. with SOLD [14]. SOLD\nfollows a similar multi-scale approach, using a convolu-\ntional framework, both for layer reconstruction and motion\nestimation – the latter wth a pre-trained PWC-Net [25].\n2.3. Flow-based Video Completion\nVideo completion is a related problem to multi-frame\nfence removal, the main difference being that the segmen-\ntation mask is assumed to be provided and the emphasis is\ntypically on longer frame sequences. Our work is motivated', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence removal, the main difference being that the segmen-\ntation mask is assumed to be provided and the emphasis is\ntypically on longer frame sequences. Our work is motivated\nby Xu et al. [29], who proposed the idea of first tackling the\neasier problem of flow inpainting, and then using the com-\npleted flows to propagate color values to a reference frame\nfrom its temporal neighbors. Since it not guaranteed that\nall occluded pixels are visible in some frame, a separate\nimage inpainting step must be used to fill any remaining\nholes. Gao et al. [6] improve this approach by synthesizing\nsharp flow edges along the object boundaries and using non-\nlocal temporal neighborhoods for propagating pixels across\nframes. These works involve a series of individual, sepa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='sharp flow edges along the object boundaries and using non-\nlocal temporal neighborhoods for propagating pixels across\nframes. These works involve a series of individual, sepa-\nrately trained processing stages, some of which are hand-\ncrafted, inefficient, and can potentially compromise perfor-\nmance of subsequent stages. Li et al. [13] address this is-\nsue by proposing an end-to-end framework for flow-guided\nvideo completion. Their approach was developed concur-\nrently to our own and shares some of its simplicity and effi-\nciency advantages. However, their framework still operates\nunder the assumption that the occlusion mask is provided.\n3. Method\nWe begin with an overview of the problem we are solv-\ning, and establish the notation that we use throughout the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='3. Method\nWe begin with an overview of the problem we are solv-\ning, and establish the notation that we use throughout the\npaper. The input to our algorithm is a burst of K RGB\nframes, {Ii}, composed from an unknown background\nscene of interest{Bi} and an unknown opaque foreground\nocclusion in the form of a fence {Fi}. Specifically,\nIi = Si · Fi + (1 − Si) · Bi,\n(1)\nwhere Si ∈ [0, 1] is a soft fence occlusion mask. Our goal\nis to train a model that removes the fence obstruction from\n{Ii} and recovers a single keyframe background image Bk,\nwhere k is the keyframe index.\nInstead of outputting the unobstructed frame directly, we', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='where k is the keyframe index.\nInstead of outputting the unobstructed frame directly, we\nbreak down the problem into the steps illustrated in Fig-\nure 2. We start by training a network that is applied indi-\nvidually on each frame in {Ii}, and outputs fence segmen-\ntation predictions {Si}. The role of these segmentations\nis two-fold: i) they mark the occluded area that needs to\nbe recovered; ii) they are used to condition a segmentation-\naware network that computes optical flows corresponding to\nthe background scene only, directly from the occluded input\n{Ii}. With this network we extract flows {fkj} between the\nkeyframe Ik and each other frame Ij in the sequence, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='{Ii}. With this network we extract flows {fkj} between the\nkeyframe Ik and each other frame Ij in the sequence, and\nalign the burst. Finally, we employ learned flow-guided im-\nage inpainting, to recover the parts of the keyframe that are\noccluded by the fence, yielding the final output ˜Bk. In the\nfollowing subsections we explain in detail each step.\n3.1. Single-frame Fence Segmentation\nOur fence segmentation model takes as input a single\nRGB frame, possibly containing a fence, and outputs a soft\nfence segmentation mask. Although this sounds like a rel-\natively simple task, there are, in fact, multiple challenges.\nFirst, datasets of large size and with high quality annota-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='atively simple task, there are, in fact, multiple challenges.\nFirst, datasets of large size and with high quality annota-\ntions for fence segmentation are surprisingly scarce. The\nmost appropriate for this task is probably the De-fencing\ndataset [4]. Fences in this dataset do not exhibit significant\nvariance in terms of appearance, scale, or structure, so we\nrely on substantial data augmentation to train a network that\nis robust to different types of fences and environments.\nMore specifically, we apply different degrees of down-\nscaling to the original image and its associated annotation,\nto effectively create fences at different scales (varying fence\nwidth/distance from the camera). To augment the limited\nscene variety in the De-fencing dataset, we also mask out', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='width/distance from the camera). To augment the limited\nscene variety in the De-fencing dataset, we also mask out\nfences, using the groundtruth segmentations, and overlay\nthem on images from the DAVIS dataset [22]. Finally, we\napply random horizontal flipping to the fence image and\ntake randomly crop a 320 × 192 window for training.\nThe segmentation network itself is a U-net [24] back-\nbone, with four encoder and four decoder blocks, that is\ntrained from scratch on our augmented fence data using a\nbinary cross entropy loss and the ADAM optimizer [12].\nTo obtain segmentation scores in the [0, 1] range, we ap-\nply a sigmoid in the output logits from the last U-net layer.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='To obtain segmentation scores in the [0, 1] range, we ap-\nply a sigmoid in the output logits from the last U-net layer.\nTable 1 lists precision-recall and f-measure scores of our\nmethod at different thresholds.\nEven though we do not\nBurst (K frames)\nFence\nSegmentation\nOptical flow\nK fence masks\nK-1 flows\nFrame inpainting\nReconstructed\nkeyframe\nFigure 2: Given a burst of K frames with fence obstructions as input, we reconstruct a single keyframe, after removing the\nfence. Our pipeline is composed of three distinct steps: a) initially, a fence mask is estimated on each input frame individually', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence. Our pipeline is composed of three distinct steps: a) initially, a fence mask is estimated on each input frame individually\nwith a U-net fence segmentation model (Sec. 3.1); b) the estimated masks are used to condition a segmentation-aware optical\nflow SPyNetm, which simultaneously computes and inpaints flows corresponding only to the background scene, ignoring\nthe repeated fence occlusion patterns (Sec. 3.2); c) finally, an image inpainting module takes the estimated masks and flows,\naligns the frames with respect to a selected keyframe and fills in the missing pixel values (Sec. 3.3).\nMethod\nPrecision\nRecall\nF-measure\nDu et al. [4]\n0.910\n0.959\n0.934', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Du et al. [4]\n0.910\n0.959\n0.934\nU-net (thresh=0.05)\n0.908\n0.958\n0.931\nU-net (thresh=0.1)\n0.934\n0.942\n0.937\nU-net (thresh=0.3)\n0.969\n0.899\n0.932\nTable 1: Segmentation results on the De-fencing test set [4].\n(a) Keyframe', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='0.932\nTable 1: Segmentation results on the De-fencing test set [4].\n(a) Keyframe\n(b) SPyNet\n(c) SPyNetm\n(d) Ground truth\nFigure 3: Standard optical flow networks fail under repeated\nocclusion patterns. Our occlusion-aware SPyNetm can re-\nliably estimate the optical flow of the background scene,\nignoring the foreground occlusion. For training, “ground-\ntruth” flows are computed using a vanilla SPyNet on the\noriginal background frames.\nuse temporal information from multiple frames like [4], we\nachieve comparable performance.\n3.2. Segmentation-aware Optical Flow Estimation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='use temporal information from multiple frames like [4], we\nachieve comparable performance.\n3.2. Segmentation-aware Optical Flow Estimation\nOptical flow computation is an integral step in many\nobstruction removal and video completion pipelines. The\nchallenge is in how to align the background regions with-\nout being distracted by foreground occlusions. SOLD [14]\nuses a pretrained PWC-Net [25] to compute flows between\nall frame pairs in the burst, and uses frames warped to the\nkeyframe to prime background reconstruction. Note that, in\nthe case of fence removal, flows are only computed for the\nbackground layers, after removing the obstruction. The rea-\nson, according to the authors, is that the flow estimation net-\nwork cannot handle the repetitive structures, and often pre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='son, according to the authors, is that the flow estimation net-\nwork cannot handle the repetitive structures, and often pre-\ndicts noisy results, which renders the alignment step unreli-\nable. This is further aggravated by the fact that the weights\nof PWC-Net are frozen, so it cannot adapt to deal with po-\ntential errors in background layer reconstructions from the\nfirst levels in the coarse-to-fine SOLD architecture; this can\nconsequently yield inaccurate flow estimates in subsequent\nlevels, compounding errors.\nFinally, PWC-Net relies on\ncost volume computation, whose runtime does not scale fa-\nvorably with input size, at least when using a publicly avail-\nable implementation [17]. On the other hand, [29, 6] com-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='vorably with input size, at least when using a publicly avail-\nable implementation [17]. On the other hand, [29, 6] com-\npute flow maps between obstructed pairs of frames using\nFlowNet [8]. One key difference in this scenario is that the\nobstruction does not follow a repetitive structure pattern, but\nis typically a large, compact area. This causes the flow maps\nto contain holes which are inpainted in a separate step.\nIn our work we drastically simplify flow estimation for\nobstructed scenes by utilizing the fence segmentation net-\nwork described in Section 3.1.\nFirst, we replace PWC-\nNet with the faster, more lightweight, SPyNet [23] architec-\nture1. Second, we modify its first convolution layer to input', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Net with the faster, more lightweight, SPyNet [23] architec-\nture1. Second, we modify its first convolution layer to input\nboth the fence segmentation masks, Si, Sj, along with their\ncorresponding input frames Ii, Ij. Our modified SPYm ar-\nchitecture then estimates the mask-conditional flow map\nf m\nij = SPYm([Ii; Si], [Ij; Sj]),\n(2)\n[·] denoting concatenation along the channel dimension. We\nuse the original pretrained weights to initialize SPYm, ex-\ncept for the modified part of the input layer, which we ini-\ntialize randomly. During training, f m\nij are computed be-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='cept for the modified part of the input layer, which we ini-\ntialize randomly. During training, f m\nij are computed be-\ntween synthetically generated frames of fence images over-\nlaid on clean background frames. Consequently, we use\nflow maps computed with the vanilla SPyNet on the clean\n1We use “SPY” in equations, for short.\nResidual\nDense Network\n(RDN)\nW\n[ ]\nW\n+\nFigure 4: Frame inpainting module. We use the predicted\nfence segmentations to mask out (⊙) the occluded areas in\nthe input frames. We then use optical flow to warp (W) the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence segmentations to mask out (⊙) the occluded areas in\nthe input frames. We then use optical flow to warp (W) the\nmasked frames and respective masks. Finally, the flows, the\nvalidity maps (see text), and the aligned frames and masks\nare concatenated ([·]) and passed as features to a CNN pre-\ndicting the keyframe residual in the occluded regions.\nbackground frames Bi, Bj as pseudo ground truth targets.\nWe use an L1 loss to finetune SPYm:\nLf =\n1\n2N\nX\nx\n|SPY(Bi, Bj)|x − f m\nij|x|,\n(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='x\n|SPY(Bi, Bj)|x − f m\nij|x|,\n(3)\nwhere N is the number of image pixels, x denotes the lo-\ncation at which we evaluate, and we average over 2N to\naccount for the u, v flow channels.\nConditioning SPyNet on segmentation predictions al-\nlows us to denote parts of the scene corresponding to ob-\nstructions and ignore them while computing background\nflows, solving a fundamental problem faced by SOLD.\nThis idea was previously explored in [4] but it involved a\ncostly optimization process. Our approach is simple but\nrobust to the presence of significant fence obstructions.\nSegmentation-aware flow estimation can be useful in a va-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='costly optimization process. Our approach is simple but\nrobust to the presence of significant fence obstructions.\nSegmentation-aware flow estimation can be useful in a va-\nriety of practical settings where one wants to ignore parts\nof the scene as distractions or sources of noise. Figure 3\ndemonstrates the effectiveness of our approach by compar-\ning outputs of the vanilla SPyNet and our segmentation-\naware SPyNetm, on the same obstructed scene.\n3.3. Flow-guided Multi-frame Fence Removal\nThe final component in our fence removal pipeline is a\nframe inpainting module, depicted in Figure 4. The frame\ninpainting module takes as input the sequence of obstructed\nframes {Ii}, forward flow maps f m\nki, computed between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='inpainting module takes as input the sequence of obstructed\nframes {Ii}, forward flow maps f m\nki, computed between\nthe keyframe Ik and each other frame in the burst using\nthe mask-conditional SPyNetm (Section 3.2), and the fence\nsegmentation masks {Si} computed using our single-frame\nsegmentation model (Section 3.1).\nWe first use {Si} to\nmask out the areas that correspond to fences in each frame,\nobtaining masked frames Im\ni\n= Ii ⊙ Si. Then flows f m\nki are\nused to warp all frames and their respective segmentations\nwith respect to the reference frame, giving rise to aligned\nmasked frames ˜Im', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='used to warp all frames and their respective segmentations\nwith respect to the reference frame, giving rise to aligned\nmasked frames ˜Im\ni\n= W(Im\ni , f m\nki) and aligned fence masks\n˜Si = W(Si, f m\nki). We also compute binary masks {Vi} that\nmark valid warped regions, and are “on” for all pixels that\nfall inside the image grid after warping.\nfin = [{˜Im\ni };\n˜\n{Si}; {Vi}] is passed as input to a Residual\nDense Network (RDN) [36] that is responsible for filling in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='{Si}; {Vi}] is passed as input to a Residual\nDense Network (RDN) [36] that is responsible for filling in\nthe missing areas in the keyframe. We also add a skip con-\nnection between the masked keyframe Im\nk and the output of\nthe RDN, so the latter only has to learn to fill in the miss-\ning areas instead of reconstructing the entire image. The\ninpainting module is trained in a supervised fashion using\nan L1 loss and the clean background as the ground truth:\nLin = 1\nN\nX\nx\n|Bk|x − (Im\nk + RDN(fin))|x|.\n(4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='x\n|Bk|x − (Im\nk + RDN(fin))|x|.\n(4)\n3.4. Implementation details\nWe implement our pipeline in Python 3 and Py-\nTorch [20].\nFor U-net, PWC-Net, SPyNet, and RDN,\nwe use their publicly available third-party implementa-\ntions [27, 18, 17, 33]. To facilitate our experiments, we have\nalso re-implemented SOLD in PyTorch (SOLDpt), follow-\ning closely the original Tensorflow implementation [14];\nwe plan to make our re-implementation publicly available\nto allow for broader use by the community and replica-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='we plan to make our re-implementation publicly available\nto allow for broader use by the community and replica-\ntion of results. Unless otherwise mentioned, we train all\nour models for 1000 epochs, using a starting learning rate\nlr = 10−4, a weight decay rate wr = 4 · 10−5, and\nthe ADAM optimizer [12] with parameters α = 10−4,\nβ1 = 0.9, β2 = 0.999, ϵ = 10−8. All three models (fence\nsegmentation, occlusion-aware flow estimation, frame in-\npainting) are trained independently.\n4. Data for Training and Evaluation\nWe use two types of data in our experiments. The first\ntype is synthetic multi-frame sequences, generated similarly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='4. Data for Training and Evaluation\nWe use two types of data in our experiments. The first\ntype is synthetic multi-frame sequences, generated similarly\nto previous works [4, 14]. These are used predominantly for\ntraining and validation experiments, but a held-out test set is\nalso used for evaluation. The second type is real bursts with\nfence obstructions, which include uncontrolled sequences,\nfor which no ground truth clean frame is available, and con-\ntrolled sequences, which come with a clean background\nscene (without the fence) as ground truth.\nSynthetic bursts are generated by overlaying obstruction\n(foreground) layers on a clean scene (background).\nWe\nsource background scenes (which are also used as ground', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(foreground) layers on a clean scene (background).\nWe\nsource background scenes (which are also used as ground\ntruth during training and evaluation) from Vimeo-90k [30],\nwhich consists of videos depicting every day activities in\nrealistic settings, often including people and other objects.\nWe specifically use the original test split of the dataset2,\n2http://data.csail.mit.edu/tofu/testset/\nwhich contains sequences of seven (7) frames. Training and\nvalidation splits are generated on the fly, but for our evalua-\ntion experiments we use a fixed test set of 100 bursts.\nThe foreground fence obstructions are sourced from the\nDe-fencing dataset [4], which contains 545 training and 100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='The foreground fence obstructions are sourced from the\nDe-fencing dataset [4], which contains 545 training and 100\ntest images with fences, along with corresponding binary\nmasks as ground truth for the fence segmentation. The se-\nquences in this dataset have been collected in various out-\ndoor conditions and have a variable frame count per scene.\nSince we have the ground truth fence masks, we can use\nthem to mask out the fence from any given frame and over-\nlay it on a clean background from Vimeo. To obtain a fence\nimage burst of size K, we mask out the fence from a sin-\ngle frame and apply K random perspective distortions to it,\nto simulate the changes caused by slightly different view-\npoints and motion. To increase the variability of fences and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='to simulate the changes caused by slightly different view-\npoints and motion. To increase the variability of fences and\nbackground scenes, we apply various forms of data aug-\nmentations before fusing them into a single frame; these are\nlisted in detail in the supplemental material.\nReal bursts. Because we want to develop a practical algo-\nrithm for fence removal, good performance under realistic\nmotion, lighting, and obstruction patterns is of paramount\nimportance. In previous works, performance on real se-\nquences is –for the most part– evaluated qualitatively, since\nobtaining the ground truth background is far from trivial.\nLiu et al. [14] include only two sequences with fence-like\nobstructions, collected in a controlled environment, which\nis too small a dataset for a proper quantiative evaluation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='obstructions, collected in a controlled environment, which\nis too small a dataset for a proper quantiative evaluation.\nIn this paper we construct a wider set of controlled se-\nquences, specifically for quantitative evaluation.\nRather\nthan collecting toy scenes as in Liu et al. [14], we capture\nreal world hand-held sequences with a fence and a corre-\nsponding background ground truth image without a fence.\nAs we cannot physically remove a fence, we instead bring\nour camera to the fence and center it in one of the fence cells\nsuch that only the background is visible. To maintain a sim-\nilar level of brightness and sharpness of the background in\nthe input and ground truth images, we fix the exposure and\nfocus of the camera on the background during capture. Due', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the input and ground truth images, we fix the exposure and\nfocus of the camera on the background during capture. Due\nto camera motion and possible changes in illumination, the\ninput keyframe and its respective ground truth may be mis-\naligned or have color discrepancies. We align crops of the\nscene using standard feature-based RANSAC fitting of ho-\nmographies, similar to [3] and correct color discrepancies\nusing color histogram matching. We then filter out any mis-\naligned crops using SSIM, PSNR, and human visual check,\navoiding mostly homogeneous regions, to promote diver-\nsity in our dataset. Our final real burst dataset consists of\n185 320 × 192 input bursts and corresponding ground truth\nkeyframes. More details on dataset generation and image', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='185 320 × 192 input bursts and corresponding ground truth\nkeyframes. More details on dataset generation and image\nsampling are provided in supplemental material.\nvimeo_test_clean.zip\n5. Experiments\nWe compare our method and other works on synthetic\nand real bursts.\nFor quantitative evaluations we use the\ntest set of our synthetically generated fence-obstructed se-\nquences, and our real bursts described in Section 4. For all\nbaselines, we use the officially released model weights, with\nthe exception of our SOLD reimplementation. We also pur-\nposedly omit the sequence-specific online optimization step\nof SOLD in our comparisons. Although online optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='posedly omit the sequence-specific online optimization step\nof SOLD in our comparisons. Although online optimization\nimproves performance, its runtime is quite slow (∼ 3 min-\nutes per burst), pushing it outside the scope of our work,\nwhich is centered around efficiency and practicality. For\nqualitative evaluations and visual comparison, we use real\nsequences from previous work and the data we collected.\n5.1. Baselines\nSingle-frame baseline.\nWe pass either ground truth or\nour (thresholded) U-net fence mask predictions as input\nto LaMa [26], a state-of-the-art CNN-based inpainting\nmethod, to create a single-frame de-fencing baseline. LaMa\ntakes as input a (possibly obstructed) image and a binary', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='method, to create a single-frame de-fencing baseline. LaMa\ntakes as input a (possibly obstructed) image and a binary\nmask and inpaints the area marked by the mask.\nSOLD [14] primarily targets reflection removal, but it can\nbe adapted to deal with opaque obstructions such as fences\nor raindrops on glass. We evaluate both the original Tensor-\nflow model (SOLDtf) and our PyTorch reimplementation\n(SOLDpt), with the latter trained on our synthetic data.\nFlow-guided video completion operates in a setting that is\ndifferent than ours in a few ways. First, the mask denoting\nthe occluded area is known, and its shape is either rectan-\ngular or in the shape of an object in the video. Second,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the occluded area is known, and its shape is either rectan-\ngular or in the shape of an object in the video. Second,\nthe number of frames in a typical input video sequence is\nK ≫ 5. Lastly, the output is the entire inpainted video.\nNevertheless, we can apply these methods for de-fencing\nin a relatively straightforward fashion by passing the fence\nsegmentation as the occlusion mask, treating the burst as a\n(short) video sequence, and keeping the inpainted result for\nthe reference frame only. In our experiments we compare\nagainst two recent flow-guided approaches, FGVC [6] and\nE2FGVI [13], using their publicly provided code.\n5.2. Fence Removal on Synthetic and Real Data\nQuantitative comparisons on synthetic and real bursts', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='5.2. Fence Removal on Synthetic and Real Data\nQuantitative comparisons on synthetic and real bursts\nare shown in Table 2. We report performance in terms of\nthe commonly used SSIM, PSNR, and LPIPS metrics. For\nLPIPS we use a VGG-16 backbone as the feature extrac-\ntor. PSNR and SSIM can be computed as an aggregation of\npixel-wise scores, so we use the fence masks (ground truth\nin the case of synthetic data, thresholded and binarized U-\nnet predictions in the case of real data3) to dissect perfor-\n3To get better pseudo ground truth fence masks, we run U-net at multi-\nple scales and compute the pixel-wise maximum across scales.\n(a) Keyframe\n(b) LaMa [26]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ple scales and compute the pixel-wise maximum across scales.\n(a) Keyframe\n(b) LaMa [26]\n(c) SOLD [14]\n(d) FGVC [6]\n(e) Ours\n(f) Ground truth\nFigure 5: Qualitative de-fencing results on real sequences. We highlight areas of interest in red (shown as zoomed insets)\nand report PSNR inside the fence mask for all methods. Last example is from Xue et al. [31] and has no ground truth.\nSynthetic data (Methods in blue use GT fence masks)\nMethod\nSSIM ↑\nPSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='PSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal\nin\nout\ntotal\n(VGG)\nSOLDtf[14]\n.783\n.970\n.941\n23.36\n37.82\n30.34\n.111\nSOLDpt [14]\n.893\n.993\n.977\n28.14\n45.70\n35.82', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.977\n28.14\n45.70\n35.82\n.040\nLaMa [26]\n.788\n.995\n.964\n24.97\n51.38\n33.10\n.039\nLaMa [26]∗\n.655\n.955\n.910\n20.96\n31.74\n27.38\n.089\nFGVC [6]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='31.74\n27.38\n.089\nFGVC [6]\n.846\n.943\n.928\n25.56\n33.85\n30.36\n.068\nFGVC [6]∗\n.784\n.896\n.879\n22.73\n27.80\n26.05\n.113\nE2FGVI [13]\n.918\n.997', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.113\nE2FGVI [13]\n.918\n.997\n.985\n30.79\n55.87\n38.89\n.030\nE2FGVI [13]∗\n.890\n.984\n.969\n29.34\n38.64\n35.25\n.044\nOurs\n.954\n.999\n.992\n33.76', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Ours\n.954\n.999\n.992\n33.76\n56.55\n41.78\n.015\nOurs-fencegt\n.957\n.999\n.993\n34.33\n58.58\n42.42\n.012\nReal bursts (Methods in blue use pseudo-GT fence masks)\nSSIM ↑\nPSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='PSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal\nin\nout\ntotal\n(VGG)\n.728\n.911\n.885\n23.50\n30.27\n27.71\n.132\n.813\n.916\n.902\n26.41\n30.90\n29.71\n.094\n.480\n.902', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='29.71\n.094\n.480\n.902\n.845\n19.95\n29.96\n26.25\n.133\n.477\n.867\n.816\n20.85\n28.02\n25.98\n.132\n.848\n.910\n.901\n27.56\n30.48\n29.73\n.095', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='27.56\n30.48\n29.73\n.095\n.856\n.907\n.900\n27.49\n29.90\n29.36\n.090\n.571\n.902\n.856\n19.69\n29.88\n25.87\n.167\n.709\n.901\n.875\n25.58\n30.25', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.901\n.875\n25.58\n30.25\n29.14\n.117\n.869\n.917\n.909\n28.60\n31.14\n30.46\n.080\n.872\n.918\n.910\n28.77\n31.15\n30.53\n.078', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='28.77\n31.15\n30.53\n.078\nTable 2: Results on synthetic test data (left) and our collected real bursts (right). Results in rows denoted with “*” are\ncomputed after thresholding the fence segmentations at t = 0.1, and dilating the binary mask 4 times with a 3 × 3 square.\nmance in three different regions: a) inside the mask (in); b)\noutside the mask (out); and c) in the entire image (total).\nPerformance outside the mask is high for all methods, since\nthis part of the image is not occluded. Performance inside\nthe mask is the most important criterion, since it quanti-\nfies the quality of reconstruction only in the occluded area.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the mask is the most important criterion, since it quanti-\nfies the quality of reconstruction only in the occluded area.\nWe outperform all single- and multi-frame baselines, based\non all metrics, with FGVC [6] performing second best at\nthe cost of much higher runtime (see Section 5.3).\nWe\nwould like to draw the reader’s attention to the results of\nLaMa [26], in particular. LaMa is a state-of-the-art inpaint-\ning method, yet it achieves surprisingly low PSNR-in and\nSSIM-in scores. The reason becomes clear if one looks at\nFigure 5 (e.g., antennae structure in the first example): even\nthough LaMa produces perfectly plausible results under the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Figure 5 (e.g., antennae structure in the first example): even\nthough LaMa produces perfectly plausible results under the\noccluded area, these are often very different than the actual\nbackground scene. These results tellingly demonstrate the\nadvantage of using multiple frames for de-fencing. Figure 5\nalso illustrates that alternatives like SOLD and FGVC can\nyield blurry or completely scrambled results, likely due to\nissues with frame alignment. For more qualitative results\nsee our supplemental material.\n5.3. Runtime Analysis\nTable 3 compares the total runtime of our pipeline with\nother approaches. In our case, timing includes all necessary\nprocessing steps: fence segmentation, optical flow compu-\ntation, alignment, and frame inpainting. LaMa, FGVC, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='processing steps: fence segmentation, optical flow compu-\ntation, alignment, and frame inpainting. LaMa, FGVC, and\nMethod\nLaMa\nSOLDpt\nFGVC\nE2FGVI\nOurs\nRuntime (s)\n0.2\n0.8\n0.7\n0.16\n0.14 (0.08)\nTable 3: Runtime comparison for a 5-frame burst (LaMa\nprocesses a single frame). Times for other methods do not\ninclude segmentation (fence segmentations are part of the\nSOLD output). We provide the runtime of our method with-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='include segmentation (fence segmentations are part of the\nSOLD output). We provide the runtime of our method with-\nout the segmentation step in parentheses, for comparison.\nE2FGVI times do not include the time spent on fence seg-\nmentation, since these methods assume that the occlusion\nmasks are precomputed. Our method is clocked at ∼ 7 fps,\nfor 320 × 192 inputs, which is ∼ 5× faster than the next\nbest performing methods, SOLD and FGVC, and compara-\nble to E2FGVI and LaMa. The runtime difference with the\nlast two may not seem large in absolute terms, but it still\namounts to a 12.5% and 30% lower runtime respectively,\nwhile our method significantly outperforms them in terms', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='amounts to a 12.5% and 30% lower runtime respectively,\nwhile our method significantly outperforms them in terms\nof reconstruction quality on real data. If we exclude the\ntime spent on segmentation from our pipeline, the speedup\nbecomes even more noticeable (50% and 60% respectively).\nAll timings are performed on a workstation equipped with\na Nvidia GTX 1080 Ti, with 12GB of GPU RAM. The de-\ntailed breakdown of timings for our pipeline is: i) segmen-\ntation (for a 5-frame burst): 0.06s; ii) flow estimation and\nalignment: 0.04s; iii) frame inpainting (RDN): 0.04s.\n5.4. Ablation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='alignment: 0.04s; iii) frame inpainting (RDN): 0.04s.\n5.4. Ablation\nChanging the frame inpainting module architecture al-\nlows us to trade-off reconstruction performance for effi-\nciency. Replacing the RDN with a simple CNN consist-\ning of 8 convolution + LeakyReLU layers decreases per-\nformance by 3.5 dB on synthetic test data but also reduces\nruntime by ∼ 30%, from 0.14s to 0.1s.\nDoes frame alignment matter? Inaccurate computation\nof the motion corresponding to the (occluded) background\ncan lead to errors in frame alignment, impacting the quality\nof frame reconstruction. We experiment with the following', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='of the motion corresponding to the (occluded) background\ncan lead to errors in frame alignment, impacting the quality\nof frame reconstruction. We experiment with the following\ntwo alternatives for computing flows: i) original SPyNet on\nobstructed frames; ii) SPyNet followed by masking the oc-\ncluded areas and using Laplacian inpainting [29] to com-\nplete the missing flows (SPyNetinp). We also consider the\noption of not aligning frames at all, and leaving our RDN\ninpainting network to learn how to complete the occluded\nareas in the reconstructed keyframe.\nThe importance of alignment and the quality of flows\nused become becomes clear when looking at the results\nin Table 4. Not aligning the input frames at all leads to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='The importance of alignment and the quality of flows\nused become becomes clear when looking at the results\nin Table 4. Not aligning the input frames at all leads to\na noticeable drop in keyframe reconstruction performance.\nStandard flow networks cannot handle the repeated fence\nobstruction patterns, and explicitly inpainting the flows un-\nder the occluded area does not help either since flow arti-\nfacts extend to non-fence areas as well, as shown in Fig-\nAlignment\nSSIM ↑\nPSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal\nin\nout\ntotal\n(VGG)\nNone\n.792', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='in\nout\ntotal\n(VGG)\nNone\n.792\n.996\n.965\n24.82\n53.64\n32.84\n.028\nSPyNet\n.841\n.997\n.973\n26.84\n54.32\n34.93\n.047\nSPyNetinp\n.841\n.997\n.973\n26.81', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.841\n.997\n.973\n26.81\n54.13\n34.89\n.048\nSPyNetm\n.954\n.999\n.992\n33.76\n56.55\n41.78\n.015\nTable 4: Effect of frame alignment on keyframe reconstruc-\ntion quality (results on our synthetic test data).\nFigure 6: Failure example. Imperfect fence predictions\n(middle) compromise the quality of the inpainting (right).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Figure 6: Failure example. Imperfect fence predictions\n(middle) compromise the quality of the inpainting (right).\nure 3. Our occlusion-aware SPyNetm, on the other hand,\ncan accurately estimate background flows, resulting in su-\nperior frame alignment and reconstruction quality.\n5.5. Limitations and Failure Cases.\nThe main limitation of our approach is that the quality of\nthe final reconstruction depends on the outputs of U-net and\nSPyNetm in the two previous stages. Errors in fence seg-\nmentation affect segmentation-aware optical flow computa-\ntion, potentially compromising frame alignment, which is\ncrucial for good inpainting (see ablation in Section 5.4). In\naddition, fence segmentations are also used in masking out', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='crucial for good inpainting (see ablation in Section 5.4). In\naddition, fence segmentations are also used in masking out\nthe occluded areas in {Ii}, The most deleterious mistakes\noccur when the fence occlusion is out of our training distri-\nbution, e.g., when the fence has an unusual shape/pattern,\nor when the contrast with the background is low. This is\nan issue that can be handled to a certain extent through bet-\nter data augmentation during training or by having access\nto richer datasets with varied types of fences, as we show in\nthe supplemental material. A failure example and its effect\non frame inpainting is shown in Figure 6.\n6. Conclusions\nWe have developed a simple, modular, and efficient', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='on frame inpainting is shown in Figure 6.\n6. Conclusions\nWe have developed a simple, modular, and efficient\npipeline for removing fence obstructions from a singe frame\nin a photo burst. Our algorithm enjoys the realism of flow-\nguided video completion methods, while addressing some\nof their practical limitations, such as complicated training\nand long runtimes. Our method runs at 7 fps for 5-frame\n320 × 192 bursts, on a Nvidia GTX 1080 Ti, and is particu-\nlarly effective on real data, outperforming other single- and\nmulti-frame de-fencing baselines on a dataset of obstructed\nbursts we collected specifically for this problem.\nReferences', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='multi-frame de-fencing baselines on a dataset of obstructed\nbursts we collected specifically for this problem.\nReferences\n[1] Jean-Baptiste Alayrac, Joao Carreira, and Andrew Zisser-\nman. The visual centrifuge: Model-free layered video rep-\nresentations. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2457–\n2466, 2019. 1, 3\n[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-\nthresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183–202, 2009. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='thresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183–202, 2009. 2\n[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte.\nDeep burst super-resolution.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9209–9218, 2021. 2, 6, 11\n[4] Chen Du, Byeongkeun Kang, Zheng Xu, Ji Dai, and Truong\nNguyen. Accurate and efficient video de-fencing using con-\nvolutional neural networks and temporal information.\nIn', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Nguyen. Accurate and efficient video de-fencing using con-\nvolutional neural networks and temporal information.\nIn\n2018 IEEE International Conference on Multimedia and\nExpo (ICME), pages 1–6. IEEE, 2018. 2, 3, 4, 5, 6, 10, 11\n[5] Yosef Gandelsman, Assaf Shocher, and Michal Irani.\n”\ndouble-dip”: Unsupervised image decomposition via cou-\npled deep-image-priors. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 11026–11035, 2019. 1, 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Conference on Computer Vision and Pattern Recognition,\npages 11026–11035, 2019. 1, 3\n[6] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf.\nFlow-edge guided video completion. In European Confer-\nence on Computer Vision, pages 713–729. Springer, 2020. 3,\n4, 6, 7, 14, 16\n[7] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo-\nhannes Kopf. Temporally coherent completion of dynamic\nvideo. ACM Transactions on Graphics (TOG), 35(6):1–11,\n2016. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='video. ACM Transactions on Graphics (TOG), 35(6):1–11,\n2016. 2\n[8] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,\nAlexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-\ntion of optical flow estimation with deep networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 2462–2470, 2017. 4\n[9] Sankaraganesh Jonna, Krishna K Nakka, and Rajiv R Sahay.\nDeep learning based fence segmentation and removal from\nan image using a video sequence. In European Conference', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Deep learning based fence segmentation and removal from\nan image using a video sequence. In European Conference\non Computer Vision, pages 836–851. Springer, 2016. 2\n[10] Sankaraganesh Jonna, Sukla Satapathy, and Rajiv R Sahay.\nStereo image de-fencing using smartphones. In 2017 IEEE\ninternational conference on acoustics, speech and signal\nprocessing (ICASSP), pages 1792–1796. IEEE, 2017. 2\n[11] Sankaraganesh Jonna, Vikram S Voleti, Rajiv R Sahay, and\nMohan S Kankanhalli. A multimodal approach for image de-\nfencing and depth inpainting. In 2015 Eighth International', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Mohan S Kankanhalli. A multimodal approach for image de-\nfencing and depth inpainting. In 2015 Eighth International\nConference on Advances in Pattern Recognition (ICAPR),\npages 1–6. IEEE, 2015. 2\n[12] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 3, 5\n[13] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and\nMing-Ming Cheng.\nTowards an end-to-end framework\nfor flow-guided video inpainting.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Ming-Ming Cheng.\nTowards an end-to-end framework\nfor flow-guided video inpainting.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17562–17571, 2022. 2, 3, 6, 7\n[14] Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu\nChuang, and Jia-Bin Huang. Learning to see through ob-\nstructions with layered decomposition. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2021. 1, 2, 3,\n4, 5, 6, 7, 11, 12, 14, 15, 16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='4, 5, 6, 7, 11, 12, 14, 15, 16\n[15] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 3431–3440, 2015. 2\n[16] Yadong Mu, Wei Liu, and Shuicheng Yan. Video de-fencing.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 24(7):1111–1121, 2013. 2\n[17] Simon Niklaus.\nA reimplementation of PWC-Net us-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='[17] Simon Niklaus.\nA reimplementation of PWC-Net us-\ning PyTorch.\nhttps://github.com/sniklaus/\npytorch-pwc, 2018. 4, 5\n[18] Simon Niklaus.\nA reimplementation of SPyNet us-\ning PyTorch.\nhttps://github.com/sniklaus/\npytorch-spynet, 2018. 5\n[19] Minwoo Park, Kyle Brocklehurst, Robert T Collins, and\nYanxi Liu. Image de-fencing revisited. In Asian Confer-\nence on Computer Vision, pages 422–434. Springer, 2010. 1,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ence on Computer Vision, pages 422–434. Springer, 2010. 1,\n2\n[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\nperative style, high-performance deep learning library. In H.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content="Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\nperative style, high-performance deep learning library. In H.\nWallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E.\nFox, and R. Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc., 2019. 5\n[21] M Pawan Kumar, Philip HS Torr, and Andrew Zisserman.\nLearning layered motion segmentations of video. Interna-\ntional Journal of Computer Vision, 76(3):301–319, 2008. 1", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tional Journal of Computer Vision, 76(3):301–319, 2008. 1\n[22] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.\nGross, and A. Sorkine-Hornung. A benchmark dataset and\nevaluation methodology for video object segmentation. In\nComputer Vision and Pattern Recognition, 2016. 3\n[23] Anurag Ranjan and Michael J Black. Optical flow estima-\ntion using a spatial pyramid network. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4161–4170, 2017. 2, 4\n[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, pages 234–241.\nSpringer, 2015. 3\n[25] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPwc-net: Cnns for optical flow using pyramid, warping, and\ncost volume.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8934–8943,\n2018. 3, 4\n[26] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='2018. 3, 4\n[26] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky.\nResolution-robust large mask inpainting with\nfourier convolutions. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision, pages\n2149–2159, 2022. 6, 7, 14, 16\n[27] Nikhil\nTomar.\nSemantic-segmentation-architecture.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='[27] Nikhil\nTomar.\nSemantic-segmentation-architecture.\nhttps://github.com/nikhilroxtomar/\nSemantic-Segmentation-Architecture, 2020. 5\n[28] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.\nDeep image prior. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 9446–9454,\n2018. 3\n[29] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy.\nDeep flow-guided video inpainting. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Deep flow-guided video inpainting. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3723–3732, 2019. 2, 3, 4, 8\n[30] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and\nWilliam T Freeman. Video enhancement with task-oriented\nflow.\nInternational Journal of Computer Vision (IJCV),\n127(8):1106–1125, 2019. 5, 10, 11\n[31] Tianfan Xue, Michael Rubinstein, Ce Liu, and William T\nFreeman.\nA computational approach for obstruction-free\nphotography.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Freeman.\nA computational approach for obstruction-free\nphotography.\nACM Transactions on Graphics (TOG),\n34(4):1–11, 2015. 2, 3, 7, 12, 15\n[32] Liu Yanxi, Belkina Tamara, H Hays James, and Roberto\nLublinerman. Image de-fencing. In Computer Vision and\nPattern Recognition, 2008. CVPR 2008. IEEE Conference\non, pages 1–8, 2008. 1, 2\n[33] Jeffrey Yeo.\nRdn.\nhttps://github.com/yjn870/\nRDN-pytorch, 2019. 5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Rdn.\nhttps://github.com/yjn870/\nRDN-pytorch, 2019. 5\n[34] Renjiao Yi, Jue Wang, and Ping Tan. Automatic fence seg-\nmentation in videos of dynamic scenes. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 705–713, 2016. 2\n[35] Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-guided\nflow completion and style fusion for video inpainting.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5982–5991, 2022. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5982–5991, 2022. 2\n[36] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image super-resolution.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2472–2481, 2018. 5\nA. Data\nA.1. Synthetic Data Augmentation\nBackground\naugmentation.\nWe\nsource\nbackground\nscenes (which are also used as ground truth during train-\ning and evaluation) from Vimeo-90k [30], which consists', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='background\nscenes (which are also used as ground truth during train-\ning and evaluation) from Vimeo-90k [30], which consists\nof videos depicting every day activities in realistic settings,\noften including people and other objects. We specifically\nuse the original test split of the dataset4, which contains se-\nquences of seven (7) frames. The original clean frames are\nused as ground truth for training and evaluation. To increase\nvariability of our synthetically generated data, we apply the\nfollowing data augmentation steps:\n1. random homography transformation\n2. center cropping to avoid any black borders caused by\n(1).\n3. random cropping of a 320 × 192 window, which are\nthe frame dimensions used during training.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(1).\n3. random cropping of a 320 × 192 window, which are\nthe frame dimensions used during training.\n4. random horizontal flip.\nForeground augmentation.\nThe foreground fence ob-\nstructions are sourced from the De-fencing dataset [4],\nwhich contains 545 training and 100 test images with\nfences, along with corresponding binary masks as ground\ntruth for the fence segmentation. The variability of fences\nin that dataset is limited, so we also apply various forms of\ndata augmentation on the fence image before fusing it with\nthe background. The types of foreground augmentation we\nconsider are:\n1. random downsample of the fence image and segmen-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the background. The types of foreground augmentation we\nconsider are:\n1. random downsample of the fence image and segmen-\ntation to create fences of different sizes and thickness.\n2. random “outer” window crop to focus on a specific\nsubregion of the fence.\n3. color jitter to make the network more robust to differ-\nent fence appearances and lighting conditions.\n4. random perspective distortion to obtain a fence se-\nquence of length K.\n5. center cropping to avoid black border effects from the\nhomographic distortion.\n6. random blur with a gaussian kernel, to simulate defo-\ncus aberrations.\nSamples from our synthetic burst dataset are shown in Fig-\nure 7.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='cus aberrations.\nSamples from our synthetic burst dataset are shown in Fig-\nure 7.\n4http://data.csail.mit.edu/tofu/testset/vimeo_\ntest_clean.zip\nFigure 7: Examples of our synthetically generated data. The leftmost column shows the clean background frame and the next\n5 columns show the background burst from Vimeo-90k [30], with overlaid fences from the De-fencing dataset [4].\nA.2. Real Burst Collection\nAlthough our synthetic data are carefully generated and\nexhibit considerable realism and diversity, they still cannot\nfully capture the variability of motion, lighting, and obstruc-\ntion patterns in scenes captured under realistic conditions,\nso we collect set of controlled sequences, specifically for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fully capture the variability of motion, lighting, and obstruc-\ntion patterns in scenes captured under realistic conditions,\nso we collect set of controlled sequences, specifically for\nquantitative evaluation. As mentioned in the main paper,\nrather than collecting toy scenes as in Liu et al. [14], we cap-\nture outdoors real world hand-held sequences with a fence\nand a corresponding background ground truth image with-\nout a fence.\nData capture.\nWe first capture one image without the\nfence as the ground-truth frame, by bringing our camera to\nthe centre of a fence cell. We then fix the focus and expo-\nsure on the background and move backwards from the fence\nto capture 5 frames with fences. To minimize misalignment\ncaused by a change in perspective, we capture the first frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='to capture 5 frames with fences. To minimize misalignment\ncaused by a change in perspective, we capture the first frame\nas the key-frame, moving backwards along the capturing\ndirection. Then, we capture the remaining four frames by\nintentionally jittering the camera around.\nKeyframe - groundtruth alignment.\nAfter capturing the\nreal bursts, we need to align the ground-truth frame to the\nobstruced key-frame. We do this following an approach\ncombining SIFT feature extraction and RANSAC homog-\nraphy estimation, similar to [3].\nWe start by computing\nand matching SIFT features in the keyframe and respective\nclean groundtruth shot. Since the resolution of the original\nimages is high, we extract 320 × 192 regions in a sliding', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='clean groundtruth shot. Since the resolution of the original\nimages is high, we extract 320 × 192 regions in a sliding\nwindow fashion, and within such window, P, we compute\nhomography parameters using matched SIFT features in\ncrops of varying sizes: 1282, 2562, 5122, and 10242 (larger\ncrop sizes extend beyond the area of the original window).\nThe computed homography parameters are used for global\nalignment of the keyframe and groundtruth frames, so we\nhave multiple homography “candidates” corresponding to\nP. The motivation behind computing homographies at dif-\nferent scales is that different parts of a given window P\nmay require different homographies to be aligned more ac-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='P. The motivation behind computing homographies at dif-\nferent scales is that different parts of a given window P\nmay require different homographies to be aligned more ac-\ncurately. We assign the best homography to each 128 × 128\ncrop C inside P, by computing its respective SSIM score\nwith respect to its warped counterpart in the groundtruth\n(we use the estimated fence masks to only include non-\nobstructed areas in the SSIM computation). To ensure a\nminimum level of quality, if there is at least one C inside P\nwith average SSIM ≤ 0.2 or PSNR ≤ 20, we discard P and\nmove to the next sliding window with stride 128. If there\nare no “failed” crops, P slides to the next non-overlapping', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='move to the next sliding window with stride 128. If there\nare no “failed” crops, P slides to the next non-overlapping\nposition. In the end, we also manually filter out the crops\nthat are misaligned on and near the fences by visual com-\nparison between input and aligned ground-truth. We also\nmanually filter out crops consisting of mostly homogeneous\nregions (sky, land, sand), to promote diversity in our dataset.\nOur final real burst dataset consists of 185 320 × 192 input\nbursts with corresponding ground truth key-frames from 29\nscenes. Samples from our real burst dataset are shown in\nFigure 9.\nB. Task Specificity and Comparison with\nSOLD [14]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Figure 9.\nB. Task Specificity and Comparison with\nSOLD [14]\nOne potential criticism towards our approach is our focus\non a specific type of obstruction (fences), and the fact that\nwe heavily rely on a specific prior (pre-trained fence seg-\nmentation model), which can harm generalization to new\ninputs, not commonsly seen in the training data. In com-\nparison, SOLD [14] is a multi-frame approach can handle\nvarious types of obstructions. However, SOLD is also lim-\nited when faced with atypical obstructions (e.g., fences), re-\nquiring scene-specific, costly online optimization that takes\n∼ 3 minutes, to achieve good results, making it impracti-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='quiring scene-specific, costly online optimization that takes\n∼ 3 minutes, to achieve good results, making it impracti-\ncal for real-world application. Our method trades-off gen-\nerality for reconstruction and runtime performance (the lat-\nter is a feature missing from previous de-fencing works),\nproducing better de-fencing results than SOLD, at a frac-\ntion of its runtime, without requiring scene-specific opti-\nmization. Besides, de-fencing is an important problem in\nits own right, with an extensive literature in computer vi-\nsion (see Section 2.1 in the main paper). Finally, we can\nmake our method more robust to a broader variety of fences\n(e.g., rhombic rotated fences, etc.) by improving our data', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='make our method more robust to a broader variety of fences\n(e.g., rhombic rotated fences, etc.) by improving our data\naugmentation protocol. To showcase this, we have added\nmore scale, rotation, shape and color variation during the\n(a) Keyframe\n(b) Output (original) (c) Improved output\nFigure 8: Better data augmentation can make the fence seg-\nmentation network more robust to varied types of fences,\nthus improving the quality of frame inpainting on real se-\nquences without the need for online finetuning.\ntraining of the fence segmentation model. As shown in Fig-\nure 8, after adding these additional data augmentations, the\nfence segmentation model can accurately segment fences', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ure 8, after adding these additional data augmentations, the\nfence segmentation model can accurately segment fences\nthat are rotated, very thin fences, or have low contrast\nwith respect to the background, subsequently improving de-\nfencing quality. Extending our method to handle other types\nof obstructions (e.g., reflections), is also a direction we are\ncurrently exploring.\nC. Qualitative Results\nFigure 10 shows additional qualitative results on se-\nquences from our synthetically generated test set.\nFig-\nure 11 shows results on real sequences, taken from previous\nworks [31, 14]. We are also including some failure cases,\nwhere the fence segmentation model encounters fences at\nscales or shapes that are out of its training distribution, re-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='where the fence segmentation model encounters fences at\nscales or shapes that are out of its training distribution, re-\nsulting in low de-fencing quality. Finally, in Figure 12 we\ncompare results from our method and other baselines on ex-\namples from the real burst dataset we collected.\nFigure 9: Examples of real bursts we have collected. These are 320 × 192 crops from the original, high resolution images,\nafter alignment.\n(a) Keyframe\n(b) LaMa [26]\n(c) SOLD [14]\n(d) FGVC [6]\n(e) Ours\n(f) Ground truth\nFigure 10: De-fencing results on sequences from our synthetic data, and respective PSNR scores inside the fence mask', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(f) Ground truth\nFigure 10: De-fencing results on sequences from our synthetic data, and respective PSNR scores inside the fence mask\narea. The leftmost column shows the obstructed keyframe, and the next 5 rows show its reconstructed version using various\nbaselines and our approach. Zoom in to notice differences in reconstructed frames.\nFigure 11: De-fencing results on sequences from [31, 14]. From top to bottom: obstructed keyframe, reconstructed keyframe\nusing our approach, estimated fence segmentation using our U-net fence segmentation model. The second group of results\nshows failure cases: when the fence obstruction is outside our training distribution (e.g., scale - very thin fences, irregular\nfence pattern, such as vertical bars, extreme blur etc.) the fence segmentation estimation fails, affecting reconstruction quality.\nAddressing unusual fence obstructions like these is our main focus for future work.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence pattern, such as vertical bars, extreme blur etc.) the fence segmentation estimation fails, affecting reconstruction quality.\nAddressing unusual fence obstructions like these is our main focus for future work.\n(a) Keyframe\n(b) LaMa [26]\n(c) SOLD [14]\n(d) FGVC [6]\n(e) Ours\n(f) Ground truth\nFigure 12: Qualitative de-fencing results on real sequences, and respective PSNR scores inside the fence mask area. The\nleftmost column shows the obstructed keyframe, and the next 5 rows show its reconstructed version using various baselines\nand our approach. Zoom in to notice differences in reconstructed frames.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc6ec150> 111
cuda:2
[Document(page_content='Efficient Flow-Guided Multi-frame De-fencing\nStavros Tsogkas\nFengjia Zhang\nAllan Jepson\nAlex Levinshtein\nSamsung AI Center Toronto\n101 College St., Toronto, ON, Canada, M5G 1L7\n{stavros.t, f.zhang2, allan.jepson, alex.lev}@samsung.com\nAbstract\nTaking photographs “in-the-wild” is often hindered by\nfence obstructions that stand between the camera user and\nthe scene of interest, and which are hard or impossible\nto avoid. De-fencing is the algorithmic process of auto-\nmatically removing such obstructions from images, reveal-\ning the invisible parts of the scene.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='to avoid. De-fencing is the algorithmic process of auto-\nmatically removing such obstructions from images, reveal-\ning the invisible parts of the scene.\nWhile this problem\ncan be formulated as a combination of fence segmentation\nand image inpainting, this often leads to implausible hal-\nlucinations of the occluded regions. Existing multi-frame\napproaches rely on propagating information to a selected\nkeyframe from its temporal neighbors, but they are often in-\nefficient and struggle with alignment of severely obstructed\nimages. In this work we draw inspiration from the video\ncompletion literature, and develop a simplified framework\nfor multi-frame de-fencing that computes high quality flow\nmaps directly from obstructed frames, and uses them to ac-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='for multi-frame de-fencing that computes high quality flow\nmaps directly from obstructed frames, and uses them to ac-\ncurately align frames. Our primary focus is efficiency and\npracticality in a real world setting: the input to our algo-\nrithm is a short image burst (5 frames) – a data modality\ncommonly available in modern smartphones– and the out-\nput is a single reconstructed keyframe, with the fence re-\nmoved. Our approach leverages simple yet effective CNN\nmodules, trained on carefully generated synthetic data, and\noutperforms more complicated alternatives real bursts, both\nquantitatively and qualitatively, while running real-time.\n1. Introduction\nRapid improvements in both the camera hardware and\nimage processing software have turned modern cell phones', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='1. Introduction\nRapid improvements in both the camera hardware and\nimage processing software have turned modern cell phones\ninto powerful yet portable image and video recording de-\nvices.\nThis has enabled and encouraged casual users to\nshoot photos without any time for special preparation,\nsetup, or framing of the shot. On the flip side, photos and\nvideos taken under these conditions rarely contain just the\nobject(s) of interest, and are hindered by various obstruc-\ntions that stand between the subject and the user.\nOne type of obstruction that is of special interest because\nFigure 1: We train a simple and efficient model for de-\nfencing: removing fence obstructions from images, reveal-\ning the underlying scene of interest. Our method is fast and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fencing: removing fence obstructions from images, reveal-\ning the underlying scene of interest. Our method is fast and\ncan accurately remove fences of varying size and appear-\nance from real bursts, without online finetuning.\nof its commonness is fences. Imagine, for instance, taking\na photo of an animal through a zoo fence, or people play-\ning basketball in a fenced-out outdoor court; these are just\na few everyday scenes that are obstructed by fence struc-\ntures that are either inconvenient or completely impossible\nto avoid. De-fencing uses computer vision algorithms to\nautomatically remove such fence obstructions from images,\nas shown in Figure 1. De-fencing is a harder problem than\nit may initially seem. Fences have varying structure and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='as shown in Figure 1. De-fencing is a harder problem than\nit may initially seem. Fences have varying structure and\nappearance patterns, and come in different thicknesses and\nsizes. Furthermore, reconstruction of the background scene\ncan become challenging because of low lighting and noise,\nor motion blur, caused by rapidly moving objects.\nThe first works that tackled de-fencing in a principled\nmanner were by Liu et al. [32, 19]. They formulate the prob-\nlem as the segmentation of a repeating foreground pattern\nthat exhibits approximate translational symmetry, followed\nby inpainting to recover the occluded image regions. The\napproach of [32] is mainly limited by the fact that it uses a\nsingle image as input. Because of the opaque nature of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='approach of [32] is mainly limited by the fact that it uses a\nsingle image as input. Because of the opaque nature of the\nfence obstruction, the occluded parts of the scene must be\nhallucinated by the inpainting algorithm; [19] partially ad-\ndresses this by using a photo taken from a different view, to\nreduce the number of pixels that must be hallucinated.\nDe-fencing can also be viewed as a special case of the\nmore general problem of layer separation [21, 1, 5, 14],\nwhich models an image as a composition of individual lay-\narXiv:2301.10759v1  [cs.CV]  25 Jan 2023\ners, e.g., a foreground layer containing the obstruction and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ers, e.g., a foreground layer containing the obstruction and\na background layer containing the scene of interest. Xue\net al. [31] formulate generic obstruction removal as a layer\nseparation problem, driven by motion parallax. Although\ntheir solution is generic and works well, it involves multi-\nple time-consuming, hand-tuned optimization steps, and the\nuse of hand-crafted motion and image priors. SOLD [14] is\na deep learning re-incarnation of [31] that achieved state-\nof-the-art results on obstruction removal, and can also be\nadapted to remove fences from a multi-frame input burst.\nUnfortunately, SOLD depends on computationally expen-\nsive networks for flow computation and frame reconstruc-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Unfortunately, SOLD depends on computationally expen-\nsive networks for flow computation and frame reconstruc-\ntion, making it impractical for use in low-powered devices.\nIt also often requires an online optimization step that takes\n∼ 3 minutes to produce acceptable results on real bursts,\nand even without this input-specific finetuning, it cannot be\nrun in real time. Finally, since the background frames are\nthe output of a reconstruction CNN module, they occasion-\nally contain inconsistencies or artifacts.\nFlow-guided completion methods [7, 29, 29, 13, 35] re-\nduce such artifacts by computing inpainted flow maps be-\ntween pairs of obstructed frames and using them to explic-\nitly transfer pixel values to a reference frame from its tem-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tween pairs of obstructed frames and using them to explic-\nitly transfer pixel values to a reference frame from its tem-\nporal neighbors. Inpainting flows in the occluded area is\neasier than directly inpainting pixel values, so a less pow-\nerful network can be used, and the results tend to look\nmore plausible because the pixel values are taken from real\nframes instead of being hallucinated by a generative net-\nwork. These works do not make any assumptions regard-\ning the shape and type of the occlusion, other the fact that\nit is completely opaque. However, the mask marking the\noccluded area is considered to be known, which is an unre-\nalistic requirement for our purposes.\nHere our goal is to develop a de-fencing algorithm that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='alistic requirement for our purposes.\nHere our goal is to develop a de-fencing algorithm that\nprioritizes efficiency and practicality. We develop a frame-\nwork that enjoys the realism and modularity of flow-based\nvideo completion approaches, while being significantly\nsimpler to train and deploy. Instead of videos, the input to\nour algorithm is shorter bursts of K = 5 frames, which are a\nvery common photo modality in modern smartphones. The\ntype of occlusion is known (fences), but we do not make\nany assumptions regarding its spatial extent or location; in-\nstead, we train a class-specific segmentation model to au-\ntomatically detect fences in images. Computing flow maps\nof scenes occluded by fences, presents us with a new chal-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tomatically detect fences in images. Computing flow maps\nof scenes occluded by fences, presents us with a new chal-\nlenge, as standard optical flow networks fail under the pres-\nence of repeated patterns [9, 14]. To solve this problem,\nwe train a segmentation-aware SPyNet [23] that can simul-\ntaneously compute and inpaint flow maps corresponding to\nthe occluded background scene, ignoring foreground occlu-\nsions. Finally, to quantitatively evaluate the performance of\nour approach on real data, we collect a dataset of multi-\nframe sequences and corresponding “pseudo-groundtruth”\nfor the reference frame using a alignment procedure, akin\nto [3]. To summarize:\nWe design a CNN pipeline for multi-frame de-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='for the reference frame using a alignment procedure, akin\nto [3]. To summarize:\nWe design a CNN pipeline for multi-frame de-fencing\nthat is simple, modular, efficient, and easy to train.\nUnlike flow-based works, which assume the occlusion\nis known, we estimate it automatically from the input.\nWe train a segmentation-aware optical flow model that\ncan reliably estimate flows corresponding to the back-\nground scene despite severe fence obstructions.\nOur method achieves state-of-the-art results on syn-\nthetic and real bursts, without requiring sequence-\nspecific finetuning.\nAs a result, it has significantly\nlower runtimes compared to alternatives.\n2. Related Work', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='specific finetuning.\nAs a result, it has significantly\nlower runtimes compared to alternatives.\n2. Related Work\n2.1. Image and Video De-fencing\nLiu et al. [32] is perhaps the first work to formally intro-\nduce de-fencing in a computer vision context, as symmetry-\ndriven automatic fence segmentation, followed by inpaint-\ning. [19] improves on this work by using online learning\nto aid lattice detection and segmentation, and by leverag-\ning a second viewpoint to improve inpainting.\nJonna et\nal. [11, 10] also improve fence segmentation, by comple-\nmenting RGB with depth data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Jonna et\nal. [11, 10] also improve fence segmentation, by comple-\nmenting RGB with depth data.\n[16, 34] extend de-fencing to video sequences of arbi-\ntrary number of frames. Mu et al [16] rely on motion par-\nallax to separate foreground fence obstructions from back-\nground (although the definition of “fence” is quite loose),\nwhile Yi et al. [34] describe a bottom-up approach for video\nde-fencing that groups pixels in each frame using color and\nmotion cues. Both methods rely on optimization techniques\nto refine their optical flow or frame inpainting results.\nMore recently, deep learning has been adopted for video', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='motion cues. Both methods rely on optimization techniques\nto refine their optical flow or frame inpainting results.\nMore recently, deep learning has been adopted for video\ndefencing [9, 4]. Jonna et al. [9] use a pretrained classifi-\ncation CNN as a feature extractor and train a SVM classi-\nfier that distinguishes fence from non-fence patches. The\nauthors reformulate an existing optical flow algorithm to\nmake it occlusion-aware and recover the de-fenced image\nusing FISTA optimization [2].\nDu et al. [4] replace the\nCNN-SVM combination with a fully convolutional network\n(FCN) [15] and apply temporal refinement to the extracted\nsegmentations by aggregating information from neighbor-\ning frames.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(FCN) [15] and apply temporal refinement to the extracted\nsegmentations by aggregating information from neighbor-\ning frames.\nOur approach shares a similar pipeline but\nsimplifies both the segmentation extraction and occlusion-\naware flow computation steps, while being considerably\nfaster, as we do not perform test-time optimization.\n2.2. Layer Separation\nA more generic formulation of the fence removal prob-\nlem views an image as a composition of layers, each with\nits own alpha map (which can be semi transparent), and\nthe goal is to separate the layers. In [5] the foreground-\nbackground layers are the output of two convolutional net-\nwork, trained per image in an unsupervised fashion, and are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='background layers are the output of two convolutional net-\nwork, trained per image in an unsupervised fashion, and are\nrecovered using a deep image prior [28]. A similar idea\nis used by Alayrac et al. [1] for video decomposition, but\nwith supervised training. Other approaches for video de-\ncomposition use explicit motion information [31, 14]. Xue\net al. [31] describe a computational method for decompos-\ning a scene into an foreground obstruction layer and a back-\nground scene from multi-scale motion cues of a multi-frame\nsequence, in an unsupervised way. They solve an optimiza-\ntion problem that alternatingly finds the constituent layers\nand the respective motion fields that, when used to align', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tion problem that alternatingly finds the constituent layers\nand the respective motion fields that, when used to align\nthe burst to a reference frame, can reconstruct the original\nframes with low error. A modern reincarnation of this ap-\nproach is proposed by Liu et al. with SOLD [14]. SOLD\nfollows a similar multi-scale approach, using a convolu-\ntional framework, both for layer reconstruction and motion\nestimation – the latter wth a pre-trained PWC-Net [25].\n2.3. Flow-based Video Completion\nVideo completion is a related problem to multi-frame\nfence removal, the main difference being that the segmen-\ntation mask is assumed to be provided and the emphasis is\ntypically on longer frame sequences. Our work is motivated', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence removal, the main difference being that the segmen-\ntation mask is assumed to be provided and the emphasis is\ntypically on longer frame sequences. Our work is motivated\nby Xu et al. [29], who proposed the idea of first tackling the\neasier problem of flow inpainting, and then using the com-\npleted flows to propagate color values to a reference frame\nfrom its temporal neighbors. Since it not guaranteed that\nall occluded pixels are visible in some frame, a separate\nimage inpainting step must be used to fill any remaining\nholes. Gao et al. [6] improve this approach by synthesizing\nsharp flow edges along the object boundaries and using non-\nlocal temporal neighborhoods for propagating pixels across\nframes. These works involve a series of individual, sepa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='sharp flow edges along the object boundaries and using non-\nlocal temporal neighborhoods for propagating pixels across\nframes. These works involve a series of individual, sepa-\nrately trained processing stages, some of which are hand-\ncrafted, inefficient, and can potentially compromise perfor-\nmance of subsequent stages. Li et al. [13] address this is-\nsue by proposing an end-to-end framework for flow-guided\nvideo completion. Their approach was developed concur-\nrently to our own and shares some of its simplicity and effi-\nciency advantages. However, their framework still operates\nunder the assumption that the occlusion mask is provided.\n3. Method\nWe begin with an overview of the problem we are solv-\ning, and establish the notation that we use throughout the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='3. Method\nWe begin with an overview of the problem we are solv-\ning, and establish the notation that we use throughout the\npaper. The input to our algorithm is a burst of K RGB\nframes, {Ii}, composed from an unknown background\nscene of interest{Bi} and an unknown opaque foreground\nocclusion in the form of a fence {Fi}. Specifically,\nIi = Si · Fi + (1 − Si) · Bi,\n(1)\nwhere Si ∈ [0, 1] is a soft fence occlusion mask. Our goal\nis to train a model that removes the fence obstruction from\n{Ii} and recovers a single keyframe background image Bk,\nwhere k is the keyframe index.\nInstead of outputting the unobstructed frame directly, we', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='where k is the keyframe index.\nInstead of outputting the unobstructed frame directly, we\nbreak down the problem into the steps illustrated in Fig-\nure 2. We start by training a network that is applied indi-\nvidually on each frame in {Ii}, and outputs fence segmen-\ntation predictions {Si}. The role of these segmentations\nis two-fold: i) they mark the occluded area that needs to\nbe recovered; ii) they are used to condition a segmentation-\naware network that computes optical flows corresponding to\nthe background scene only, directly from the occluded input\n{Ii}. With this network we extract flows {fkj} between the\nkeyframe Ik and each other frame Ij in the sequence, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='{Ii}. With this network we extract flows {fkj} between the\nkeyframe Ik and each other frame Ij in the sequence, and\nalign the burst. Finally, we employ learned flow-guided im-\nage inpainting, to recover the parts of the keyframe that are\noccluded by the fence, yielding the final output ˜Bk. In the\nfollowing subsections we explain in detail each step.\n3.1. Single-frame Fence Segmentation\nOur fence segmentation model takes as input a single\nRGB frame, possibly containing a fence, and outputs a soft\nfence segmentation mask. Although this sounds like a rel-\natively simple task, there are, in fact, multiple challenges.\nFirst, datasets of large size and with high quality annota-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='atively simple task, there are, in fact, multiple challenges.\nFirst, datasets of large size and with high quality annota-\ntions for fence segmentation are surprisingly scarce. The\nmost appropriate for this task is probably the De-fencing\ndataset [4]. Fences in this dataset do not exhibit significant\nvariance in terms of appearance, scale, or structure, so we\nrely on substantial data augmentation to train a network that\nis robust to different types of fences and environments.\nMore specifically, we apply different degrees of down-\nscaling to the original image and its associated annotation,\nto effectively create fences at different scales (varying fence\nwidth/distance from the camera). To augment the limited\nscene variety in the De-fencing dataset, we also mask out', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='width/distance from the camera). To augment the limited\nscene variety in the De-fencing dataset, we also mask out\nfences, using the groundtruth segmentations, and overlay\nthem on images from the DAVIS dataset [22]. Finally, we\napply random horizontal flipping to the fence image and\ntake randomly crop a 320 × 192 window for training.\nThe segmentation network itself is a U-net [24] back-\nbone, with four encoder and four decoder blocks, that is\ntrained from scratch on our augmented fence data using a\nbinary cross entropy loss and the ADAM optimizer [12].\nTo obtain segmentation scores in the [0, 1] range, we ap-\nply a sigmoid in the output logits from the last U-net layer.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='To obtain segmentation scores in the [0, 1] range, we ap-\nply a sigmoid in the output logits from the last U-net layer.\nTable 1 lists precision-recall and f-measure scores of our\nmethod at different thresholds.\nEven though we do not\nBurst (K frames)\nFence\nSegmentation\nOptical flow\nK fence masks\nK-1 flows\nFrame inpainting\nReconstructed\nkeyframe\nFigure 2: Given a burst of K frames with fence obstructions as input, we reconstruct a single keyframe, after removing the\nfence. Our pipeline is composed of three distinct steps: a) initially, a fence mask is estimated on each input frame individually', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence. Our pipeline is composed of three distinct steps: a) initially, a fence mask is estimated on each input frame individually\nwith a U-net fence segmentation model (Sec. 3.1); b) the estimated masks are used to condition a segmentation-aware optical\nflow SPyNetm, which simultaneously computes and inpaints flows corresponding only to the background scene, ignoring\nthe repeated fence occlusion patterns (Sec. 3.2); c) finally, an image inpainting module takes the estimated masks and flows,\naligns the frames with respect to a selected keyframe and fills in the missing pixel values (Sec. 3.3).\nMethod\nPrecision\nRecall\nF-measure\nDu et al. [4]\n0.910\n0.959\n0.934', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Du et al. [4]\n0.910\n0.959\n0.934\nU-net (thresh=0.05)\n0.908\n0.958\n0.931\nU-net (thresh=0.1)\n0.934\n0.942\n0.937\nU-net (thresh=0.3)\n0.969\n0.899\n0.932\nTable 1: Segmentation results on the De-fencing test set [4].\n(a) Keyframe', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='0.932\nTable 1: Segmentation results on the De-fencing test set [4].\n(a) Keyframe\n(b) SPyNet\n(c) SPyNetm\n(d) Ground truth\nFigure 3: Standard optical flow networks fail under repeated\nocclusion patterns. Our occlusion-aware SPyNetm can re-\nliably estimate the optical flow of the background scene,\nignoring the foreground occlusion. For training, “ground-\ntruth” flows are computed using a vanilla SPyNet on the\noriginal background frames.\nuse temporal information from multiple frames like [4], we\nachieve comparable performance.\n3.2. Segmentation-aware Optical Flow Estimation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='use temporal information from multiple frames like [4], we\nachieve comparable performance.\n3.2. Segmentation-aware Optical Flow Estimation\nOptical flow computation is an integral step in many\nobstruction removal and video completion pipelines. The\nchallenge is in how to align the background regions with-\nout being distracted by foreground occlusions. SOLD [14]\nuses a pretrained PWC-Net [25] to compute flows between\nall frame pairs in the burst, and uses frames warped to the\nkeyframe to prime background reconstruction. Note that, in\nthe case of fence removal, flows are only computed for the\nbackground layers, after removing the obstruction. The rea-\nson, according to the authors, is that the flow estimation net-\nwork cannot handle the repetitive structures, and often pre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='son, according to the authors, is that the flow estimation net-\nwork cannot handle the repetitive structures, and often pre-\ndicts noisy results, which renders the alignment step unreli-\nable. This is further aggravated by the fact that the weights\nof PWC-Net are frozen, so it cannot adapt to deal with po-\ntential errors in background layer reconstructions from the\nfirst levels in the coarse-to-fine SOLD architecture; this can\nconsequently yield inaccurate flow estimates in subsequent\nlevels, compounding errors.\nFinally, PWC-Net relies on\ncost volume computation, whose runtime does not scale fa-\nvorably with input size, at least when using a publicly avail-\nable implementation [17]. On the other hand, [29, 6] com-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='vorably with input size, at least when using a publicly avail-\nable implementation [17]. On the other hand, [29, 6] com-\npute flow maps between obstructed pairs of frames using\nFlowNet [8]. One key difference in this scenario is that the\nobstruction does not follow a repetitive structure pattern, but\nis typically a large, compact area. This causes the flow maps\nto contain holes which are inpainted in a separate step.\nIn our work we drastically simplify flow estimation for\nobstructed scenes by utilizing the fence segmentation net-\nwork described in Section 3.1.\nFirst, we replace PWC-\nNet with the faster, more lightweight, SPyNet [23] architec-\nture1. Second, we modify its first convolution layer to input', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Net with the faster, more lightweight, SPyNet [23] architec-\nture1. Second, we modify its first convolution layer to input\nboth the fence segmentation masks, Si, Sj, along with their\ncorresponding input frames Ii, Ij. Our modified SPYm ar-\nchitecture then estimates the mask-conditional flow map\nf m\nij = SPYm([Ii; Si], [Ij; Sj]),\n(2)\n[·] denoting concatenation along the channel dimension. We\nuse the original pretrained weights to initialize SPYm, ex-\ncept for the modified part of the input layer, which we ini-\ntialize randomly. During training, f m\nij are computed be-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='cept for the modified part of the input layer, which we ini-\ntialize randomly. During training, f m\nij are computed be-\ntween synthetically generated frames of fence images over-\nlaid on clean background frames. Consequently, we use\nflow maps computed with the vanilla SPyNet on the clean\n1We use “SPY” in equations, for short.\nResidual\nDense Network\n(RDN)\nW\n[ ]\nW\n+\nFigure 4: Frame inpainting module. We use the predicted\nfence segmentations to mask out (⊙) the occluded areas in\nthe input frames. We then use optical flow to warp (W) the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence segmentations to mask out (⊙) the occluded areas in\nthe input frames. We then use optical flow to warp (W) the\nmasked frames and respective masks. Finally, the flows, the\nvalidity maps (see text), and the aligned frames and masks\nare concatenated ([·]) and passed as features to a CNN pre-\ndicting the keyframe residual in the occluded regions.\nbackground frames Bi, Bj as pseudo ground truth targets.\nWe use an L1 loss to finetune SPYm:\nLf =\n1\n2N\nX\nx\n|SPY(Bi, Bj)|x − f m\nij|x|,\n(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='x\n|SPY(Bi, Bj)|x − f m\nij|x|,\n(3)\nwhere N is the number of image pixels, x denotes the lo-\ncation at which we evaluate, and we average over 2N to\naccount for the u, v flow channels.\nConditioning SPyNet on segmentation predictions al-\nlows us to denote parts of the scene corresponding to ob-\nstructions and ignore them while computing background\nflows, solving a fundamental problem faced by SOLD.\nThis idea was previously explored in [4] but it involved a\ncostly optimization process. Our approach is simple but\nrobust to the presence of significant fence obstructions.\nSegmentation-aware flow estimation can be useful in a va-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='costly optimization process. Our approach is simple but\nrobust to the presence of significant fence obstructions.\nSegmentation-aware flow estimation can be useful in a va-\nriety of practical settings where one wants to ignore parts\nof the scene as distractions or sources of noise. Figure 3\ndemonstrates the effectiveness of our approach by compar-\ning outputs of the vanilla SPyNet and our segmentation-\naware SPyNetm, on the same obstructed scene.\n3.3. Flow-guided Multi-frame Fence Removal\nThe final component in our fence removal pipeline is a\nframe inpainting module, depicted in Figure 4. The frame\ninpainting module takes as input the sequence of obstructed\nframes {Ii}, forward flow maps f m\nki, computed between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='inpainting module takes as input the sequence of obstructed\nframes {Ii}, forward flow maps f m\nki, computed between\nthe keyframe Ik and each other frame in the burst using\nthe mask-conditional SPyNetm (Section 3.2), and the fence\nsegmentation masks {Si} computed using our single-frame\nsegmentation model (Section 3.1).\nWe first use {Si} to\nmask out the areas that correspond to fences in each frame,\nobtaining masked frames Im\ni\n= Ii ⊙ Si. Then flows f m\nki are\nused to warp all frames and their respective segmentations\nwith respect to the reference frame, giving rise to aligned\nmasked frames ˜Im', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='used to warp all frames and their respective segmentations\nwith respect to the reference frame, giving rise to aligned\nmasked frames ˜Im\ni\n= W(Im\ni , f m\nki) and aligned fence masks\n˜Si = W(Si, f m\nki). We also compute binary masks {Vi} that\nmark valid warped regions, and are “on” for all pixels that\nfall inside the image grid after warping.\nfin = [{˜Im\ni };\n˜\n{Si}; {Vi}] is passed as input to a Residual\nDense Network (RDN) [36] that is responsible for filling in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='{Si}; {Vi}] is passed as input to a Residual\nDense Network (RDN) [36] that is responsible for filling in\nthe missing areas in the keyframe. We also add a skip con-\nnection between the masked keyframe Im\nk and the output of\nthe RDN, so the latter only has to learn to fill in the miss-\ning areas instead of reconstructing the entire image. The\ninpainting module is trained in a supervised fashion using\nan L1 loss and the clean background as the ground truth:\nLin = 1\nN\nX\nx\n|Bk|x − (Im\nk + RDN(fin))|x|.\n(4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='x\n|Bk|x − (Im\nk + RDN(fin))|x|.\n(4)\n3.4. Implementation details\nWe implement our pipeline in Python 3 and Py-\nTorch [20].\nFor U-net, PWC-Net, SPyNet, and RDN,\nwe use their publicly available third-party implementa-\ntions [27, 18, 17, 33]. To facilitate our experiments, we have\nalso re-implemented SOLD in PyTorch (SOLDpt), follow-\ning closely the original Tensorflow implementation [14];\nwe plan to make our re-implementation publicly available\nto allow for broader use by the community and replica-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='we plan to make our re-implementation publicly available\nto allow for broader use by the community and replica-\ntion of results. Unless otherwise mentioned, we train all\nour models for 1000 epochs, using a starting learning rate\nlr = 10−4, a weight decay rate wr = 4 · 10−5, and\nthe ADAM optimizer [12] with parameters α = 10−4,\nβ1 = 0.9, β2 = 0.999, ϵ = 10−8. All three models (fence\nsegmentation, occlusion-aware flow estimation, frame in-\npainting) are trained independently.\n4. Data for Training and Evaluation\nWe use two types of data in our experiments. The first\ntype is synthetic multi-frame sequences, generated similarly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='4. Data for Training and Evaluation\nWe use two types of data in our experiments. The first\ntype is synthetic multi-frame sequences, generated similarly\nto previous works [4, 14]. These are used predominantly for\ntraining and validation experiments, but a held-out test set is\nalso used for evaluation. The second type is real bursts with\nfence obstructions, which include uncontrolled sequences,\nfor which no ground truth clean frame is available, and con-\ntrolled sequences, which come with a clean background\nscene (without the fence) as ground truth.\nSynthetic bursts are generated by overlaying obstruction\n(foreground) layers on a clean scene (background).\nWe\nsource background scenes (which are also used as ground', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(foreground) layers on a clean scene (background).\nWe\nsource background scenes (which are also used as ground\ntruth during training and evaluation) from Vimeo-90k [30],\nwhich consists of videos depicting every day activities in\nrealistic settings, often including people and other objects.\nWe specifically use the original test split of the dataset2,\n2http://data.csail.mit.edu/tofu/testset/\nwhich contains sequences of seven (7) frames. Training and\nvalidation splits are generated on the fly, but for our evalua-\ntion experiments we use a fixed test set of 100 bursts.\nThe foreground fence obstructions are sourced from the\nDe-fencing dataset [4], which contains 545 training and 100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='The foreground fence obstructions are sourced from the\nDe-fencing dataset [4], which contains 545 training and 100\ntest images with fences, along with corresponding binary\nmasks as ground truth for the fence segmentation. The se-\nquences in this dataset have been collected in various out-\ndoor conditions and have a variable frame count per scene.\nSince we have the ground truth fence masks, we can use\nthem to mask out the fence from any given frame and over-\nlay it on a clean background from Vimeo. To obtain a fence\nimage burst of size K, we mask out the fence from a sin-\ngle frame and apply K random perspective distortions to it,\nto simulate the changes caused by slightly different view-\npoints and motion. To increase the variability of fences and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='to simulate the changes caused by slightly different view-\npoints and motion. To increase the variability of fences and\nbackground scenes, we apply various forms of data aug-\nmentations before fusing them into a single frame; these are\nlisted in detail in the supplemental material.\nReal bursts. Because we want to develop a practical algo-\nrithm for fence removal, good performance under realistic\nmotion, lighting, and obstruction patterns is of paramount\nimportance. In previous works, performance on real se-\nquences is –for the most part– evaluated qualitatively, since\nobtaining the ground truth background is far from trivial.\nLiu et al. [14] include only two sequences with fence-like\nobstructions, collected in a controlled environment, which\nis too small a dataset for a proper quantiative evaluation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='obstructions, collected in a controlled environment, which\nis too small a dataset for a proper quantiative evaluation.\nIn this paper we construct a wider set of controlled se-\nquences, specifically for quantitative evaluation.\nRather\nthan collecting toy scenes as in Liu et al. [14], we capture\nreal world hand-held sequences with a fence and a corre-\nsponding background ground truth image without a fence.\nAs we cannot physically remove a fence, we instead bring\nour camera to the fence and center it in one of the fence cells\nsuch that only the background is visible. To maintain a sim-\nilar level of brightness and sharpness of the background in\nthe input and ground truth images, we fix the exposure and\nfocus of the camera on the background during capture. Due', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the input and ground truth images, we fix the exposure and\nfocus of the camera on the background during capture. Due\nto camera motion and possible changes in illumination, the\ninput keyframe and its respective ground truth may be mis-\naligned or have color discrepancies. We align crops of the\nscene using standard feature-based RANSAC fitting of ho-\nmographies, similar to [3] and correct color discrepancies\nusing color histogram matching. We then filter out any mis-\naligned crops using SSIM, PSNR, and human visual check,\navoiding mostly homogeneous regions, to promote diver-\nsity in our dataset. Our final real burst dataset consists of\n185 320 × 192 input bursts and corresponding ground truth\nkeyframes. More details on dataset generation and image', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='185 320 × 192 input bursts and corresponding ground truth\nkeyframes. More details on dataset generation and image\nsampling are provided in supplemental material.\nvimeo_test_clean.zip\n5. Experiments\nWe compare our method and other works on synthetic\nand real bursts.\nFor quantitative evaluations we use the\ntest set of our synthetically generated fence-obstructed se-\nquences, and our real bursts described in Section 4. For all\nbaselines, we use the officially released model weights, with\nthe exception of our SOLD reimplementation. We also pur-\nposedly omit the sequence-specific online optimization step\nof SOLD in our comparisons. Although online optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='posedly omit the sequence-specific online optimization step\nof SOLD in our comparisons. Although online optimization\nimproves performance, its runtime is quite slow (∼ 3 min-\nutes per burst), pushing it outside the scope of our work,\nwhich is centered around efficiency and practicality. For\nqualitative evaluations and visual comparison, we use real\nsequences from previous work and the data we collected.\n5.1. Baselines\nSingle-frame baseline.\nWe pass either ground truth or\nour (thresholded) U-net fence mask predictions as input\nto LaMa [26], a state-of-the-art CNN-based inpainting\nmethod, to create a single-frame de-fencing baseline. LaMa\ntakes as input a (possibly obstructed) image and a binary', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='method, to create a single-frame de-fencing baseline. LaMa\ntakes as input a (possibly obstructed) image and a binary\nmask and inpaints the area marked by the mask.\nSOLD [14] primarily targets reflection removal, but it can\nbe adapted to deal with opaque obstructions such as fences\nor raindrops on glass. We evaluate both the original Tensor-\nflow model (SOLDtf) and our PyTorch reimplementation\n(SOLDpt), with the latter trained on our synthetic data.\nFlow-guided video completion operates in a setting that is\ndifferent than ours in a few ways. First, the mask denoting\nthe occluded area is known, and its shape is either rectan-\ngular or in the shape of an object in the video. Second,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the occluded area is known, and its shape is either rectan-\ngular or in the shape of an object in the video. Second,\nthe number of frames in a typical input video sequence is\nK ≫ 5. Lastly, the output is the entire inpainted video.\nNevertheless, we can apply these methods for de-fencing\nin a relatively straightforward fashion by passing the fence\nsegmentation as the occlusion mask, treating the burst as a\n(short) video sequence, and keeping the inpainted result for\nthe reference frame only. In our experiments we compare\nagainst two recent flow-guided approaches, FGVC [6] and\nE2FGVI [13], using their publicly provided code.\n5.2. Fence Removal on Synthetic and Real Data\nQuantitative comparisons on synthetic and real bursts', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='5.2. Fence Removal on Synthetic and Real Data\nQuantitative comparisons on synthetic and real bursts\nare shown in Table 2. We report performance in terms of\nthe commonly used SSIM, PSNR, and LPIPS metrics. For\nLPIPS we use a VGG-16 backbone as the feature extrac-\ntor. PSNR and SSIM can be computed as an aggregation of\npixel-wise scores, so we use the fence masks (ground truth\nin the case of synthetic data, thresholded and binarized U-\nnet predictions in the case of real data3) to dissect perfor-\n3To get better pseudo ground truth fence masks, we run U-net at multi-\nple scales and compute the pixel-wise maximum across scales.\n(a) Keyframe\n(b) LaMa [26]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ple scales and compute the pixel-wise maximum across scales.\n(a) Keyframe\n(b) LaMa [26]\n(c) SOLD [14]\n(d) FGVC [6]\n(e) Ours\n(f) Ground truth\nFigure 5: Qualitative de-fencing results on real sequences. We highlight areas of interest in red (shown as zoomed insets)\nand report PSNR inside the fence mask for all methods. Last example is from Xue et al. [31] and has no ground truth.\nSynthetic data (Methods in blue use GT fence masks)\nMethod\nSSIM ↑\nPSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='PSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal\nin\nout\ntotal\n(VGG)\nSOLDtf[14]\n.783\n.970\n.941\n23.36\n37.82\n30.34\n.111\nSOLDpt [14]\n.893\n.993\n.977\n28.14\n45.70\n35.82', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.977\n28.14\n45.70\n35.82\n.040\nLaMa [26]\n.788\n.995\n.964\n24.97\n51.38\n33.10\n.039\nLaMa [26]∗\n.655\n.955\n.910\n20.96\n31.74\n27.38\n.089\nFGVC [6]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='31.74\n27.38\n.089\nFGVC [6]\n.846\n.943\n.928\n25.56\n33.85\n30.36\n.068\nFGVC [6]∗\n.784\n.896\n.879\n22.73\n27.80\n26.05\n.113\nE2FGVI [13]\n.918\n.997', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.113\nE2FGVI [13]\n.918\n.997\n.985\n30.79\n55.87\n38.89\n.030\nE2FGVI [13]∗\n.890\n.984\n.969\n29.34\n38.64\n35.25\n.044\nOurs\n.954\n.999\n.992\n33.76', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Ours\n.954\n.999\n.992\n33.76\n56.55\n41.78\n.015\nOurs-fencegt\n.957\n.999\n.993\n34.33\n58.58\n42.42\n.012\nReal bursts (Methods in blue use pseudo-GT fence masks)\nSSIM ↑\nPSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='PSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal\nin\nout\ntotal\n(VGG)\n.728\n.911\n.885\n23.50\n30.27\n27.71\n.132\n.813\n.916\n.902\n26.41\n30.90\n29.71\n.094\n.480\n.902', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='29.71\n.094\n.480\n.902\n.845\n19.95\n29.96\n26.25\n.133\n.477\n.867\n.816\n20.85\n28.02\n25.98\n.132\n.848\n.910\n.901\n27.56\n30.48\n29.73\n.095', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='27.56\n30.48\n29.73\n.095\n.856\n.907\n.900\n27.49\n29.90\n29.36\n.090\n.571\n.902\n.856\n19.69\n29.88\n25.87\n.167\n.709\n.901\n.875\n25.58\n30.25', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.901\n.875\n25.58\n30.25\n29.14\n.117\n.869\n.917\n.909\n28.60\n31.14\n30.46\n.080\n.872\n.918\n.910\n28.77\n31.15\n30.53\n.078', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='28.77\n31.15\n30.53\n.078\nTable 2: Results on synthetic test data (left) and our collected real bursts (right). Results in rows denoted with “*” are\ncomputed after thresholding the fence segmentations at t = 0.1, and dilating the binary mask 4 times with a 3 × 3 square.\nmance in three different regions: a) inside the mask (in); b)\noutside the mask (out); and c) in the entire image (total).\nPerformance outside the mask is high for all methods, since\nthis part of the image is not occluded. Performance inside\nthe mask is the most important criterion, since it quanti-\nfies the quality of reconstruction only in the occluded area.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the mask is the most important criterion, since it quanti-\nfies the quality of reconstruction only in the occluded area.\nWe outperform all single- and multi-frame baselines, based\non all metrics, with FGVC [6] performing second best at\nthe cost of much higher runtime (see Section 5.3).\nWe\nwould like to draw the reader’s attention to the results of\nLaMa [26], in particular. LaMa is a state-of-the-art inpaint-\ning method, yet it achieves surprisingly low PSNR-in and\nSSIM-in scores. The reason becomes clear if one looks at\nFigure 5 (e.g., antennae structure in the first example): even\nthough LaMa produces perfectly plausible results under the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Figure 5 (e.g., antennae structure in the first example): even\nthough LaMa produces perfectly plausible results under the\noccluded area, these are often very different than the actual\nbackground scene. These results tellingly demonstrate the\nadvantage of using multiple frames for de-fencing. Figure 5\nalso illustrates that alternatives like SOLD and FGVC can\nyield blurry or completely scrambled results, likely due to\nissues with frame alignment. For more qualitative results\nsee our supplemental material.\n5.3. Runtime Analysis\nTable 3 compares the total runtime of our pipeline with\nother approaches. In our case, timing includes all necessary\nprocessing steps: fence segmentation, optical flow compu-\ntation, alignment, and frame inpainting. LaMa, FGVC, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='processing steps: fence segmentation, optical flow compu-\ntation, alignment, and frame inpainting. LaMa, FGVC, and\nMethod\nLaMa\nSOLDpt\nFGVC\nE2FGVI\nOurs\nRuntime (s)\n0.2\n0.8\n0.7\n0.16\n0.14 (0.08)\nTable 3: Runtime comparison for a 5-frame burst (LaMa\nprocesses a single frame). Times for other methods do not\ninclude segmentation (fence segmentations are part of the\nSOLD output). We provide the runtime of our method with-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='include segmentation (fence segmentations are part of the\nSOLD output). We provide the runtime of our method with-\nout the segmentation step in parentheses, for comparison.\nE2FGVI times do not include the time spent on fence seg-\nmentation, since these methods assume that the occlusion\nmasks are precomputed. Our method is clocked at ∼ 7 fps,\nfor 320 × 192 inputs, which is ∼ 5× faster than the next\nbest performing methods, SOLD and FGVC, and compara-\nble to E2FGVI and LaMa. The runtime difference with the\nlast two may not seem large in absolute terms, but it still\namounts to a 12.5% and 30% lower runtime respectively,\nwhile our method significantly outperforms them in terms', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='amounts to a 12.5% and 30% lower runtime respectively,\nwhile our method significantly outperforms them in terms\nof reconstruction quality on real data. If we exclude the\ntime spent on segmentation from our pipeline, the speedup\nbecomes even more noticeable (50% and 60% respectively).\nAll timings are performed on a workstation equipped with\na Nvidia GTX 1080 Ti, with 12GB of GPU RAM. The de-\ntailed breakdown of timings for our pipeline is: i) segmen-\ntation (for a 5-frame burst): 0.06s; ii) flow estimation and\nalignment: 0.04s; iii) frame inpainting (RDN): 0.04s.\n5.4. Ablation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='alignment: 0.04s; iii) frame inpainting (RDN): 0.04s.\n5.4. Ablation\nChanging the frame inpainting module architecture al-\nlows us to trade-off reconstruction performance for effi-\nciency. Replacing the RDN with a simple CNN consist-\ning of 8 convolution + LeakyReLU layers decreases per-\nformance by 3.5 dB on synthetic test data but also reduces\nruntime by ∼ 30%, from 0.14s to 0.1s.\nDoes frame alignment matter? Inaccurate computation\nof the motion corresponding to the (occluded) background\ncan lead to errors in frame alignment, impacting the quality\nof frame reconstruction. We experiment with the following', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='of the motion corresponding to the (occluded) background\ncan lead to errors in frame alignment, impacting the quality\nof frame reconstruction. We experiment with the following\ntwo alternatives for computing flows: i) original SPyNet on\nobstructed frames; ii) SPyNet followed by masking the oc-\ncluded areas and using Laplacian inpainting [29] to com-\nplete the missing flows (SPyNetinp). We also consider the\noption of not aligning frames at all, and leaving our RDN\ninpainting network to learn how to complete the occluded\nareas in the reconstructed keyframe.\nThe importance of alignment and the quality of flows\nused become becomes clear when looking at the results\nin Table 4. Not aligning the input frames at all leads to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='The importance of alignment and the quality of flows\nused become becomes clear when looking at the results\nin Table 4. Not aligning the input frames at all leads to\na noticeable drop in keyframe reconstruction performance.\nStandard flow networks cannot handle the repeated fence\nobstruction patterns, and explicitly inpainting the flows un-\nder the occluded area does not help either since flow arti-\nfacts extend to non-fence areas as well, as shown in Fig-\nAlignment\nSSIM ↑\nPSNR (dB) ↑\nLPIPS ↓\nin\nout\ntotal\nin\nout\ntotal\n(VGG)\nNone\n.792', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='in\nout\ntotal\n(VGG)\nNone\n.792\n.996\n.965\n24.82\n53.64\n32.84\n.028\nSPyNet\n.841\n.997\n.973\n26.84\n54.32\n34.93\n.047\nSPyNetinp\n.841\n.997\n.973\n26.81', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='.841\n.997\n.973\n26.81\n54.13\n34.89\n.048\nSPyNetm\n.954\n.999\n.992\n33.76\n56.55\n41.78\n.015\nTable 4: Effect of frame alignment on keyframe reconstruc-\ntion quality (results on our synthetic test data).\nFigure 6: Failure example. Imperfect fence predictions\n(middle) compromise the quality of the inpainting (right).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Figure 6: Failure example. Imperfect fence predictions\n(middle) compromise the quality of the inpainting (right).\nure 3. Our occlusion-aware SPyNetm, on the other hand,\ncan accurately estimate background flows, resulting in su-\nperior frame alignment and reconstruction quality.\n5.5. Limitations and Failure Cases.\nThe main limitation of our approach is that the quality of\nthe final reconstruction depends on the outputs of U-net and\nSPyNetm in the two previous stages. Errors in fence seg-\nmentation affect segmentation-aware optical flow computa-\ntion, potentially compromising frame alignment, which is\ncrucial for good inpainting (see ablation in Section 5.4). In\naddition, fence segmentations are also used in masking out', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='crucial for good inpainting (see ablation in Section 5.4). In\naddition, fence segmentations are also used in masking out\nthe occluded areas in {Ii}, The most deleterious mistakes\noccur when the fence occlusion is out of our training distri-\nbution, e.g., when the fence has an unusual shape/pattern,\nor when the contrast with the background is low. This is\nan issue that can be handled to a certain extent through bet-\nter data augmentation during training or by having access\nto richer datasets with varied types of fences, as we show in\nthe supplemental material. A failure example and its effect\non frame inpainting is shown in Figure 6.\n6. Conclusions\nWe have developed a simple, modular, and efficient', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='on frame inpainting is shown in Figure 6.\n6. Conclusions\nWe have developed a simple, modular, and efficient\npipeline for removing fence obstructions from a singe frame\nin a photo burst. Our algorithm enjoys the realism of flow-\nguided video completion methods, while addressing some\nof their practical limitations, such as complicated training\nand long runtimes. Our method runs at 7 fps for 5-frame\n320 × 192 bursts, on a Nvidia GTX 1080 Ti, and is particu-\nlarly effective on real data, outperforming other single- and\nmulti-frame de-fencing baselines on a dataset of obstructed\nbursts we collected specifically for this problem.\nReferences', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='multi-frame de-fencing baselines on a dataset of obstructed\nbursts we collected specifically for this problem.\nReferences\n[1] Jean-Baptiste Alayrac, Joao Carreira, and Andrew Zisser-\nman. The visual centrifuge: Model-free layered video rep-\nresentations. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2457–\n2466, 2019. 1, 3\n[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-\nthresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183–202, 2009. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='thresholding algorithm for linear inverse problems. SIAM\njournal on imaging sciences, 2(1):183–202, 2009. 2\n[3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu\nTimofte.\nDeep burst super-resolution.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9209–9218, 2021. 2, 6, 11\n[4] Chen Du, Byeongkeun Kang, Zheng Xu, Ji Dai, and Truong\nNguyen. Accurate and efficient video de-fencing using con-\nvolutional neural networks and temporal information.\nIn', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Nguyen. Accurate and efficient video de-fencing using con-\nvolutional neural networks and temporal information.\nIn\n2018 IEEE International Conference on Multimedia and\nExpo (ICME), pages 1–6. IEEE, 2018. 2, 3, 4, 5, 6, 10, 11\n[5] Yosef Gandelsman, Assaf Shocher, and Michal Irani.\n”\ndouble-dip”: Unsupervised image decomposition via cou-\npled deep-image-priors. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 11026–11035, 2019. 1, 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Conference on Computer Vision and Pattern Recognition,\npages 11026–11035, 2019. 1, 3\n[6] Chen Gao, Ayush Saraf, Jia-Bin Huang, and Johannes Kopf.\nFlow-edge guided video completion. In European Confer-\nence on Computer Vision, pages 713–729. Springer, 2020. 3,\n4, 6, 7, 14, 16\n[7] Jia-Bin Huang, Sing Bing Kang, Narendra Ahuja, and Jo-\nhannes Kopf. Temporally coherent completion of dynamic\nvideo. ACM Transactions on Graphics (TOG), 35(6):1–11,\n2016. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='video. ACM Transactions on Graphics (TOG), 35(6):1–11,\n2016. 2\n[8] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,\nAlexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-\ntion of optical flow estimation with deep networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 2462–2470, 2017. 4\n[9] Sankaraganesh Jonna, Krishna K Nakka, and Rajiv R Sahay.\nDeep learning based fence segmentation and removal from\nan image using a video sequence. In European Conference', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Deep learning based fence segmentation and removal from\nan image using a video sequence. In European Conference\non Computer Vision, pages 836–851. Springer, 2016. 2\n[10] Sankaraganesh Jonna, Sukla Satapathy, and Rajiv R Sahay.\nStereo image de-fencing using smartphones. In 2017 IEEE\ninternational conference on acoustics, speech and signal\nprocessing (ICASSP), pages 1792–1796. IEEE, 2017. 2\n[11] Sankaraganesh Jonna, Vikram S Voleti, Rajiv R Sahay, and\nMohan S Kankanhalli. A multimodal approach for image de-\nfencing and depth inpainting. In 2015 Eighth International', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Mohan S Kankanhalli. A multimodal approach for image de-\nfencing and depth inpainting. In 2015 Eighth International\nConference on Advances in Pattern Recognition (ICAPR),\npages 1–6. IEEE, 2015. 2\n[12] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 3, 5\n[13] Zhen Li, Cheng-Ze Lu, Jianhua Qin, Chun-Le Guo, and\nMing-Ming Cheng.\nTowards an end-to-end framework\nfor flow-guided video inpainting.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Ming-Ming Cheng.\nTowards an end-to-end framework\nfor flow-guided video inpainting.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17562–17571, 2022. 2, 3, 6, 7\n[14] Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu\nChuang, and Jia-Bin Huang. Learning to see through ob-\nstructions with layered decomposition. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2021. 1, 2, 3,\n4, 5, 6, 7, 11, 12, 14, 15, 16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='4, 5, 6, 7, 11, 12, 14, 15, 16\n[15] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 3431–3440, 2015. 2\n[16] Yadong Mu, Wei Liu, and Shuicheng Yan. Video de-fencing.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 24(7):1111–1121, 2013. 2\n[17] Simon Niklaus.\nA reimplementation of PWC-Net us-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='[17] Simon Niklaus.\nA reimplementation of PWC-Net us-\ning PyTorch.\nhttps://github.com/sniklaus/\npytorch-pwc, 2018. 4, 5\n[18] Simon Niklaus.\nA reimplementation of SPyNet us-\ning PyTorch.\nhttps://github.com/sniklaus/\npytorch-spynet, 2018. 5\n[19] Minwoo Park, Kyle Brocklehurst, Robert T Collins, and\nYanxi Liu. Image de-fencing revisited. In Asian Confer-\nence on Computer Vision, pages 422–434. Springer, 2010. 1,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ence on Computer Vision, pages 422–434. Springer, 2010. 1,\n2\n[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\nperative style, high-performance deep learning library. In H.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content="Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-\nperative style, high-performance deep learning library. In H.\nWallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E.\nFox, and R. Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc., 2019. 5\n[21] M Pawan Kumar, Philip HS Torr, and Andrew Zisserman.\nLearning layered motion segmentations of video. Interna-\ntional Journal of Computer Vision, 76(3):301–319, 2008. 1", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='tional Journal of Computer Vision, 76(3):301–319, 2008. 1\n[22] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M.\nGross, and A. Sorkine-Hornung. A benchmark dataset and\nevaluation methodology for video object segmentation. In\nComputer Vision and Pattern Recognition, 2016. 3\n[23] Anurag Ranjan and Michael J Black. Optical flow estima-\ntion using a spatial pyramid network. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4161–4170, 2017. 2, 4\n[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, pages 234–241.\nSpringer, 2015. 3\n[25] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPwc-net: Cnns for optical flow using pyramid, warping, and\ncost volume.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8934–8943,\n2018. 3, 4\n[26] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='2018. 3, 4\n[26] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky.\nResolution-robust large mask inpainting with\nfourier convolutions. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision, pages\n2149–2159, 2022. 6, 7, 14, 16\n[27] Nikhil\nTomar.\nSemantic-segmentation-architecture.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='[27] Nikhil\nTomar.\nSemantic-segmentation-architecture.\nhttps://github.com/nikhilroxtomar/\nSemantic-Segmentation-Architecture, 2020. 5\n[28] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.\nDeep image prior. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 9446–9454,\n2018. 3\n[29] Rui Xu, Xiaoxiao Li, Bolei Zhou, and Chen Change Loy.\nDeep flow-guided video inpainting. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Deep flow-guided video inpainting. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3723–3732, 2019. 2, 3, 4, 8\n[30] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and\nWilliam T Freeman. Video enhancement with task-oriented\nflow.\nInternational Journal of Computer Vision (IJCV),\n127(8):1106–1125, 2019. 5, 10, 11\n[31] Tianfan Xue, Michael Rubinstein, Ce Liu, and William T\nFreeman.\nA computational approach for obstruction-free\nphotography.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Freeman.\nA computational approach for obstruction-free\nphotography.\nACM Transactions on Graphics (TOG),\n34(4):1–11, 2015. 2, 3, 7, 12, 15\n[32] Liu Yanxi, Belkina Tamara, H Hays James, and Roberto\nLublinerman. Image de-fencing. In Computer Vision and\nPattern Recognition, 2008. CVPR 2008. IEEE Conference\non, pages 1–8, 2008. 1, 2\n[33] Jeffrey Yeo.\nRdn.\nhttps://github.com/yjn870/\nRDN-pytorch, 2019. 5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Rdn.\nhttps://github.com/yjn870/\nRDN-pytorch, 2019. 5\n[34] Renjiao Yi, Jue Wang, and Ping Tan. Automatic fence seg-\nmentation in videos of dynamic scenes. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 705–713, 2016. 2\n[35] Kaidong Zhang, Jingjing Fu, and Dong Liu. Inertia-guided\nflow completion and style fusion for video inpainting.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5982–5991, 2022. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Proceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5982–5991, 2022. 2\n[36] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image super-resolution.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2472–2481, 2018. 5\nA. Data\nA.1. Synthetic Data Augmentation\nBackground\naugmentation.\nWe\nsource\nbackground\nscenes (which are also used as ground truth during train-\ning and evaluation) from Vimeo-90k [30], which consists', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='background\nscenes (which are also used as ground truth during train-\ning and evaluation) from Vimeo-90k [30], which consists\nof videos depicting every day activities in realistic settings,\noften including people and other objects. We specifically\nuse the original test split of the dataset4, which contains se-\nquences of seven (7) frames. The original clean frames are\nused as ground truth for training and evaluation. To increase\nvariability of our synthetically generated data, we apply the\nfollowing data augmentation steps:\n1. random homography transformation\n2. center cropping to avoid any black borders caused by\n(1).\n3. random cropping of a 320 × 192 window, which are\nthe frame dimensions used during training.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(1).\n3. random cropping of a 320 × 192 window, which are\nthe frame dimensions used during training.\n4. random horizontal flip.\nForeground augmentation.\nThe foreground fence ob-\nstructions are sourced from the De-fencing dataset [4],\nwhich contains 545 training and 100 test images with\nfences, along with corresponding binary masks as ground\ntruth for the fence segmentation. The variability of fences\nin that dataset is limited, so we also apply various forms of\ndata augmentation on the fence image before fusing it with\nthe background. The types of foreground augmentation we\nconsider are:\n1. random downsample of the fence image and segmen-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='the background. The types of foreground augmentation we\nconsider are:\n1. random downsample of the fence image and segmen-\ntation to create fences of different sizes and thickness.\n2. random “outer” window crop to focus on a specific\nsubregion of the fence.\n3. color jitter to make the network more robust to differ-\nent fence appearances and lighting conditions.\n4. random perspective distortion to obtain a fence se-\nquence of length K.\n5. center cropping to avoid black border effects from the\nhomographic distortion.\n6. random blur with a gaussian kernel, to simulate defo-\ncus aberrations.\nSamples from our synthetic burst dataset are shown in Fig-\nure 7.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='cus aberrations.\nSamples from our synthetic burst dataset are shown in Fig-\nure 7.\n4http://data.csail.mit.edu/tofu/testset/vimeo_\ntest_clean.zip\nFigure 7: Examples of our synthetically generated data. The leftmost column shows the clean background frame and the next\n5 columns show the background burst from Vimeo-90k [30], with overlaid fences from the De-fencing dataset [4].\nA.2. Real Burst Collection\nAlthough our synthetic data are carefully generated and\nexhibit considerable realism and diversity, they still cannot\nfully capture the variability of motion, lighting, and obstruc-\ntion patterns in scenes captured under realistic conditions,\nso we collect set of controlled sequences, specifically for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fully capture the variability of motion, lighting, and obstruc-\ntion patterns in scenes captured under realistic conditions,\nso we collect set of controlled sequences, specifically for\nquantitative evaluation. As mentioned in the main paper,\nrather than collecting toy scenes as in Liu et al. [14], we cap-\nture outdoors real world hand-held sequences with a fence\nand a corresponding background ground truth image with-\nout a fence.\nData capture.\nWe first capture one image without the\nfence as the ground-truth frame, by bringing our camera to\nthe centre of a fence cell. We then fix the focus and expo-\nsure on the background and move backwards from the fence\nto capture 5 frames with fences. To minimize misalignment\ncaused by a change in perspective, we capture the first frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='to capture 5 frames with fences. To minimize misalignment\ncaused by a change in perspective, we capture the first frame\nas the key-frame, moving backwards along the capturing\ndirection. Then, we capture the remaining four frames by\nintentionally jittering the camera around.\nKeyframe - groundtruth alignment.\nAfter capturing the\nreal bursts, we need to align the ground-truth frame to the\nobstruced key-frame. We do this following an approach\ncombining SIFT feature extraction and RANSAC homog-\nraphy estimation, similar to [3].\nWe start by computing\nand matching SIFT features in the keyframe and respective\nclean groundtruth shot. Since the resolution of the original\nimages is high, we extract 320 × 192 regions in a sliding', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='clean groundtruth shot. Since the resolution of the original\nimages is high, we extract 320 × 192 regions in a sliding\nwindow fashion, and within such window, P, we compute\nhomography parameters using matched SIFT features in\ncrops of varying sizes: 1282, 2562, 5122, and 10242 (larger\ncrop sizes extend beyond the area of the original window).\nThe computed homography parameters are used for global\nalignment of the keyframe and groundtruth frames, so we\nhave multiple homography “candidates” corresponding to\nP. The motivation behind computing homographies at dif-\nferent scales is that different parts of a given window P\nmay require different homographies to be aligned more ac-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='P. The motivation behind computing homographies at dif-\nferent scales is that different parts of a given window P\nmay require different homographies to be aligned more ac-\ncurately. We assign the best homography to each 128 × 128\ncrop C inside P, by computing its respective SSIM score\nwith respect to its warped counterpart in the groundtruth\n(we use the estimated fence masks to only include non-\nobstructed areas in the SSIM computation). To ensure a\nminimum level of quality, if there is at least one C inside P\nwith average SSIM ≤ 0.2 or PSNR ≤ 20, we discard P and\nmove to the next sliding window with stride 128. If there\nare no “failed” crops, P slides to the next non-overlapping', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='move to the next sliding window with stride 128. If there\nare no “failed” crops, P slides to the next non-overlapping\nposition. In the end, we also manually filter out the crops\nthat are misaligned on and near the fences by visual com-\nparison between input and aligned ground-truth. We also\nmanually filter out crops consisting of mostly homogeneous\nregions (sky, land, sand), to promote diversity in our dataset.\nOur final real burst dataset consists of 185 320 × 192 input\nbursts with corresponding ground truth key-frames from 29\nscenes. Samples from our real burst dataset are shown in\nFigure 9.\nB. Task Specificity and Comparison with\nSOLD [14]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='Figure 9.\nB. Task Specificity and Comparison with\nSOLD [14]\nOne potential criticism towards our approach is our focus\non a specific type of obstruction (fences), and the fact that\nwe heavily rely on a specific prior (pre-trained fence seg-\nmentation model), which can harm generalization to new\ninputs, not commonsly seen in the training data. In com-\nparison, SOLD [14] is a multi-frame approach can handle\nvarious types of obstructions. However, SOLD is also lim-\nited when faced with atypical obstructions (e.g., fences), re-\nquiring scene-specific, costly online optimization that takes\n∼ 3 minutes, to achieve good results, making it impracti-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='quiring scene-specific, costly online optimization that takes\n∼ 3 minutes, to achieve good results, making it impracti-\ncal for real-world application. Our method trades-off gen-\nerality for reconstruction and runtime performance (the lat-\nter is a feature missing from previous de-fencing works),\nproducing better de-fencing results than SOLD, at a frac-\ntion of its runtime, without requiring scene-specific opti-\nmization. Besides, de-fencing is an important problem in\nits own right, with an extensive literature in computer vi-\nsion (see Section 2.1 in the main paper). Finally, we can\nmake our method more robust to a broader variety of fences\n(e.g., rhombic rotated fences, etc.) by improving our data', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='make our method more robust to a broader variety of fences\n(e.g., rhombic rotated fences, etc.) by improving our data\naugmentation protocol. To showcase this, we have added\nmore scale, rotation, shape and color variation during the\n(a) Keyframe\n(b) Output (original) (c) Improved output\nFigure 8: Better data augmentation can make the fence seg-\nmentation network more robust to varied types of fences,\nthus improving the quality of frame inpainting on real se-\nquences without the need for online finetuning.\ntraining of the fence segmentation model. As shown in Fig-\nure 8, after adding these additional data augmentations, the\nfence segmentation model can accurately segment fences', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='ure 8, after adding these additional data augmentations, the\nfence segmentation model can accurately segment fences\nthat are rotated, very thin fences, or have low contrast\nwith respect to the background, subsequently improving de-\nfencing quality. Extending our method to handle other types\nof obstructions (e.g., reflections), is also a direction we are\ncurrently exploring.\nC. Qualitative Results\nFigure 10 shows additional qualitative results on se-\nquences from our synthetically generated test set.\nFig-\nure 11 shows results on real sequences, taken from previous\nworks [31, 14]. We are also including some failure cases,\nwhere the fence segmentation model encounters fences at\nscales or shapes that are out of its training distribution, re-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='where the fence segmentation model encounters fences at\nscales or shapes that are out of its training distribution, re-\nsulting in low de-fencing quality. Finally, in Figure 12 we\ncompare results from our method and other baselines on ex-\namples from the real burst dataset we collected.\nFigure 9: Examples of real bursts we have collected. These are 320 × 192 crops from the original, high resolution images,\nafter alignment.\n(a) Keyframe\n(b) LaMa [26]\n(c) SOLD [14]\n(d) FGVC [6]\n(e) Ours\n(f) Ground truth\nFigure 10: De-fencing results on sequences from our synthetic data, and respective PSNR scores inside the fence mask', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='(f) Ground truth\nFigure 10: De-fencing results on sequences from our synthetic data, and respective PSNR scores inside the fence mask\narea. The leftmost column shows the obstructed keyframe, and the next 5 rows show its reconstructed version using various\nbaselines and our approach. Zoom in to notice differences in reconstructed frames.\nFigure 11: De-fencing results on sequences from [31, 14]. From top to bottom: obstructed keyframe, reconstructed keyframe\nusing our approach, estimated fence segmentation using our U-net fence segmentation model. The second group of results\nshows failure cases: when the fence obstruction is outside our training distribution (e.g., scale - very thin fences, irregular\nfence pattern, such as vertical bars, extreme blur etc.) the fence segmentation estimation fails, affecting reconstruction quality.\nAddressing unusual fence obstructions like these is our main focus for future work.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'}), Document(page_content='fence pattern, such as vertical bars, extreme blur etc.) the fence segmentation estimation fails, affecting reconstruction quality.\nAddressing unusual fence obstructions like these is our main focus for future work.\n(a) Keyframe\n(b) LaMa [26]\n(c) SOLD [14]\n(d) FGVC [6]\n(e) Ours\n(f) Ground truth\nFigure 12: Qualitative de-fencing results on real sequences, and respective PSNR scores inside the fence mask area. The\nleftmost column shows the obstructed keyframe, and the next 5 rows show its reconstructed version using various baselines\nand our approach. Zoom in to notice differences in reconstructed frames.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwqlm1x6o/Efficient Flow-Guided Multi-frame De-fencing.pdf'})]
cuda:2
[UploadFile(filename='高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf', size=1920751, headers=Headers({'content-disposition': 'form-data; name="files"; filename="é«\x98æ\x80§è\x83½è®¡ç®\x97 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmplwmbp1ut, tmplwmbp1ut
File: 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, msg: 成功上传文件 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, docs: [Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc704250> 111
cuda:2
[Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwmbp1ut/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
cuda:2
[UploadFile(filename='高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf', size=1920751, headers=Headers({'content-disposition': 'form-data; name="files"; filename="é«\x98æ\x80§è\x83½è®¡ç®\x97 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpsjjomlno, tmpsjjomlno
File: 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, msg: 成功上传文件 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, docs: [Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90d34c92d0> 111
cuda:2
[Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsjjomlno/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
cuda:2
[UploadFile(filename='高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf', size=1920751, headers=Headers({'content-disposition': 'form-data; name="files"; filename="é«\x98æ\x80§è\x83½è®¡ç®\x97 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp_r162505, tmp_r162505
File: 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, msg: 成功上传文件 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, docs: [Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8ff44e3150> 111
cuda:2
[Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_r162505/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
cuda:2
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp1l3mns61, tmp1l3mns61
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd58e4d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1l3mns61/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpjuir6u30, tmpjuir6u30
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5eacd0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpjuir6u30/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
[UploadFile(filename='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf', size=11976858, headers=Headers({'content-disposition': 'form-data; name="files"; filename="Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpyccd_2uw, tmpyccd_2uw
File: Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf, msg: 成功上传文件 Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf, docs: [Document(page_content='Window Detection In Facade Imagery:\nA Deep Learning Approach Using Mask R-CNN\nNils Nordmark, University of Gothenburg\nMola Ayenew, University of Gothenburg\nAbstract\nThe parsing of windows in building facades is a long-\ndesired but challenging task in computer vision. It is\ncrucial to urban analysis, semantic reconstruction, life\ncycle analysis, digital twins and scene parsing amongst\nother building-related tasks that require high-quality se-\nmantic data. In this article, we investigate the usage of the\nMask R-CNN framework to be used for window detection\nof facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='of facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to\nproduce instance segmentations of our new window class.\nExperimental results show that our suggested approach can\nwith a relatively small dataset train the network only with\ntransfer learning and augmentation achieve results on par\nwith prior state-of-the-art window detection approaches,\neven without post-optimization techniques.\n1. INTRODUCTION\nGathering semantic data regarding building fea-\ntures from images is desired as manual gathering\ncan often be a challenging, time consuming and\nresource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and\nbalconies are often not available, can change over\ntime and the data gathering is usually a manual\nprocess.\nMany different approaches have been applied to\nsolve the gathering of semantic data within many\ndifferent domains. Within building facade parsing\nthis task has mostly been treated as a process of\nsemantic segmentation of facade imagery into archi-\ntectural structural classes such as windows, doors,\nbalconies and so on. This semantic segmentation\napproach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='approach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the\nsame object as each pixel is classiﬁed to belong\nto a particular class. Speciﬁcally for parsing of\nwindows instances in building facades, we suggest\nan approach that can decouple segmentation and\nclassiﬁcation to detect windows in an image with\na bounding box and segmentation mask for each\nwindow instance.\nWith instance segmentation, we can analyze the\nwindows independently with detailed and compre-\nhensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='hensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,\nwe propose an approach based on the Mask R-CNN\nframework.\n2. RELATED WORK\nFacade parsing has been studied for a long time\nand there exists a large body of work on how to\ndeal with such a problem.\nEarlier approaches\nZhao et al. (2010) proposed parsing ground-\nlevel images into architectural units which could\nbe used for city modelling. Wendel, Donoser, and\nBischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-\nmeans clustering to tackle the same problem.\nMany others proposed a procedural shape gram-\nmar approach. This approach could at a high-level\nbe explained as a hand-crafted set of rules of basic\nshapes which represents structured geometries, i.e.\nthe structure of the object is encoded as a set of\nparametric rules. Models could then be generated\nby iteratively applying the rules on a starting shape.\nMüller et al. (2007) used a procedural modelling\npipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.\nHan and Zhu (2008) extended the work with an\nattribute grammar which used a more advanced\narXiv:2107.10006v1  [cs.CV]  21 Jul 2021\nbottom-up and top-down mechanism for recogniz-\ning objects. Their method was based on previous\nobject recognition work, such as Zhu, Zhang, and\nTu (2000) and Tu et al. (2005).\nTeboul et al. (2010) combined machine learning\nalgorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='algorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve\nthe works of Müller et al. (2007) and Han and\nZhu (2008), the learning process to some extent still\nwas based on hand-crafted rules.\nMathias et al. (2011) took another approach based\non architectural styles for automatic classiﬁcation.\nThey tried to extend Müller et al. (2007) work\nwith appropriate style information as they argued\nthat it could be used to select a more appropriate\nprocedural grammar for the task of building recon-\nstruction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='struction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on\nthe assumption that the building style was known\nand needed to be conducted as a manual task.\nInstead, they tried to automate the classiﬁcation of\narchitectural styles, by using facade images to train\ntheir classiﬁer to distinguish between three distinct\narchitectural styles. For classifying those speciﬁc\nstyles they were quite accurate but it also required\na rather speciﬁc set of circumstances to succeed.\nAs most of the earlier approaches used hand-\ncrafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more\ncomplex objects, i.e. could not handle deviations\nfrom these set of user-deﬁned rules well. They also\nsuffered in accuracy when the input image was\nmore complex, such as street-view images where the\nimage was not perfectly rectiﬁed. Another common\nlimitation was that they often suffered from heavy\ncomputational costs and slow running times (Bala\nand Kumar (2016), Koch et al. (2018)).\nConvolutional Neural Networks Approaches\nMore recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='More recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success\nof AlexNet in 2012, many semantic segmentation\napproaches which used to apply traditional ma-\nchine learning algorithms have instead been pri-\nmarily based on CNNs Bala and Kumar (2016);\nParthasarathy (2017).\nImproved models quickly followed AlexNet such\nas Segnet (Badrinarayanan, Kendall, Cipolla, et\nal. (2015)) which was one of the ﬁrst to tackle\nthe problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-\nhamer, and Darrell (2015) showed that a fully\nconvolutional network (FCN) trained end-to-end,\npixels-to-pixels could exceed then state-of-the-art\nsemantic image segmentation methods by retaining\nspatial information of an image. In more detail,\nthey used three pre-trained base models (AlexNet,\nVGGNet and GoogleNet) and transferred them from\nclassiﬁers by replacing fully connected layers with\nconvolutional layers. Since then a semantic seg-\nmentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.\nDutta (2020)).\nChen et al. (2017) use dilated convolution instead\nof plain convolution to set new state-of-art results\nwith their proposed DeepLab system.\nRecently, more automated methods and entirely\nbased on deep learning have achieved remarkable\nimprovements in image analysis, such as Penatti,\nNogueira, and Dos Santos (2015) and Nogueira,\nPenatti, and Dos Santos (2017).\nDeep Learning Approaches\nDeep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Deep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision\ntasks (classiﬁcation, object detection, semantic seg-\nmentation and instance segmentation) in a lot of\nbenchmarks and a variety of ﬁelds, e.g. medicine,\nsurveillance, economics, sociology. They have been\napplied to a diversity of tasks such as face detection,\nspeech recognition, autonomous vehicles and so on.\nDeep Learning Approaches Related to Facade\nParsing\nNaturally, deep learning was also applied to\nbuilding-related image analysis problems as well.\n2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing\nquite speciﬁcally and rather successfully.\nSchmitz and Mayer (2016) method for semantic\ninterpretation of facade images used a convolutional\nnetwork and a key insight was that even smaller\ndatasets could train the network when transfer learn-\ning could be employed. For validation, they used the\neTRIMS dataset to measure the overall accuracy,\nprecision and recall. The F1 score had an accuracy\nof 0.82± 0.09 for the facade categories, e.g. win-\ndows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='dows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of\nfacade parsing even further with their method Deep-\nFacade which combined both the learning capacity\nof deep convolutional neural networks with rules\nand a loss function based on symmetry found in\nbuilding facades structures. They trained an FCN-\n8s network with their novel loss function to obtain\nexperimental results on both the ECP dataset and\nthe eTRIMS dataset that signiﬁcantly outperformed\nprevious state-of-the-art methods. Pixel accuracies\non the eTRIMS dataset reached as high as 0.91 for\nwindows.\nFathalla and Vogiatzis (2017) used appearance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='windows.\nFathalla and Vogiatzis (2017) used appearance\nand layout cues combined with the VGG-model.\nTheir method only had a single object of focus and\noverall accuracy was on par with other methods,\nalthough supplementary materials had to be asked\nfor an in-depth understanding of the results.\nKoch et al. (2018) developed a multi-scale patch-\nbased pattern extraction that combined with a CNN\n(AlexNet), to automatically estimate the condition\nof buildings. They showed to some degree their\nmethod could serve as a proxy for condition esti-\nmates by appraisers.\nBacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with\na deep learning-based facade segmentation stage\nbased on generative adversarial networks (GANs).\nThe task was to reconstruct 3D models of build-\nings facades, in the urban environment for cultural\nheritage.\nHu et al. (2020) used the bounding boxes detected\nby YOLO architecture in real-time to guide facade\nreconstruction in an interactive environment. Results\nshowed accuracies above 0.82, which they deemed\nas sufﬁcient for their 3D reconstruction task.\nRahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of\nboth traditional and modern machine learning ap-\nproaches. His system was trained for windows delin-\neation segmentation, i.e. detecting window frames,\nmullions, transoms and so on. For small datasets\nit showed promising results, however, this method\nis not competitive compared to complete end-to-\nend state-of-the-art deep learning approaches such\nas Liu et al. (2017).\nWenguang Ma and Wei Ma (2020) used Faster\nR-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.\nThey called their method WD-Net which showed\n0.5 per cent improved accuracy compared with the\nbaseline Faster R-CNN.\n3. OUR APPROACH\nMask R-CNN Architecture\nIn this paper, we suggested the Mask R-CNN\nmodel, which is an intuitive extension of the Faster\nR-CNN model and outputs the predicting segmenta-\ntion masks on each Region of Interest (RoI), in par-\nallel with the existing branch for classiﬁcation and\nbounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='bounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed\nfor pixel-to-pixel alignment between network inputs\nand outputs. The segmentation mask in Mask R-\nCNN represents the pixel-level segmentation and\nalignment of each object that helps to extract each\ninstance of an object separately without background.\nMask R-CNN is a two-stage framework. The ﬁrst\nstage scans the image and generates proposals, the\nsecond stage classiﬁes the proposals and generates\nbounding boxes and masks Abdulla (2018); He\net al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='et al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal\nNetwork (RPN), ROI Classiﬁer & Bounding Box\nRegressor, and Segmentation Masks as shown in the\n(Figure 2) below.\n3\nFigure 1. Mask R-CNN framework\nNote. Image from (He et al. (2017))\nFigure 2. Mask R-CNN Internal Architecture\nNote. Figure 2 shows the steps until the instance\nsegmentation processes takes place from the ﬁrst\nstage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='stage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,\ncalled the backbone. It is a pre-trained standard con-\nvolutional neural network, which is either ResNet50\nor ResNet101. The backbone network serves as a\nfeature extractor over an entire image, at the early\nlayers the backbone detects low-level features like\nedges and corners, and later layers successively\ndetect higher-level features like the car, person,\nsky. When the image passes through the backbone\nnetwork, it is converted from 1024x1024px x 3\n(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.\nAnother more effective backbone, called a Fea-\nture Pyramid Network (FPN) applied on the top of\nthe ResNet backbone to improve the feature extrac-\ntion method. FPN uses a top-down architecture with\nlateral connections to build an in-network feature\npyramid from a single-scale input. It improves the\nstandard feature extraction pyramid by adding a\nsecond pyramid that takes the high-level features\nfrom the ﬁrst pyramid and passes them down to\nlower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='lower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level\nfeatures. Using a ResNet-FPN backbone for feature\nextraction with Mask R-CNN gives excellent gains\nin both accuracy and speed Abdulla (2018).\nRegion Proposal Network (RPN)\nA Region Proposal Network (RPN) was intro-\nduced by Ren et al. (2016) that takes an image as\ninput and outputs a set of rectangular object propos-\nals and ﬁnds areas that contain objects. To generate\nregion proposals, the author scans the images in\na sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='a sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×\nn spatial window of the input convolutional feature\nmap. Each sliding window is mapped to a lower-\ndimensional feature. The RPN doesn’t scan over\nthe image directly, instead, the RPN scans over the\nbackbone feature map. This allows the RPN to reuse\nthe extracted features efﬁciently and avoid duplicate\ncalculations.Then this feature is fed into two fully\nconnected layers, a box-regression layer, and a box-\nclassiﬁcation layer.\nRPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='RPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the\nRPN scans over are called anchors, which are boxes\ndistributed over the image area. The anchor is the\ncentral point of the sliding window. For the ZF\nModel which was an extension of AlexNet, the\ndimensions are 256-d and for VGG-16, it was 512-d.\nThe classiﬁer determines the probability of a pro-\nposal having the target object. Regression regresses\nthe coordinates of the proposals. For any image,\n4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN\nproposed by passing a sliding window over the\nCNN feature maps and at each window outputting\nk potential bounding boxes and scores for how\ngood each bounding box expected to be.\nscale and aspect-ratio are two important parameters.\nAspect ratio is width of image/height of the image,\nthe scale is the size of the image. (Figure 3) shows\nthat if 3 scales and 3 aspect-ratio are chosen, the\ntotal of 9 proposals are possible for each pixel,\nthis is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='this is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole\nimage, the number of anchors will be W*H*K,\nwhere W and H are the width and height of the\nimage respectively Karmarkar (2018).\nThe RPN generates two outputs for each anchor:\nAnchor Class:\nThe anchor class can be either\nforeground or background classes. The foreground\nclass implies that there is likely an object in that\nbox.\nBounding Box Reﬁnement: A foreground an-\nchor might not be centered perfectly over the object.\nSo the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='So the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN\npredictions, the model picks the top anchors that are\nlikely to contain objects and reﬁne their location and\nsize.If several anchors overlap too much, it keeps the\none with the highest foreground score and discards\nthe rest (Non-max Suppression). After getting the\nﬁnal proposals (regions of interest) then, it passes\nto the next stage.\nFigure 4. Example, a) Anchor boxes per object b)\nTop three anchor boxes pre object\n(a)\n(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and\naspect ratios and they overlap each other to cover\nthe object as much as possible. (b) shows that the\ntop three anchor boxes that are more likely to\ncontain an object, after some reﬁnement on their\nlocation and size\nROI Classiﬁer & Bounding Box Regressor\nThis\narchitecture\nwas\nintroduced\nby\n(Gir-\nshick (2015)), by removing the SVM classiﬁers for\ntraining to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='training to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)\nproposed by the RPN.\nFirst, the network processes all images with sev-\neral convolutional and max-pooling layers to pro-\nduce convolutional feature map. Then a region of\ninterest (RoI) pooling layer extracts a ﬁxed length\nfeature vector for each object proposal from the fea-\nture map. Then, each feature vectored into the fully\nconvolutional layers and produces softmax proba-\nbility over different object classes with background\nclass and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='class and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:\nClass: The class of the object in the ROI. Since this\nnetwork is deeper and has the capacity to classify\nregions to speciﬁc classes like person, car, chair,etc.\nIt can also generate a background class, which\ncauses the ROI to be discarded.\nBounding Box Reﬁnement: Further reﬁne the\nlocation and size of the bounding box to encapsulate\n5\nthe object.\nRoI pooling layer: Due to the bounding box\nreﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='reﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop\na part of a feature map and resize it to a ﬁxed size.\nThe RoI pooling layer uses max pooling to convert\nthe features inside any valid region of interest into\na small feature map with a ﬁxed spatial extent of\nH×W (e.g., 7 × 7), where H and W are layer hyper-\nparameters that are independent of any particular\nRoI Girshick (2015).\nSegmentation Masks\nThe mask network is the addition that the Mask\nR-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks\nfor them. The generated masks are low resolution:\n28x28 pixels. But they are soft masks, represented\nby ﬂoat numbers, so they hold more details than\nbinary masks. The small mask size helps to keep the\nmask branch light. During training, the ground-truth\nmasks. scale down to 28x28 to compute the loss,\nand during inference, the predicted masks scale up\nto the size of the ROI bounding box and that gives\nus the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='us the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted\nsum of different losses at different stages of the\nmodel. Five general losses occur during training the\nweight Abdulla (2018).\nRpn class loss: Occur when an improper clas-\nsiﬁcation of anchor boxes by the Region Proposal\nNetwork. This loss increases when the multiple\nobjects are not being detected by the model in the\nlast output. This loss determines how well the Re-\ngion Proposal Network separates background with\nobjects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='objects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being\ndetected but the bounding box is not properly ﬁt\nwith the position of the object.\nMrcnn class loss: Occurs when an improper clas-\nsiﬁcation of objects that are present in the region\nproposal. This is to be increased in case the object\nis being detected from the image, but misclassiﬁed.\nIt determines how well the Mask RCNN recognizes\neach class of object.\nMrcnn bbox loss: This loss occurs during the local-\nization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of\nthe object is done, but localization is not precise.\nMrcnn mask loss: This loss corresponds to masks\ncreated on the identiﬁed objects, which is related to\nhow well the Mask RCNN segment objects.\nPublic datasets\nAs for current dataset resources, the Ecole\nCentrale Paris (ECP) Facades dataset Teboul et\nal. (2010) and the eTRIMS Korc and Först-\nner (2009) database are the two most relevant public\nfacade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='facade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.\nThe ECP dataset consists of 104 annotated im-\nages which are rectiﬁed and cropped facades of\nHaussmannian style buildings in Paris. Images are\nannotated with seven classes (balcony, chimney,\ndoor, roof, shop, wall and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image\nthat consists of a colourmap (Fig. 5). The original\nannotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='annotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.\nThe eTRIMS consists of 60 annotated street\nview images but different from the ECP dataset\nthe images are not rectiﬁed and the facades are of\nvarious architectural styles. Images are annotated\nwith eight classes (sky, building, door, vegetation,\ncar, road, pavement and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image that\nconsists of an array and a color-map matrix (Fig.\n6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to\ndetect windows in real street view images because\n6\nFigure 5. Example from ECP dataset, a) the input\nimage b) the annotation\n(a)\n(b)\nNote. Image from (Teboul et al. (2010))\nFigure 6. Example from eTRIMS, (a) the input\nimage (b) the annotation\n(a)\n(b)\nNote. In the middle part of (b), the upper window\nboundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='boundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).\nof multiple reasons. The ECP dataset only contains\nfacades that are manually rectiﬁed and viewed in a\nfront-parallel direction, thus using it as training data\nwould not be wise since it is not highly representa-\ntive of real street view images, which is the imagery\nwe want to detect windows instances in. However,\nthe most important reasons were that we found both\nto be imprecise or even wrongly annotated regarding\nwhat exactly deﬁnes the window boundaries. For\nexample, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='example, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior\ncasing (See Figure 6).\nWe searched and found a few more public\ndatasets that contained window annotations but none\nwere of very high quality and many had the same\nissues as mentioned above. Therefore we had to take\na decision to build a quality dataset from scratch.\nOur dataset\nWe decided to use Google Street View images\n(Map data ©2020 Google) of facades mainly from\ncentral Gothenburg, Sweden. The cropped images\nwere manually collected with an intention to con-\ntain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.\nWe annotated them manually with the VGG\nImage Annotator (VIA) tool A. Dutta and Zisser-\nman (2019). There exists a lot of other tools to\nannotate images, e.g. LabelMe, RectLabel, Label-\nBox and COCO UI, but we chose VIA because\nof its efﬁciency and simplicity. For example, it\nruns in most modern web browsers so it does not\nrequire any other installation or setup. Regarding\nthe annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others\nstore them in XML-format, and so on. The VIA tool\nsaves its annotations in a JSON-format ﬁle where\neach mask is a set of polygon points (Figure 7).\nSince we tried to be consistent to follow each\nwindow’s exact window frame and not to include\nother window parts as window boundaries it made\nannotation a very time-consuming and complex\ntask, especially as speciﬁc decisions had to be taken\nfor each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='for each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user\ninterface and ﬁgured out most of the different win-\n7\nFigure 7. Two examples from images in our dataset\nannotated with the VIA tool\n(a)\n(b)\nNote. The yellow lines are the manually annotated\npolygons which deﬁne each window boundary.\ndow types and their frames, we were annotating one\nwindow about every 5-10 seconds. However, there\nwere a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='were a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended\nup with a collection of 100 images that contained\n1540 annotated windows (an average of 15,4 win-\ndows per image). We then divide our dataset into a\ntraining set (80 images) and a validation set (20 im-\nages), i.e. we split the generated JSON-ﬁle into two\ndifferent ﬁles (Figure 8) with their corresponding\nimages.\nOptimization Methods\nTo use the Mask R-CNN framework we used\nan open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='an open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by\nAbdulla (2017). Then we hosted the implementation\non the Google Colab platform for free GPU usage\nand easy of shareability amongst other positive\nfactors.\nAssume to start with the structure of the Mask\nR-CNN as described in the paper He et al. (2017)\nand the source code provided in Abdulla (2017), we\nneeded to investigate the similarities and differences\nbetween the Matterport implementation running on\nKeras and TensorFlow and the ofﬁcial paper imple-\nFigure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Figure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the\ntraining set and the right to the validation set.\nBoth are viewed with JSON Editor Online.\nmentation build on the Caffe deep learning frame-\nwork.\nFrom our understanding, the implementation fol-\nlowed the paper with only a few exceptions and\nthose were done mostly for code simplicity and\ngeneralization.\nFor example, to support training multiple images\nper batch, the Matterport implementation resizes all\nimages to the same size and preserve the aspect ratio\nby padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='by padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-\ntations they chose to generate bounding boxes in the\ncode on the ﬂy, thus making image augmentations\neasier to apply. The downside is a slight decrease in\nbounding box placement accuracy, about only 2%\nof bounding boxes differed by 1px or more.\nThe paper uses a more aggressive learning rate of\n0.02, but for the Matterport implementation that was\ntoo high and causes the weights to explode in Ten-\nsorFlow as it computes gradients differently from\nCaffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Caffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between\nMatterport’s and the paper’s features, parameters,\nhyper-parameters and so on, we could start to\noptimize our model. The optimization adjustments\nand modiﬁcations were updated continuously during\n8\nthe whole process, but we can perhaps give some\nintuition on how they were done by describing them\nas iteratively separate steps. For example, optimiza-\ntion of the network structure with its corresponding\nconﬁguration can be considered ’low level’, while\noptimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='optimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to\nadjust our result within the context of our research\nproblem or goal. Furthermore, each step often\nneeded distinct conﬁguration adjustment depending\non which model weights and dataset changes were\nused, i.e. training resumed with COCO-weights,\naugmentation ﬂip, augmentation afﬁne and so on,\nall needed different conﬁgurations to train properly.\nFigure 9. Examples of the many conﬁgurations\noptions in the Matterport implementation\nNote. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and\nFigure 12).\n4. EXPERIMENTS\nIn this section, we will ﬁrst present some metrics\nused in our experiments, then give insight into the\nthought process of some of our structure and con-\nﬁguration optimization, training optimization and\ninference optimization. Finally, we will present our\nexperimental settings and some of the best results\nwe achieved.\nMetrics\nTo use the networks inference results for detect-\ning windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ing windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly\ndetected windows. In other words, recall is a crucial\nvalue to improve. To evaluate the images overall\ndetection results, we ﬁrst use IoU (Intersection\nover Union) to measure how much our predicted\nboundary overlaps with the ground truth (the real\nobject boundary). A predeﬁned IoU threshold of 0.5\nor greater is used for classifying the prediction as a\ntrue positive otherwise a false positive.\nFigure 10. Examples of IoU.\nNote. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence\nscore and measured IoU.\nThen we use the typical precision and recall rate:\nPrecision =\nTP\nTP + FP\n(1)\nRecall =\nTP\nTP + FN\n(2)\nwhere, TP, FP, FN denotes the true positive, false\npositive and false negative, respectively.\nFinally, we use the mean Average Precision\n(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR\n(3)\nwhere the P represent the precision rate and and R\nthe recall rate.\nStructure and conﬁguration optimization\nOverall, we found it a challenging task to test\nand tune the many parameters, mainly due to the\n9\nadaptive process of such a large search space, the\ncontinuous updates to our training data, but also the\ntime it takes to train and do inference to calculate\nthe accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-\nurations of the neural network, for example, we\ninvestigated Mask R-CNN in combination with both\nthe ResNet50 and the ResNet101 Backbone as\ndescribed in He et al. (2017). Those experiments\nshowed that the latter often yielded better results.\nWe kept the base conﬁguration of input images of\nsize 1024x1024 px for best accuracy even if some\nof our images were a bit smaller since the model\nresized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.\nWe found the gradient clipping norm to be ap-\npropriately set a 5 with a learning rate at 0.001 and\nmomentum of 0.9 and so on.\nBy plotting each test we could observe that train-\ning and validation losses indicated overﬁtting, i.e.\nwe got an increasingly lower training loss but the\nvalidation loss at certain points started to ﬂuctuate\nor stagnate (Figure 11).\nFigure 11. Examples of plots from our training\nNote. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with\nweight decay and dropout amongst other things. Un-\nfortunately, GridSearch with cross-validation, to au-\ntomatically search for the optimal hyper-parameter\nvalues, required more computational power than we\nhad access to, but we believe a valid option given\nthe appropriate resources.\nAnother option and the most intuitive, to prevent\noverﬁtting is often to extend the dataset. However,\nthis was neither feasible given the remaining project\ntime and also did not ﬁt well with our interest in\ninvestigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='investigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is\na resampling procedure used to evaluate machine\nlearning models on a limited data sample, without\nany major improvements on our test set in terms of\naccuracy.\nTraining optimization\nFor the training optimization we assume the given\nMask-RCNN loss function as:\nL = αL1 + βL2 + γL3 + δL4 + ϵL5\n(4)\nwhere the ﬁrst loss is the RPN class loss and\nthe second the RPN bounding box prediction loss.\nThe other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='The other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization\ngoal is to ﬁnd a corresponding conﬁguration such\nthat it minimizes the overall loss. This is an adaptive\nprocess of matching the results of the loss function\nwith the given dataset and the parameters as a result\nof a trial process.\nGiven a loss Li and equation (4), optimize K =\n(α, β, γ, δ, ϵ) ∈ R5 and L1, L2, L3, L4, L5 loss func-\ntions so that the average precision of our model so\nthat the average precision of our model is improved\nwith a threshold of AP50 :\nmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='that the average precision of our model is improved\nwith a threshold of AP50 :\nmax\nK,L1,L2,L3,L4,L5 AP50\n(5)\nUltimately, the ﬁnal choice of parameters chosen\ndepends on the structure conﬁgurations discussed\nin the previous section and also on our input data\n. While technically for K any combination of real\nvalues can be combined:\n(α, β, γ, δ, ϵ) ∼ n.(α, β, γ, δ, ϵ), ∀n ∈ R\n(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10\non the loss gradient and not the loss value. Further-\nmore, several other conﬁguration parameters need\nto be tested and changed to improve accuracy. The\npractical approach for tuning loss weights was to\nstart with a default weight of 1 for each loss and\nevaluate the model on the validation set by inferring\nthe model performance image by image. We had to\nevaluate the true positives, false positives, and false\nnegatives combined with IoU of their masks and\ntheir score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='their score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In\nother words, we are using transfer learning, which\nmeans we don’t need to train a new model from\nscratch and that instead utilize a lot of the already\nlearned features from the COCO dataset, which\ncontains around 120K images. After the training for\n40 epochs with k-fold cross-validation have been\ncompleted, we save the model’s parameters and\nevaluate the new weights (one for each epoch is\ngenerated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='generated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-\ntating new images would take too long. Since the\npolygons are calculated internally this was easy to\nimplement with the imgaug library.\nFirst, we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith augmentation that ﬂip/mirror each image in\nthe training set horizontally right/left. Then we\nagain save the model’s parameters and evaluate the\nperformance of the new weights.\nThen we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Then we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the\ntraining set by rotation (-45, 45) and shearing (-16,\n16). Finally, we save the model’s parameters and\nevaluate the performance of the new weights with\nrunning inference on the weights that yielded the\nlowest training loss.\nInference optimization\nIn the process of optimizing the trained network,\nwe constantly adjusted and modiﬁed the training set\nand other parameters. Since the output of the infer-\nence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust\naccordingly with regards to the properties of the test\nset images and the overall accuracy. We found a\ndetection minimum conﬁdence of 0.9 to yield as\ngood trade-off of the precision and recall values for\nour windows.\n5. RESULTS\nWe perform our test result experiments on the\neTrims dataset and not on the ECP dataset. The\nreason being that the ECP dataset doesn’t contain\nstreet-view images and the images are all manually\nrectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='rectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.\nHowever, to properly use the eTRIMS dataset\nwith our model, we had to reﬁne their annotation\nto overcome inaccuracies and mistakes, and also\naccommodate our deﬁned window boundary which\nfollows the window frame. We, therefore, annotated\n10 randomly chosen images for our test set to\nevaluate our model’s performances. Therefore, any\ncomparisons should note these differences.\nBoth the training and inference was done on\nGoogle Colab with the free Tesla K80 GPU of about\n12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of\nthose we present the best three models with regards\nto the highest mAP on our test set (Table 1) and\nfor visualization we present the inference done on\nthe model achieving the highest mAP below (Figure\n12).\n11\nFigure 12.\nWindow detection results from a model trained on coco-weights, then resumed with ﬂiplr.\nImage\nGround truth\nPrediction\nOverlap\ngreen colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='green colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score\ncaptions: score/IoU\n12\nREFERENCES\nREFERENCES\nTable 1.\nA comparison of pixel accuracies on the eTRIMS\ndataset. Accuracies are shown in percentage.\nClass\n[1]\n[2]\n[3]\n[4]\n[5]\nOur*\nOur**\nOur***\nWindow[%] 75\n73\n71\n86', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Our***\nWindow[%] 75\n73\n71\n86\n90.91 91.93\n94.55\n94.76\nNote: Our result is only from 10 randomly chosen images from the eTRIMS\ndataset with reﬁned annotation.\nmodel trained on coco-weights then resumed with ﬂiplr and afﬁne using\nk-fold cross-validation (the resumed weights were chosen by the folds\nhighest mAP)\n** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)\n*** model trained on coco-weights then resumed with ﬂiplr (the resumed\nweights were chosen by the lowest training loss)\n6. CONCLUSIONS\nIn this paper, we propose to use the state-of-\nthe-art Mask R-CNN framework He et al. (2017)\nas an end-to-end network capable of instance seg-\nmentation without further machinery. Previous fa-\ncade parsing detection work often only produced\nsemantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='semantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is\ncapable of detecting and producing a bounding box\nand segmentation mask for each window instance.\nExperimental results show that our suggested\napproach can with only a relatively small but precise\ndataset train the network—only with transfer learn-\ning and augmentation—to produce instance segmen-\ntation of windows and a comparable result with\nprior state-of-the-art window detection approaches,\neven without pre- or post-optimization techniques.\nAs far as we know, we are the ﬁrst to have anno-\ntated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window\nboundary and not to include other window parts. We\nbelieve that our consequent and precise annotation\nof windows helped our model training. We also\nhope it can beneﬁt the research community as a free\nresource for more precise window detection.\n7. FUTURE WORK\nIn this paper, we have got a sense of how to tune\nthe hyper-parameters effectively with our training\ndata. However, further work is recommended to\ninvestigate this with platforms, such as Wandb or\nOnepanel Jobs for easier coordination, synchroniza-\ntion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types\nwere the model has low accuracy.\nFurther work could also beneﬁt from training\nmore facade classes, such as building, balcony,\ndoors, chimney, to reduce false positives, false neg-\natives. The same goes for the window class, i.e.\nsplitting it into distinct window type classes, such\nas double-hung, transom etc. A model capable of\na more complete facade parsing we believe will\nalso make application easier as the model has a\nﬁner granularity of the facades properties. A caution\nis that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='is that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of\nevery facade structure.\nTo further reﬁne the results and perhaps mitigate\nsome of the typical limitations we experienced,\nsuch as dealing with occlusion, overconﬁdent false\npositives predictions, occasional mask overlaps, fu-\nture work should investigate both in pre- and post-\noptimization methods of previous approaches or\ntesting their novel approaches. Also, code refactor-\ning and optimization, e.g. rewriting some Python\ncode in TensorFlow to bring all the beneﬁts of the\nTensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS\nThis paper was the ﬁnal written report in the\ncourse DIT891 Project in Data Science. The course\nis offered within the Applied Data Science Master’s\nProgramme by the University of Gothenburg. We\nwould like to thank Shirin Tavara, teacher and the\ncourse responsible and Richard Johansson, teacher.\nA special thanks goes to our supervisors Alexan-\nder Hollberg, Sanjay Somanath and Yinan Yu for\ntheir congeniality and support.\n13\nREFERENCES\nREFERENCES\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='13\nREFERENCES\nREFERENCES\nREFERENCES\nAbdulla, Waleed (2017). “Mask R-CNN for object\ndetection and instance segmentation on Keras and\nTensorFlow https://github. com/matterport”. In:\nMask _ RCNN.\nAbdulla, Waleed (2018). Instance Segmentation\nwith Mask R-CNN and TensorFlow. https : / /\nengineering . matterport . com / splash - of - color -\ninstance- segmentation- with- mask- r- cnn- and-\ntensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia\nRagia (2020). “3D building façade reconstruction\nusing deep learning”. In: ISPRS International\nJournal of Geo-Information 9(5), p. 322.\nBadrinarayanan,\nVijay,\nAlex\nKendall,\nRoberto\nCipolla, et al. (2015). “1SegNet: A Deep Convo-\nlutional Encoder-Decoder Architecture for Image\nSegmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Segmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:\nInternational\nJournal\nof\nAdvanced\nResearch\nin Electronics and Communication Engineering\n(IJARECE) Volume 5, pp. 1448–1454.\nChen, Liang-Chieh et al. (2017). “Deeplab: Seman-\ntic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected\ncrfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.\nDutta, Abhishek and Andrew Zisserman (2019).\n“The VIA annotation software for images, audio\nand video”. In: Proceedings of the 27th ACM in-\nternational conference on multimedia, pp. 2276–\n2279.\nFathalla, Radwa and George Vogiatzis (2017). “A\ndeep learning pipeline for semantic facade seg-\nmentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer\nvision, pp. 1440–1448.\nHan, Feng and Song-Chun Zhu (2008). “Bottom-\nup/top-down image parsing with attribute gram-\nmar”. In: IEEE transactions on pattern analysis\nand machine intelligence 31(1), pp. 59–73.\nHe, Kaiming et al. (2017). “Mask r-cnn”. In: Pro-\nceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.\nHu, Han et al. (2020). “Fast and Regularized Re-\nconstruction of Building Fa\\c {c} ades from\nStreet-View Images using Binary Integer Pro-\ngramming”. In: arXiv preprint arXiv:2002.08549.\nKarmarkar, T (2018). “Regional Proposal network\n(RPN)—Backbone of Faster R-CNN”. In: Aug\n18, p. 6.\nKoch, David et al. (2018). “Visual estimation of\nbuilding condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='building condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on\nMultimedia for Real Estate Tech, pp. 12–17.\nKorc,\nFilip\nand\nWolfgang\nFörstner\n(2009).\n“eTRIMS\nImage\nDatabase\nfor\ninterpreting\nimages of man-made scenes”. In: Dept. of\nPhotogrammetry, University of Bonn, Tech. Rep.\nTR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep\nlearning approach to facade parsing”. In.\nLong, Jonathan, Evan Shelhamer, and Trevor Dar-\nrell (2015). “Fully convolutional networks for\nsemantic segmentation”. In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pp. 3431–3440.\nMa, Wenguang and Wei Ma (2020). “Deep window\ndetection in street scenes”. In: KSII Transactions\non Internet and Information Systems (TIIS) 14(2),\npp. 855–870.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='on Internet and Information Systems (TIIS) 14(2),\npp. 855–870.\nMathias, Markus et al. (2011). “Automatic architec-\ntural style recognition”. In: ISPRS-International\nArchives of the Photogrammetry, Remote Sensing\nand Spatial Information Sciences 3816, pp. 171–\n176.\nMüller, Pascal et al. (2007). “Image-based procedu-\nral modeling of facades”. In: ACM Trans. Graph.\n26(3), p. 85.\nNogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Nogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting\nconvolutional neural networks for remote sensing\nscene classiﬁcation”. In: Pattern Recognition 61,\npp. 539–556.\nParthasarathy,\nDhruv\n(2017).\n“A\nbrief\nhistory\nof CNNs in image segmentation: From R-\n14\nCNN to Mask R-CNN”. In: Available online:\nblog.\nathelas.\ncom/a-brief-history-of-cnns-in-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='blog.\nathelas.\ncom/a-brief-history-of-cnns-in-\nimage-segmentation-from-r-cnn-to-mask-rcnn-\n34ea83205de4 (accessed on 24 February 2019).\nPenatti, Otávio AB, Keiller Nogueira, and Jefersson\nA Dos Santos (2015). “Do deep features general-\nize from everyday objects to remote sensing and\naerial scenes domains?” In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition workshops, pp. 44–51.\nRahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der\nBundeswehr München.” In.\nRecky, Michal and Franz Leberl (2010). “Windows\ndetection using k-means in cie-lab color space”.\nIn: 2010 20th International Conference on Pat-\ntern Recognition. IEEE, pp. 356–359.\nRen, Shaoqing et al. (2016). “Faster R-CNN: to-\nwards real-time object detection with region pro-\nposal networks”. In: IEEE transactions on pat-\ntern analysis and machine intelligence 39(6),\npp. 1137–1149.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tern analysis and machine intelligence 39(6),\npp. 1137–1149.\nSchmitz, Matthias and Helmut Mayer (2016). “A\nconvolutional network for semantic facade seg-\nmentation and interpretation”. In: The Interna-\ntional Archives of Photogrammetry, Remote Sens-\ning and Spatial Information Sciences 41, p. 709.\nSultana, F, A Suﬁan, and P Dutta (2020). “Evolution\nof Image Segmentation using Deep Convolutional\nNeural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Neural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.\nTeboul, Olivier et al. (2010). “Segmentation of\nbuilding facades using procedural shape priors”.\nIn: 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition. IEEE,\npp. 3105–3112.\nTu, Zhuowen et al. (2005). “Image parsing: Unify-\ning segmentation, detection, and recognition”. In:\nInternational Journal of computer vision 63(2),\npp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst\nBischof (2010). “Unsupervised facade segmenta-\ntion using repetitive patterns”. In: Joint Pattern\nRecognition Symposium. Springer, pp. 51–60.\nZhao, Peng et al. (2010). “Rectilinear parsing of ar-\nchitecture in urban environment”. In: 2010 IEEE\nComputer Society Conference on Computer Vi-\nsion and Pattern Recognition. IEEE, pp. 342–349.\nZhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Zhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-\nject recognition by data driven Markov chain\nMonte Carlo”. In: Proceedings IEEE Conference\non Computer Vision and Pattern Recognition.\nCVPR 2000 (Cat. No. PR00662). Vol. 1. IEEE,\npp. 738–745.\n15', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5e91d0> 111
cuda:2
[Document(page_content='Window Detection In Facade Imagery:\nA Deep Learning Approach Using Mask R-CNN\nNils Nordmark, University of Gothenburg\nMola Ayenew, University of Gothenburg\nAbstract\nThe parsing of windows in building facades is a long-\ndesired but challenging task in computer vision. It is\ncrucial to urban analysis, semantic reconstruction, life\ncycle analysis, digital twins and scene parsing amongst\nother building-related tasks that require high-quality se-\nmantic data. In this article, we investigate the usage of the\nMask R-CNN framework to be used for window detection\nof facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='of facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to\nproduce instance segmentations of our new window class.\nExperimental results show that our suggested approach can\nwith a relatively small dataset train the network only with\ntransfer learning and augmentation achieve results on par\nwith prior state-of-the-art window detection approaches,\neven without post-optimization techniques.\n1. INTRODUCTION\nGathering semantic data regarding building fea-\ntures from images is desired as manual gathering\ncan often be a challenging, time consuming and\nresource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and\nbalconies are often not available, can change over\ntime and the data gathering is usually a manual\nprocess.\nMany different approaches have been applied to\nsolve the gathering of semantic data within many\ndifferent domains. Within building facade parsing\nthis task has mostly been treated as a process of\nsemantic segmentation of facade imagery into archi-\ntectural structural classes such as windows, doors,\nbalconies and so on. This semantic segmentation\napproach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='approach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the\nsame object as each pixel is classiﬁed to belong\nto a particular class. Speciﬁcally for parsing of\nwindows instances in building facades, we suggest\nan approach that can decouple segmentation and\nclassiﬁcation to detect windows in an image with\na bounding box and segmentation mask for each\nwindow instance.\nWith instance segmentation, we can analyze the\nwindows independently with detailed and compre-\nhensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='hensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,\nwe propose an approach based on the Mask R-CNN\nframework.\n2. RELATED WORK\nFacade parsing has been studied for a long time\nand there exists a large body of work on how to\ndeal with such a problem.\nEarlier approaches\nZhao et al. (2010) proposed parsing ground-\nlevel images into architectural units which could\nbe used for city modelling. Wendel, Donoser, and\nBischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-\nmeans clustering to tackle the same problem.\nMany others proposed a procedural shape gram-\nmar approach. This approach could at a high-level\nbe explained as a hand-crafted set of rules of basic\nshapes which represents structured geometries, i.e.\nthe structure of the object is encoded as a set of\nparametric rules. Models could then be generated\nby iteratively applying the rules on a starting shape.\nMüller et al. (2007) used a procedural modelling\npipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.\nHan and Zhu (2008) extended the work with an\nattribute grammar which used a more advanced\narXiv:2107.10006v1  [cs.CV]  21 Jul 2021\nbottom-up and top-down mechanism for recogniz-\ning objects. Their method was based on previous\nobject recognition work, such as Zhu, Zhang, and\nTu (2000) and Tu et al. (2005).\nTeboul et al. (2010) combined machine learning\nalgorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='algorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve\nthe works of Müller et al. (2007) and Han and\nZhu (2008), the learning process to some extent still\nwas based on hand-crafted rules.\nMathias et al. (2011) took another approach based\non architectural styles for automatic classiﬁcation.\nThey tried to extend Müller et al. (2007) work\nwith appropriate style information as they argued\nthat it could be used to select a more appropriate\nprocedural grammar for the task of building recon-\nstruction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='struction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on\nthe assumption that the building style was known\nand needed to be conducted as a manual task.\nInstead, they tried to automate the classiﬁcation of\narchitectural styles, by using facade images to train\ntheir classiﬁer to distinguish between three distinct\narchitectural styles. For classifying those speciﬁc\nstyles they were quite accurate but it also required\na rather speciﬁc set of circumstances to succeed.\nAs most of the earlier approaches used hand-\ncrafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more\ncomplex objects, i.e. could not handle deviations\nfrom these set of user-deﬁned rules well. They also\nsuffered in accuracy when the input image was\nmore complex, such as street-view images where the\nimage was not perfectly rectiﬁed. Another common\nlimitation was that they often suffered from heavy\ncomputational costs and slow running times (Bala\nand Kumar (2016), Koch et al. (2018)).\nConvolutional Neural Networks Approaches\nMore recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='More recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success\nof AlexNet in 2012, many semantic segmentation\napproaches which used to apply traditional ma-\nchine learning algorithms have instead been pri-\nmarily based on CNNs Bala and Kumar (2016);\nParthasarathy (2017).\nImproved models quickly followed AlexNet such\nas Segnet (Badrinarayanan, Kendall, Cipolla, et\nal. (2015)) which was one of the ﬁrst to tackle\nthe problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-\nhamer, and Darrell (2015) showed that a fully\nconvolutional network (FCN) trained end-to-end,\npixels-to-pixels could exceed then state-of-the-art\nsemantic image segmentation methods by retaining\nspatial information of an image. In more detail,\nthey used three pre-trained base models (AlexNet,\nVGGNet and GoogleNet) and transferred them from\nclassiﬁers by replacing fully connected layers with\nconvolutional layers. Since then a semantic seg-\nmentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.\nDutta (2020)).\nChen et al. (2017) use dilated convolution instead\nof plain convolution to set new state-of-art results\nwith their proposed DeepLab system.\nRecently, more automated methods and entirely\nbased on deep learning have achieved remarkable\nimprovements in image analysis, such as Penatti,\nNogueira, and Dos Santos (2015) and Nogueira,\nPenatti, and Dos Santos (2017).\nDeep Learning Approaches\nDeep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Deep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision\ntasks (classiﬁcation, object detection, semantic seg-\nmentation and instance segmentation) in a lot of\nbenchmarks and a variety of ﬁelds, e.g. medicine,\nsurveillance, economics, sociology. They have been\napplied to a diversity of tasks such as face detection,\nspeech recognition, autonomous vehicles and so on.\nDeep Learning Approaches Related to Facade\nParsing\nNaturally, deep learning was also applied to\nbuilding-related image analysis problems as well.\n2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing\nquite speciﬁcally and rather successfully.\nSchmitz and Mayer (2016) method for semantic\ninterpretation of facade images used a convolutional\nnetwork and a key insight was that even smaller\ndatasets could train the network when transfer learn-\ning could be employed. For validation, they used the\neTRIMS dataset to measure the overall accuracy,\nprecision and recall. The F1 score had an accuracy\nof 0.82± 0.09 for the facade categories, e.g. win-\ndows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='dows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of\nfacade parsing even further with their method Deep-\nFacade which combined both the learning capacity\nof deep convolutional neural networks with rules\nand a loss function based on symmetry found in\nbuilding facades structures. They trained an FCN-\n8s network with their novel loss function to obtain\nexperimental results on both the ECP dataset and\nthe eTRIMS dataset that signiﬁcantly outperformed\nprevious state-of-the-art methods. Pixel accuracies\non the eTRIMS dataset reached as high as 0.91 for\nwindows.\nFathalla and Vogiatzis (2017) used appearance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='windows.\nFathalla and Vogiatzis (2017) used appearance\nand layout cues combined with the VGG-model.\nTheir method only had a single object of focus and\noverall accuracy was on par with other methods,\nalthough supplementary materials had to be asked\nfor an in-depth understanding of the results.\nKoch et al. (2018) developed a multi-scale patch-\nbased pattern extraction that combined with a CNN\n(AlexNet), to automatically estimate the condition\nof buildings. They showed to some degree their\nmethod could serve as a proxy for condition esti-\nmates by appraisers.\nBacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with\na deep learning-based facade segmentation stage\nbased on generative adversarial networks (GANs).\nThe task was to reconstruct 3D models of build-\nings facades, in the urban environment for cultural\nheritage.\nHu et al. (2020) used the bounding boxes detected\nby YOLO architecture in real-time to guide facade\nreconstruction in an interactive environment. Results\nshowed accuracies above 0.82, which they deemed\nas sufﬁcient for their 3D reconstruction task.\nRahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of\nboth traditional and modern machine learning ap-\nproaches. His system was trained for windows delin-\neation segmentation, i.e. detecting window frames,\nmullions, transoms and so on. For small datasets\nit showed promising results, however, this method\nis not competitive compared to complete end-to-\nend state-of-the-art deep learning approaches such\nas Liu et al. (2017).\nWenguang Ma and Wei Ma (2020) used Faster\nR-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.\nThey called their method WD-Net which showed\n0.5 per cent improved accuracy compared with the\nbaseline Faster R-CNN.\n3. OUR APPROACH\nMask R-CNN Architecture\nIn this paper, we suggested the Mask R-CNN\nmodel, which is an intuitive extension of the Faster\nR-CNN model and outputs the predicting segmenta-\ntion masks on each Region of Interest (RoI), in par-\nallel with the existing branch for classiﬁcation and\nbounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='bounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed\nfor pixel-to-pixel alignment between network inputs\nand outputs. The segmentation mask in Mask R-\nCNN represents the pixel-level segmentation and\nalignment of each object that helps to extract each\ninstance of an object separately without background.\nMask R-CNN is a two-stage framework. The ﬁrst\nstage scans the image and generates proposals, the\nsecond stage classiﬁes the proposals and generates\nbounding boxes and masks Abdulla (2018); He\net al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='et al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal\nNetwork (RPN), ROI Classiﬁer & Bounding Box\nRegressor, and Segmentation Masks as shown in the\n(Figure 2) below.\n3\nFigure 1. Mask R-CNN framework\nNote. Image from (He et al. (2017))\nFigure 2. Mask R-CNN Internal Architecture\nNote. Figure 2 shows the steps until the instance\nsegmentation processes takes place from the ﬁrst\nstage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='stage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,\ncalled the backbone. It is a pre-trained standard con-\nvolutional neural network, which is either ResNet50\nor ResNet101. The backbone network serves as a\nfeature extractor over an entire image, at the early\nlayers the backbone detects low-level features like\nedges and corners, and later layers successively\ndetect higher-level features like the car, person,\nsky. When the image passes through the backbone\nnetwork, it is converted from 1024x1024px x 3\n(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.\nAnother more effective backbone, called a Fea-\nture Pyramid Network (FPN) applied on the top of\nthe ResNet backbone to improve the feature extrac-\ntion method. FPN uses a top-down architecture with\nlateral connections to build an in-network feature\npyramid from a single-scale input. It improves the\nstandard feature extraction pyramid by adding a\nsecond pyramid that takes the high-level features\nfrom the ﬁrst pyramid and passes them down to\nlower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='lower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level\nfeatures. Using a ResNet-FPN backbone for feature\nextraction with Mask R-CNN gives excellent gains\nin both accuracy and speed Abdulla (2018).\nRegion Proposal Network (RPN)\nA Region Proposal Network (RPN) was intro-\nduced by Ren et al. (2016) that takes an image as\ninput and outputs a set of rectangular object propos-\nals and ﬁnds areas that contain objects. To generate\nregion proposals, the author scans the images in\na sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='a sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×\nn spatial window of the input convolutional feature\nmap. Each sliding window is mapped to a lower-\ndimensional feature. The RPN doesn’t scan over\nthe image directly, instead, the RPN scans over the\nbackbone feature map. This allows the RPN to reuse\nthe extracted features efﬁciently and avoid duplicate\ncalculations.Then this feature is fed into two fully\nconnected layers, a box-regression layer, and a box-\nclassiﬁcation layer.\nRPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='RPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the\nRPN scans over are called anchors, which are boxes\ndistributed over the image area. The anchor is the\ncentral point of the sliding window. For the ZF\nModel which was an extension of AlexNet, the\ndimensions are 256-d and for VGG-16, it was 512-d.\nThe classiﬁer determines the probability of a pro-\nposal having the target object. Regression regresses\nthe coordinates of the proposals. For any image,\n4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN\nproposed by passing a sliding window over the\nCNN feature maps and at each window outputting\nk potential bounding boxes and scores for how\ngood each bounding box expected to be.\nscale and aspect-ratio are two important parameters.\nAspect ratio is width of image/height of the image,\nthe scale is the size of the image. (Figure 3) shows\nthat if 3 scales and 3 aspect-ratio are chosen, the\ntotal of 9 proposals are possible for each pixel,\nthis is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='this is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole\nimage, the number of anchors will be W*H*K,\nwhere W and H are the width and height of the\nimage respectively Karmarkar (2018).\nThe RPN generates two outputs for each anchor:\nAnchor Class:\nThe anchor class can be either\nforeground or background classes. The foreground\nclass implies that there is likely an object in that\nbox.\nBounding Box Reﬁnement: A foreground an-\nchor might not be centered perfectly over the object.\nSo the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='So the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN\npredictions, the model picks the top anchors that are\nlikely to contain objects and reﬁne their location and\nsize.If several anchors overlap too much, it keeps the\none with the highest foreground score and discards\nthe rest (Non-max Suppression). After getting the\nﬁnal proposals (regions of interest) then, it passes\nto the next stage.\nFigure 4. Example, a) Anchor boxes per object b)\nTop three anchor boxes pre object\n(a)\n(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and\naspect ratios and they overlap each other to cover\nthe object as much as possible. (b) shows that the\ntop three anchor boxes that are more likely to\ncontain an object, after some reﬁnement on their\nlocation and size\nROI Classiﬁer & Bounding Box Regressor\nThis\narchitecture\nwas\nintroduced\nby\n(Gir-\nshick (2015)), by removing the SVM classiﬁers for\ntraining to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='training to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)\nproposed by the RPN.\nFirst, the network processes all images with sev-\neral convolutional and max-pooling layers to pro-\nduce convolutional feature map. Then a region of\ninterest (RoI) pooling layer extracts a ﬁxed length\nfeature vector for each object proposal from the fea-\nture map. Then, each feature vectored into the fully\nconvolutional layers and produces softmax proba-\nbility over different object classes with background\nclass and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='class and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:\nClass: The class of the object in the ROI. Since this\nnetwork is deeper and has the capacity to classify\nregions to speciﬁc classes like person, car, chair,etc.\nIt can also generate a background class, which\ncauses the ROI to be discarded.\nBounding Box Reﬁnement: Further reﬁne the\nlocation and size of the bounding box to encapsulate\n5\nthe object.\nRoI pooling layer: Due to the bounding box\nreﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='reﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop\na part of a feature map and resize it to a ﬁxed size.\nThe RoI pooling layer uses max pooling to convert\nthe features inside any valid region of interest into\na small feature map with a ﬁxed spatial extent of\nH×W (e.g., 7 × 7), where H and W are layer hyper-\nparameters that are independent of any particular\nRoI Girshick (2015).\nSegmentation Masks\nThe mask network is the addition that the Mask\nR-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks\nfor them. The generated masks are low resolution:\n28x28 pixels. But they are soft masks, represented\nby ﬂoat numbers, so they hold more details than\nbinary masks. The small mask size helps to keep the\nmask branch light. During training, the ground-truth\nmasks. scale down to 28x28 to compute the loss,\nand during inference, the predicted masks scale up\nto the size of the ROI bounding box and that gives\nus the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='us the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted\nsum of different losses at different stages of the\nmodel. Five general losses occur during training the\nweight Abdulla (2018).\nRpn class loss: Occur when an improper clas-\nsiﬁcation of anchor boxes by the Region Proposal\nNetwork. This loss increases when the multiple\nobjects are not being detected by the model in the\nlast output. This loss determines how well the Re-\ngion Proposal Network separates background with\nobjects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='objects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being\ndetected but the bounding box is not properly ﬁt\nwith the position of the object.\nMrcnn class loss: Occurs when an improper clas-\nsiﬁcation of objects that are present in the region\nproposal. This is to be increased in case the object\nis being detected from the image, but misclassiﬁed.\nIt determines how well the Mask RCNN recognizes\neach class of object.\nMrcnn bbox loss: This loss occurs during the local-\nization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of\nthe object is done, but localization is not precise.\nMrcnn mask loss: This loss corresponds to masks\ncreated on the identiﬁed objects, which is related to\nhow well the Mask RCNN segment objects.\nPublic datasets\nAs for current dataset resources, the Ecole\nCentrale Paris (ECP) Facades dataset Teboul et\nal. (2010) and the eTRIMS Korc and Först-\nner (2009) database are the two most relevant public\nfacade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='facade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.\nThe ECP dataset consists of 104 annotated im-\nages which are rectiﬁed and cropped facades of\nHaussmannian style buildings in Paris. Images are\nannotated with seven classes (balcony, chimney,\ndoor, roof, shop, wall and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image\nthat consists of a colourmap (Fig. 5). The original\nannotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='annotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.\nThe eTRIMS consists of 60 annotated street\nview images but different from the ECP dataset\nthe images are not rectiﬁed and the facades are of\nvarious architectural styles. Images are annotated\nwith eight classes (sky, building, door, vegetation,\ncar, road, pavement and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image that\nconsists of an array and a color-map matrix (Fig.\n6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to\ndetect windows in real street view images because\n6\nFigure 5. Example from ECP dataset, a) the input\nimage b) the annotation\n(a)\n(b)\nNote. Image from (Teboul et al. (2010))\nFigure 6. Example from eTRIMS, (a) the input\nimage (b) the annotation\n(a)\n(b)\nNote. In the middle part of (b), the upper window\nboundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='boundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).\nof multiple reasons. The ECP dataset only contains\nfacades that are manually rectiﬁed and viewed in a\nfront-parallel direction, thus using it as training data\nwould not be wise since it is not highly representa-\ntive of real street view images, which is the imagery\nwe want to detect windows instances in. However,\nthe most important reasons were that we found both\nto be imprecise or even wrongly annotated regarding\nwhat exactly deﬁnes the window boundaries. For\nexample, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='example, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior\ncasing (See Figure 6).\nWe searched and found a few more public\ndatasets that contained window annotations but none\nwere of very high quality and many had the same\nissues as mentioned above. Therefore we had to take\na decision to build a quality dataset from scratch.\nOur dataset\nWe decided to use Google Street View images\n(Map data ©2020 Google) of facades mainly from\ncentral Gothenburg, Sweden. The cropped images\nwere manually collected with an intention to con-\ntain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.\nWe annotated them manually with the VGG\nImage Annotator (VIA) tool A. Dutta and Zisser-\nman (2019). There exists a lot of other tools to\nannotate images, e.g. LabelMe, RectLabel, Label-\nBox and COCO UI, but we chose VIA because\nof its efﬁciency and simplicity. For example, it\nruns in most modern web browsers so it does not\nrequire any other installation or setup. Regarding\nthe annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others\nstore them in XML-format, and so on. The VIA tool\nsaves its annotations in a JSON-format ﬁle where\neach mask is a set of polygon points (Figure 7).\nSince we tried to be consistent to follow each\nwindow’s exact window frame and not to include\nother window parts as window boundaries it made\nannotation a very time-consuming and complex\ntask, especially as speciﬁc decisions had to be taken\nfor each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='for each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user\ninterface and ﬁgured out most of the different win-\n7\nFigure 7. Two examples from images in our dataset\nannotated with the VIA tool\n(a)\n(b)\nNote. The yellow lines are the manually annotated\npolygons which deﬁne each window boundary.\ndow types and their frames, we were annotating one\nwindow about every 5-10 seconds. However, there\nwere a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='were a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended\nup with a collection of 100 images that contained\n1540 annotated windows (an average of 15,4 win-\ndows per image). We then divide our dataset into a\ntraining set (80 images) and a validation set (20 im-\nages), i.e. we split the generated JSON-ﬁle into two\ndifferent ﬁles (Figure 8) with their corresponding\nimages.\nOptimization Methods\nTo use the Mask R-CNN framework we used\nan open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='an open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by\nAbdulla (2017). Then we hosted the implementation\non the Google Colab platform for free GPU usage\nand easy of shareability amongst other positive\nfactors.\nAssume to start with the structure of the Mask\nR-CNN as described in the paper He et al. (2017)\nand the source code provided in Abdulla (2017), we\nneeded to investigate the similarities and differences\nbetween the Matterport implementation running on\nKeras and TensorFlow and the ofﬁcial paper imple-\nFigure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Figure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the\ntraining set and the right to the validation set.\nBoth are viewed with JSON Editor Online.\nmentation build on the Caffe deep learning frame-\nwork.\nFrom our understanding, the implementation fol-\nlowed the paper with only a few exceptions and\nthose were done mostly for code simplicity and\ngeneralization.\nFor example, to support training multiple images\nper batch, the Matterport implementation resizes all\nimages to the same size and preserve the aspect ratio\nby padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='by padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-\ntations they chose to generate bounding boxes in the\ncode on the ﬂy, thus making image augmentations\neasier to apply. The downside is a slight decrease in\nbounding box placement accuracy, about only 2%\nof bounding boxes differed by 1px or more.\nThe paper uses a more aggressive learning rate of\n0.02, but for the Matterport implementation that was\ntoo high and causes the weights to explode in Ten-\nsorFlow as it computes gradients differently from\nCaffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Caffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between\nMatterport’s and the paper’s features, parameters,\nhyper-parameters and so on, we could start to\noptimize our model. The optimization adjustments\nand modiﬁcations were updated continuously during\n8\nthe whole process, but we can perhaps give some\nintuition on how they were done by describing them\nas iteratively separate steps. For example, optimiza-\ntion of the network structure with its corresponding\nconﬁguration can be considered ’low level’, while\noptimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='optimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to\nadjust our result within the context of our research\nproblem or goal. Furthermore, each step often\nneeded distinct conﬁguration adjustment depending\non which model weights and dataset changes were\nused, i.e. training resumed with COCO-weights,\naugmentation ﬂip, augmentation afﬁne and so on,\nall needed different conﬁgurations to train properly.\nFigure 9. Examples of the many conﬁgurations\noptions in the Matterport implementation\nNote. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and\nFigure 12).\n4. EXPERIMENTS\nIn this section, we will ﬁrst present some metrics\nused in our experiments, then give insight into the\nthought process of some of our structure and con-\nﬁguration optimization, training optimization and\ninference optimization. Finally, we will present our\nexperimental settings and some of the best results\nwe achieved.\nMetrics\nTo use the networks inference results for detect-\ning windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ing windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly\ndetected windows. In other words, recall is a crucial\nvalue to improve. To evaluate the images overall\ndetection results, we ﬁrst use IoU (Intersection\nover Union) to measure how much our predicted\nboundary overlaps with the ground truth (the real\nobject boundary). A predeﬁned IoU threshold of 0.5\nor greater is used for classifying the prediction as a\ntrue positive otherwise a false positive.\nFigure 10. Examples of IoU.\nNote. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence\nscore and measured IoU.\nThen we use the typical precision and recall rate:\nPrecision =\nTP\nTP + FP\n(1)\nRecall =\nTP\nTP + FN\n(2)\nwhere, TP, FP, FN denotes the true positive, false\npositive and false negative, respectively.\nFinally, we use the mean Average Precision\n(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR\n(3)\nwhere the P represent the precision rate and and R\nthe recall rate.\nStructure and conﬁguration optimization\nOverall, we found it a challenging task to test\nand tune the many parameters, mainly due to the\n9\nadaptive process of such a large search space, the\ncontinuous updates to our training data, but also the\ntime it takes to train and do inference to calculate\nthe accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-\nurations of the neural network, for example, we\ninvestigated Mask R-CNN in combination with both\nthe ResNet50 and the ResNet101 Backbone as\ndescribed in He et al. (2017). Those experiments\nshowed that the latter often yielded better results.\nWe kept the base conﬁguration of input images of\nsize 1024x1024 px for best accuracy even if some\nof our images were a bit smaller since the model\nresized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.\nWe found the gradient clipping norm to be ap-\npropriately set a 5 with a learning rate at 0.001 and\nmomentum of 0.9 and so on.\nBy plotting each test we could observe that train-\ning and validation losses indicated overﬁtting, i.e.\nwe got an increasingly lower training loss but the\nvalidation loss at certain points started to ﬂuctuate\nor stagnate (Figure 11).\nFigure 11. Examples of plots from our training\nNote. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with\nweight decay and dropout amongst other things. Un-\nfortunately, GridSearch with cross-validation, to au-\ntomatically search for the optimal hyper-parameter\nvalues, required more computational power than we\nhad access to, but we believe a valid option given\nthe appropriate resources.\nAnother option and the most intuitive, to prevent\noverﬁtting is often to extend the dataset. However,\nthis was neither feasible given the remaining project\ntime and also did not ﬁt well with our interest in\ninvestigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='investigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is\na resampling procedure used to evaluate machine\nlearning models on a limited data sample, without\nany major improvements on our test set in terms of\naccuracy.\nTraining optimization\nFor the training optimization we assume the given\nMask-RCNN loss function as:\nL = αL1 + βL2 + γL3 + δL4 + ϵL5\n(4)\nwhere the ﬁrst loss is the RPN class loss and\nthe second the RPN bounding box prediction loss.\nThe other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='The other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization\ngoal is to ﬁnd a corresponding conﬁguration such\nthat it minimizes the overall loss. This is an adaptive\nprocess of matching the results of the loss function\nwith the given dataset and the parameters as a result\nof a trial process.\nGiven a loss Li and equation (4), optimize K =\n(α, β, γ, δ, ϵ) ∈ R5 and L1, L2, L3, L4, L5 loss func-\ntions so that the average precision of our model so\nthat the average precision of our model is improved\nwith a threshold of AP50 :\nmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='that the average precision of our model is improved\nwith a threshold of AP50 :\nmax\nK,L1,L2,L3,L4,L5 AP50\n(5)\nUltimately, the ﬁnal choice of parameters chosen\ndepends on the structure conﬁgurations discussed\nin the previous section and also on our input data\n. While technically for K any combination of real\nvalues can be combined:\n(α, β, γ, δ, ϵ) ∼ n.(α, β, γ, δ, ϵ), ∀n ∈ R\n(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10\non the loss gradient and not the loss value. Further-\nmore, several other conﬁguration parameters need\nto be tested and changed to improve accuracy. The\npractical approach for tuning loss weights was to\nstart with a default weight of 1 for each loss and\nevaluate the model on the validation set by inferring\nthe model performance image by image. We had to\nevaluate the true positives, false positives, and false\nnegatives combined with IoU of their masks and\ntheir score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='their score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In\nother words, we are using transfer learning, which\nmeans we don’t need to train a new model from\nscratch and that instead utilize a lot of the already\nlearned features from the COCO dataset, which\ncontains around 120K images. After the training for\n40 epochs with k-fold cross-validation have been\ncompleted, we save the model’s parameters and\nevaluate the new weights (one for each epoch is\ngenerated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='generated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-\ntating new images would take too long. Since the\npolygons are calculated internally this was easy to\nimplement with the imgaug library.\nFirst, we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith augmentation that ﬂip/mirror each image in\nthe training set horizontally right/left. Then we\nagain save the model’s parameters and evaluate the\nperformance of the new weights.\nThen we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Then we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the\ntraining set by rotation (-45, 45) and shearing (-16,\n16). Finally, we save the model’s parameters and\nevaluate the performance of the new weights with\nrunning inference on the weights that yielded the\nlowest training loss.\nInference optimization\nIn the process of optimizing the trained network,\nwe constantly adjusted and modiﬁed the training set\nand other parameters. Since the output of the infer-\nence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust\naccordingly with regards to the properties of the test\nset images and the overall accuracy. We found a\ndetection minimum conﬁdence of 0.9 to yield as\ngood trade-off of the precision and recall values for\nour windows.\n5. RESULTS\nWe perform our test result experiments on the\neTrims dataset and not on the ECP dataset. The\nreason being that the ECP dataset doesn’t contain\nstreet-view images and the images are all manually\nrectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='rectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.\nHowever, to properly use the eTRIMS dataset\nwith our model, we had to reﬁne their annotation\nto overcome inaccuracies and mistakes, and also\naccommodate our deﬁned window boundary which\nfollows the window frame. We, therefore, annotated\n10 randomly chosen images for our test set to\nevaluate our model’s performances. Therefore, any\ncomparisons should note these differences.\nBoth the training and inference was done on\nGoogle Colab with the free Tesla K80 GPU of about\n12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of\nthose we present the best three models with regards\nto the highest mAP on our test set (Table 1) and\nfor visualization we present the inference done on\nthe model achieving the highest mAP below (Figure\n12).\n11\nFigure 12.\nWindow detection results from a model trained on coco-weights, then resumed with ﬂiplr.\nImage\nGround truth\nPrediction\nOverlap\ngreen colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='green colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score\ncaptions: score/IoU\n12\nREFERENCES\nREFERENCES\nTable 1.\nA comparison of pixel accuracies on the eTRIMS\ndataset. Accuracies are shown in percentage.\nClass\n[1]\n[2]\n[3]\n[4]\n[5]\nOur*\nOur**\nOur***\nWindow[%] 75\n73\n71\n86', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Our***\nWindow[%] 75\n73\n71\n86\n90.91 91.93\n94.55\n94.76\nNote: Our result is only from 10 randomly chosen images from the eTRIMS\ndataset with reﬁned annotation.\nmodel trained on coco-weights then resumed with ﬂiplr and afﬁne using\nk-fold cross-validation (the resumed weights were chosen by the folds\nhighest mAP)\n** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)\n*** model trained on coco-weights then resumed with ﬂiplr (the resumed\nweights were chosen by the lowest training loss)\n6. CONCLUSIONS\nIn this paper, we propose to use the state-of-\nthe-art Mask R-CNN framework He et al. (2017)\nas an end-to-end network capable of instance seg-\nmentation without further machinery. Previous fa-\ncade parsing detection work often only produced\nsemantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='semantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is\ncapable of detecting and producing a bounding box\nand segmentation mask for each window instance.\nExperimental results show that our suggested\napproach can with only a relatively small but precise\ndataset train the network—only with transfer learn-\ning and augmentation—to produce instance segmen-\ntation of windows and a comparable result with\nprior state-of-the-art window detection approaches,\neven without pre- or post-optimization techniques.\nAs far as we know, we are the ﬁrst to have anno-\ntated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window\nboundary and not to include other window parts. We\nbelieve that our consequent and precise annotation\nof windows helped our model training. We also\nhope it can beneﬁt the research community as a free\nresource for more precise window detection.\n7. FUTURE WORK\nIn this paper, we have got a sense of how to tune\nthe hyper-parameters effectively with our training\ndata. However, further work is recommended to\ninvestigate this with platforms, such as Wandb or\nOnepanel Jobs for easier coordination, synchroniza-\ntion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types\nwere the model has low accuracy.\nFurther work could also beneﬁt from training\nmore facade classes, such as building, balcony,\ndoors, chimney, to reduce false positives, false neg-\natives. The same goes for the window class, i.e.\nsplitting it into distinct window type classes, such\nas double-hung, transom etc. A model capable of\na more complete facade parsing we believe will\nalso make application easier as the model has a\nﬁner granularity of the facades properties. A caution\nis that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='is that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of\nevery facade structure.\nTo further reﬁne the results and perhaps mitigate\nsome of the typical limitations we experienced,\nsuch as dealing with occlusion, overconﬁdent false\npositives predictions, occasional mask overlaps, fu-\nture work should investigate both in pre- and post-\noptimization methods of previous approaches or\ntesting their novel approaches. Also, code refactor-\ning and optimization, e.g. rewriting some Python\ncode in TensorFlow to bring all the beneﬁts of the\nTensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS\nThis paper was the ﬁnal written report in the\ncourse DIT891 Project in Data Science. The course\nis offered within the Applied Data Science Master’s\nProgramme by the University of Gothenburg. We\nwould like to thank Shirin Tavara, teacher and the\ncourse responsible and Richard Johansson, teacher.\nA special thanks goes to our supervisors Alexan-\nder Hollberg, Sanjay Somanath and Yinan Yu for\ntheir congeniality and support.\n13\nREFERENCES\nREFERENCES\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='13\nREFERENCES\nREFERENCES\nREFERENCES\nAbdulla, Waleed (2017). “Mask R-CNN for object\ndetection and instance segmentation on Keras and\nTensorFlow https://github. com/matterport”. In:\nMask _ RCNN.\nAbdulla, Waleed (2018). Instance Segmentation\nwith Mask R-CNN and TensorFlow. https : / /\nengineering . matterport . com / splash - of - color -\ninstance- segmentation- with- mask- r- cnn- and-\ntensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia\nRagia (2020). “3D building façade reconstruction\nusing deep learning”. In: ISPRS International\nJournal of Geo-Information 9(5), p. 322.\nBadrinarayanan,\nVijay,\nAlex\nKendall,\nRoberto\nCipolla, et al. (2015). “1SegNet: A Deep Convo-\nlutional Encoder-Decoder Architecture for Image\nSegmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Segmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:\nInternational\nJournal\nof\nAdvanced\nResearch\nin Electronics and Communication Engineering\n(IJARECE) Volume 5, pp. 1448–1454.\nChen, Liang-Chieh et al. (2017). “Deeplab: Seman-\ntic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected\ncrfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.\nDutta, Abhishek and Andrew Zisserman (2019).\n“The VIA annotation software for images, audio\nand video”. In: Proceedings of the 27th ACM in-\nternational conference on multimedia, pp. 2276–\n2279.\nFathalla, Radwa and George Vogiatzis (2017). “A\ndeep learning pipeline for semantic facade seg-\nmentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer\nvision, pp. 1440–1448.\nHan, Feng and Song-Chun Zhu (2008). “Bottom-\nup/top-down image parsing with attribute gram-\nmar”. In: IEEE transactions on pattern analysis\nand machine intelligence 31(1), pp. 59–73.\nHe, Kaiming et al. (2017). “Mask r-cnn”. In: Pro-\nceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.\nHu, Han et al. (2020). “Fast and Regularized Re-\nconstruction of Building Fa\\c {c} ades from\nStreet-View Images using Binary Integer Pro-\ngramming”. In: arXiv preprint arXiv:2002.08549.\nKarmarkar, T (2018). “Regional Proposal network\n(RPN)—Backbone of Faster R-CNN”. In: Aug\n18, p. 6.\nKoch, David et al. (2018). “Visual estimation of\nbuilding condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='building condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on\nMultimedia for Real Estate Tech, pp. 12–17.\nKorc,\nFilip\nand\nWolfgang\nFörstner\n(2009).\n“eTRIMS\nImage\nDatabase\nfor\ninterpreting\nimages of man-made scenes”. In: Dept. of\nPhotogrammetry, University of Bonn, Tech. Rep.\nTR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep\nlearning approach to facade parsing”. In.\nLong, Jonathan, Evan Shelhamer, and Trevor Dar-\nrell (2015). “Fully convolutional networks for\nsemantic segmentation”. In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pp. 3431–3440.\nMa, Wenguang and Wei Ma (2020). “Deep window\ndetection in street scenes”. In: KSII Transactions\non Internet and Information Systems (TIIS) 14(2),\npp. 855–870.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='on Internet and Information Systems (TIIS) 14(2),\npp. 855–870.\nMathias, Markus et al. (2011). “Automatic architec-\ntural style recognition”. In: ISPRS-International\nArchives of the Photogrammetry, Remote Sensing\nand Spatial Information Sciences 3816, pp. 171–\n176.\nMüller, Pascal et al. (2007). “Image-based procedu-\nral modeling of facades”. In: ACM Trans. Graph.\n26(3), p. 85.\nNogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Nogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting\nconvolutional neural networks for remote sensing\nscene classiﬁcation”. In: Pattern Recognition 61,\npp. 539–556.\nParthasarathy,\nDhruv\n(2017).\n“A\nbrief\nhistory\nof CNNs in image segmentation: From R-\n14\nCNN to Mask R-CNN”. In: Available online:\nblog.\nathelas.\ncom/a-brief-history-of-cnns-in-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='blog.\nathelas.\ncom/a-brief-history-of-cnns-in-\nimage-segmentation-from-r-cnn-to-mask-rcnn-\n34ea83205de4 (accessed on 24 February 2019).\nPenatti, Otávio AB, Keiller Nogueira, and Jefersson\nA Dos Santos (2015). “Do deep features general-\nize from everyday objects to remote sensing and\naerial scenes domains?” In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition workshops, pp. 44–51.\nRahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der\nBundeswehr München.” In.\nRecky, Michal and Franz Leberl (2010). “Windows\ndetection using k-means in cie-lab color space”.\nIn: 2010 20th International Conference on Pat-\ntern Recognition. IEEE, pp. 356–359.\nRen, Shaoqing et al. (2016). “Faster R-CNN: to-\nwards real-time object detection with region pro-\nposal networks”. In: IEEE transactions on pat-\ntern analysis and machine intelligence 39(6),\npp. 1137–1149.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tern analysis and machine intelligence 39(6),\npp. 1137–1149.\nSchmitz, Matthias and Helmut Mayer (2016). “A\nconvolutional network for semantic facade seg-\nmentation and interpretation”. In: The Interna-\ntional Archives of Photogrammetry, Remote Sens-\ning and Spatial Information Sciences 41, p. 709.\nSultana, F, A Suﬁan, and P Dutta (2020). “Evolution\nof Image Segmentation using Deep Convolutional\nNeural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Neural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.\nTeboul, Olivier et al. (2010). “Segmentation of\nbuilding facades using procedural shape priors”.\nIn: 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition. IEEE,\npp. 3105–3112.\nTu, Zhuowen et al. (2005). “Image parsing: Unify-\ning segmentation, detection, and recognition”. In:\nInternational Journal of computer vision 63(2),\npp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst\nBischof (2010). “Unsupervised facade segmenta-\ntion using repetitive patterns”. In: Joint Pattern\nRecognition Symposium. Springer, pp. 51–60.\nZhao, Peng et al. (2010). “Rectilinear parsing of ar-\nchitecture in urban environment”. In: 2010 IEEE\nComputer Society Conference on Computer Vi-\nsion and Pattern Recognition. IEEE, pp. 342–349.\nZhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Zhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-\nject recognition by data driven Markov chain\nMonte Carlo”. In: Proceedings IEEE Conference\non Computer Vision and Pattern Recognition.\nCVPR 2000 (Cat. No. PR00662). Vol. 1. IEEE,\npp. 738–745.\n15', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyccd_2uw/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'})]
cuda:2
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmps483xnja, tmps483xnja
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90c36ef090> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmps483xnja/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
cuda:2
[UploadFile(filename='None.txt', size=6808, headers=Headers({'content-disposition': 'form-data; name="files"; filename="None.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpo80gwju9, tmpo80gwju9
File: None.txt, msg: 成功上传文件 None.txt, docs: [Document(page_content='TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='In recent years, multimodal large language models (MLLMs) such as GPT-4V havedemonstrated remarkable advancements, excelling in a variety of vision-languagetasks. Despite their prowess, the closed-source nature and computationaldemands of such models limit their accessibility and applicability. This studyintroduces TinyGPT-V, a novel open-source MLLM, designed for efficient trainingand inference across various vision-language tasks, including image captioning(IC) and visual question answering (VQA). Leveraging a compact yet powerfularchitecture, TinyGPT-V integrates the Phi-2 language model with pre-trainedvision encoders, utilizing a unique mapping module for visual and linguisticinformation fusion. With a training regimen optimized for small backbones andemploying a diverse dataset amalgam, TinyGPT-V requires significantly lowercomputational resources 24GB for training and as little as 8GB for inferencewithout compromising on performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='Our experiments demonstrate thatTinyGPT-V, with its language model 2.8 billion parameters, achieves comparableresults in VQA and image inference tasks to its larger counterparts while beinguniquely suited for deployment on resource-constrained devices throughinnovative quantization techniques. This work not only paves the way for moreaccessible and efficient MLLMs but also underscores the potential of smaller,optimized models in bridging the gap between high performance and computationalefficiency in real-world applications.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='Additionally, this paper introduces anew approach to multimodal large language models using smaller backbones. Ourcode and training weights are available in\\url{https://github.com/DLYuanGod/TinyGPT-V}.\nSCP: Scene Completion Pre-training for 3D Object Detection', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='3D object detection using LiDAR point clouds is a fundamental task in thefields of computer vision, robotics, and autonomous driving. However, existing3D detectors heavily rely on annotated datasets, which are both time-consumingand prone to errors during the process of labeling 3D bounding boxes. In thispaper, we propose a Scene Completion Pre-training (SCP) method to enhance theperformance of 3D object detectors with less labeled data. SCP offers three keyadvantages: (1) Improved initialization of the point cloud model. By completingthe scene point clouds, SCP effectively captures the spatial and semanticrelationships among objects within urban environments. (2) Elimination of theneed for additional datasets. SCP serves as a valuable auxiliary network thatdoes not impose any additional efforts or data requirements on the 3Ddetectors. (3) Reduction of the amount of labeled data for detection. With thehelp of SCP, the existing state-of-the-art 3D detectors can achieve comparableperformance while only relying on 20% labeled data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='ThreshNet: An Efficient DenseNet Using Threshold Mechanism to Reduce  Connections', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='With the continuous development of neural networks for computer vision tasks,more and more network architectures have achieved outstanding success. As oneof the most advanced neural network architectures, DenseNet shortcuts allfeature maps to solve the model depth problem. Although this networkarchitecture has excellent accuracy with low parameters, it requires anexcessive inference time. To solve this problem, HarDNet reduces theconnections between the feature maps, making the remaining connections resembleharmonic waves. However, this compression method may result in a decrease inthe model accuracy and an increase in the parameters and model size. Thisnetwork architecture may reduce the memory access time, but its overallperformance can still be improved. Therefore, we propose a new networkarchitecture, ThreshNet, using a threshold mechanism to further optimize theconnection method. Different numbers of connections for different convolutionlayers are discarded to accelerate the inference of the network.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='Different numbers of connections for different convolutionlayers are discarded to accelerate the inference of the network. The proposednetwork has been evaluated with image classification using CIFAR 10 and SVHNdatasets under platforms of NVIDIA RTX 3050 and Raspberry Pi 4. Theexperimental results show that, compared with HarDNet68, GhostNet, MobileNetV2,ShuffleNet, and EfficientNet, the inference time of the proposed ThreshNet79 is5%, 9%, 10%, 18%, and 20% faster, respectively. The number of parameters ofThreshNet95 is 55% less than that of HarDNet85.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='The new model compression andmodel acceleration methods can speed up the inference time, enabling networkmodels to operate on mobile devices.\nToken Turing Machines', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content="Token Turing Machines\nWe propose Token Turing Machines (TTM), a sequential, autoregressiveTransformer model with memory for real-world sequential visual understanding.Our model is inspired by the seminal Neural Turing Machine, and has an externalmemory consisting of a set of tokens which summarise the previous history(i.e., frames). This memory is efficiently addressed, read and written using aTransformer as the processing unit/controller at each step. The model's memorymodule ensures that a new observation will only be processed with the contentsof the memory (and not the entire history), meaning that it can efficientlyprocess long sequences with a bounded computational cost at each step. We showthat TTM outperforms other alternatives, such as other Transformer modelsdesigned for long sequences and recurrent neural networks, on two real-worldsequential visual understanding tasks: online temporal activity detection fromvideos and vision-based robot action policy learning. Code is publicly available at:https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers\nAs transformers are equivariant to the permutation of input tokens, encodingthe positional information of tokens is necessary for many tasks. However,since existing positional encoding schemes have been initially designed for NLPtasks, their suitability for vision tasks, which typically exhibit differentstructural properties in their data, is questionable. We argue that existingpositional encoding schemes are suboptimal for 3D vision tasks, as they do notrespect their underlying 3D geometric structure. Based on this hypothesis, wepropose a geometry-aware attention mechanism that encodes the geometricstructure of tokens as relative transformation determined by the geometricrelationship between queries and key-value pairs. By evaluating on multiplenovel view synthesis (NVS) datasets in the sparse wide-baseline multi-viewsetting, we show that our attention, called Geometric Transform Attention(GTA), improves learning efficiency and performance of state-of-the-arttransformer-based NVS models without any additional learned parameters and onlyminor computational overhead.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f90ca5e47d0> 111
cuda:2
[Document(page_content='TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='In recent years, multimodal large language models (MLLMs) such as GPT-4V havedemonstrated remarkable advancements, excelling in a variety of vision-languagetasks. Despite their prowess, the closed-source nature and computationaldemands of such models limit their accessibility and applicability. This studyintroduces TinyGPT-V, a novel open-source MLLM, designed for efficient trainingand inference across various vision-language tasks, including image captioning(IC) and visual question answering (VQA). Leveraging a compact yet powerfularchitecture, TinyGPT-V integrates the Phi-2 language model with pre-trainedvision encoders, utilizing a unique mapping module for visual and linguisticinformation fusion. With a training regimen optimized for small backbones andemploying a diverse dataset amalgam, TinyGPT-V requires significantly lowercomputational resources 24GB for training and as little as 8GB for inferencewithout compromising on performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='Our experiments demonstrate thatTinyGPT-V, with its language model 2.8 billion parameters, achieves comparableresults in VQA and image inference tasks to its larger counterparts while beinguniquely suited for deployment on resource-constrained devices throughinnovative quantization techniques. This work not only paves the way for moreaccessible and efficient MLLMs but also underscores the potential of smaller,optimized models in bridging the gap between high performance and computationalefficiency in real-world applications.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='Additionally, this paper introduces anew approach to multimodal large language models using smaller backbones. Ourcode and training weights are available in\\url{https://github.com/DLYuanGod/TinyGPT-V}.\nSCP: Scene Completion Pre-training for 3D Object Detection', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='3D object detection using LiDAR point clouds is a fundamental task in thefields of computer vision, robotics, and autonomous driving. However, existing3D detectors heavily rely on annotated datasets, which are both time-consumingand prone to errors during the process of labeling 3D bounding boxes. In thispaper, we propose a Scene Completion Pre-training (SCP) method to enhance theperformance of 3D object detectors with less labeled data. SCP offers three keyadvantages: (1) Improved initialization of the point cloud model. By completingthe scene point clouds, SCP effectively captures the spatial and semanticrelationships among objects within urban environments. (2) Elimination of theneed for additional datasets. SCP serves as a valuable auxiliary network thatdoes not impose any additional efforts or data requirements on the 3Ddetectors. (3) Reduction of the amount of labeled data for detection. With thehelp of SCP, the existing state-of-the-art 3D detectors can achieve comparableperformance while only relying on 20% labeled data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='ThreshNet: An Efficient DenseNet Using Threshold Mechanism to Reduce  Connections', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='With the continuous development of neural networks for computer vision tasks,more and more network architectures have achieved outstanding success. As oneof the most advanced neural network architectures, DenseNet shortcuts allfeature maps to solve the model depth problem. Although this networkarchitecture has excellent accuracy with low parameters, it requires anexcessive inference time. To solve this problem, HarDNet reduces theconnections between the feature maps, making the remaining connections resembleharmonic waves. However, this compression method may result in a decrease inthe model accuracy and an increase in the parameters and model size. Thisnetwork architecture may reduce the memory access time, but its overallperformance can still be improved. Therefore, we propose a new networkarchitecture, ThreshNet, using a threshold mechanism to further optimize theconnection method. Different numbers of connections for different convolutionlayers are discarded to accelerate the inference of the network.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='Different numbers of connections for different convolutionlayers are discarded to accelerate the inference of the network. The proposednetwork has been evaluated with image classification using CIFAR 10 and SVHNdatasets under platforms of NVIDIA RTX 3050 and Raspberry Pi 4. Theexperimental results show that, compared with HarDNet68, GhostNet, MobileNetV2,ShuffleNet, and EfficientNet, the inference time of the proposed ThreshNet79 is5%, 9%, 10%, 18%, and 20% faster, respectively. The number of parameters ofThreshNet95 is 55% less than that of HarDNet85.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='The new model compression andmodel acceleration methods can speed up the inference time, enabling networkmodels to operate on mobile devices.\nToken Turing Machines', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content="Token Turing Machines\nWe propose Token Turing Machines (TTM), a sequential, autoregressiveTransformer model with memory for real-world sequential visual understanding.Our model is inspired by the seminal Neural Turing Machine, and has an externalmemory consisting of a set of tokens which summarise the previous history(i.e., frames). This memory is efficiently addressed, read and written using aTransformer as the processing unit/controller at each step. The model's memorymodule ensures that a new observation will only be processed with the contentsof the memory (and not the entire history), meaning that it can efficientlyprocess long sequences with a bounded computational cost at each step. We showthat TTM outperforms other alternatives, such as other Transformer modelsdesigned for long sequences and recurrent neural networks, on two real-worldsequential visual understanding tasks: online temporal activity detection fromvideos and vision-based robot action policy learning. Code is publicly available at:https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'}), Document(page_content='GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers\nAs transformers are equivariant to the permutation of input tokens, encodingthe positional information of tokens is necessary for many tasks. However,since existing positional encoding schemes have been initially designed for NLPtasks, their suitability for vision tasks, which typically exhibit differentstructural properties in their data, is questionable. We argue that existingpositional encoding schemes are suboptimal for 3D vision tasks, as they do notrespect their underlying 3D geometric structure. Based on this hypothesis, wepropose a geometry-aware attention mechanism that encodes the geometricstructure of tokens as relative transformation determined by the geometricrelationship between queries and key-value pairs. By evaluating on multiplenovel view synthesis (NVS) datasets in the sparse wide-baseline multi-viewsetting, we show that our attention, called Geometric Transform Attention(GTA), improves learning efficiency and performance of state-of-the-arttransformer-based NVS models without any additional learned parameters and onlyminor computational overhead.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpo80gwju9/None.txt'})]
cuda:2
[UploadFile(filename='tmp_kb.txt', size=2393, headers=Headers({'content-disposition': 'form-data; name="files"; filename="tmp_kb.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmptwk6ktti, tmptwk6ktti
File: tmp_kb.txt, msg: 成功上传文件 tmp_kb.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptwk6ktti/tmp_kb.txt'}), Document(page_content='In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction. Automatic Infectious Disease Classification Analysis with Concept  Discovery Automatic infectious disease classification from images can facilitate neededmedical diagnoses.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptwk6ktti/tmp_kb.txt'}), Document(page_content='Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptwk6ktti/tmp_kb.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd58f3d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptwk6ktti/tmp_kb.txt'}), Document(page_content='In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction. Automatic Infectious Disease Classification Analysis with Concept  Discovery Automatic infectious disease classification from images can facilitate neededmedical diagnoses.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptwk6ktti/tmp_kb.txt'}), Document(page_content='Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptwk6ktti/tmp_kb.txt'})]
cuda:2
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp_lo93hw5, tmp_lo93hw5
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8ff42f54d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_lo93hw5/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
[UploadFile(filename='Quantization of Deep Neural Networks for Accurate Edge Computing.pdf', size=893520, headers=Headers({'content-disposition': 'form-data; name="files"; filename="Quantization of Deep Neural Networks for Accurate Edge Computing.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpu6k23zja, tmpu6k23zja
File: Quantization of Deep Neural Networks for Accurate Edge Computing.pdf, msg: 成功上传文件 Quantization of Deep Neural Networks for Accurate Edge Computing.pdf, docs: [Document(page_content='1\nQuantization of Deep Neural Networks for Accurate Edge\nComputing\nWENTAO CHEN∗, HAILONG QIU∗, and JIAN ZHUANG, Guangdong Cardiovascular Institute,\nGuangdong Provincial Key Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s\nHospital, Guangdong Academy of Medical Sciences\nCHUTONG ZHANG∗ and YU HU, Easylink Technology Co., Ltd\nQING LU, TIANCHEN WANG, and YIYU SHI†, Department of Computer Science and Engineering,\nUniversity of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='University of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong\nAcademy of Medical Sciences\nDeep neural networks (DNNs) have demonstrated their great potential in recent years, exceeding the per-\nformance of human experts in a wide range of applications. Due to their large sizes, however, compression\ntechniques such as weight quantization and pruning are usually applied before they can be accommodated on\nthe edge. It is generally believed that quantization leads to performance degradation, and plenty of existing\nworks have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='works have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to\nimprove accuracy. We conduct comprehensive experiments on three widely used applications: fully con-\nnected network (FCN) for biomedical image segmentation, convolutional neural network (CNN) for image\nclassification on ImageNet, and recurrent neural network (RNN) for automatic speech recognition, and experi-\nmental results show that quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three applications\nrespectively with 3.5x-6.4x memory reduction.\nCCS Concepts: • Computing methodologies → Artificial intelligence; • Hardware → Emerging tech-\nnologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='nologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization\n∗Both authors contributed equally to this research. Wentao Chen works at Easylink Technology Co., Ltd, and this work is\ndone when he was an visiting scholar at Guangdong Provincial People’s Hospital.\n†Corresponding authors.\nAuthors’ addresses: Wentao Chen; Hailong Qiu; Jian Zhuang, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong Academy\nof Medical Sciences, 106 Zhongshan Second Road, Guangzhou, Guangdong, 510080, chenwentaokl@gmail.com; Chutong', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Zhang; Yu Hu, Easylink Technology Co., Ltd, Wuhan, Hubei, 43000; Qing Lu; Tianchen Wang; Yiyu Shi, yshi4@nd.edu,\nDepartment of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, 46556; Meiping Huang;\nXiaowe Xu, Guangdong Cardiovascular Institute, Guangdong Provincial Key Laboratory of South China Structural Heart\nDisease, Guangdong Provincial People’s Hospital, Guangdong Academy of Medical Sciences, 106 Zhongshan Second Road,\nGuangzhou, Guangdong, 510080, xiao.wei.xu@foxmail.com.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2021 Association for Computing Machinery.\n1550-4832/2021/1-ART1 $15.00\nhttps://doi.org/10.1145/3451211', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='https://doi.org/10.1145/3451211\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\narXiv:2104.12046v2  [cs.CV]  14 Oct 2021\n1:2\nW. Chen and C. Zhang, et al.\nACM Reference Format:\nWentao Chen, Hailong Qiu, Jian Zhuang, Chutong Zhang, Yu Hu, Qing Lu, Tianchen Wang, Yiyu Shi, Meiping\nHuang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Huang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.\nEmerg. Technol. Comput. Syst. 1, 1, Article 1 (January 2021), 11 pages. https://doi.org/10.1145/3451211\n1\nINTRODUCTION\nDeep Neural Networks (DNNs) have been widely used in various applications and show its great\npotential to tackle complex problems [1–3, 19, 26, 32]. Furthermore, it has been and continue to\nbe instrumental in enabling/advancing breakthroughs in various disciplines, including disease', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='be instrumental in enabling/advancing breakthroughs in various disciplines, including disease\ndiagnosis, real-time language translation, autonomous driving, etc. [8, 9, 15, 20, 22, 25, 30, 31, 33, 33].\nMeanwhile, edge computing for Internet of Things (IoT) has been widely studied which requires\nDNN models with small memory size and efficient computation [17, 27, 28]. Thus, there is a huge\ngap between current DNN models and the requirenment of edge computing.\nRecently, in order to accommodate DNNs on the edge, DNN quantization has become an active\nresearch topic [4, 5, 7, 11, 13, 13, 16, 18, 34–36], which aims to represent DNN weights with less', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='memory (precision) while maintaining acceptable accuracy with efficient memory and computation\ncosts. It has been observed in the literature, however, that sometimes quantization can improve\naccuracy which can be credited to the reduction of overfitting [23]. Dynamic fixed point [29]\ncan achieve 4x less memory operation cost with only 0.4-0.6% Top-5 accuracy loss for ImageNet\nclassification [6]. Ternary weight network and binaryConnect [29] have further reduced the bit-\nwidth of weights to 2 bits or even 1 bit with a relatively larger accuracy loss. Recently, their enhanced\nversion, trained ternary training and binary weight network [29] have reduced the accuracy loss to\nonly 0.6-0.8%. There also exists some works using non-linear quantization to represent the parameter\ndistribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='distribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not\nonly the weights but also the activations. Quantized neural networks, binarized neural networks,\nand XNOR-net [29] reduced the weights to only 1 bit and the activations to 1-2 bits resulting in\na large reduction on memory and computation cost yet with significant accuracy loss. Two-Step\nquantization [24] has 2% drop for VGG-16 on ImageNet-2012 dataset and Integer-Arithmetic-Only\nquantization[14] has 3.1% accuracy loss. In some of the above works, we notice that quantization\ncan sometimes improve the performance [29], however, there is no comprehensive studies to verify\nthis point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='this point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.\nFor example, two-step quantization [24] has a 2% accuracy drop on ILSVRC-2012 dataset and\nInteger-Arithmetic-Only quantization [14] has a 3.1% accuracy loss. Various works in the literature\nhave explored the best ways to quantize the weights with minimum accuracy loss. To some extent,\nquantization essentially imposes regularization on weight representations. As such, it should also\nsometimes help to improve accuracy when overfitting issue presents in large neural networks.\nTo demonstrate this point, in this paper, we conduct extensive experiments using incremental\nquantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='quantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed\nevaluations under the same hardware, deep learning frameworks and configurations. In medical\nimage segmentation, our method can achieve 1% accuracy improvement compared with the current\nstate-of-the-art method on the MICCIA Gland dataset. In image classification, extensive experiments\nare presented on ImageNet-2012 using a widely used CNN model VGG-16 [21], and the result shows\nthat our proposed method exceeds the current best performance using VGG-16 by up to 1.95%. In\nautomatic speech recognition, we quantize Deep Speech [12] network on the TIMIT dataset[10] and\nimproves the accuracy by 4.23%. We also discuss the incremental quantization on the performance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='improves the accuracy by 4.23%. We also discuss the incremental quantization on the performance\nof simplified network with different bit widths, and the experimental results show that incremental\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:3\nquantization can no longer improve the performance of simplified networks with less overfitting.\nIn addition, we get 3.5x-6.4x memory reduction, which is extremely beneficial in the context of\nedge computing.\nFCN\nFCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='FCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set\nSuggested Training Set\nSelect Representative\nbased on Uncertainty and\nSimilarity\nQuantization\nQuantization\nSegmentation FCN\nInference of Suggestive FCN\nTraining of Suggestive FCN\nNetwork Training\nWith Quantization\n(QNT)\nSuggestive Annotation with Quantization (QSA)\nFig. 1. Illustration of quantization framework based on the suggestive annotation framework. In suggestive\nannotation with quantization, better training samples (suggestive training set) can be extracted from the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='annotation with quantization, better training samples (suggestive training set) can be extracted from the\noriginal training set. In network training with quantization, better performance can be achieved by reduce\noverfitting.\n2\nPRELIMINARIES\nIn this section, we briefly review the incremental quantization method used in the experiments.\nThe main goal of incremental quantization is to convert 32-bit-floating-point weights 𝑊 into\nlow-precision weights ˆ𝑊 either power of two or zero with minimum accuracy loss. Each of ˆ𝑊 is\nchosen from 𝑃𝑙 = {±2𝑛1, · · ·, ±2𝑛2}, where 𝑛1 and 𝑛2 are two integer numbers determined by the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='max absolute value of 𝑊𝑙 and expected quantized bit-width, and 𝑛2 ≤ 𝑛1.\nThe training procedure of incremental quantization [34] is consisted of three operations: weight\npartition, group-wise quantization and re-training. Weights partition is the most crucial part of the\nwhole process. We adopt pruning-inspired partition strategy to divide weights into two disjoint\ngroups by comparing the absolute value with layer-wise threshold, which usually achieves better\nperformance than random partition ([34]).\nFor the 𝑙-th layer, we define weights partition as shown in Eq. (1), where 𝐴(1)\n𝑙\nis the first group\nweights that need to be quantized and 𝐴(2)\n𝑙', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='is the first group\nweights that need to be quantized and 𝐴(2)\n𝑙\ndenotes the second group with remaining unquantized\nvalues.\n𝐴(1)\n𝑙\n∪ 𝐴(2)\n𝑙\n= 𝑊𝑙 (𝑖, 𝑗)\n𝐴(1)\n𝑙\n∩ 𝐴(2)\n𝑙\n= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,\nthus they are quantized using Eq. (2),\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:4\nW. Chen and C. Zhang, et al.\n𝑤𝑞 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑝', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='𝑠𝑖𝑔𝑛(𝑤) × 2𝑝\n𝑖𝑓 3 × 2𝑝−2 ≤ |𝑤| ≤ 3 × 2𝑝−1;\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑚 𝑖𝑓 |𝑤| ≥ 2𝑢;\n0\n𝑖𝑓 |𝑤| < 2−𝑙−1\n(2)\nwhere 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='where 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds\nof the quantized set(𝑙 ≤ 𝑝 ≤ 𝑢).\nBy contrast, the second group remains floating-point values to compensate the accuracy loss in\nmodel, and will be re-trained. After one round, these three steps are further adopted only on the\nsecond group in the rest of the training process. As a result, all weights are quantized or to zeros\nhence we gain memory reduction with slight accuracy loss.\n3\nCASE STUDIES\n3.1\nBiomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Biomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit\nwidth on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + Quantized NT\n86.25\n65.55\n79.37\n85.87\n86.12\n86.33', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='79.37\n85.87\n86.12\n86.33\n86.51\n85.93\n85.26\nQuantized SA + Float NT\n86.25\n62.98\n85.39\n86.17\n86.52\n86.66\n86.51\n86.22\n86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the\nMICCAI Gland dataset.\nConfigurations\nParallel number\nOrignal\n2\n3\n4\n5\n6\n7\nFloat SA + Float NT\n86.25\n86.23\n85.93\n85.81\n86.24\n87.25\n86.32\nFloat SA + 7 bits NT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='86.24\n87.25\n86.32\nFloat SA + 7 bits NT\n86.25\n86.2\n85.39\n85.32\n86.84\n86.08\n85.74\nTable 3. Segmentation accuracy (averaged Dice score and F1 score in %) using different bit width and 5\nparallel FCNs on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Orignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + 5 FCNs\n87.85\n64.40\n85.94\n88.20\n88.51\n88.77\n89.12\n87.86\n87.55\n7 bits SA + 5 FCNs\n87.55\n71.88', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='87.55\n7 bits SA + 5 FCNs\n87.55\n71.88\n80.13\n87.55\n88.63\n88.73\n89.2\n88.67\n87.53\n3.1.1\nNetwork Quantization. As shown in Fig. 1, the network quantization for biomedical image\nsegmentation has two steps: suggestive annotation (SA) with quantization and network training\n(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for\nhigh uncertainty. In the second step, quantization of segmentation FCNs are performed with\nthe suggestive training samples for higher accuracy. The FCN is a 34 layer network, and more\ndetails can refer to [32]. In order to obtain high representativeness, each FCN in suggestive FCNs\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:5\nshould be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='should be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple\nsuggestive FCNs will have very small variance of the final prediction though with different weight\ninitialization. The adopted regularization techniques including weight decay and dropout scheme\nfurther make the multiple suggestive FCNs to be almost the same. By adding quantization to\nsuggestive annotation, the above requirement can be satisfied. Though it may be a little offensive\nsince most of the time it will degrade the accuracy, it is particularly appreciated by suggestive FCNs\nthat focus on uncertainty. Particularly quantization transforms the originally continuous weight\nspace, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='space, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of\nthe outputs. Note that accuracy should be also considered and too offensive quantization methods\nshould be avoided.\n3.1.2\nExperimental Setup. We adopt the 2015 MICCAI Gland Challenge dataset [22] which has\n85 training images (Part A: 37 normal glands, and Part B: 48 abnormal glands) and 80 testing\nimages (Part A: 60 normal glands, and Part B: 20 abnormal glands). In suggestive annotation [32],\n16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in\neach iteration. Totally there are 120 iterations in suggestive annotation, and totally 960 suggested\ntraining samples are produced. We quantize the weights to 2 bits to 9 bits. For ensembling method,\nWe test 2 FCNs to 7 FCNs. We also discuss the overfitting problem with a simplified version of FCN\nby reducing the filter numbers by half. All the experiments are evaluated considering detection (F1\nscore) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='score) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set\nlearning rate to 5×10−4 in the initial stage, and to 5×10−5 when the iteration time reaches a threshold.\nAll the configurations are repeated 4 times and the one with the optimal performance is selected\nfor comparison.\n3.1.3\nResults and Analysis. As there are two networks in suggestive annotation, we discussed both\nof the two with incremental quantization. As shown in Table 1, we first analysis the performance\nof quantization methods in SA with quantization and NT with quantization. We can notice that\ncompared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='compared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for\nquantized NT and quantized SA, respectively.\nAs shown in Table 2, we can discover that ensembling method can improve the segmentation\naccuracy most of the time. Similar to Table 1, the optimal accuracy is achieved in some median\nparallel number, e.g., 6 for float SA + 7 bits NT.\nAs shown in Table 3, we further discussed quantization on both SA and NT with the optimal\nparallel number. The trend is the same as that in Table 1, and the median bit width (7 bits) obtains\nthe best performance. With both quantization and ensemble method, we can achieve a promising\nperformance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='performance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method\nby 1%. In addition, our method can also obtain 4.6x and 6.4x reduction on memory usage for\nincremental quantization with 7 bits and 5 bits, respectively. Note that as activations are in floating\npoint representation, the runtime are not affected.\nAs shown in Table 4, we also discuss quantization with simplified networks, e.g., small models.\nWe can notice that when the network model is smaller, the segmentation accuracy with 4 bits and\n8 bits are no longer improved, however, there is a significant accuracy degradation. This is due to\nthe fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='the fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:6\nW. Chen and C. Zhang, et al.\nTable 4. Segmentation accuracy (averaged Dice score and F1 score in %) using quantization with 4 bits and 8\nbits with small models.\nNetwork\nfloat\n4 bit\n8 bit\nsimplified (Quantized SA + Float NT)\n82.53\n78.14\n82.14', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='82.53\n78.14\n82.14\nsimplified (Float SA + Quantized NT)\n82.53\n76.12\n81.27\n3.2\nImage Classification\n3.2.1\nNetwork Quantization. We use incremental quantization [34] on VGG-16 model for image\nclassification. The original VGG-16 has 16 layers (13 conv-layers and 3 FC layers) as shown in Fig.\n2. Besides that, we focus on the parameter-redundancy problem on VGG-16. We re-train two small\nVGG-16 models with the first five convolutional layers of the original eight layers) and the first', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='VGG-16 models with the first five convolutional layers of the original eight layers) and the first\neight convolutional layers of the original 11 layers), respectively for overfitting analysis.\n3.2.2\nExperimental Setup. We adopt the ILSVRC-2012 dataset [6] which contains over 1.2 million\ntraining images and over 50,000 testing images. VGG-16 [21] is adopted for classification. We\nquantize the weights from 2 bits to 7 bits for discussion. Additionally, we test two VGG models to 9\nVGG models to further clarify the compensation of ensemble learning in terms of accuracy loss\nbrought by weight quantization. And for small VGG-16 models, we compare 4 bits quantization\nand 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='and 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate\ndecay as 1 × 10−6. Stochastic gradient descent (SGD) function is selected to be the optimizer and\nMomentun of 0.9 is used to accelerate training. In order to unify the variables and discuss how the\nbit width impact the network performance, the max value of the quantized weights is set to 4.0,\nand it takes 4 iterations from full floating-point weights to quantized weights, and the accumulated\nportions of quantized weights at four iterative steps are {50%, 75%, 87.5%, 100%}.\nTable 5. Image classification performance (top-1 error in %) of VGG-16 model [14] with weights from 2 bits\nto 9 bits on ImageNet dataset.\nConfigurations', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='to 9 bits on ImageNet dataset.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nVGG-16\n69.29\n42.61\n63.72\n67.70\n68.64\n69.68\n70.36\n70.48\n70.28', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='69.68\n70.36\n70.48\n70.28\nTable 6. Image classification performance (top-1 error in %) of VGG-16 model [14] with parallel numbers\nfrom 2 to 7 on ImageNet dataset.\nConfigurations\nparallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nVGG-16\n69.29\n70.18\n70.10\n70.38\n69.31', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='70.18\n70.10\n70.38\n69.31\n69.82\n68.58\nVGG-16 with 8 bits\n69.29\n70.10\n71.24\n71.03\n69.80\n69.17\n69.85\n3.2.3\nResults and Analysis. We compare our result with the original and small VGG-16 models. As\nshown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='shown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%\nimprovement. We set a variety of ensemble experiments as the control group to discuss the effect\nof ensemble methods. The result of six configurations from two to seven parallel models using\n32-bit floating-point weights are shown in Table 6. With three parallel models, we can get a 0.81%\nperformance improvement. While for the VGG-16 model with 8 bits representation and parallel\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:7\n224x224x64\n112x112x128', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='1:7\n224x224x64\n112x112x128\n56x56x256\n28x28x512\n14x14x512\n7x7x512\n1x1x4096\n1x1x1000\nConvolutional + ReLu\nMaxpooling\nFully connected + ReLu\nSoftmax\nIncremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Incremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the\nfirst layers (with a feature map size of 112x112x128) and halved the channels of the rest layers except for\nthe first two input layers. In small VGG-16 models with 5 layers, we remove the third and forth layers (with\nfeature map sizes of 28x28x512 and 14x14x512, respectively). In small VGG-16 models with eight layers, we\nonly remove the forth layer.\nTable 7. Classification performance (top-1 error in %) comparison between floating pointss, 4 bits and 8 bits\nrepresentations using small VGG-16 models on ImageNet.\nNetwork\nfloat', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='representations using small VGG-16 models on ImageNet.\nNetwork\nfloat\n4 bit\n8 bit\nVGG-16 with 5 conv-layers\n64.14\n64.12\n64.05\nVGG-16 with 8 conv-layers\n66.04\n66.00\n66.02\nnumber from two to seven, we get a 1.95% improvement with 2 parallel models compared with the\noriginal model. In addition, 4x memory reduction is obtained with quantization.\nAs for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='As for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16\nmodels. We shorten the original VGG-16 model to contain 5/8 conv-layers with 3 FC layers. As\nshown in Table. 7, we get 0.02% precision loss after quantization on both two simplified VGG-16\nmodels. Comparing to the results in Table. 5, it can be concluded that quantization can benefit the\nover-parameterized models to some extent. However, if the original model is already simplified,\nthe inhibiting effect of quantization will be faded.\n3.3\nAutomatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Automatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech\nrecognition task. As shown in Fig. 3, the original Deep Speech has 5 hidden layers, composed of 3\ntime-distributed FC layers, 1 bi-directional RNN layer, 1 time-distributed FC layer followed by 1\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:8\nW. Chen and C. Zhang, et al.\nIncremental Network Quantization\nh(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='h(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax\nh(t+1)\nFig. 3. Illustration of quantization framework on the Deep Speech. For small Deep Speech models, we remove\nthe second and third FC layers.\nTable 8. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nweights from 2 bits to 9 bits.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech\n48.13\n60.33\n64.30\n69.53\n52.93\n47.20\n49.50\n52.40\n44.70\nTable 9. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nparallel numbers from 2 to 7.\nConfigurations\nParallel numbers\nOrignal\n2\n3\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Configurations\nParallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nDeep Speech\n48.13\n45.37\n47.30\n43.93\n42.83\n46.23\n43.30\nDeep Speech with 9 bits\n48.13\n43.90\n46.17\n48.07\n48.83\n53.90', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='46.17\n48.07\n48.83\n53.90\n45.53\nTable 10. Automatic speech recognition performance (classification error rate in %) comparison between\nfloating pointss, 4 bits and 8 bits representations using small Deep Speech models\nNetwork\nfloat\n4 bit\n8 bit\nSmall Deep Speech\n50.04\n47.05\n47.08\nsoftmax layer as output. We remove 2 time-distributed FC layers before the RNN layer to simplify\nthe model, and re-train the small model on TIMIT.\n3.3.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='the model, and re-train the small model on TIMIT.\n3.3.2\nExperiment Setup. We adopt the DARPA TIMIT Acoustic-Phonetic Continuous Speech\nCorpus dataset (TIMIT) [10] which contains 6300 sentences read by 630 different Americans for\nDeep Speech (DS) network [12]. We quantize the weights to 2 bits to 7 bits, and we test two to\nnine models in ensemble method. For training parameters, we set learning rate to 1 × 10−4 with a\nlearning rate decay of 1 × 10−6. The predicted error rate will be evaluated by the word error rate\n(WER).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='ACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:9\n3.3.3\nResults and Analysis. As shown in Table 8, the quantized model with 9 bits obtains the\noptimal performance which is 3.43% higher than the model with floating point representation. As\nshown in Table. 9, With three parallel networks, we can get an improvement of 4.2%. Using two\nparallel models, our method obtains the optimal performance among all the configurations which is\n4.23% higher than the original method, which is 0.03% better than the ensemble model. In addition,\n3.6x reduction on memory usage can be achieved.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='3.6x reduction on memory usage can be achieved.\nIn Table. 10, we discuss the quantization effectiveness on small Deep Speech models using 4 bits\nand 8 bits. Our model can get an improvement of 3% compared with the original model.\n3.4\nDiscussion\nAs for the accuracy, we can notice that incremental quantization can improve the segmentation\nperformance by around 1% on the MICCIA Gland dataset. For image classification, We ended up\nusing the 8-bit fixed-point VGG-16 model for two parallel models to achieve a 1.95% improvement\ncompared with the original model. For speech recognition, the optimal performance obtained an\nimprovement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='improvement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-\n6.4x reduction on memory usage. By comparing network training with and without quantization in\nTable 1, Table 5 and Table 8, we can observe that network training with quantization will not always\nimprove the accuracy, especially in extremely situations with 2-4 bits representation. In addition,\ntoo large bit widths do not improve the performance, and the optimal performance is achieved\nwith some median bit width, e.g., 6 bits and 7 bits. Thus, with proper incremental quantization, the\naccuracy can usually be improved with memory reduction.\nAs for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='As for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and\nautomatic speech recognition, respectively. Also the optimal accuracy is achieved in some median\nparallel number. Thus, a proper parallel number can improve the accuracy by scarifying some\nmemory operation and computation cost.\nFor overfitting, we discuss the corresponding small models in the three applications with quan-\ntiztaion. As shown in Table 4, Table 7 and Table 10, we can notice that when the model is much\nsmaller than the original networks, the accuracy degrades seriously such as the results in medical\nimage segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='image segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to\nthe fact that large models usually have more overfitting than small ones. Thus, when quantization is\napplied, the overfitting in large models is reduced, resulting with improved performance. However,\nfor small models with less overfitting, the overfitting is reduced and the representation capability\nis also degraded resulting with accuracy loss. Therefore, incremental quantization may be used as\na regulation method to reduce overfitting.\nIncremental quantization can not only reduce memory consumption of 3.5x-6.4x, but also speedup\nthe processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='the processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup\ndepends on specific hardware such as CPU, GPU and FPGAs. If no specific hardware module is\nimplemented for such transforming, no speedup is obtained, e.g., on existing CPUs and GPUs. If we\nimplement specific hardware module for 2D convolution operation on FPGAs, the speedup can be\n1.7x-7.8x according to [28]. Note that the speedup depends on a variety of factors such as hardware\nplatforms and network structures.\n4\nCONCLUSION\nAmong deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Among deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:10\nW. Chen and C. Zhang, et al.\nthat quantization leads to performance degradation, and plenty of existing works have explored\nquantization strategies aiming at minimum accuracy loss. In this paper, we show that quantization\ncan sometimes help to improve accuracy by imposing regularization on weight representations. We\nconduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='conduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-\ncation, and recurrent neural network (RNN) for automatic speech recognition, and experimental\nresults show that incremental quantization can improve the accuracy by 1%, 1.95%, 4.23% on the\nthree applications respectively with 3.5x-6.4x memory reduction. As a case in compression tech-\nniques, incremental quantization shows great potential to reduce onverfitting, and there may exist\nsome general rules and strategies to enable other compression techniques to have the capability of\nreducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='reducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS\nThis work was supported by the National key Research and Development Program of China (No.\n2018YFC1002600), the Science and Technology Planning Project of Guangdong Province, China\n(No. 2017B090904034, No. 2017B030314109, No. 2018B090944002, No. 2019B020230003), Guangdong\nPeak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Peak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES\n[1] Beatriz Blanco-Filgueira, Daniel García-Lesta, Mauro Fernández-Sanjurjo, Víctor Manuel Brea, and Paula López.\n2019. Deep learning-based multiple object visual tracking on embedded system for iot and mobile edge computing\napplications. IEEE Internet of Things Journal 6, 3 (2019), 5423–5431.\n[2] Hao Chen, Xiaojuan Qi, Jie-Zhi Cheng, Pheng-Ann Heng, et al. 2016. Deep Contextual Networks for Neuronal Structure\nSegmentation.. In AAAI. 1167–1173.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Segmentation.. In AAAI. 1167–1173.\n[3] Hao Chen, Xiaojuan Qi, Lequan Yu, and Pheng-Ann Heng. 2016. Dcan: Deep contour-aware networks for accurate\ngland segmentation. In CVPR. 2487–2496.\n[4] M Courbariaux and Y Bengio. [n.d.]. Binarynet: Training deep neural networks with weights and activations constrained\nto+ 1 or-1. CoRR abs/1602.02830 (2016).\n[5] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In NIPS. 3123–3131.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='with binary weights during propagations. In NIPS. 3123–3131.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR. IEEE, 248–255.\n[7] Yukun Ding, Jinglan Liu, and Yiyu Shi. 2018. On the Universal Approximability of Quantized ReLU Neural Networks.\narXiv preprint arXiv:1802.03646 (2018).\n[8] Marc Egger and Detlef Schoder. 2017. Consumer-oriented tech mining: Integrating the consumer perspective into\norganizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='organizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International\nConference on System Sciences.\n[9] Ge Gao, Chengyan Wang, Xiaodong Zhang, Juan Hu, Xuedong Yang, He Wang, Jue Zhang, and Xiaoying Wang. 2017.\nQuantitative analysis of diffusion-weighted magnetic resonance images: differentiation between prostate cancer and\nnormal tissue based on a computer-aided diagnosis system. Science China Life Sciences 60, 1 (2017), 37–43.\n[10] John S Garofolo. 1993. TIMIT acoustic phonetic continuous speech corpus. Linguistic Data Consortium, 1993 (1993).\n[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).\n[12] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh,\nShubho Sengupta, Adam Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint\narXiv:1412.5567 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='arXiv:1412.5567 (2014).\n[13] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Quantized neural networks:\nTraining neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061 (2016).\n[14] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry\nKalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:11\n[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2012. 3D convolutional neural networks for human action recognition.\nIEEE transactions on pattern analysis and machine intelligence 35, 1 (2012), 221–231.\n[16] Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks. arXiv preprint arXiv:1605.04711 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='[17] Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie Wen, Meiping Huang, Haiyun Yuan, and\nJian Zhuang. 2019. Machine vision guided 3d medical image compression for efficient transmission and accurate\nsegmentation in the clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n12687–12696.\n[18] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. Xnor-net: Imagenet classification\nusing binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='using binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.\n[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI. Springer, 234–241.\n[20] Chuck Rosenberg. 2013. Improving photo search: A step across the semantic gap. Google Research Blog 12 (2013).\n[21] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='arXiv preprint arXiv:1409.1556 (2014).\n[22] Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang,\nBogdan J Matuszewski, Elia Bruni, Urko Sanchez, et al. 2017. Gland segmentation in colon histology images: The glas\nchallenge contest. Medical image analysis 35 (2017), 489–502.\n[23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.\n[24] Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. 2018. Two-step quantization for\nlow-bit neural networks. In Proceedings of the IEEE Conference on computer vision and pattern recognition. 4376–4384.\n[25] Tianchen Wang, Xiaowei Xu, Jinjun Xiong, Qianjun Jia, Haiyun Yuan, Meiping Huang, Jian Zhuang, and Yiyu Shi.\n2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference\non Medical Image Computing and Computer-Assisted Intervention. Springer, 447–457.\n[26] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael Niemier, Jason Cong, Yu Hu, and Yiyu Shi. 2018. Scaling for\nedge inference of deep neural networks. Nature Electronics 1, 4 (2018), 216–222.\n[27] Xiaowei Xu, Qing Lu, Tianchen Wang, Yu Hu, Chen Zhuo, Jinglan Liu, and Yiyu Shi. 2018. Efficient hardware\nimplementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='implementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging\nTechnologies in Computing Systems (JETC) 14, 4 (2018), 1–20.\n[28] Xiaowei Xu, Qing Lu, Tianchen Wang, Jinglan Liu, Cheng Zhuo, Xiaobo Sharon Hu, and Yiyu Shi. 2017. Edge segmen-\ntation: Empowering mobile telemedicine with compressed cellular neural networks. In 2017 IEEE/ACM International\nConference on Computer-Aided Design (ICCAD). IEEE, 880–887.\n[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional\nnetworks for accurate biomedical image segmentation. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 8300–8308.\n[30] Xiaowei Xu, Tianchen Wang, Yiyu Shi, Haiyun Yuan, Qianjun Jia, Meiping Huang, and Jian Zhuang. 2019. Whole\nheart and great vessel segmentation in congenital heart disease using deep neural networks and graph matching. In\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.\n[31] Xiaowei Xu, Tianchen Wang, Jian Zhuang, Haiyun Yuan, Meiping Huang, Jianzheng Cen, Qianjun Jia, Yuhao Dong,\nand Yiyu Shi. 2020. Imagechd: A 3d computed tomography image dataset for classification of congenital heart disease.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 77–87.\n[32] Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and Danny Z Chen. 2017. Suggestive annotation: A deep active\nlearning framework for biomedical image segmentation. In International conference on medical image computing and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='learning framework for biomedical image segmentation. In International conference on medical image computing and\ncomputer-assisted intervention. Springer, 399–407.\n[33] Li Zhang, Haixin Ai, Wen Chen, Zimo Yin, Huan Hu, Junfeng Zhu, Jian Zhao, Qi Zhao, and Hongsheng Liu. 2017.\nCarcinoPred-EL: novel models for predicting the carcinogenicity of chemicals using molecular fingerprints and\nensemble learning methods. Scientific reports 7, 1 (2017), 1–14.\n[34] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. 2017. Incremental network quantization: Towards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044 (2017).\n[35] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016. DoReFa-Net: Training low\nbitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 (2016).\n[36] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. 2016. Trained ternary quantization. arXiv preprint\narXiv:1612.01064 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='arXiv:1612.01064 (2016).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa57a2d90> 111
cuda:2
[Document(page_content='1\nQuantization of Deep Neural Networks for Accurate Edge\nComputing\nWENTAO CHEN∗, HAILONG QIU∗, and JIAN ZHUANG, Guangdong Cardiovascular Institute,\nGuangdong Provincial Key Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s\nHospital, Guangdong Academy of Medical Sciences\nCHUTONG ZHANG∗ and YU HU, Easylink Technology Co., Ltd\nQING LU, TIANCHEN WANG, and YIYU SHI†, Department of Computer Science and Engineering,\nUniversity of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='University of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong\nAcademy of Medical Sciences\nDeep neural networks (DNNs) have demonstrated their great potential in recent years, exceeding the per-\nformance of human experts in a wide range of applications. Due to their large sizes, however, compression\ntechniques such as weight quantization and pruning are usually applied before they can be accommodated on\nthe edge. It is generally believed that quantization leads to performance degradation, and plenty of existing\nworks have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='works have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to\nimprove accuracy. We conduct comprehensive experiments on three widely used applications: fully con-\nnected network (FCN) for biomedical image segmentation, convolutional neural network (CNN) for image\nclassification on ImageNet, and recurrent neural network (RNN) for automatic speech recognition, and experi-\nmental results show that quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three applications\nrespectively with 3.5x-6.4x memory reduction.\nCCS Concepts: • Computing methodologies → Artificial intelligence; • Hardware → Emerging tech-\nnologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='nologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization\n∗Both authors contributed equally to this research. Wentao Chen works at Easylink Technology Co., Ltd, and this work is\ndone when he was an visiting scholar at Guangdong Provincial People’s Hospital.\n†Corresponding authors.\nAuthors’ addresses: Wentao Chen; Hailong Qiu; Jian Zhuang, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong Academy\nof Medical Sciences, 106 Zhongshan Second Road, Guangzhou, Guangdong, 510080, chenwentaokl@gmail.com; Chutong', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Zhang; Yu Hu, Easylink Technology Co., Ltd, Wuhan, Hubei, 43000; Qing Lu; Tianchen Wang; Yiyu Shi, yshi4@nd.edu,\nDepartment of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, 46556; Meiping Huang;\nXiaowe Xu, Guangdong Cardiovascular Institute, Guangdong Provincial Key Laboratory of South China Structural Heart\nDisease, Guangdong Provincial People’s Hospital, Guangdong Academy of Medical Sciences, 106 Zhongshan Second Road,\nGuangzhou, Guangdong, 510080, xiao.wei.xu@foxmail.com.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2021 Association for Computing Machinery.\n1550-4832/2021/1-ART1 $15.00\nhttps://doi.org/10.1145/3451211', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='https://doi.org/10.1145/3451211\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\narXiv:2104.12046v2  [cs.CV]  14 Oct 2021\n1:2\nW. Chen and C. Zhang, et al.\nACM Reference Format:\nWentao Chen, Hailong Qiu, Jian Zhuang, Chutong Zhang, Yu Hu, Qing Lu, Tianchen Wang, Yiyu Shi, Meiping\nHuang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Huang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.\nEmerg. Technol. Comput. Syst. 1, 1, Article 1 (January 2021), 11 pages. https://doi.org/10.1145/3451211\n1\nINTRODUCTION\nDeep Neural Networks (DNNs) have been widely used in various applications and show its great\npotential to tackle complex problems [1–3, 19, 26, 32]. Furthermore, it has been and continue to\nbe instrumental in enabling/advancing breakthroughs in various disciplines, including disease', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='be instrumental in enabling/advancing breakthroughs in various disciplines, including disease\ndiagnosis, real-time language translation, autonomous driving, etc. [8, 9, 15, 20, 22, 25, 30, 31, 33, 33].\nMeanwhile, edge computing for Internet of Things (IoT) has been widely studied which requires\nDNN models with small memory size and efficient computation [17, 27, 28]. Thus, there is a huge\ngap between current DNN models and the requirenment of edge computing.\nRecently, in order to accommodate DNNs on the edge, DNN quantization has become an active\nresearch topic [4, 5, 7, 11, 13, 13, 16, 18, 34–36], which aims to represent DNN weights with less', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='memory (precision) while maintaining acceptable accuracy with efficient memory and computation\ncosts. It has been observed in the literature, however, that sometimes quantization can improve\naccuracy which can be credited to the reduction of overfitting [23]. Dynamic fixed point [29]\ncan achieve 4x less memory operation cost with only 0.4-0.6% Top-5 accuracy loss for ImageNet\nclassification [6]. Ternary weight network and binaryConnect [29] have further reduced the bit-\nwidth of weights to 2 bits or even 1 bit with a relatively larger accuracy loss. Recently, their enhanced\nversion, trained ternary training and binary weight network [29] have reduced the accuracy loss to\nonly 0.6-0.8%. There also exists some works using non-linear quantization to represent the parameter\ndistribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='distribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not\nonly the weights but also the activations. Quantized neural networks, binarized neural networks,\nand XNOR-net [29] reduced the weights to only 1 bit and the activations to 1-2 bits resulting in\na large reduction on memory and computation cost yet with significant accuracy loss. Two-Step\nquantization [24] has 2% drop for VGG-16 on ImageNet-2012 dataset and Integer-Arithmetic-Only\nquantization[14] has 3.1% accuracy loss. In some of the above works, we notice that quantization\ncan sometimes improve the performance [29], however, there is no comprehensive studies to verify\nthis point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='this point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.\nFor example, two-step quantization [24] has a 2% accuracy drop on ILSVRC-2012 dataset and\nInteger-Arithmetic-Only quantization [14] has a 3.1% accuracy loss. Various works in the literature\nhave explored the best ways to quantize the weights with minimum accuracy loss. To some extent,\nquantization essentially imposes regularization on weight representations. As such, it should also\nsometimes help to improve accuracy when overfitting issue presents in large neural networks.\nTo demonstrate this point, in this paper, we conduct extensive experiments using incremental\nquantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='quantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed\nevaluations under the same hardware, deep learning frameworks and configurations. In medical\nimage segmentation, our method can achieve 1% accuracy improvement compared with the current\nstate-of-the-art method on the MICCIA Gland dataset. In image classification, extensive experiments\nare presented on ImageNet-2012 using a widely used CNN model VGG-16 [21], and the result shows\nthat our proposed method exceeds the current best performance using VGG-16 by up to 1.95%. In\nautomatic speech recognition, we quantize Deep Speech [12] network on the TIMIT dataset[10] and\nimproves the accuracy by 4.23%. We also discuss the incremental quantization on the performance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='improves the accuracy by 4.23%. We also discuss the incremental quantization on the performance\nof simplified network with different bit widths, and the experimental results show that incremental\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:3\nquantization can no longer improve the performance of simplified networks with less overfitting.\nIn addition, we get 3.5x-6.4x memory reduction, which is extremely beneficial in the context of\nedge computing.\nFCN\nFCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='FCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set\nSuggested Training Set\nSelect Representative\nbased on Uncertainty and\nSimilarity\nQuantization\nQuantization\nSegmentation FCN\nInference of Suggestive FCN\nTraining of Suggestive FCN\nNetwork Training\nWith Quantization\n(QNT)\nSuggestive Annotation with Quantization (QSA)\nFig. 1. Illustration of quantization framework based on the suggestive annotation framework. In suggestive\nannotation with quantization, better training samples (suggestive training set) can be extracted from the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='annotation with quantization, better training samples (suggestive training set) can be extracted from the\noriginal training set. In network training with quantization, better performance can be achieved by reduce\noverfitting.\n2\nPRELIMINARIES\nIn this section, we briefly review the incremental quantization method used in the experiments.\nThe main goal of incremental quantization is to convert 32-bit-floating-point weights 𝑊 into\nlow-precision weights ˆ𝑊 either power of two or zero with minimum accuracy loss. Each of ˆ𝑊 is\nchosen from 𝑃𝑙 = {±2𝑛1, · · ·, ±2𝑛2}, where 𝑛1 and 𝑛2 are two integer numbers determined by the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='max absolute value of 𝑊𝑙 and expected quantized bit-width, and 𝑛2 ≤ 𝑛1.\nThe training procedure of incremental quantization [34] is consisted of three operations: weight\npartition, group-wise quantization and re-training. Weights partition is the most crucial part of the\nwhole process. We adopt pruning-inspired partition strategy to divide weights into two disjoint\ngroups by comparing the absolute value with layer-wise threshold, which usually achieves better\nperformance than random partition ([34]).\nFor the 𝑙-th layer, we define weights partition as shown in Eq. (1), where 𝐴(1)\n𝑙\nis the first group\nweights that need to be quantized and 𝐴(2)\n𝑙', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='is the first group\nweights that need to be quantized and 𝐴(2)\n𝑙\ndenotes the second group with remaining unquantized\nvalues.\n𝐴(1)\n𝑙\n∪ 𝐴(2)\n𝑙\n= 𝑊𝑙 (𝑖, 𝑗)\n𝐴(1)\n𝑙\n∩ 𝐴(2)\n𝑙\n= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,\nthus they are quantized using Eq. (2),\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:4\nW. Chen and C. Zhang, et al.\n𝑤𝑞 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑝', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='𝑠𝑖𝑔𝑛(𝑤) × 2𝑝\n𝑖𝑓 3 × 2𝑝−2 ≤ |𝑤| ≤ 3 × 2𝑝−1;\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑚 𝑖𝑓 |𝑤| ≥ 2𝑢;\n0\n𝑖𝑓 |𝑤| < 2−𝑙−1\n(2)\nwhere 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='where 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds\nof the quantized set(𝑙 ≤ 𝑝 ≤ 𝑢).\nBy contrast, the second group remains floating-point values to compensate the accuracy loss in\nmodel, and will be re-trained. After one round, these three steps are further adopted only on the\nsecond group in the rest of the training process. As a result, all weights are quantized or to zeros\nhence we gain memory reduction with slight accuracy loss.\n3\nCASE STUDIES\n3.1\nBiomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Biomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit\nwidth on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + Quantized NT\n86.25\n65.55\n79.37\n85.87\n86.12\n86.33', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='79.37\n85.87\n86.12\n86.33\n86.51\n85.93\n85.26\nQuantized SA + Float NT\n86.25\n62.98\n85.39\n86.17\n86.52\n86.66\n86.51\n86.22\n86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the\nMICCAI Gland dataset.\nConfigurations\nParallel number\nOrignal\n2\n3\n4\n5\n6\n7\nFloat SA + Float NT\n86.25\n86.23\n85.93\n85.81\n86.24\n87.25\n86.32\nFloat SA + 7 bits NT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='86.24\n87.25\n86.32\nFloat SA + 7 bits NT\n86.25\n86.2\n85.39\n85.32\n86.84\n86.08\n85.74\nTable 3. Segmentation accuracy (averaged Dice score and F1 score in %) using different bit width and 5\nparallel FCNs on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Orignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + 5 FCNs\n87.85\n64.40\n85.94\n88.20\n88.51\n88.77\n89.12\n87.86\n87.55\n7 bits SA + 5 FCNs\n87.55\n71.88', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='87.55\n7 bits SA + 5 FCNs\n87.55\n71.88\n80.13\n87.55\n88.63\n88.73\n89.2\n88.67\n87.53\n3.1.1\nNetwork Quantization. As shown in Fig. 1, the network quantization for biomedical image\nsegmentation has two steps: suggestive annotation (SA) with quantization and network training\n(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for\nhigh uncertainty. In the second step, quantization of segmentation FCNs are performed with\nthe suggestive training samples for higher accuracy. The FCN is a 34 layer network, and more\ndetails can refer to [32]. In order to obtain high representativeness, each FCN in suggestive FCNs\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:5\nshould be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='should be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple\nsuggestive FCNs will have very small variance of the final prediction though with different weight\ninitialization. The adopted regularization techniques including weight decay and dropout scheme\nfurther make the multiple suggestive FCNs to be almost the same. By adding quantization to\nsuggestive annotation, the above requirement can be satisfied. Though it may be a little offensive\nsince most of the time it will degrade the accuracy, it is particularly appreciated by suggestive FCNs\nthat focus on uncertainty. Particularly quantization transforms the originally continuous weight\nspace, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='space, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of\nthe outputs. Note that accuracy should be also considered and too offensive quantization methods\nshould be avoided.\n3.1.2\nExperimental Setup. We adopt the 2015 MICCAI Gland Challenge dataset [22] which has\n85 training images (Part A: 37 normal glands, and Part B: 48 abnormal glands) and 80 testing\nimages (Part A: 60 normal glands, and Part B: 20 abnormal glands). In suggestive annotation [32],\n16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in\neach iteration. Totally there are 120 iterations in suggestive annotation, and totally 960 suggested\ntraining samples are produced. We quantize the weights to 2 bits to 9 bits. For ensembling method,\nWe test 2 FCNs to 7 FCNs. We also discuss the overfitting problem with a simplified version of FCN\nby reducing the filter numbers by half. All the experiments are evaluated considering detection (F1\nscore) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='score) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set\nlearning rate to 5×10−4 in the initial stage, and to 5×10−5 when the iteration time reaches a threshold.\nAll the configurations are repeated 4 times and the one with the optimal performance is selected\nfor comparison.\n3.1.3\nResults and Analysis. As there are two networks in suggestive annotation, we discussed both\nof the two with incremental quantization. As shown in Table 1, we first analysis the performance\nof quantization methods in SA with quantization and NT with quantization. We can notice that\ncompared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='compared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for\nquantized NT and quantized SA, respectively.\nAs shown in Table 2, we can discover that ensembling method can improve the segmentation\naccuracy most of the time. Similar to Table 1, the optimal accuracy is achieved in some median\nparallel number, e.g., 6 for float SA + 7 bits NT.\nAs shown in Table 3, we further discussed quantization on both SA and NT with the optimal\nparallel number. The trend is the same as that in Table 1, and the median bit width (7 bits) obtains\nthe best performance. With both quantization and ensemble method, we can achieve a promising\nperformance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='performance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method\nby 1%. In addition, our method can also obtain 4.6x and 6.4x reduction on memory usage for\nincremental quantization with 7 bits and 5 bits, respectively. Note that as activations are in floating\npoint representation, the runtime are not affected.\nAs shown in Table 4, we also discuss quantization with simplified networks, e.g., small models.\nWe can notice that when the network model is smaller, the segmentation accuracy with 4 bits and\n8 bits are no longer improved, however, there is a significant accuracy degradation. This is due to\nthe fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='the fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:6\nW. Chen and C. Zhang, et al.\nTable 4. Segmentation accuracy (averaged Dice score and F1 score in %) using quantization with 4 bits and 8\nbits with small models.\nNetwork\nfloat\n4 bit\n8 bit\nsimplified (Quantized SA + Float NT)\n82.53\n78.14\n82.14', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='82.53\n78.14\n82.14\nsimplified (Float SA + Quantized NT)\n82.53\n76.12\n81.27\n3.2\nImage Classification\n3.2.1\nNetwork Quantization. We use incremental quantization [34] on VGG-16 model for image\nclassification. The original VGG-16 has 16 layers (13 conv-layers and 3 FC layers) as shown in Fig.\n2. Besides that, we focus on the parameter-redundancy problem on VGG-16. We re-train two small\nVGG-16 models with the first five convolutional layers of the original eight layers) and the first', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='VGG-16 models with the first five convolutional layers of the original eight layers) and the first\neight convolutional layers of the original 11 layers), respectively for overfitting analysis.\n3.2.2\nExperimental Setup. We adopt the ILSVRC-2012 dataset [6] which contains over 1.2 million\ntraining images and over 50,000 testing images. VGG-16 [21] is adopted for classification. We\nquantize the weights from 2 bits to 7 bits for discussion. Additionally, we test two VGG models to 9\nVGG models to further clarify the compensation of ensemble learning in terms of accuracy loss\nbrought by weight quantization. And for small VGG-16 models, we compare 4 bits quantization\nand 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='and 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate\ndecay as 1 × 10−6. Stochastic gradient descent (SGD) function is selected to be the optimizer and\nMomentun of 0.9 is used to accelerate training. In order to unify the variables and discuss how the\nbit width impact the network performance, the max value of the quantized weights is set to 4.0,\nand it takes 4 iterations from full floating-point weights to quantized weights, and the accumulated\nportions of quantized weights at four iterative steps are {50%, 75%, 87.5%, 100%}.\nTable 5. Image classification performance (top-1 error in %) of VGG-16 model [14] with weights from 2 bits\nto 9 bits on ImageNet dataset.\nConfigurations', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='to 9 bits on ImageNet dataset.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nVGG-16\n69.29\n42.61\n63.72\n67.70\n68.64\n69.68\n70.36\n70.48\n70.28', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='69.68\n70.36\n70.48\n70.28\nTable 6. Image classification performance (top-1 error in %) of VGG-16 model [14] with parallel numbers\nfrom 2 to 7 on ImageNet dataset.\nConfigurations\nparallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nVGG-16\n69.29\n70.18\n70.10\n70.38\n69.31', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='70.18\n70.10\n70.38\n69.31\n69.82\n68.58\nVGG-16 with 8 bits\n69.29\n70.10\n71.24\n71.03\n69.80\n69.17\n69.85\n3.2.3\nResults and Analysis. We compare our result with the original and small VGG-16 models. As\nshown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='shown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%\nimprovement. We set a variety of ensemble experiments as the control group to discuss the effect\nof ensemble methods. The result of six configurations from two to seven parallel models using\n32-bit floating-point weights are shown in Table 6. With three parallel models, we can get a 0.81%\nperformance improvement. While for the VGG-16 model with 8 bits representation and parallel\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:7\n224x224x64\n112x112x128', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='1:7\n224x224x64\n112x112x128\n56x56x256\n28x28x512\n14x14x512\n7x7x512\n1x1x4096\n1x1x1000\nConvolutional + ReLu\nMaxpooling\nFully connected + ReLu\nSoftmax\nIncremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Incremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the\nfirst layers (with a feature map size of 112x112x128) and halved the channels of the rest layers except for\nthe first two input layers. In small VGG-16 models with 5 layers, we remove the third and forth layers (with\nfeature map sizes of 28x28x512 and 14x14x512, respectively). In small VGG-16 models with eight layers, we\nonly remove the forth layer.\nTable 7. Classification performance (top-1 error in %) comparison between floating pointss, 4 bits and 8 bits\nrepresentations using small VGG-16 models on ImageNet.\nNetwork\nfloat', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='representations using small VGG-16 models on ImageNet.\nNetwork\nfloat\n4 bit\n8 bit\nVGG-16 with 5 conv-layers\n64.14\n64.12\n64.05\nVGG-16 with 8 conv-layers\n66.04\n66.00\n66.02\nnumber from two to seven, we get a 1.95% improvement with 2 parallel models compared with the\noriginal model. In addition, 4x memory reduction is obtained with quantization.\nAs for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='As for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16\nmodels. We shorten the original VGG-16 model to contain 5/8 conv-layers with 3 FC layers. As\nshown in Table. 7, we get 0.02% precision loss after quantization on both two simplified VGG-16\nmodels. Comparing to the results in Table. 5, it can be concluded that quantization can benefit the\nover-parameterized models to some extent. However, if the original model is already simplified,\nthe inhibiting effect of quantization will be faded.\n3.3\nAutomatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Automatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech\nrecognition task. As shown in Fig. 3, the original Deep Speech has 5 hidden layers, composed of 3\ntime-distributed FC layers, 1 bi-directional RNN layer, 1 time-distributed FC layer followed by 1\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:8\nW. Chen and C. Zhang, et al.\nIncremental Network Quantization\nh(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='h(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax\nh(t+1)\nFig. 3. Illustration of quantization framework on the Deep Speech. For small Deep Speech models, we remove\nthe second and third FC layers.\nTable 8. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nweights from 2 bits to 9 bits.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech\n48.13\n60.33\n64.30\n69.53\n52.93\n47.20\n49.50\n52.40\n44.70\nTable 9. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nparallel numbers from 2 to 7.\nConfigurations\nParallel numbers\nOrignal\n2\n3\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Configurations\nParallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nDeep Speech\n48.13\n45.37\n47.30\n43.93\n42.83\n46.23\n43.30\nDeep Speech with 9 bits\n48.13\n43.90\n46.17\n48.07\n48.83\n53.90', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='46.17\n48.07\n48.83\n53.90\n45.53\nTable 10. Automatic speech recognition performance (classification error rate in %) comparison between\nfloating pointss, 4 bits and 8 bits representations using small Deep Speech models\nNetwork\nfloat\n4 bit\n8 bit\nSmall Deep Speech\n50.04\n47.05\n47.08\nsoftmax layer as output. We remove 2 time-distributed FC layers before the RNN layer to simplify\nthe model, and re-train the small model on TIMIT.\n3.3.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='the model, and re-train the small model on TIMIT.\n3.3.2\nExperiment Setup. We adopt the DARPA TIMIT Acoustic-Phonetic Continuous Speech\nCorpus dataset (TIMIT) [10] which contains 6300 sentences read by 630 different Americans for\nDeep Speech (DS) network [12]. We quantize the weights to 2 bits to 7 bits, and we test two to\nnine models in ensemble method. For training parameters, we set learning rate to 1 × 10−4 with a\nlearning rate decay of 1 × 10−6. The predicted error rate will be evaluated by the word error rate\n(WER).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='ACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:9\n3.3.3\nResults and Analysis. As shown in Table 8, the quantized model with 9 bits obtains the\noptimal performance which is 3.43% higher than the model with floating point representation. As\nshown in Table. 9, With three parallel networks, we can get an improvement of 4.2%. Using two\nparallel models, our method obtains the optimal performance among all the configurations which is\n4.23% higher than the original method, which is 0.03% better than the ensemble model. In addition,\n3.6x reduction on memory usage can be achieved.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='3.6x reduction on memory usage can be achieved.\nIn Table. 10, we discuss the quantization effectiveness on small Deep Speech models using 4 bits\nand 8 bits. Our model can get an improvement of 3% compared with the original model.\n3.4\nDiscussion\nAs for the accuracy, we can notice that incremental quantization can improve the segmentation\nperformance by around 1% on the MICCIA Gland dataset. For image classification, We ended up\nusing the 8-bit fixed-point VGG-16 model for two parallel models to achieve a 1.95% improvement\ncompared with the original model. For speech recognition, the optimal performance obtained an\nimprovement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='improvement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-\n6.4x reduction on memory usage. By comparing network training with and without quantization in\nTable 1, Table 5 and Table 8, we can observe that network training with quantization will not always\nimprove the accuracy, especially in extremely situations with 2-4 bits representation. In addition,\ntoo large bit widths do not improve the performance, and the optimal performance is achieved\nwith some median bit width, e.g., 6 bits and 7 bits. Thus, with proper incremental quantization, the\naccuracy can usually be improved with memory reduction.\nAs for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='As for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and\nautomatic speech recognition, respectively. Also the optimal accuracy is achieved in some median\nparallel number. Thus, a proper parallel number can improve the accuracy by scarifying some\nmemory operation and computation cost.\nFor overfitting, we discuss the corresponding small models in the three applications with quan-\ntiztaion. As shown in Table 4, Table 7 and Table 10, we can notice that when the model is much\nsmaller than the original networks, the accuracy degrades seriously such as the results in medical\nimage segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='image segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to\nthe fact that large models usually have more overfitting than small ones. Thus, when quantization is\napplied, the overfitting in large models is reduced, resulting with improved performance. However,\nfor small models with less overfitting, the overfitting is reduced and the representation capability\nis also degraded resulting with accuracy loss. Therefore, incremental quantization may be used as\na regulation method to reduce overfitting.\nIncremental quantization can not only reduce memory consumption of 3.5x-6.4x, but also speedup\nthe processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='the processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup\ndepends on specific hardware such as CPU, GPU and FPGAs. If no specific hardware module is\nimplemented for such transforming, no speedup is obtained, e.g., on existing CPUs and GPUs. If we\nimplement specific hardware module for 2D convolution operation on FPGAs, the speedup can be\n1.7x-7.8x according to [28]. Note that the speedup depends on a variety of factors such as hardware\nplatforms and network structures.\n4\nCONCLUSION\nAmong deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Among deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:10\nW. Chen and C. Zhang, et al.\nthat quantization leads to performance degradation, and plenty of existing works have explored\nquantization strategies aiming at minimum accuracy loss. In this paper, we show that quantization\ncan sometimes help to improve accuracy by imposing regularization on weight representations. We\nconduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='conduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-\ncation, and recurrent neural network (RNN) for automatic speech recognition, and experimental\nresults show that incremental quantization can improve the accuracy by 1%, 1.95%, 4.23% on the\nthree applications respectively with 3.5x-6.4x memory reduction. As a case in compression tech-\nniques, incremental quantization shows great potential to reduce onverfitting, and there may exist\nsome general rules and strategies to enable other compression techniques to have the capability of\nreducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='reducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS\nThis work was supported by the National key Research and Development Program of China (No.\n2018YFC1002600), the Science and Technology Planning Project of Guangdong Province, China\n(No. 2017B090904034, No. 2017B030314109, No. 2018B090944002, No. 2019B020230003), Guangdong\nPeak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Peak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES\n[1] Beatriz Blanco-Filgueira, Daniel García-Lesta, Mauro Fernández-Sanjurjo, Víctor Manuel Brea, and Paula López.\n2019. Deep learning-based multiple object visual tracking on embedded system for iot and mobile edge computing\napplications. IEEE Internet of Things Journal 6, 3 (2019), 5423–5431.\n[2] Hao Chen, Xiaojuan Qi, Jie-Zhi Cheng, Pheng-Ann Heng, et al. 2016. Deep Contextual Networks for Neuronal Structure\nSegmentation.. In AAAI. 1167–1173.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Segmentation.. In AAAI. 1167–1173.\n[3] Hao Chen, Xiaojuan Qi, Lequan Yu, and Pheng-Ann Heng. 2016. Dcan: Deep contour-aware networks for accurate\ngland segmentation. In CVPR. 2487–2496.\n[4] M Courbariaux and Y Bengio. [n.d.]. Binarynet: Training deep neural networks with weights and activations constrained\nto+ 1 or-1. CoRR abs/1602.02830 (2016).\n[5] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In NIPS. 3123–3131.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='with binary weights during propagations. In NIPS. 3123–3131.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR. IEEE, 248–255.\n[7] Yukun Ding, Jinglan Liu, and Yiyu Shi. 2018. On the Universal Approximability of Quantized ReLU Neural Networks.\narXiv preprint arXiv:1802.03646 (2018).\n[8] Marc Egger and Detlef Schoder. 2017. Consumer-oriented tech mining: Integrating the consumer perspective into\norganizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='organizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International\nConference on System Sciences.\n[9] Ge Gao, Chengyan Wang, Xiaodong Zhang, Juan Hu, Xuedong Yang, He Wang, Jue Zhang, and Xiaoying Wang. 2017.\nQuantitative analysis of diffusion-weighted magnetic resonance images: differentiation between prostate cancer and\nnormal tissue based on a computer-aided diagnosis system. Science China Life Sciences 60, 1 (2017), 37–43.\n[10] John S Garofolo. 1993. TIMIT acoustic phonetic continuous speech corpus. Linguistic Data Consortium, 1993 (1993).\n[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).\n[12] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh,\nShubho Sengupta, Adam Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint\narXiv:1412.5567 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='arXiv:1412.5567 (2014).\n[13] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Quantized neural networks:\nTraining neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061 (2016).\n[14] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry\nKalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:11\n[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2012. 3D convolutional neural networks for human action recognition.\nIEEE transactions on pattern analysis and machine intelligence 35, 1 (2012), 221–231.\n[16] Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks. arXiv preprint arXiv:1605.04711 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='[17] Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie Wen, Meiping Huang, Haiyun Yuan, and\nJian Zhuang. 2019. Machine vision guided 3d medical image compression for efficient transmission and accurate\nsegmentation in the clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n12687–12696.\n[18] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. Xnor-net: Imagenet classification\nusing binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='using binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.\n[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI. Springer, 234–241.\n[20] Chuck Rosenberg. 2013. Improving photo search: A step across the semantic gap. Google Research Blog 12 (2013).\n[21] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='arXiv preprint arXiv:1409.1556 (2014).\n[22] Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang,\nBogdan J Matuszewski, Elia Bruni, Urko Sanchez, et al. 2017. Gland segmentation in colon histology images: The glas\nchallenge contest. Medical image analysis 35 (2017), 489–502.\n[23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.\n[24] Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. 2018. Two-step quantization for\nlow-bit neural networks. In Proceedings of the IEEE Conference on computer vision and pattern recognition. 4376–4384.\n[25] Tianchen Wang, Xiaowei Xu, Jinjun Xiong, Qianjun Jia, Haiyun Yuan, Meiping Huang, Jian Zhuang, and Yiyu Shi.\n2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference\non Medical Image Computing and Computer-Assisted Intervention. Springer, 447–457.\n[26] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael Niemier, Jason Cong, Yu Hu, and Yiyu Shi. 2018. Scaling for\nedge inference of deep neural networks. Nature Electronics 1, 4 (2018), 216–222.\n[27] Xiaowei Xu, Qing Lu, Tianchen Wang, Yu Hu, Chen Zhuo, Jinglan Liu, and Yiyu Shi. 2018. Efficient hardware\nimplementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='implementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging\nTechnologies in Computing Systems (JETC) 14, 4 (2018), 1–20.\n[28] Xiaowei Xu, Qing Lu, Tianchen Wang, Jinglan Liu, Cheng Zhuo, Xiaobo Sharon Hu, and Yiyu Shi. 2017. Edge segmen-\ntation: Empowering mobile telemedicine with compressed cellular neural networks. In 2017 IEEE/ACM International\nConference on Computer-Aided Design (ICCAD). IEEE, 880–887.\n[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional\nnetworks for accurate biomedical image segmentation. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 8300–8308.\n[30] Xiaowei Xu, Tianchen Wang, Yiyu Shi, Haiyun Yuan, Qianjun Jia, Meiping Huang, and Jian Zhuang. 2019. Whole\nheart and great vessel segmentation in congenital heart disease using deep neural networks and graph matching. In\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.\n[31] Xiaowei Xu, Tianchen Wang, Jian Zhuang, Haiyun Yuan, Meiping Huang, Jianzheng Cen, Qianjun Jia, Yuhao Dong,\nand Yiyu Shi. 2020. Imagechd: A 3d computed tomography image dataset for classification of congenital heart disease.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 77–87.\n[32] Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and Danny Z Chen. 2017. Suggestive annotation: A deep active\nlearning framework for biomedical image segmentation. In International conference on medical image computing and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='learning framework for biomedical image segmentation. In International conference on medical image computing and\ncomputer-assisted intervention. Springer, 399–407.\n[33] Li Zhang, Haixin Ai, Wen Chen, Zimo Yin, Huan Hu, Junfeng Zhu, Jian Zhao, Qi Zhao, and Hongsheng Liu. 2017.\nCarcinoPred-EL: novel models for predicting the carcinogenicity of chemicals using molecular fingerprints and\nensemble learning methods. Scientific reports 7, 1 (2017), 1–14.\n[34] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. 2017. Incremental network quantization: Towards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044 (2017).\n[35] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016. DoReFa-Net: Training low\nbitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 (2016).\n[36] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. 2016. Trained ternary quantization. arXiv preprint\narXiv:1612.01064 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'}), Document(page_content='arXiv:1612.01064 (2016).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu6k23zja/Quantization of Deep Neural Networks for Accurate Edge Computing.pdf'})]
cuda:2
[UploadFile(filename='0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf', size=893520, headers=Headers({'content-disposition': 'form-data; name="files"; filename="0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpwfi5kred, tmpwfi5kred
File: 0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf, msg: 成功上传文件 0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf, docs: [Document(page_content='1\nQuantization of Deep Neural Networks for Accurate Edge\nComputing\nWENTAO CHEN∗, HAILONG QIU∗, and JIAN ZHUANG, Guangdong Cardiovascular Institute,\nGuangdong Provincial Key Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s\nHospital, Guangdong Academy of Medical Sciences\nCHUTONG ZHANG∗ and YU HU, Easylink Technology Co., Ltd\nQING LU, TIANCHEN WANG, and YIYU SHI†, Department of Computer Science and Engineering,\nUniversity of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='University of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong\nAcademy of Medical Sciences\nDeep neural networks (DNNs) have demonstrated their great potential in recent years, exceeding the per-\nformance of human experts in a wide range of applications. Due to their large sizes, however, compression\ntechniques such as weight quantization and pruning are usually applied before they can be accommodated on\nthe edge. It is generally believed that quantization leads to performance degradation, and plenty of existing\nworks have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='works have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to\nimprove accuracy. We conduct comprehensive experiments on three widely used applications: fully con-\nnected network (FCN) for biomedical image segmentation, convolutional neural network (CNN) for image\nclassification on ImageNet, and recurrent neural network (RNN) for automatic speech recognition, and experi-\nmental results show that quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three applications\nrespectively with 3.5x-6.4x memory reduction.\nCCS Concepts: • Computing methodologies → Artificial intelligence; • Hardware → Emerging tech-\nnologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='nologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization\n∗Both authors contributed equally to this research. Wentao Chen works at Easylink Technology Co., Ltd, and this work is\ndone when he was an visiting scholar at Guangdong Provincial People’s Hospital.\n†Corresponding authors.\nAuthors’ addresses: Wentao Chen; Hailong Qiu; Jian Zhuang, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong Academy\nof Medical Sciences, 106 Zhongshan Second Road, Guangzhou, Guangdong, 510080, chenwentaokl@gmail.com; Chutong', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Zhang; Yu Hu, Easylink Technology Co., Ltd, Wuhan, Hubei, 43000; Qing Lu; Tianchen Wang; Yiyu Shi, yshi4@nd.edu,\nDepartment of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, 46556; Meiping Huang;\nXiaowe Xu, Guangdong Cardiovascular Institute, Guangdong Provincial Key Laboratory of South China Structural Heart\nDisease, Guangdong Provincial People’s Hospital, Guangdong Academy of Medical Sciences, 106 Zhongshan Second Road,\nGuangzhou, Guangdong, 510080, xiao.wei.xu@foxmail.com.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2021 Association for Computing Machinery.\n1550-4832/2021/1-ART1 $15.00\nhttps://doi.org/10.1145/3451211', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='https://doi.org/10.1145/3451211\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\narXiv:2104.12046v2  [cs.CV]  14 Oct 2021\n1:2\nW. Chen and C. Zhang, et al.\nACM Reference Format:\nWentao Chen, Hailong Qiu, Jian Zhuang, Chutong Zhang, Yu Hu, Qing Lu, Tianchen Wang, Yiyu Shi, Meiping\nHuang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Huang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.\nEmerg. Technol. Comput. Syst. 1, 1, Article 1 (January 2021), 11 pages. https://doi.org/10.1145/3451211\n1\nINTRODUCTION\nDeep Neural Networks (DNNs) have been widely used in various applications and show its great\npotential to tackle complex problems [1–3, 19, 26, 32]. Furthermore, it has been and continue to\nbe instrumental in enabling/advancing breakthroughs in various disciplines, including disease', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='be instrumental in enabling/advancing breakthroughs in various disciplines, including disease\ndiagnosis, real-time language translation, autonomous driving, etc. [8, 9, 15, 20, 22, 25, 30, 31, 33, 33].\nMeanwhile, edge computing for Internet of Things (IoT) has been widely studied which requires\nDNN models with small memory size and efficient computation [17, 27, 28]. Thus, there is a huge\ngap between current DNN models and the requirenment of edge computing.\nRecently, in order to accommodate DNNs on the edge, DNN quantization has become an active\nresearch topic [4, 5, 7, 11, 13, 13, 16, 18, 34–36], which aims to represent DNN weights with less', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='memory (precision) while maintaining acceptable accuracy with efficient memory and computation\ncosts. It has been observed in the literature, however, that sometimes quantization can improve\naccuracy which can be credited to the reduction of overfitting [23]. Dynamic fixed point [29]\ncan achieve 4x less memory operation cost with only 0.4-0.6% Top-5 accuracy loss for ImageNet\nclassification [6]. Ternary weight network and binaryConnect [29] have further reduced the bit-\nwidth of weights to 2 bits or even 1 bit with a relatively larger accuracy loss. Recently, their enhanced\nversion, trained ternary training and binary weight network [29] have reduced the accuracy loss to\nonly 0.6-0.8%. There also exists some works using non-linear quantization to represent the parameter\ndistribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='distribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not\nonly the weights but also the activations. Quantized neural networks, binarized neural networks,\nand XNOR-net [29] reduced the weights to only 1 bit and the activations to 1-2 bits resulting in\na large reduction on memory and computation cost yet with significant accuracy loss. Two-Step\nquantization [24] has 2% drop for VGG-16 on ImageNet-2012 dataset and Integer-Arithmetic-Only\nquantization[14] has 3.1% accuracy loss. In some of the above works, we notice that quantization\ncan sometimes improve the performance [29], however, there is no comprehensive studies to verify\nthis point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='this point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.\nFor example, two-step quantization [24] has a 2% accuracy drop on ILSVRC-2012 dataset and\nInteger-Arithmetic-Only quantization [14] has a 3.1% accuracy loss. Various works in the literature\nhave explored the best ways to quantize the weights with minimum accuracy loss. To some extent,\nquantization essentially imposes regularization on weight representations. As such, it should also\nsometimes help to improve accuracy when overfitting issue presents in large neural networks.\nTo demonstrate this point, in this paper, we conduct extensive experiments using incremental\nquantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='quantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed\nevaluations under the same hardware, deep learning frameworks and configurations. In medical\nimage segmentation, our method can achieve 1% accuracy improvement compared with the current\nstate-of-the-art method on the MICCIA Gland dataset. In image classification, extensive experiments\nare presented on ImageNet-2012 using a widely used CNN model VGG-16 [21], and the result shows\nthat our proposed method exceeds the current best performance using VGG-16 by up to 1.95%. In\nautomatic speech recognition, we quantize Deep Speech [12] network on the TIMIT dataset[10] and\nimproves the accuracy by 4.23%. We also discuss the incremental quantization on the performance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='improves the accuracy by 4.23%. We also discuss the incremental quantization on the performance\nof simplified network with different bit widths, and the experimental results show that incremental\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:3\nquantization can no longer improve the performance of simplified networks with less overfitting.\nIn addition, we get 3.5x-6.4x memory reduction, which is extremely beneficial in the context of\nedge computing.\nFCN\nFCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='FCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set\nSuggested Training Set\nSelect Representative\nbased on Uncertainty and\nSimilarity\nQuantization\nQuantization\nSegmentation FCN\nInference of Suggestive FCN\nTraining of Suggestive FCN\nNetwork Training\nWith Quantization\n(QNT)\nSuggestive Annotation with Quantization (QSA)\nFig. 1. Illustration of quantization framework based on the suggestive annotation framework. In suggestive\nannotation with quantization, better training samples (suggestive training set) can be extracted from the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='annotation with quantization, better training samples (suggestive training set) can be extracted from the\noriginal training set. In network training with quantization, better performance can be achieved by reduce\noverfitting.\n2\nPRELIMINARIES\nIn this section, we briefly review the incremental quantization method used in the experiments.\nThe main goal of incremental quantization is to convert 32-bit-floating-point weights 𝑊 into\nlow-precision weights ˆ𝑊 either power of two or zero with minimum accuracy loss. Each of ˆ𝑊 is\nchosen from 𝑃𝑙 = {±2𝑛1, · · ·, ±2𝑛2}, where 𝑛1 and 𝑛2 are two integer numbers determined by the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='max absolute value of 𝑊𝑙 and expected quantized bit-width, and 𝑛2 ≤ 𝑛1.\nThe training procedure of incremental quantization [34] is consisted of three operations: weight\npartition, group-wise quantization and re-training. Weights partition is the most crucial part of the\nwhole process. We adopt pruning-inspired partition strategy to divide weights into two disjoint\ngroups by comparing the absolute value with layer-wise threshold, which usually achieves better\nperformance than random partition ([34]).\nFor the 𝑙-th layer, we define weights partition as shown in Eq. (1), where 𝐴(1)\n𝑙\nis the first group\nweights that need to be quantized and 𝐴(2)\n𝑙', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='is the first group\nweights that need to be quantized and 𝐴(2)\n𝑙\ndenotes the second group with remaining unquantized\nvalues.\n𝐴(1)\n𝑙\n∪ 𝐴(2)\n𝑙\n= 𝑊𝑙 (𝑖, 𝑗)\n𝐴(1)\n𝑙\n∩ 𝐴(2)\n𝑙\n= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,\nthus they are quantized using Eq. (2),\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:4\nW. Chen and C. Zhang, et al.\n𝑤𝑞 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑝', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='𝑠𝑖𝑔𝑛(𝑤) × 2𝑝\n𝑖𝑓 3 × 2𝑝−2 ≤ |𝑤| ≤ 3 × 2𝑝−1;\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑚 𝑖𝑓 |𝑤| ≥ 2𝑢;\n0\n𝑖𝑓 |𝑤| < 2−𝑙−1\n(2)\nwhere 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='where 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds\nof the quantized set(𝑙 ≤ 𝑝 ≤ 𝑢).\nBy contrast, the second group remains floating-point values to compensate the accuracy loss in\nmodel, and will be re-trained. After one round, these three steps are further adopted only on the\nsecond group in the rest of the training process. As a result, all weights are quantized or to zeros\nhence we gain memory reduction with slight accuracy loss.\n3\nCASE STUDIES\n3.1\nBiomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Biomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit\nwidth on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + Quantized NT\n86.25\n65.55\n79.37\n85.87\n86.12\n86.33', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='79.37\n85.87\n86.12\n86.33\n86.51\n85.93\n85.26\nQuantized SA + Float NT\n86.25\n62.98\n85.39\n86.17\n86.52\n86.66\n86.51\n86.22\n86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the\nMICCAI Gland dataset.\nConfigurations\nParallel number\nOrignal\n2\n3\n4\n5\n6\n7\nFloat SA + Float NT\n86.25\n86.23\n85.93\n85.81\n86.24\n87.25\n86.32\nFloat SA + 7 bits NT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='86.24\n87.25\n86.32\nFloat SA + 7 bits NT\n86.25\n86.2\n85.39\n85.32\n86.84\n86.08\n85.74\nTable 3. Segmentation accuracy (averaged Dice score and F1 score in %) using different bit width and 5\nparallel FCNs on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Orignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + 5 FCNs\n87.85\n64.40\n85.94\n88.20\n88.51\n88.77\n89.12\n87.86\n87.55\n7 bits SA + 5 FCNs\n87.55\n71.88', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='87.55\n7 bits SA + 5 FCNs\n87.55\n71.88\n80.13\n87.55\n88.63\n88.73\n89.2\n88.67\n87.53\n3.1.1\nNetwork Quantization. As shown in Fig. 1, the network quantization for biomedical image\nsegmentation has two steps: suggestive annotation (SA) with quantization and network training\n(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for\nhigh uncertainty. In the second step, quantization of segmentation FCNs are performed with\nthe suggestive training samples for higher accuracy. The FCN is a 34 layer network, and more\ndetails can refer to [32]. In order to obtain high representativeness, each FCN in suggestive FCNs\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:5\nshould be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='should be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple\nsuggestive FCNs will have very small variance of the final prediction though with different weight\ninitialization. The adopted regularization techniques including weight decay and dropout scheme\nfurther make the multiple suggestive FCNs to be almost the same. By adding quantization to\nsuggestive annotation, the above requirement can be satisfied. Though it may be a little offensive\nsince most of the time it will degrade the accuracy, it is particularly appreciated by suggestive FCNs\nthat focus on uncertainty. Particularly quantization transforms the originally continuous weight\nspace, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='space, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of\nthe outputs. Note that accuracy should be also considered and too offensive quantization methods\nshould be avoided.\n3.1.2\nExperimental Setup. We adopt the 2015 MICCAI Gland Challenge dataset [22] which has\n85 training images (Part A: 37 normal glands, and Part B: 48 abnormal glands) and 80 testing\nimages (Part A: 60 normal glands, and Part B: 20 abnormal glands). In suggestive annotation [32],\n16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in\neach iteration. Totally there are 120 iterations in suggestive annotation, and totally 960 suggested\ntraining samples are produced. We quantize the weights to 2 bits to 9 bits. For ensembling method,\nWe test 2 FCNs to 7 FCNs. We also discuss the overfitting problem with a simplified version of FCN\nby reducing the filter numbers by half. All the experiments are evaluated considering detection (F1\nscore) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='score) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set\nlearning rate to 5×10−4 in the initial stage, and to 5×10−5 when the iteration time reaches a threshold.\nAll the configurations are repeated 4 times and the one with the optimal performance is selected\nfor comparison.\n3.1.3\nResults and Analysis. As there are two networks in suggestive annotation, we discussed both\nof the two with incremental quantization. As shown in Table 1, we first analysis the performance\nof quantization methods in SA with quantization and NT with quantization. We can notice that\ncompared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='compared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for\nquantized NT and quantized SA, respectively.\nAs shown in Table 2, we can discover that ensembling method can improve the segmentation\naccuracy most of the time. Similar to Table 1, the optimal accuracy is achieved in some median\nparallel number, e.g., 6 for float SA + 7 bits NT.\nAs shown in Table 3, we further discussed quantization on both SA and NT with the optimal\nparallel number. The trend is the same as that in Table 1, and the median bit width (7 bits) obtains\nthe best performance. With both quantization and ensemble method, we can achieve a promising\nperformance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='performance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method\nby 1%. In addition, our method can also obtain 4.6x and 6.4x reduction on memory usage for\nincremental quantization with 7 bits and 5 bits, respectively. Note that as activations are in floating\npoint representation, the runtime are not affected.\nAs shown in Table 4, we also discuss quantization with simplified networks, e.g., small models.\nWe can notice that when the network model is smaller, the segmentation accuracy with 4 bits and\n8 bits are no longer improved, however, there is a significant accuracy degradation. This is due to\nthe fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='the fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:6\nW. Chen and C. Zhang, et al.\nTable 4. Segmentation accuracy (averaged Dice score and F1 score in %) using quantization with 4 bits and 8\nbits with small models.\nNetwork\nfloat\n4 bit\n8 bit\nsimplified (Quantized SA + Float NT)\n82.53\n78.14\n82.14', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='82.53\n78.14\n82.14\nsimplified (Float SA + Quantized NT)\n82.53\n76.12\n81.27\n3.2\nImage Classification\n3.2.1\nNetwork Quantization. We use incremental quantization [34] on VGG-16 model for image\nclassification. The original VGG-16 has 16 layers (13 conv-layers and 3 FC layers) as shown in Fig.\n2. Besides that, we focus on the parameter-redundancy problem on VGG-16. We re-train two small\nVGG-16 models with the first five convolutional layers of the original eight layers) and the first', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='VGG-16 models with the first five convolutional layers of the original eight layers) and the first\neight convolutional layers of the original 11 layers), respectively for overfitting analysis.\n3.2.2\nExperimental Setup. We adopt the ILSVRC-2012 dataset [6] which contains over 1.2 million\ntraining images and over 50,000 testing images. VGG-16 [21] is adopted for classification. We\nquantize the weights from 2 bits to 7 bits for discussion. Additionally, we test two VGG models to 9\nVGG models to further clarify the compensation of ensemble learning in terms of accuracy loss\nbrought by weight quantization. And for small VGG-16 models, we compare 4 bits quantization\nand 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='and 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate\ndecay as 1 × 10−6. Stochastic gradient descent (SGD) function is selected to be the optimizer and\nMomentun of 0.9 is used to accelerate training. In order to unify the variables and discuss how the\nbit width impact the network performance, the max value of the quantized weights is set to 4.0,\nand it takes 4 iterations from full floating-point weights to quantized weights, and the accumulated\nportions of quantized weights at four iterative steps are {50%, 75%, 87.5%, 100%}.\nTable 5. Image classification performance (top-1 error in %) of VGG-16 model [14] with weights from 2 bits\nto 9 bits on ImageNet dataset.\nConfigurations', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='to 9 bits on ImageNet dataset.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nVGG-16\n69.29\n42.61\n63.72\n67.70\n68.64\n69.68\n70.36\n70.48\n70.28', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='69.68\n70.36\n70.48\n70.28\nTable 6. Image classification performance (top-1 error in %) of VGG-16 model [14] with parallel numbers\nfrom 2 to 7 on ImageNet dataset.\nConfigurations\nparallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nVGG-16\n69.29\n70.18\n70.10\n70.38\n69.31', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='70.18\n70.10\n70.38\n69.31\n69.82\n68.58\nVGG-16 with 8 bits\n69.29\n70.10\n71.24\n71.03\n69.80\n69.17\n69.85\n3.2.3\nResults and Analysis. We compare our result with the original and small VGG-16 models. As\nshown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='shown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%\nimprovement. We set a variety of ensemble experiments as the control group to discuss the effect\nof ensemble methods. The result of six configurations from two to seven parallel models using\n32-bit floating-point weights are shown in Table 6. With three parallel models, we can get a 0.81%\nperformance improvement. While for the VGG-16 model with 8 bits representation and parallel\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:7\n224x224x64\n112x112x128', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='1:7\n224x224x64\n112x112x128\n56x56x256\n28x28x512\n14x14x512\n7x7x512\n1x1x4096\n1x1x1000\nConvolutional + ReLu\nMaxpooling\nFully connected + ReLu\nSoftmax\nIncremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Incremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the\nfirst layers (with a feature map size of 112x112x128) and halved the channels of the rest layers except for\nthe first two input layers. In small VGG-16 models with 5 layers, we remove the third and forth layers (with\nfeature map sizes of 28x28x512 and 14x14x512, respectively). In small VGG-16 models with eight layers, we\nonly remove the forth layer.\nTable 7. Classification performance (top-1 error in %) comparison between floating pointss, 4 bits and 8 bits\nrepresentations using small VGG-16 models on ImageNet.\nNetwork\nfloat', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='representations using small VGG-16 models on ImageNet.\nNetwork\nfloat\n4 bit\n8 bit\nVGG-16 with 5 conv-layers\n64.14\n64.12\n64.05\nVGG-16 with 8 conv-layers\n66.04\n66.00\n66.02\nnumber from two to seven, we get a 1.95% improvement with 2 parallel models compared with the\noriginal model. In addition, 4x memory reduction is obtained with quantization.\nAs for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='As for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16\nmodels. We shorten the original VGG-16 model to contain 5/8 conv-layers with 3 FC layers. As\nshown in Table. 7, we get 0.02% precision loss after quantization on both two simplified VGG-16\nmodels. Comparing to the results in Table. 5, it can be concluded that quantization can benefit the\nover-parameterized models to some extent. However, if the original model is already simplified,\nthe inhibiting effect of quantization will be faded.\n3.3\nAutomatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Automatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech\nrecognition task. As shown in Fig. 3, the original Deep Speech has 5 hidden layers, composed of 3\ntime-distributed FC layers, 1 bi-directional RNN layer, 1 time-distributed FC layer followed by 1\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:8\nW. Chen and C. Zhang, et al.\nIncremental Network Quantization\nh(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='h(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax\nh(t+1)\nFig. 3. Illustration of quantization framework on the Deep Speech. For small Deep Speech models, we remove\nthe second and third FC layers.\nTable 8. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nweights from 2 bits to 9 bits.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech\n48.13\n60.33\n64.30\n69.53\n52.93\n47.20\n49.50\n52.40\n44.70\nTable 9. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nparallel numbers from 2 to 7.\nConfigurations\nParallel numbers\nOrignal\n2\n3\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Configurations\nParallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nDeep Speech\n48.13\n45.37\n47.30\n43.93\n42.83\n46.23\n43.30\nDeep Speech with 9 bits\n48.13\n43.90\n46.17\n48.07\n48.83\n53.90', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='46.17\n48.07\n48.83\n53.90\n45.53\nTable 10. Automatic speech recognition performance (classification error rate in %) comparison between\nfloating pointss, 4 bits and 8 bits representations using small Deep Speech models\nNetwork\nfloat\n4 bit\n8 bit\nSmall Deep Speech\n50.04\n47.05\n47.08\nsoftmax layer as output. We remove 2 time-distributed FC layers before the RNN layer to simplify\nthe model, and re-train the small model on TIMIT.\n3.3.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='the model, and re-train the small model on TIMIT.\n3.3.2\nExperiment Setup. We adopt the DARPA TIMIT Acoustic-Phonetic Continuous Speech\nCorpus dataset (TIMIT) [10] which contains 6300 sentences read by 630 different Americans for\nDeep Speech (DS) network [12]. We quantize the weights to 2 bits to 7 bits, and we test two to\nnine models in ensemble method. For training parameters, we set learning rate to 1 × 10−4 with a\nlearning rate decay of 1 × 10−6. The predicted error rate will be evaluated by the word error rate\n(WER).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='ACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:9\n3.3.3\nResults and Analysis. As shown in Table 8, the quantized model with 9 bits obtains the\noptimal performance which is 3.43% higher than the model with floating point representation. As\nshown in Table. 9, With three parallel networks, we can get an improvement of 4.2%. Using two\nparallel models, our method obtains the optimal performance among all the configurations which is\n4.23% higher than the original method, which is 0.03% better than the ensemble model. In addition,\n3.6x reduction on memory usage can be achieved.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='3.6x reduction on memory usage can be achieved.\nIn Table. 10, we discuss the quantization effectiveness on small Deep Speech models using 4 bits\nand 8 bits. Our model can get an improvement of 3% compared with the original model.\n3.4\nDiscussion\nAs for the accuracy, we can notice that incremental quantization can improve the segmentation\nperformance by around 1% on the MICCIA Gland dataset. For image classification, We ended up\nusing the 8-bit fixed-point VGG-16 model for two parallel models to achieve a 1.95% improvement\ncompared with the original model. For speech recognition, the optimal performance obtained an\nimprovement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='improvement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-\n6.4x reduction on memory usage. By comparing network training with and without quantization in\nTable 1, Table 5 and Table 8, we can observe that network training with quantization will not always\nimprove the accuracy, especially in extremely situations with 2-4 bits representation. In addition,\ntoo large bit widths do not improve the performance, and the optimal performance is achieved\nwith some median bit width, e.g., 6 bits and 7 bits. Thus, with proper incremental quantization, the\naccuracy can usually be improved with memory reduction.\nAs for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='As for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and\nautomatic speech recognition, respectively. Also the optimal accuracy is achieved in some median\nparallel number. Thus, a proper parallel number can improve the accuracy by scarifying some\nmemory operation and computation cost.\nFor overfitting, we discuss the corresponding small models in the three applications with quan-\ntiztaion. As shown in Table 4, Table 7 and Table 10, we can notice that when the model is much\nsmaller than the original networks, the accuracy degrades seriously such as the results in medical\nimage segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='image segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to\nthe fact that large models usually have more overfitting than small ones. Thus, when quantization is\napplied, the overfitting in large models is reduced, resulting with improved performance. However,\nfor small models with less overfitting, the overfitting is reduced and the representation capability\nis also degraded resulting with accuracy loss. Therefore, incremental quantization may be used as\na regulation method to reduce overfitting.\nIncremental quantization can not only reduce memory consumption of 3.5x-6.4x, but also speedup\nthe processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='the processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup\ndepends on specific hardware such as CPU, GPU and FPGAs. If no specific hardware module is\nimplemented for such transforming, no speedup is obtained, e.g., on existing CPUs and GPUs. If we\nimplement specific hardware module for 2D convolution operation on FPGAs, the speedup can be\n1.7x-7.8x according to [28]. Note that the speedup depends on a variety of factors such as hardware\nplatforms and network structures.\n4\nCONCLUSION\nAmong deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Among deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:10\nW. Chen and C. Zhang, et al.\nthat quantization leads to performance degradation, and plenty of existing works have explored\nquantization strategies aiming at minimum accuracy loss. In this paper, we show that quantization\ncan sometimes help to improve accuracy by imposing regularization on weight representations. We\nconduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='conduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-\ncation, and recurrent neural network (RNN) for automatic speech recognition, and experimental\nresults show that incremental quantization can improve the accuracy by 1%, 1.95%, 4.23% on the\nthree applications respectively with 3.5x-6.4x memory reduction. As a case in compression tech-\nniques, incremental quantization shows great potential to reduce onverfitting, and there may exist\nsome general rules and strategies to enable other compression techniques to have the capability of\nreducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='reducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS\nThis work was supported by the National key Research and Development Program of China (No.\n2018YFC1002600), the Science and Technology Planning Project of Guangdong Province, China\n(No. 2017B090904034, No. 2017B030314109, No. 2018B090944002, No. 2019B020230003), Guangdong\nPeak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Peak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES\n[1] Beatriz Blanco-Filgueira, Daniel García-Lesta, Mauro Fernández-Sanjurjo, Víctor Manuel Brea, and Paula López.\n2019. Deep learning-based multiple object visual tracking on embedded system for iot and mobile edge computing\napplications. IEEE Internet of Things Journal 6, 3 (2019), 5423–5431.\n[2] Hao Chen, Xiaojuan Qi, Jie-Zhi Cheng, Pheng-Ann Heng, et al. 2016. Deep Contextual Networks for Neuronal Structure\nSegmentation.. In AAAI. 1167–1173.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Segmentation.. In AAAI. 1167–1173.\n[3] Hao Chen, Xiaojuan Qi, Lequan Yu, and Pheng-Ann Heng. 2016. Dcan: Deep contour-aware networks for accurate\ngland segmentation. In CVPR. 2487–2496.\n[4] M Courbariaux and Y Bengio. [n.d.]. Binarynet: Training deep neural networks with weights and activations constrained\nto+ 1 or-1. CoRR abs/1602.02830 (2016).\n[5] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In NIPS. 3123–3131.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='with binary weights during propagations. In NIPS. 3123–3131.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR. IEEE, 248–255.\n[7] Yukun Ding, Jinglan Liu, and Yiyu Shi. 2018. On the Universal Approximability of Quantized ReLU Neural Networks.\narXiv preprint arXiv:1802.03646 (2018).\n[8] Marc Egger and Detlef Schoder. 2017. Consumer-oriented tech mining: Integrating the consumer perspective into\norganizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='organizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International\nConference on System Sciences.\n[9] Ge Gao, Chengyan Wang, Xiaodong Zhang, Juan Hu, Xuedong Yang, He Wang, Jue Zhang, and Xiaoying Wang. 2017.\nQuantitative analysis of diffusion-weighted magnetic resonance images: differentiation between prostate cancer and\nnormal tissue based on a computer-aided diagnosis system. Science China Life Sciences 60, 1 (2017), 37–43.\n[10] John S Garofolo. 1993. TIMIT acoustic phonetic continuous speech corpus. Linguistic Data Consortium, 1993 (1993).\n[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).\n[12] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh,\nShubho Sengupta, Adam Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint\narXiv:1412.5567 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='arXiv:1412.5567 (2014).\n[13] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Quantized neural networks:\nTraining neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061 (2016).\n[14] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry\nKalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:11\n[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2012. 3D convolutional neural networks for human action recognition.\nIEEE transactions on pattern analysis and machine intelligence 35, 1 (2012), 221–231.\n[16] Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks. arXiv preprint arXiv:1605.04711 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='[17] Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie Wen, Meiping Huang, Haiyun Yuan, and\nJian Zhuang. 2019. Machine vision guided 3d medical image compression for efficient transmission and accurate\nsegmentation in the clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n12687–12696.\n[18] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. Xnor-net: Imagenet classification\nusing binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='using binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.\n[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI. Springer, 234–241.\n[20] Chuck Rosenberg. 2013. Improving photo search: A step across the semantic gap. Google Research Blog 12 (2013).\n[21] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='arXiv preprint arXiv:1409.1556 (2014).\n[22] Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang,\nBogdan J Matuszewski, Elia Bruni, Urko Sanchez, et al. 2017. Gland segmentation in colon histology images: The glas\nchallenge contest. Medical image analysis 35 (2017), 489–502.\n[23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.\n[24] Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. 2018. Two-step quantization for\nlow-bit neural networks. In Proceedings of the IEEE Conference on computer vision and pattern recognition. 4376–4384.\n[25] Tianchen Wang, Xiaowei Xu, Jinjun Xiong, Qianjun Jia, Haiyun Yuan, Meiping Huang, Jian Zhuang, and Yiyu Shi.\n2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference\non Medical Image Computing and Computer-Assisted Intervention. Springer, 447–457.\n[26] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael Niemier, Jason Cong, Yu Hu, and Yiyu Shi. 2018. Scaling for\nedge inference of deep neural networks. Nature Electronics 1, 4 (2018), 216–222.\n[27] Xiaowei Xu, Qing Lu, Tianchen Wang, Yu Hu, Chen Zhuo, Jinglan Liu, and Yiyu Shi. 2018. Efficient hardware\nimplementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='implementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging\nTechnologies in Computing Systems (JETC) 14, 4 (2018), 1–20.\n[28] Xiaowei Xu, Qing Lu, Tianchen Wang, Jinglan Liu, Cheng Zhuo, Xiaobo Sharon Hu, and Yiyu Shi. 2017. Edge segmen-\ntation: Empowering mobile telemedicine with compressed cellular neural networks. In 2017 IEEE/ACM International\nConference on Computer-Aided Design (ICCAD). IEEE, 880–887.\n[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional\nnetworks for accurate biomedical image segmentation. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 8300–8308.\n[30] Xiaowei Xu, Tianchen Wang, Yiyu Shi, Haiyun Yuan, Qianjun Jia, Meiping Huang, and Jian Zhuang. 2019. Whole\nheart and great vessel segmentation in congenital heart disease using deep neural networks and graph matching. In\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.\n[31] Xiaowei Xu, Tianchen Wang, Jian Zhuang, Haiyun Yuan, Meiping Huang, Jianzheng Cen, Qianjun Jia, Yuhao Dong,\nand Yiyu Shi. 2020. Imagechd: A 3d computed tomography image dataset for classification of congenital heart disease.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 77–87.\n[32] Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and Danny Z Chen. 2017. Suggestive annotation: A deep active\nlearning framework for biomedical image segmentation. In International conference on medical image computing and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='learning framework for biomedical image segmentation. In International conference on medical image computing and\ncomputer-assisted intervention. Springer, 399–407.\n[33] Li Zhang, Haixin Ai, Wen Chen, Zimo Yin, Huan Hu, Junfeng Zhu, Jian Zhao, Qi Zhao, and Hongsheng Liu. 2017.\nCarcinoPred-EL: novel models for predicting the carcinogenicity of chemicals using molecular fingerprints and\nensemble learning methods. Scientific reports 7, 1 (2017), 1–14.\n[34] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. 2017. Incremental network quantization: Towards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044 (2017).\n[35] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016. DoReFa-Net: Training low\nbitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 (2016).\n[36] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. 2016. Trained ternary quantization. arXiv preprint\narXiv:1612.01064 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='arXiv:1612.01064 (2016).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc1fdf10> 111
cuda:2
[Document(page_content='1\nQuantization of Deep Neural Networks for Accurate Edge\nComputing\nWENTAO CHEN∗, HAILONG QIU∗, and JIAN ZHUANG, Guangdong Cardiovascular Institute,\nGuangdong Provincial Key Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s\nHospital, Guangdong Academy of Medical Sciences\nCHUTONG ZHANG∗ and YU HU, Easylink Technology Co., Ltd\nQING LU, TIANCHEN WANG, and YIYU SHI†, Department of Computer Science and Engineering,\nUniversity of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='University of Notre Dame\nMEIPING HUANG† and XIAOWE XU†, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong\nAcademy of Medical Sciences\nDeep neural networks (DNNs) have demonstrated their great potential in recent years, exceeding the per-\nformance of human experts in a wide range of applications. Due to their large sizes, however, compression\ntechniques such as weight quantization and pruning are usually applied before they can be accommodated on\nthe edge. It is generally believed that quantization leads to performance degradation, and plenty of existing\nworks have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='works have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that\nquantization, which essentially imposes regularization on weight representations, can sometimes help to\nimprove accuracy. We conduct comprehensive experiments on three widely used applications: fully con-\nnected network (FCN) for biomedical image segmentation, convolutional neural network (CNN) for image\nclassification on ImageNet, and recurrent neural network (RNN) for automatic speech recognition, and experi-\nmental results show that quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three applications\nrespectively with 3.5x-6.4x memory reduction.\nCCS Concepts: • Computing methodologies → Artificial intelligence; • Hardware → Emerging tech-\nnologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='nologies.\nAdditional Key Words and Phrases: Edge computing, Deep neural networks, Quantization\n∗Both authors contributed equally to this research. Wentao Chen works at Easylink Technology Co., Ltd, and this work is\ndone when he was an visiting scholar at Guangdong Provincial People’s Hospital.\n†Corresponding authors.\nAuthors’ addresses: Wentao Chen; Hailong Qiu; Jian Zhuang, Guangdong Cardiovascular Institute, Guangdong Provincial\nKey Laboratory of South China Structural Heart Disease, Guangdong Provincial People’s Hospital, Guangdong Academy\nof Medical Sciences, 106 Zhongshan Second Road, Guangzhou, Guangdong, 510080, chenwentaokl@gmail.com; Chutong', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Zhang; Yu Hu, Easylink Technology Co., Ltd, Wuhan, Hubei, 43000; Qing Lu; Tianchen Wang; Yiyu Shi, yshi4@nd.edu,\nDepartment of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, 46556; Meiping Huang;\nXiaowe Xu, Guangdong Cardiovascular Institute, Guangdong Provincial Key Laboratory of South China Structural Heart\nDisease, Guangdong Provincial People’s Hospital, Guangdong Academy of Medical Sciences, 106 Zhongshan Second Road,\nGuangzhou, Guangdong, 510080, xiao.wei.xu@foxmail.com.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2021 Association for Computing Machinery.\n1550-4832/2021/1-ART1 $15.00\nhttps://doi.org/10.1145/3451211', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='https://doi.org/10.1145/3451211\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\narXiv:2104.12046v2  [cs.CV]  14 Oct 2021\n1:2\nW. Chen and C. Zhang, et al.\nACM Reference Format:\nWentao Chen, Hailong Qiu, Jian Zhuang, Chutong Zhang, Yu Hu, Qing Lu, Tianchen Wang, Yiyu Shi, Meiping\nHuang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Huang, and Xiaowe Xu. 2021. Quantization of Deep Neural Networks for Accurate Edge Computing. ACM J.\nEmerg. Technol. Comput. Syst. 1, 1, Article 1 (January 2021), 11 pages. https://doi.org/10.1145/3451211\n1\nINTRODUCTION\nDeep Neural Networks (DNNs) have been widely used in various applications and show its great\npotential to tackle complex problems [1–3, 19, 26, 32]. Furthermore, it has been and continue to\nbe instrumental in enabling/advancing breakthroughs in various disciplines, including disease', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='be instrumental in enabling/advancing breakthroughs in various disciplines, including disease\ndiagnosis, real-time language translation, autonomous driving, etc. [8, 9, 15, 20, 22, 25, 30, 31, 33, 33].\nMeanwhile, edge computing for Internet of Things (IoT) has been widely studied which requires\nDNN models with small memory size and efficient computation [17, 27, 28]. Thus, there is a huge\ngap between current DNN models and the requirenment of edge computing.\nRecently, in order to accommodate DNNs on the edge, DNN quantization has become an active\nresearch topic [4, 5, 7, 11, 13, 13, 16, 18, 34–36], which aims to represent DNN weights with less', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='memory (precision) while maintaining acceptable accuracy with efficient memory and computation\ncosts. It has been observed in the literature, however, that sometimes quantization can improve\naccuracy which can be credited to the reduction of overfitting [23]. Dynamic fixed point [29]\ncan achieve 4x less memory operation cost with only 0.4-0.6% Top-5 accuracy loss for ImageNet\nclassification [6]. Ternary weight network and binaryConnect [29] have further reduced the bit-\nwidth of weights to 2 bits or even 1 bit with a relatively larger accuracy loss. Recently, their enhanced\nversion, trained ternary training and binary weight network [29] have reduced the accuracy loss to\nonly 0.6-0.8%. There also exists some works using non-linear quantization to represent the parameter\ndistribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='distribution for better accuracy [29]. Unlike the above works, some studies aims to quantize not\nonly the weights but also the activations. Quantized neural networks, binarized neural networks,\nand XNOR-net [29] reduced the weights to only 1 bit and the activations to 1-2 bits resulting in\na large reduction on memory and computation cost yet with significant accuracy loss. Two-Step\nquantization [24] has 2% drop for VGG-16 on ImageNet-2012 dataset and Integer-Arithmetic-Only\nquantization[14] has 3.1% accuracy loss. In some of the above works, we notice that quantization\ncan sometimes improve the performance [29], however, there is no comprehensive studies to verify\nthis point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='this point.\nIt is generally believed that quantization of DNN weights lead to performance degradation.\nFor example, two-step quantization [24] has a 2% accuracy drop on ILSVRC-2012 dataset and\nInteger-Arithmetic-Only quantization [14] has a 3.1% accuracy loss. Various works in the literature\nhave explored the best ways to quantize the weights with minimum accuracy loss. To some extent,\nquantization essentially imposes regularization on weight representations. As such, it should also\nsometimes help to improve accuracy when overfitting issue presents in large neural networks.\nTo demonstrate this point, in this paper, we conduct extensive experiments using incremental\nquantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='quantization on three applications: medical image segmentation, image classification and automatic\nspeech recognition. For fair comparison, we have re-implemented related methods and performed\nevaluations under the same hardware, deep learning frameworks and configurations. In medical\nimage segmentation, our method can achieve 1% accuracy improvement compared with the current\nstate-of-the-art method on the MICCIA Gland dataset. In image classification, extensive experiments\nare presented on ImageNet-2012 using a widely used CNN model VGG-16 [21], and the result shows\nthat our proposed method exceeds the current best performance using VGG-16 by up to 1.95%. In\nautomatic speech recognition, we quantize Deep Speech [12] network on the TIMIT dataset[10] and\nimproves the accuracy by 4.23%. We also discuss the incremental quantization on the performance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='improves the accuracy by 4.23%. We also discuss the incremental quantization on the performance\nof simplified network with different bit widths, and the experimental results show that incremental\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:3\nquantization can no longer improve the performance of simplified networks with less overfitting.\nIn addition, we get 3.5x-6.4x memory reduction, which is extremely beneficial in the context of\nedge computing.\nFCN\nFCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='FCN\nFCN\n...\nSuggestive\nFCN\nOriginal Training Set\nSuggested Training Set\nSelect Representative\nbased on Uncertainty and\nSimilarity\nQuantization\nQuantization\nSegmentation FCN\nInference of Suggestive FCN\nTraining of Suggestive FCN\nNetwork Training\nWith Quantization\n(QNT)\nSuggestive Annotation with Quantization (QSA)\nFig. 1. Illustration of quantization framework based on the suggestive annotation framework. In suggestive\nannotation with quantization, better training samples (suggestive training set) can be extracted from the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='annotation with quantization, better training samples (suggestive training set) can be extracted from the\noriginal training set. In network training with quantization, better performance can be achieved by reduce\noverfitting.\n2\nPRELIMINARIES\nIn this section, we briefly review the incremental quantization method used in the experiments.\nThe main goal of incremental quantization is to convert 32-bit-floating-point weights 𝑊 into\nlow-precision weights ˆ𝑊 either power of two or zero with minimum accuracy loss. Each of ˆ𝑊 is\nchosen from 𝑃𝑙 = {±2𝑛1, · · ·, ±2𝑛2}, where 𝑛1 and 𝑛2 are two integer numbers determined by the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='max absolute value of 𝑊𝑙 and expected quantized bit-width, and 𝑛2 ≤ 𝑛1.\nThe training procedure of incremental quantization [34] is consisted of three operations: weight\npartition, group-wise quantization and re-training. Weights partition is the most crucial part of the\nwhole process. We adopt pruning-inspired partition strategy to divide weights into two disjoint\ngroups by comparing the absolute value with layer-wise threshold, which usually achieves better\nperformance than random partition ([34]).\nFor the 𝑙-th layer, we define weights partition as shown in Eq. (1), where 𝐴(1)\n𝑙\nis the first group\nweights that need to be quantized and 𝐴(2)\n𝑙', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='is the first group\nweights that need to be quantized and 𝐴(2)\n𝑙\ndenotes the second group with remaining unquantized\nvalues.\n𝐴(1)\n𝑙\n∪ 𝐴(2)\n𝑙\n= 𝑊𝑙 (𝑖, 𝑗)\n𝐴(1)\n𝑙\n∩ 𝐴(2)\n𝑙\n= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='= ∅\n(1)\nThe weights in the first group are expected to form a low-precision base for the original model,\nthus they are quantized using Eq. (2),\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:4\nW. Chen and C. Zhang, et al.\n𝑤𝑞 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑝', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='𝑠𝑖𝑔𝑛(𝑤) × 2𝑝\n𝑖𝑓 3 × 2𝑝−2 ≤ |𝑤| ≤ 3 × 2𝑝−1;\n𝑠𝑖𝑔𝑛(𝑤) × 2𝑚 𝑖𝑓 |𝑤| ≥ 2𝑢;\n0\n𝑖𝑓 |𝑤| < 2−𝑙−1\n(2)\nwhere 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='where 𝑤𝑞 are quantized weights, 𝑤 represent original weights, 𝑢 and 𝑙 are upper and lower bounds\nof the quantized set(𝑙 ≤ 𝑝 ≤ 𝑢).\nBy contrast, the second group remains floating-point values to compensate the accuracy loss in\nmodel, and will be re-trained. After one round, these three steps are further adopted only on the\nsecond group in the rest of the training process. As a result, all weights are quantized or to zeros\nhence we gain memory reduction with slight accuracy loss.\n3\nCASE STUDIES\n3.1\nBiomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Biomedical Image Segmentation\nTable 1. Segmentation accuracy (averaged Dice score and F1 score in %) using different quantization bit\nwidth on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + Quantized NT\n86.25\n65.55\n79.37\n85.87\n86.12\n86.33', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='79.37\n85.87\n86.12\n86.33\n86.51\n85.93\n85.26\nQuantized SA + Float NT\n86.25\n62.98\n85.39\n86.17\n86.52\n86.66\n86.51\n86.22\n86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='86.12\nTable 2. Segmentation accuracy (averaged Dice score and F1 score in %) using different parallel FCNs on the\nMICCAI Gland dataset.\nConfigurations\nParallel number\nOrignal\n2\n3\n4\n5\n6\n7\nFloat SA + Float NT\n86.25\n86.23\n85.93\n85.81\n86.24\n87.25\n86.32\nFloat SA + 7 bits NT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='86.24\n87.25\n86.32\nFloat SA + 7 bits NT\n86.25\n86.2\n85.39\n85.32\n86.84\n86.08\n85.74\nTable 3. Segmentation accuracy (averaged Dice score and F1 score in %) using different bit width and 5\nparallel FCNs on the MICCAI Gland dataset.\nConfigurations\nBit width\nOrignal\n2 bits\n3 bits\n4 bits\n5 bits', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Orignal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nFloat SA + 5 FCNs\n87.85\n64.40\n85.94\n88.20\n88.51\n88.77\n89.12\n87.86\n87.55\n7 bits SA + 5 FCNs\n87.55\n71.88', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='87.55\n7 bits SA + 5 FCNs\n87.55\n71.88\n80.13\n87.55\n88.63\n88.73\n89.2\n88.67\n87.53\n3.1.1\nNetwork Quantization. As shown in Fig. 1, the network quantization for biomedical image\nsegmentation has two steps: suggestive annotation (SA) with quantization and network training\n(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='(NT) with quantization. In the first step, we add a quantization module to suggestive FCNs for\nhigh uncertainty. In the second step, quantization of segmentation FCNs are performed with\nthe suggestive training samples for higher accuracy. The FCN is a 34 layer network, and more\ndetails can refer to [32]. In order to obtain high representativeness, each FCN in suggestive FCNs\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:5\nshould be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='should be diverse for high uncertainty with acceptable accuracy. However, usually DNNs including\nFCNs are over-parameterized, and a large portion of the parameters is redundant. Thus, multiple\nsuggestive FCNs will have very small variance of the final prediction though with different weight\ninitialization. The adopted regularization techniques including weight decay and dropout scheme\nfurther make the multiple suggestive FCNs to be almost the same. By adding quantization to\nsuggestive annotation, the above requirement can be satisfied. Though it may be a little offensive\nsince most of the time it will degrade the accuracy, it is particularly appreciated by suggestive FCNs\nthat focus on uncertainty. Particularly quantization transforms the originally continuous weight\nspace, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='space, where the weights of several networks can be arbitrarily close, into a sparse and discrete one,\nand thus increasing the distances between the trained networks and accordingly the diversity of\nthe outputs. Note that accuracy should be also considered and too offensive quantization methods\nshould be avoided.\n3.1.2\nExperimental Setup. We adopt the 2015 MICCAI Gland Challenge dataset [22] which has\n85 training images (Part A: 37 normal glands, and Part B: 48 abnormal glands) and 80 testing\nimages (Part A: 60 normal glands, and Part B: 20 abnormal glands). In suggestive annotation [32],\n16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='16 images with the highest uncertainty scores are extracted first, and then 8 images are collected\nbased on their representativeness using similarity, which are added to the suggested training set in\neach iteration. Totally there are 120 iterations in suggestive annotation, and totally 960 suggested\ntraining samples are produced. We quantize the weights to 2 bits to 9 bits. For ensembling method,\nWe test 2 FCNs to 7 FCNs. We also discuss the overfitting problem with a simplified version of FCN\nby reducing the filter numbers by half. All the experiments are evaluated considering detection (F1\nscore) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='score) and segmentation (dice score) [29], and their averaged results are reported.\nThe training parameters are as follows. We adopt a simple learning rate scaling strategy: set\nlearning rate to 5×10−4 in the initial stage, and to 5×10−5 when the iteration time reaches a threshold.\nAll the configurations are repeated 4 times and the one with the optimal performance is selected\nfor comparison.\n3.1.3\nResults and Analysis. As there are two networks in suggestive annotation, we discussed both\nof the two with incremental quantization. As shown in Table 1, we first analysis the performance\nof quantization methods in SA with quantization and NT with quantization. We can notice that\ncompared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='compared with the original setting, quantization can improve the accuracy by 0.26% and 0.31% for\nquantized NT and quantized SA, respectively.\nAs shown in Table 2, we can discover that ensembling method can improve the segmentation\naccuracy most of the time. Similar to Table 1, the optimal accuracy is achieved in some median\nparallel number, e.g., 6 for float SA + 7 bits NT.\nAs shown in Table 3, we further discussed quantization on both SA and NT with the optimal\nparallel number. The trend is the same as that in Table 1, and the median bit width (7 bits) obtains\nthe best performance. With both quantization and ensemble method, we can achieve a promising\nperformance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='performance on the MICCAI 2015 Gland dataset, which outperforms the state-of-the-art method\nby 1%. In addition, our method can also obtain 4.6x and 6.4x reduction on memory usage for\nincremental quantization with 7 bits and 5 bits, respectively. Note that as activations are in floating\npoint representation, the runtime are not affected.\nAs shown in Table 4, we also discuss quantization with simplified networks, e.g., small models.\nWe can notice that when the network model is smaller, the segmentation accuracy with 4 bits and\n8 bits are no longer improved, however, there is a significant accuracy degradation. This is due to\nthe fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='the fact that smaller network with less parameters has less overfitting, and quantization further\nreduce the network’s representation capability resulting with degraded performance.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:6\nW. Chen and C. Zhang, et al.\nTable 4. Segmentation accuracy (averaged Dice score and F1 score in %) using quantization with 4 bits and 8\nbits with small models.\nNetwork\nfloat\n4 bit\n8 bit\nsimplified (Quantized SA + Float NT)\n82.53\n78.14\n82.14', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='82.53\n78.14\n82.14\nsimplified (Float SA + Quantized NT)\n82.53\n76.12\n81.27\n3.2\nImage Classification\n3.2.1\nNetwork Quantization. We use incremental quantization [34] on VGG-16 model for image\nclassification. The original VGG-16 has 16 layers (13 conv-layers and 3 FC layers) as shown in Fig.\n2. Besides that, we focus on the parameter-redundancy problem on VGG-16. We re-train two small\nVGG-16 models with the first five convolutional layers of the original eight layers) and the first', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='VGG-16 models with the first five convolutional layers of the original eight layers) and the first\neight convolutional layers of the original 11 layers), respectively for overfitting analysis.\n3.2.2\nExperimental Setup. We adopt the ILSVRC-2012 dataset [6] which contains over 1.2 million\ntraining images and over 50,000 testing images. VGG-16 [21] is adopted for classification. We\nquantize the weights from 2 bits to 7 bits for discussion. Additionally, we test two VGG models to 9\nVGG models to further clarify the compensation of ensemble learning in terms of accuracy loss\nbrought by weight quantization. And for small VGG-16 models, we compare 4 bits quantization\nand 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='and 8 bits quantization with floating weights. Learning rate is set to 1 × 10−4 with learning rate\ndecay as 1 × 10−6. Stochastic gradient descent (SGD) function is selected to be the optimizer and\nMomentun of 0.9 is used to accelerate training. In order to unify the variables and discuss how the\nbit width impact the network performance, the max value of the quantized weights is set to 4.0,\nand it takes 4 iterations from full floating-point weights to quantized weights, and the accumulated\nportions of quantized weights at four iterative steps are {50%, 75%, 87.5%, 100%}.\nTable 5. Image classification performance (top-1 error in %) of VGG-16 model [14] with weights from 2 bits\nto 9 bits on ImageNet dataset.\nConfigurations', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='to 9 bits on ImageNet dataset.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nVGG-16\n69.29\n42.61\n63.72\n67.70\n68.64\n69.68\n70.36\n70.48\n70.28', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='69.68\n70.36\n70.48\n70.28\nTable 6. Image classification performance (top-1 error in %) of VGG-16 model [14] with parallel numbers\nfrom 2 to 7 on ImageNet dataset.\nConfigurations\nparallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nVGG-16\n69.29\n70.18\n70.10\n70.38\n69.31', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='70.18\n70.10\n70.38\n69.31\n69.82\n68.58\nVGG-16 with 8 bits\n69.29\n70.10\n71.24\n71.03\n69.80\n69.17\n69.85\n3.2.3\nResults and Analysis. We compare our result with the original and small VGG-16 models. As\nshown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='shown in Table 5, the configuration with 8 bits obtains the optimal performance, which gets a 1.1%\nimprovement. We set a variety of ensemble experiments as the control group to discuss the effect\nof ensemble methods. The result of six configurations from two to seven parallel models using\n32-bit floating-point weights are shown in Table 6. With three parallel models, we can get a 0.81%\nperformance improvement. While for the VGG-16 model with 8 bits representation and parallel\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:7\n224x224x64\n112x112x128', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='1:7\n224x224x64\n112x112x128\n56x56x256\n28x28x512\n14x14x512\n7x7x512\n1x1x4096\n1x1x1000\nConvolutional + ReLu\nMaxpooling\nFully connected + ReLu\nSoftmax\nIncremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Incremental Network Quantization\nFig. 2. Illustration of quantization framework on the VGG-16 model. In small VGG models, we remove the\nfirst layers (with a feature map size of 112x112x128) and halved the channels of the rest layers except for\nthe first two input layers. In small VGG-16 models with 5 layers, we remove the third and forth layers (with\nfeature map sizes of 28x28x512 and 14x14x512, respectively). In small VGG-16 models with eight layers, we\nonly remove the forth layer.\nTable 7. Classification performance (top-1 error in %) comparison between floating pointss, 4 bits and 8 bits\nrepresentations using small VGG-16 models on ImageNet.\nNetwork\nfloat', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='representations using small VGG-16 models on ImageNet.\nNetwork\nfloat\n4 bit\n8 bit\nVGG-16 with 5 conv-layers\n64.14\n64.12\n64.05\nVGG-16 with 8 conv-layers\n66.04\n66.00\n66.02\nnumber from two to seven, we get a 1.95% improvement with 2 parallel models compared with the\noriginal model. In addition, 4x memory reduction is obtained with quantization.\nAs for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='As for overfitting, we discuss the performance of 4 bits and 8 bits quantization on small VGG-16\nmodels. We shorten the original VGG-16 model to contain 5/8 conv-layers with 3 FC layers. As\nshown in Table. 7, we get 0.02% precision loss after quantization on both two simplified VGG-16\nmodels. Comparing to the results in Table. 5, it can be concluded that quantization can benefit the\nover-parameterized models to some extent. However, if the original model is already simplified,\nthe inhibiting effect of quantization will be faded.\n3.3\nAutomatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Automatic Speech Recognition\n3.3.1\nNetwork Quantization. We use incremental quantization [34] on Deep Speech for speech\nrecognition task. As shown in Fig. 3, the original Deep Speech has 5 hidden layers, composed of 3\ntime-distributed FC layers, 1 bi-directional RNN layer, 1 time-distributed FC layer followed by 1\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:8\nW. Chen and C. Zhang, et al.\nIncremental Network Quantization\nh(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='h(t-1)\nTime-distributed FC\nBi-directional RNN\nSoftmax\nh(t+1)\nFig. 3. Illustration of quantization framework on the Deep Speech. For small Deep Speech models, we remove\nthe second and third FC layers.\nTable 8. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nweights from 2 bits to 9 bits.\nConfigurations\nQuantization bit width\nOriginal\n2 bits\n3 bits\n4 bits\n5 bits\n6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='6 bits\n7 bits\n8 bits\n9 bits\nDeep Speech\n48.13\n60.33\n64.30\n69.53\n52.93\n47.20\n49.50\n52.40\n44.70\nTable 9. Automatic speech recognition performance (classification error rate in %) on Deep Speech [34] with\nparallel numbers from 2 to 7.\nConfigurations\nParallel numbers\nOrignal\n2\n3\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Configurations\nParallel numbers\nOrignal\n2\n3\n4\n5\n6\n7\nDeep Speech\n48.13\n45.37\n47.30\n43.93\n42.83\n46.23\n43.30\nDeep Speech with 9 bits\n48.13\n43.90\n46.17\n48.07\n48.83\n53.90', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='46.17\n48.07\n48.83\n53.90\n45.53\nTable 10. Automatic speech recognition performance (classification error rate in %) comparison between\nfloating pointss, 4 bits and 8 bits representations using small Deep Speech models\nNetwork\nfloat\n4 bit\n8 bit\nSmall Deep Speech\n50.04\n47.05\n47.08\nsoftmax layer as output. We remove 2 time-distributed FC layers before the RNN layer to simplify\nthe model, and re-train the small model on TIMIT.\n3.3.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='the model, and re-train the small model on TIMIT.\n3.3.2\nExperiment Setup. We adopt the DARPA TIMIT Acoustic-Phonetic Continuous Speech\nCorpus dataset (TIMIT) [10] which contains 6300 sentences read by 630 different Americans for\nDeep Speech (DS) network [12]. We quantize the weights to 2 bits to 7 bits, and we test two to\nnine models in ensemble method. For training parameters, we set learning rate to 1 × 10−4 with a\nlearning rate decay of 1 × 10−6. The predicted error rate will be evaluated by the word error rate\n(WER).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='ACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:9\n3.3.3\nResults and Analysis. As shown in Table 8, the quantized model with 9 bits obtains the\noptimal performance which is 3.43% higher than the model with floating point representation. As\nshown in Table. 9, With three parallel networks, we can get an improvement of 4.2%. Using two\nparallel models, our method obtains the optimal performance among all the configurations which is\n4.23% higher than the original method, which is 0.03% better than the ensemble model. In addition,\n3.6x reduction on memory usage can be achieved.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='3.6x reduction on memory usage can be achieved.\nIn Table. 10, we discuss the quantization effectiveness on small Deep Speech models using 4 bits\nand 8 bits. Our model can get an improvement of 3% compared with the original model.\n3.4\nDiscussion\nAs for the accuracy, we can notice that incremental quantization can improve the segmentation\nperformance by around 1% on the MICCIA Gland dataset. For image classification, We ended up\nusing the 8-bit fixed-point VGG-16 model for two parallel models to achieve a 1.95% improvement\ncompared with the original model. For speech recognition, the optimal performance obtained an\nimprovement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='improvement of 4.23% over the original Deep Speech model. In addition, our method has a up to 3.5-\n6.4x reduction on memory usage. By comparing network training with and without quantization in\nTable 1, Table 5 and Table 8, we can observe that network training with quantization will not always\nimprove the accuracy, especially in extremely situations with 2-4 bits representation. In addition,\ntoo large bit widths do not improve the performance, and the optimal performance is achieved\nwith some median bit width, e.g., 6 bits and 7 bits. Thus, with proper incremental quantization, the\naccuracy can usually be improved with memory reduction.\nAs for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='As for the impact on ensemble method, six parallel FCNs, two parallel CNNs, and two parallel\nRNNs with quantization reaches the optimal performance on segmentation, classification and\nautomatic speech recognition, respectively. Also the optimal accuracy is achieved in some median\nparallel number. Thus, a proper parallel number can improve the accuracy by scarifying some\nmemory operation and computation cost.\nFor overfitting, we discuss the corresponding small models in the three applications with quan-\ntiztaion. As shown in Table 4, Table 7 and Table 10, we can notice that when the model is much\nsmaller than the original networks, the accuracy degrades seriously such as the results in medical\nimage segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='image segmentation and image classification. When the model is only simplified a bit such as the\nsmall model in automatic speech recognition, the accuracy is still improved. This is possibly due to\nthe fact that large models usually have more overfitting than small ones. Thus, when quantization is\napplied, the overfitting in large models is reduced, resulting with improved performance. However,\nfor small models with less overfitting, the overfitting is reduced and the representation capability\nis also degraded resulting with accuracy loss. Therefore, incremental quantization may be used as\na regulation method to reduce overfitting.\nIncremental quantization can not only reduce memory consumption of 3.5x-6.4x, but also speedup\nthe processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='the processing. As the optimization mainly comes from transforming multiplication to addition in\nthe convolution calculation, specific hardware modules are required for speedup. Thus, the speedup\ndepends on specific hardware such as CPU, GPU and FPGAs. If no specific hardware module is\nimplemented for such transforming, no speedup is obtained, e.g., on existing CPUs and GPUs. If we\nimplement specific hardware module for 2D convolution operation on FPGAs, the speedup can be\n1.7x-7.8x according to [28]. Note that the speedup depends on a variety of factors such as hardware\nplatforms and network structures.\n4\nCONCLUSION\nAmong deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Among deep neural networks (DNNs), compression techniques such as weight quantization and\npruning are usually applied before they can be accommodated on the edge. It is generally believed\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\n1:10\nW. Chen and C. Zhang, et al.\nthat quantization leads to performance degradation, and plenty of existing works have explored\nquantization strategies aiming at minimum accuracy loss. In this paper, we show that quantization\ncan sometimes help to improve accuracy by imposing regularization on weight representations. We\nconduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='conduct comprehensive experiments on three widely used applications: fully connected network\n(FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classifi-\ncation, and recurrent neural network (RNN) for automatic speech recognition, and experimental\nresults show that incremental quantization can improve the accuracy by 1%, 1.95%, 4.23% on the\nthree applications respectively with 3.5x-6.4x memory reduction. As a case in compression tech-\nniques, incremental quantization shows great potential to reduce onverfitting, and there may exist\nsome general rules and strategies to enable other compression techniques to have the capability of\nreducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='reducing overfitting. We encourage related researchers to explore this interesting topic, which is\nalso our future work.\nACKNOWLEDGMENTS\nThis work was supported by the National key Research and Development Program of China (No.\n2018YFC1002600), the Science and Technology Planning Project of Guangdong Province, China\n(No. 2017B090904034, No. 2017B030314109, No. 2018B090944002, No. 2019B020230003), Guangdong\nPeak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Peak Project (No. DFJH201802), the National Natural Science Foundation of China (No. 62006050).\nREFERENCES\n[1] Beatriz Blanco-Filgueira, Daniel García-Lesta, Mauro Fernández-Sanjurjo, Víctor Manuel Brea, and Paula López.\n2019. Deep learning-based multiple object visual tracking on embedded system for iot and mobile edge computing\napplications. IEEE Internet of Things Journal 6, 3 (2019), 5423–5431.\n[2] Hao Chen, Xiaojuan Qi, Jie-Zhi Cheng, Pheng-Ann Heng, et al. 2016. Deep Contextual Networks for Neuronal Structure\nSegmentation.. In AAAI. 1167–1173.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Segmentation.. In AAAI. 1167–1173.\n[3] Hao Chen, Xiaojuan Qi, Lequan Yu, and Pheng-Ann Heng. 2016. Dcan: Deep contour-aware networks for accurate\ngland segmentation. In CVPR. 2487–2496.\n[4] M Courbariaux and Y Bengio. [n.d.]. Binarynet: Training deep neural networks with weights and activations constrained\nto+ 1 or-1. CoRR abs/1602.02830 (2016).\n[5] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. 2015. Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In NIPS. 3123–3131.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='with binary weights during propagations. In NIPS. 3123–3131.\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR. IEEE, 248–255.\n[7] Yukun Ding, Jinglan Liu, and Yiyu Shi. 2018. On the Universal Approximability of Quantized ReLU Neural Networks.\narXiv preprint arXiv:1802.03646 (2018).\n[8] Marc Egger and Detlef Schoder. 2017. Consumer-oriented tech mining: Integrating the consumer perspective into\norganizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='organizational technology intelligence-the case of autonomous driving. In Proceedings of the 50th Hawaii International\nConference on System Sciences.\n[9] Ge Gao, Chengyan Wang, Xiaodong Zhang, Juan Hu, Xuedong Yang, He Wang, Jue Zhang, and Xiaoying Wang. 2017.\nQuantitative analysis of diffusion-weighted magnetic resonance images: differentiation between prostate cancer and\nnormal tissue based on a computer-aided diagnosis system. Science China Life Sciences 60, 1 (2017), 37–43.\n[10] John S Garofolo. 1993. TIMIT acoustic phonetic continuous speech corpus. Linguistic Data Consortium, 1993 (1993).\n[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='[11] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).\n[12] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh,\nShubho Sengupta, Adam Coates, et al. 2014. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint\narXiv:1412.5567 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='arXiv:1412.5567 (2014).\n[13] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Quantized neural networks:\nTraining neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061 (2016).\n[14] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry\nKalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2704–2713.\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.\nQuantization of Deep Neural Networks for Accurate Edge Computing\n1:11\n[15] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2012. 3D convolutional neural networks for human action recognition.\nIEEE transactions on pattern analysis and machine intelligence 35, 1 (2012), 221–231.\n[16] Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary weight networks. arXiv preprint arXiv:1605.04711 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='[17] Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie Wen, Meiping Huang, Haiyun Yuan, and\nJian Zhuang. 2019. Machine vision guided 3d medical image compression for efficient transmission and accurate\nsegmentation in the clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n12687–12696.\n[18] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. Xnor-net: Imagenet classification\nusing binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='using binary convolutional neural networks. In European Conference on Computer Vision. Springer, 525–542.\n[19] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI. Springer, 234–241.\n[20] Chuck Rosenberg. 2013. Improving photo search: A step across the semantic gap. Google Research Blog 12 (2013).\n[21] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition.\narXiv preprint arXiv:1409.1556 (2014).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='arXiv preprint arXiv:1409.1556 (2014).\n[22] Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang,\nBogdan J Matuszewski, Elia Bruni, Urko Sanchez, et al. 2017. Gland segmentation in colon histology images: The glas\nchallenge contest. Medical image analysis 35 (2017), 489–502.\n[23] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.\n[24] Peisong Wang, Qinghao Hu, Yifan Zhang, Chunjie Zhang, Yang Liu, and Jian Cheng. 2018. Two-step quantization for\nlow-bit neural networks. In Proceedings of the IEEE Conference on computer vision and pattern recognition. 4376–4384.\n[25] Tianchen Wang, Xiaowei Xu, Jinjun Xiong, Qianjun Jia, Haiyun Yuan, Meiping Huang, Jian Zhuang, and Yiyu Shi.\n2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='2020. Ica-unet: Ica inspired statistical unet for real-time 3d cardiac cine mri segmentation. In International Conference\non Medical Image Computing and Computer-Assisted Intervention. Springer, 447–457.\n[26] Xiaowei Xu, Yukun Ding, Sharon Xiaobo Hu, Michael Niemier, Jason Cong, Yu Hu, and Yiyu Shi. 2018. Scaling for\nedge inference of deep neural networks. Nature Electronics 1, 4 (2018), 216–222.\n[27] Xiaowei Xu, Qing Lu, Tianchen Wang, Yu Hu, Chen Zhuo, Jinglan Liu, and Yiyu Shi. 2018. Efficient hardware\nimplementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='implementation of cellular neural networks with incremental quantization and early exit. ACM Journal on Emerging\nTechnologies in Computing Systems (JETC) 14, 4 (2018), 1–20.\n[28] Xiaowei Xu, Qing Lu, Tianchen Wang, Jinglan Liu, Cheng Zhuo, Xiaobo Sharon Hu, and Yiyu Shi. 2017. Edge segmen-\ntation: Empowering mobile telemedicine with compressed cellular neural networks. In 2017 IEEE/ACM International\nConference on Computer-Aided Design (ICCAD). IEEE, 880–887.\n[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='[29] Xiaowei Xu, Qing Lu, Lin Yang, Sharon Hu, Danny Chen, Yu Hu, and Yiyu Shi. 2018. Quantization of fully convolutional\nnetworks for accurate biomedical image segmentation. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. 8300–8308.\n[30] Xiaowei Xu, Tianchen Wang, Yiyu Shi, Haiyun Yuan, Qianjun Jia, Meiping Huang, and Jian Zhuang. 2019. Whole\nheart and great vessel segmentation in congenital heart disease using deep neural networks and graph matching. In\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 477–485.\n[31] Xiaowei Xu, Tianchen Wang, Jian Zhuang, Haiyun Yuan, Meiping Huang, Jianzheng Cen, Qianjun Jia, Yuhao Dong,\nand Yiyu Shi. 2020. Imagechd: A 3d computed tomography image dataset for classification of congenital heart disease.\nIn International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 77–87.\n[32] Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and Danny Z Chen. 2017. Suggestive annotation: A deep active\nlearning framework for biomedical image segmentation. In International conference on medical image computing and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='learning framework for biomedical image segmentation. In International conference on medical image computing and\ncomputer-assisted intervention. Springer, 399–407.\n[33] Li Zhang, Haixin Ai, Wen Chen, Zimo Yin, Huan Hu, Junfeng Zhu, Jian Zhao, Qi Zhao, and Hongsheng Liu. 2017.\nCarcinoPred-EL: novel models for predicting the carcinogenicity of chemicals using molecular fingerprints and\nensemble learning methods. Scientific reports 7, 1 (2017), 1–14.\n[34] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. 2017. Incremental network quantization: Towards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044 (2017).\n[35] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. 2016. DoReFa-Net: Training low\nbitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160 (2016).\n[36] Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. 2016. Trained ternary quantization. arXiv preprint\narXiv:1612.01064 (2016).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'}), Document(page_content='arXiv:1612.01064 (2016).\nACM J. Emerg. Technol. Comput. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpwfi5kred/0000f570-04bc-49fa-b2d4-56447ca1bd9b.pdf'})]
cuda:2
请讲述研究现状部分
 tmpwfi5kred
[]
cuda:2
[0.9394711, 0.94568217, 0.9489053, 0.95719475, 0.9692631, 0.98716176]
请讲讲这篇论文解决的问题
 tmpwfi5kred
[]
cuda:2
[1.1182556, 1.1190737, 1.1264346, 1.140867, 1.1452272, 1.145381]
请讲讲这篇论文解决的问题
 tmpwfi5kred
[]
cuda:2
[1.1182556, 1.1190737, 1.1264346, 1.140867, 1.1452272, 1.145381]
请讲讲这篇论文实验得到的结果
 tmpwfi5kred
[]
cuda:2
[1.0491071, 1.1176666, 1.1300519, 1.1313919, 1.1346862, 1.1360748]
请讲讲这篇论文得出的结论
 tmpwfi5kred
[]
cuda:2
[1.0507694, 1.0610565, 1.0669796, 1.0750844, 1.0883445, 1.092176]
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpk5p038x4, tmpk5p038x4
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd59c790> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk5p038x4/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpp1xce4sg, tmpp1xce4sg
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f9088940650> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpp1xce4sg/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
[UploadFile(filename='a6448dca-a906-464e-adbc-6210acce64a6.txt', size=5175, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a6448dca-a906-464e-adbc-6210acce64a6.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp_z25wg6f, tmp_z25wg6f
File: a6448dca-a906-464e-adbc-6210acce64a6.txt, msg: 成功上传文件 a6448dca-a906-464e-adbc-6210acce64a6.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8ff44c6350> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nEfficient Flow-Guided Multi-frame De-fencing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content="Taking photographs ''in-the-wild'' is often hindered by fence obstructionsthat stand between the camera user and the scene of interest, and which arehard or impossible to avoid. De-fencing is the algorithmic process ofautomatically removing such obstructions from images, revealing the invisibleparts of the scene. While this problem can be formulated as a combination offence segmentation and image inpainting, this often leads to implausiblehallucinations of the occluded regions. Existing multi-frame approaches rely onpropagating information to a selected keyframe from its temporal neighbors, butthey are often inefficient and struggle with alignment of severely obstructedimages. In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='In this work we draw inspiration from the video completion literatureand develop a simplified framework for multi-frame de-fencing that computeshigh quality flow maps directly from obstructed frames and uses them toaccurately align frames. Our primary focus is efficiency and practicality in areal-world setting: the input to our algorithm is a short image burst (5frames) - a data modality commonly available in modern smartphones - and theoutput is a single reconstructed keyframe, with the fence removed. Our approachleverages simple yet effective CNN modules, trained on carefully generatedsynthetic data, and outperforms more complicated alternatives real bursts, bothquantitatively and qualitatively, while running real-time.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_z25wg6f/a6448dca-a906-464e-adbc-6210acce64a6.txt'})]
cuda:2
[UploadFile(filename='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf', size=11976858, headers=Headers({'content-disposition': 'form-data; name="files"; filename="Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpunkyn4bc, tmpunkyn4bc
File: Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf, msg: 成功上传文件 Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf, docs: [Document(page_content='Window Detection In Facade Imagery:\nA Deep Learning Approach Using Mask R-CNN\nNils Nordmark, University of Gothenburg\nMola Ayenew, University of Gothenburg\nAbstract\nThe parsing of windows in building facades is a long-\ndesired but challenging task in computer vision. It is\ncrucial to urban analysis, semantic reconstruction, life\ncycle analysis, digital twins and scene parsing amongst\nother building-related tasks that require high-quality se-\nmantic data. In this article, we investigate the usage of the\nMask R-CNN framework to be used for window detection\nof facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='of facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to\nproduce instance segmentations of our new window class.\nExperimental results show that our suggested approach can\nwith a relatively small dataset train the network only with\ntransfer learning and augmentation achieve results on par\nwith prior state-of-the-art window detection approaches,\neven without post-optimization techniques.\n1. INTRODUCTION\nGathering semantic data regarding building fea-\ntures from images is desired as manual gathering\ncan often be a challenging, time consuming and\nresource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and\nbalconies are often not available, can change over\ntime and the data gathering is usually a manual\nprocess.\nMany different approaches have been applied to\nsolve the gathering of semantic data within many\ndifferent domains. Within building facade parsing\nthis task has mostly been treated as a process of\nsemantic segmentation of facade imagery into archi-\ntectural structural classes such as windows, doors,\nbalconies and so on. This semantic segmentation\napproach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='approach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the\nsame object as each pixel is classiﬁed to belong\nto a particular class. Speciﬁcally for parsing of\nwindows instances in building facades, we suggest\nan approach that can decouple segmentation and\nclassiﬁcation to detect windows in an image with\na bounding box and segmentation mask for each\nwindow instance.\nWith instance segmentation, we can analyze the\nwindows independently with detailed and compre-\nhensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='hensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,\nwe propose an approach based on the Mask R-CNN\nframework.\n2. RELATED WORK\nFacade parsing has been studied for a long time\nand there exists a large body of work on how to\ndeal with such a problem.\nEarlier approaches\nZhao et al. (2010) proposed parsing ground-\nlevel images into architectural units which could\nbe used for city modelling. Wendel, Donoser, and\nBischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-\nmeans clustering to tackle the same problem.\nMany others proposed a procedural shape gram-\nmar approach. This approach could at a high-level\nbe explained as a hand-crafted set of rules of basic\nshapes which represents structured geometries, i.e.\nthe structure of the object is encoded as a set of\nparametric rules. Models could then be generated\nby iteratively applying the rules on a starting shape.\nMüller et al. (2007) used a procedural modelling\npipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.\nHan and Zhu (2008) extended the work with an\nattribute grammar which used a more advanced\narXiv:2107.10006v1  [cs.CV]  21 Jul 2021\nbottom-up and top-down mechanism for recogniz-\ning objects. Their method was based on previous\nobject recognition work, such as Zhu, Zhang, and\nTu (2000) and Tu et al. (2005).\nTeboul et al. (2010) combined machine learning\nalgorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='algorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve\nthe works of Müller et al. (2007) and Han and\nZhu (2008), the learning process to some extent still\nwas based on hand-crafted rules.\nMathias et al. (2011) took another approach based\non architectural styles for automatic classiﬁcation.\nThey tried to extend Müller et al. (2007) work\nwith appropriate style information as they argued\nthat it could be used to select a more appropriate\nprocedural grammar for the task of building recon-\nstruction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='struction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on\nthe assumption that the building style was known\nand needed to be conducted as a manual task.\nInstead, they tried to automate the classiﬁcation of\narchitectural styles, by using facade images to train\ntheir classiﬁer to distinguish between three distinct\narchitectural styles. For classifying those speciﬁc\nstyles they were quite accurate but it also required\na rather speciﬁc set of circumstances to succeed.\nAs most of the earlier approaches used hand-\ncrafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more\ncomplex objects, i.e. could not handle deviations\nfrom these set of user-deﬁned rules well. They also\nsuffered in accuracy when the input image was\nmore complex, such as street-view images where the\nimage was not perfectly rectiﬁed. Another common\nlimitation was that they often suffered from heavy\ncomputational costs and slow running times (Bala\nand Kumar (2016), Koch et al. (2018)).\nConvolutional Neural Networks Approaches\nMore recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='More recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success\nof AlexNet in 2012, many semantic segmentation\napproaches which used to apply traditional ma-\nchine learning algorithms have instead been pri-\nmarily based on CNNs Bala and Kumar (2016);\nParthasarathy (2017).\nImproved models quickly followed AlexNet such\nas Segnet (Badrinarayanan, Kendall, Cipolla, et\nal. (2015)) which was one of the ﬁrst to tackle\nthe problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-\nhamer, and Darrell (2015) showed that a fully\nconvolutional network (FCN) trained end-to-end,\npixels-to-pixels could exceed then state-of-the-art\nsemantic image segmentation methods by retaining\nspatial information of an image. In more detail,\nthey used three pre-trained base models (AlexNet,\nVGGNet and GoogleNet) and transferred them from\nclassiﬁers by replacing fully connected layers with\nconvolutional layers. Since then a semantic seg-\nmentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.\nDutta (2020)).\nChen et al. (2017) use dilated convolution instead\nof plain convolution to set new state-of-art results\nwith their proposed DeepLab system.\nRecently, more automated methods and entirely\nbased on deep learning have achieved remarkable\nimprovements in image analysis, such as Penatti,\nNogueira, and Dos Santos (2015) and Nogueira,\nPenatti, and Dos Santos (2017).\nDeep Learning Approaches\nDeep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Deep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision\ntasks (classiﬁcation, object detection, semantic seg-\nmentation and instance segmentation) in a lot of\nbenchmarks and a variety of ﬁelds, e.g. medicine,\nsurveillance, economics, sociology. They have been\napplied to a diversity of tasks such as face detection,\nspeech recognition, autonomous vehicles and so on.\nDeep Learning Approaches Related to Facade\nParsing\nNaturally, deep learning was also applied to\nbuilding-related image analysis problems as well.\n2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing\nquite speciﬁcally and rather successfully.\nSchmitz and Mayer (2016) method for semantic\ninterpretation of facade images used a convolutional\nnetwork and a key insight was that even smaller\ndatasets could train the network when transfer learn-\ning could be employed. For validation, they used the\neTRIMS dataset to measure the overall accuracy,\nprecision and recall. The F1 score had an accuracy\nof 0.82± 0.09 for the facade categories, e.g. win-\ndows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='dows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of\nfacade parsing even further with their method Deep-\nFacade which combined both the learning capacity\nof deep convolutional neural networks with rules\nand a loss function based on symmetry found in\nbuilding facades structures. They trained an FCN-\n8s network with their novel loss function to obtain\nexperimental results on both the ECP dataset and\nthe eTRIMS dataset that signiﬁcantly outperformed\nprevious state-of-the-art methods. Pixel accuracies\non the eTRIMS dataset reached as high as 0.91 for\nwindows.\nFathalla and Vogiatzis (2017) used appearance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='windows.\nFathalla and Vogiatzis (2017) used appearance\nand layout cues combined with the VGG-model.\nTheir method only had a single object of focus and\noverall accuracy was on par with other methods,\nalthough supplementary materials had to be asked\nfor an in-depth understanding of the results.\nKoch et al. (2018) developed a multi-scale patch-\nbased pattern extraction that combined with a CNN\n(AlexNet), to automatically estimate the condition\nof buildings. They showed to some degree their\nmethod could serve as a proxy for condition esti-\nmates by appraisers.\nBacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with\na deep learning-based facade segmentation stage\nbased on generative adversarial networks (GANs).\nThe task was to reconstruct 3D models of build-\nings facades, in the urban environment for cultural\nheritage.\nHu et al. (2020) used the bounding boxes detected\nby YOLO architecture in real-time to guide facade\nreconstruction in an interactive environment. Results\nshowed accuracies above 0.82, which they deemed\nas sufﬁcient for their 3D reconstruction task.\nRahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of\nboth traditional and modern machine learning ap-\nproaches. His system was trained for windows delin-\neation segmentation, i.e. detecting window frames,\nmullions, transoms and so on. For small datasets\nit showed promising results, however, this method\nis not competitive compared to complete end-to-\nend state-of-the-art deep learning approaches such\nas Liu et al. (2017).\nWenguang Ma and Wei Ma (2020) used Faster\nR-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.\nThey called their method WD-Net which showed\n0.5 per cent improved accuracy compared with the\nbaseline Faster R-CNN.\n3. OUR APPROACH\nMask R-CNN Architecture\nIn this paper, we suggested the Mask R-CNN\nmodel, which is an intuitive extension of the Faster\nR-CNN model and outputs the predicting segmenta-\ntion masks on each Region of Interest (RoI), in par-\nallel with the existing branch for classiﬁcation and\nbounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='bounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed\nfor pixel-to-pixel alignment between network inputs\nand outputs. The segmentation mask in Mask R-\nCNN represents the pixel-level segmentation and\nalignment of each object that helps to extract each\ninstance of an object separately without background.\nMask R-CNN is a two-stage framework. The ﬁrst\nstage scans the image and generates proposals, the\nsecond stage classiﬁes the proposals and generates\nbounding boxes and masks Abdulla (2018); He\net al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='et al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal\nNetwork (RPN), ROI Classiﬁer & Bounding Box\nRegressor, and Segmentation Masks as shown in the\n(Figure 2) below.\n3\nFigure 1. Mask R-CNN framework\nNote. Image from (He et al. (2017))\nFigure 2. Mask R-CNN Internal Architecture\nNote. Figure 2 shows the steps until the instance\nsegmentation processes takes place from the ﬁrst\nstage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='stage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,\ncalled the backbone. It is a pre-trained standard con-\nvolutional neural network, which is either ResNet50\nor ResNet101. The backbone network serves as a\nfeature extractor over an entire image, at the early\nlayers the backbone detects low-level features like\nedges and corners, and later layers successively\ndetect higher-level features like the car, person,\nsky. When the image passes through the backbone\nnetwork, it is converted from 1024x1024px x 3\n(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.\nAnother more effective backbone, called a Fea-\nture Pyramid Network (FPN) applied on the top of\nthe ResNet backbone to improve the feature extrac-\ntion method. FPN uses a top-down architecture with\nlateral connections to build an in-network feature\npyramid from a single-scale input. It improves the\nstandard feature extraction pyramid by adding a\nsecond pyramid that takes the high-level features\nfrom the ﬁrst pyramid and passes them down to\nlower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='lower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level\nfeatures. Using a ResNet-FPN backbone for feature\nextraction with Mask R-CNN gives excellent gains\nin both accuracy and speed Abdulla (2018).\nRegion Proposal Network (RPN)\nA Region Proposal Network (RPN) was intro-\nduced by Ren et al. (2016) that takes an image as\ninput and outputs a set of rectangular object propos-\nals and ﬁnds areas that contain objects. To generate\nregion proposals, the author scans the images in\na sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='a sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×\nn spatial window of the input convolutional feature\nmap. Each sliding window is mapped to a lower-\ndimensional feature. The RPN doesn’t scan over\nthe image directly, instead, the RPN scans over the\nbackbone feature map. This allows the RPN to reuse\nthe extracted features efﬁciently and avoid duplicate\ncalculations.Then this feature is fed into two fully\nconnected layers, a box-regression layer, and a box-\nclassiﬁcation layer.\nRPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='RPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the\nRPN scans over are called anchors, which are boxes\ndistributed over the image area. The anchor is the\ncentral point of the sliding window. For the ZF\nModel which was an extension of AlexNet, the\ndimensions are 256-d and for VGG-16, it was 512-d.\nThe classiﬁer determines the probability of a pro-\nposal having the target object. Regression regresses\nthe coordinates of the proposals. For any image,\n4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN\nproposed by passing a sliding window over the\nCNN feature maps and at each window outputting\nk potential bounding boxes and scores for how\ngood each bounding box expected to be.\nscale and aspect-ratio are two important parameters.\nAspect ratio is width of image/height of the image,\nthe scale is the size of the image. (Figure 3) shows\nthat if 3 scales and 3 aspect-ratio are chosen, the\ntotal of 9 proposals are possible for each pixel,\nthis is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='this is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole\nimage, the number of anchors will be W*H*K,\nwhere W and H are the width and height of the\nimage respectively Karmarkar (2018).\nThe RPN generates two outputs for each anchor:\nAnchor Class:\nThe anchor class can be either\nforeground or background classes. The foreground\nclass implies that there is likely an object in that\nbox.\nBounding Box Reﬁnement: A foreground an-\nchor might not be centered perfectly over the object.\nSo the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='So the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN\npredictions, the model picks the top anchors that are\nlikely to contain objects and reﬁne their location and\nsize.If several anchors overlap too much, it keeps the\none with the highest foreground score and discards\nthe rest (Non-max Suppression). After getting the\nﬁnal proposals (regions of interest) then, it passes\nto the next stage.\nFigure 4. Example, a) Anchor boxes per object b)\nTop three anchor boxes pre object\n(a)\n(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and\naspect ratios and they overlap each other to cover\nthe object as much as possible. (b) shows that the\ntop three anchor boxes that are more likely to\ncontain an object, after some reﬁnement on their\nlocation and size\nROI Classiﬁer & Bounding Box Regressor\nThis\narchitecture\nwas\nintroduced\nby\n(Gir-\nshick (2015)), by removing the SVM classiﬁers for\ntraining to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='training to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)\nproposed by the RPN.\nFirst, the network processes all images with sev-\neral convolutional and max-pooling layers to pro-\nduce convolutional feature map. Then a region of\ninterest (RoI) pooling layer extracts a ﬁxed length\nfeature vector for each object proposal from the fea-\nture map. Then, each feature vectored into the fully\nconvolutional layers and produces softmax proba-\nbility over different object classes with background\nclass and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='class and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:\nClass: The class of the object in the ROI. Since this\nnetwork is deeper and has the capacity to classify\nregions to speciﬁc classes like person, car, chair,etc.\nIt can also generate a background class, which\ncauses the ROI to be discarded.\nBounding Box Reﬁnement: Further reﬁne the\nlocation and size of the bounding box to encapsulate\n5\nthe object.\nRoI pooling layer: Due to the bounding box\nreﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='reﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop\na part of a feature map and resize it to a ﬁxed size.\nThe RoI pooling layer uses max pooling to convert\nthe features inside any valid region of interest into\na small feature map with a ﬁxed spatial extent of\nH×W (e.g., 7 × 7), where H and W are layer hyper-\nparameters that are independent of any particular\nRoI Girshick (2015).\nSegmentation Masks\nThe mask network is the addition that the Mask\nR-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks\nfor them. The generated masks are low resolution:\n28x28 pixels. But they are soft masks, represented\nby ﬂoat numbers, so they hold more details than\nbinary masks. The small mask size helps to keep the\nmask branch light. During training, the ground-truth\nmasks. scale down to 28x28 to compute the loss,\nand during inference, the predicted masks scale up\nto the size of the ROI bounding box and that gives\nus the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='us the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted\nsum of different losses at different stages of the\nmodel. Five general losses occur during training the\nweight Abdulla (2018).\nRpn class loss: Occur when an improper clas-\nsiﬁcation of anchor boxes by the Region Proposal\nNetwork. This loss increases when the multiple\nobjects are not being detected by the model in the\nlast output. This loss determines how well the Re-\ngion Proposal Network separates background with\nobjects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='objects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being\ndetected but the bounding box is not properly ﬁt\nwith the position of the object.\nMrcnn class loss: Occurs when an improper clas-\nsiﬁcation of objects that are present in the region\nproposal. This is to be increased in case the object\nis being detected from the image, but misclassiﬁed.\nIt determines how well the Mask RCNN recognizes\neach class of object.\nMrcnn bbox loss: This loss occurs during the local-\nization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of\nthe object is done, but localization is not precise.\nMrcnn mask loss: This loss corresponds to masks\ncreated on the identiﬁed objects, which is related to\nhow well the Mask RCNN segment objects.\nPublic datasets\nAs for current dataset resources, the Ecole\nCentrale Paris (ECP) Facades dataset Teboul et\nal. (2010) and the eTRIMS Korc and Först-\nner (2009) database are the two most relevant public\nfacade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='facade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.\nThe ECP dataset consists of 104 annotated im-\nages which are rectiﬁed and cropped facades of\nHaussmannian style buildings in Paris. Images are\nannotated with seven classes (balcony, chimney,\ndoor, roof, shop, wall and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image\nthat consists of a colourmap (Fig. 5). The original\nannotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='annotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.\nThe eTRIMS consists of 60 annotated street\nview images but different from the ECP dataset\nthe images are not rectiﬁed and the facades are of\nvarious architectural styles. Images are annotated\nwith eight classes (sky, building, door, vegetation,\ncar, road, pavement and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image that\nconsists of an array and a color-map matrix (Fig.\n6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to\ndetect windows in real street view images because\n6\nFigure 5. Example from ECP dataset, a) the input\nimage b) the annotation\n(a)\n(b)\nNote. Image from (Teboul et al. (2010))\nFigure 6. Example from eTRIMS, (a) the input\nimage (b) the annotation\n(a)\n(b)\nNote. In the middle part of (b), the upper window\nboundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='boundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).\nof multiple reasons. The ECP dataset only contains\nfacades that are manually rectiﬁed and viewed in a\nfront-parallel direction, thus using it as training data\nwould not be wise since it is not highly representa-\ntive of real street view images, which is the imagery\nwe want to detect windows instances in. However,\nthe most important reasons were that we found both\nto be imprecise or even wrongly annotated regarding\nwhat exactly deﬁnes the window boundaries. For\nexample, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='example, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior\ncasing (See Figure 6).\nWe searched and found a few more public\ndatasets that contained window annotations but none\nwere of very high quality and many had the same\nissues as mentioned above. Therefore we had to take\na decision to build a quality dataset from scratch.\nOur dataset\nWe decided to use Google Street View images\n(Map data ©2020 Google) of facades mainly from\ncentral Gothenburg, Sweden. The cropped images\nwere manually collected with an intention to con-\ntain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.\nWe annotated them manually with the VGG\nImage Annotator (VIA) tool A. Dutta and Zisser-\nman (2019). There exists a lot of other tools to\nannotate images, e.g. LabelMe, RectLabel, Label-\nBox and COCO UI, but we chose VIA because\nof its efﬁciency and simplicity. For example, it\nruns in most modern web browsers so it does not\nrequire any other installation or setup. Regarding\nthe annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others\nstore them in XML-format, and so on. The VIA tool\nsaves its annotations in a JSON-format ﬁle where\neach mask is a set of polygon points (Figure 7).\nSince we tried to be consistent to follow each\nwindow’s exact window frame and not to include\nother window parts as window boundaries it made\nannotation a very time-consuming and complex\ntask, especially as speciﬁc decisions had to be taken\nfor each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='for each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user\ninterface and ﬁgured out most of the different win-\n7\nFigure 7. Two examples from images in our dataset\nannotated with the VIA tool\n(a)\n(b)\nNote. The yellow lines are the manually annotated\npolygons which deﬁne each window boundary.\ndow types and their frames, we were annotating one\nwindow about every 5-10 seconds. However, there\nwere a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='were a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended\nup with a collection of 100 images that contained\n1540 annotated windows (an average of 15,4 win-\ndows per image). We then divide our dataset into a\ntraining set (80 images) and a validation set (20 im-\nages), i.e. we split the generated JSON-ﬁle into two\ndifferent ﬁles (Figure 8) with their corresponding\nimages.\nOptimization Methods\nTo use the Mask R-CNN framework we used\nan open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='an open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by\nAbdulla (2017). Then we hosted the implementation\non the Google Colab platform for free GPU usage\nand easy of shareability amongst other positive\nfactors.\nAssume to start with the structure of the Mask\nR-CNN as described in the paper He et al. (2017)\nand the source code provided in Abdulla (2017), we\nneeded to investigate the similarities and differences\nbetween the Matterport implementation running on\nKeras and TensorFlow and the ofﬁcial paper imple-\nFigure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Figure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the\ntraining set and the right to the validation set.\nBoth are viewed with JSON Editor Online.\nmentation build on the Caffe deep learning frame-\nwork.\nFrom our understanding, the implementation fol-\nlowed the paper with only a few exceptions and\nthose were done mostly for code simplicity and\ngeneralization.\nFor example, to support training multiple images\nper batch, the Matterport implementation resizes all\nimages to the same size and preserve the aspect ratio\nby padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='by padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-\ntations they chose to generate bounding boxes in the\ncode on the ﬂy, thus making image augmentations\neasier to apply. The downside is a slight decrease in\nbounding box placement accuracy, about only 2%\nof bounding boxes differed by 1px or more.\nThe paper uses a more aggressive learning rate of\n0.02, but for the Matterport implementation that was\ntoo high and causes the weights to explode in Ten-\nsorFlow as it computes gradients differently from\nCaffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Caffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between\nMatterport’s and the paper’s features, parameters,\nhyper-parameters and so on, we could start to\noptimize our model. The optimization adjustments\nand modiﬁcations were updated continuously during\n8\nthe whole process, but we can perhaps give some\nintuition on how they were done by describing them\nas iteratively separate steps. For example, optimiza-\ntion of the network structure with its corresponding\nconﬁguration can be considered ’low level’, while\noptimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='optimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to\nadjust our result within the context of our research\nproblem or goal. Furthermore, each step often\nneeded distinct conﬁguration adjustment depending\non which model weights and dataset changes were\nused, i.e. training resumed with COCO-weights,\naugmentation ﬂip, augmentation afﬁne and so on,\nall needed different conﬁgurations to train properly.\nFigure 9. Examples of the many conﬁgurations\noptions in the Matterport implementation\nNote. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and\nFigure 12).\n4. EXPERIMENTS\nIn this section, we will ﬁrst present some metrics\nused in our experiments, then give insight into the\nthought process of some of our structure and con-\nﬁguration optimization, training optimization and\ninference optimization. Finally, we will present our\nexperimental settings and some of the best results\nwe achieved.\nMetrics\nTo use the networks inference results for detect-\ning windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ing windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly\ndetected windows. In other words, recall is a crucial\nvalue to improve. To evaluate the images overall\ndetection results, we ﬁrst use IoU (Intersection\nover Union) to measure how much our predicted\nboundary overlaps with the ground truth (the real\nobject boundary). A predeﬁned IoU threshold of 0.5\nor greater is used for classifying the prediction as a\ntrue positive otherwise a false positive.\nFigure 10. Examples of IoU.\nNote. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence\nscore and measured IoU.\nThen we use the typical precision and recall rate:\nPrecision =\nTP\nTP + FP\n(1)\nRecall =\nTP\nTP + FN\n(2)\nwhere, TP, FP, FN denotes the true positive, false\npositive and false negative, respectively.\nFinally, we use the mean Average Precision\n(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR\n(3)\nwhere the P represent the precision rate and and R\nthe recall rate.\nStructure and conﬁguration optimization\nOverall, we found it a challenging task to test\nand tune the many parameters, mainly due to the\n9\nadaptive process of such a large search space, the\ncontinuous updates to our training data, but also the\ntime it takes to train and do inference to calculate\nthe accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-\nurations of the neural network, for example, we\ninvestigated Mask R-CNN in combination with both\nthe ResNet50 and the ResNet101 Backbone as\ndescribed in He et al. (2017). Those experiments\nshowed that the latter often yielded better results.\nWe kept the base conﬁguration of input images of\nsize 1024x1024 px for best accuracy even if some\nof our images were a bit smaller since the model\nresized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.\nWe found the gradient clipping norm to be ap-\npropriately set a 5 with a learning rate at 0.001 and\nmomentum of 0.9 and so on.\nBy plotting each test we could observe that train-\ning and validation losses indicated overﬁtting, i.e.\nwe got an increasingly lower training loss but the\nvalidation loss at certain points started to ﬂuctuate\nor stagnate (Figure 11).\nFigure 11. Examples of plots from our training\nNote. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with\nweight decay and dropout amongst other things. Un-\nfortunately, GridSearch with cross-validation, to au-\ntomatically search for the optimal hyper-parameter\nvalues, required more computational power than we\nhad access to, but we believe a valid option given\nthe appropriate resources.\nAnother option and the most intuitive, to prevent\noverﬁtting is often to extend the dataset. However,\nthis was neither feasible given the remaining project\ntime and also did not ﬁt well with our interest in\ninvestigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='investigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is\na resampling procedure used to evaluate machine\nlearning models on a limited data sample, without\nany major improvements on our test set in terms of\naccuracy.\nTraining optimization\nFor the training optimization we assume the given\nMask-RCNN loss function as:\nL = αL1 + βL2 + γL3 + δL4 + ϵL5\n(4)\nwhere the ﬁrst loss is the RPN class loss and\nthe second the RPN bounding box prediction loss.\nThe other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='The other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization\ngoal is to ﬁnd a corresponding conﬁguration such\nthat it minimizes the overall loss. This is an adaptive\nprocess of matching the results of the loss function\nwith the given dataset and the parameters as a result\nof a trial process.\nGiven a loss Li and equation (4), optimize K =\n(α, β, γ, δ, ϵ) ∈ R5 and L1, L2, L3, L4, L5 loss func-\ntions so that the average precision of our model so\nthat the average precision of our model is improved\nwith a threshold of AP50 :\nmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='that the average precision of our model is improved\nwith a threshold of AP50 :\nmax\nK,L1,L2,L3,L4,L5 AP50\n(5)\nUltimately, the ﬁnal choice of parameters chosen\ndepends on the structure conﬁgurations discussed\nin the previous section and also on our input data\n. While technically for K any combination of real\nvalues can be combined:\n(α, β, γ, δ, ϵ) ∼ n.(α, β, γ, δ, ϵ), ∀n ∈ R\n(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10\non the loss gradient and not the loss value. Further-\nmore, several other conﬁguration parameters need\nto be tested and changed to improve accuracy. The\npractical approach for tuning loss weights was to\nstart with a default weight of 1 for each loss and\nevaluate the model on the validation set by inferring\nthe model performance image by image. We had to\nevaluate the true positives, false positives, and false\nnegatives combined with IoU of their masks and\ntheir score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='their score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In\nother words, we are using transfer learning, which\nmeans we don’t need to train a new model from\nscratch and that instead utilize a lot of the already\nlearned features from the COCO dataset, which\ncontains around 120K images. After the training for\n40 epochs with k-fold cross-validation have been\ncompleted, we save the model’s parameters and\nevaluate the new weights (one for each epoch is\ngenerated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='generated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-\ntating new images would take too long. Since the\npolygons are calculated internally this was easy to\nimplement with the imgaug library.\nFirst, we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith augmentation that ﬂip/mirror each image in\nthe training set horizontally right/left. Then we\nagain save the model’s parameters and evaluate the\nperformance of the new weights.\nThen we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Then we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the\ntraining set by rotation (-45, 45) and shearing (-16,\n16). Finally, we save the model’s parameters and\nevaluate the performance of the new weights with\nrunning inference on the weights that yielded the\nlowest training loss.\nInference optimization\nIn the process of optimizing the trained network,\nwe constantly adjusted and modiﬁed the training set\nand other parameters. Since the output of the infer-\nence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust\naccordingly with regards to the properties of the test\nset images and the overall accuracy. We found a\ndetection minimum conﬁdence of 0.9 to yield as\ngood trade-off of the precision and recall values for\nour windows.\n5. RESULTS\nWe perform our test result experiments on the\neTrims dataset and not on the ECP dataset. The\nreason being that the ECP dataset doesn’t contain\nstreet-view images and the images are all manually\nrectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='rectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.\nHowever, to properly use the eTRIMS dataset\nwith our model, we had to reﬁne their annotation\nto overcome inaccuracies and mistakes, and also\naccommodate our deﬁned window boundary which\nfollows the window frame. We, therefore, annotated\n10 randomly chosen images for our test set to\nevaluate our model’s performances. Therefore, any\ncomparisons should note these differences.\nBoth the training and inference was done on\nGoogle Colab with the free Tesla K80 GPU of about\n12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of\nthose we present the best three models with regards\nto the highest mAP on our test set (Table 1) and\nfor visualization we present the inference done on\nthe model achieving the highest mAP below (Figure\n12).\n11\nFigure 12.\nWindow detection results from a model trained on coco-weights, then resumed with ﬂiplr.\nImage\nGround truth\nPrediction\nOverlap\ngreen colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='green colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score\ncaptions: score/IoU\n12\nREFERENCES\nREFERENCES\nTable 1.\nA comparison of pixel accuracies on the eTRIMS\ndataset. Accuracies are shown in percentage.\nClass\n[1]\n[2]\n[3]\n[4]\n[5]\nOur*\nOur**\nOur***\nWindow[%] 75\n73\n71\n86', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Our***\nWindow[%] 75\n73\n71\n86\n90.91 91.93\n94.55\n94.76\nNote: Our result is only from 10 randomly chosen images from the eTRIMS\ndataset with reﬁned annotation.\nmodel trained on coco-weights then resumed with ﬂiplr and afﬁne using\nk-fold cross-validation (the resumed weights were chosen by the folds\nhighest mAP)\n** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)\n*** model trained on coco-weights then resumed with ﬂiplr (the resumed\nweights were chosen by the lowest training loss)\n6. CONCLUSIONS\nIn this paper, we propose to use the state-of-\nthe-art Mask R-CNN framework He et al. (2017)\nas an end-to-end network capable of instance seg-\nmentation without further machinery. Previous fa-\ncade parsing detection work often only produced\nsemantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='semantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is\ncapable of detecting and producing a bounding box\nand segmentation mask for each window instance.\nExperimental results show that our suggested\napproach can with only a relatively small but precise\ndataset train the network—only with transfer learn-\ning and augmentation—to produce instance segmen-\ntation of windows and a comparable result with\nprior state-of-the-art window detection approaches,\neven without pre- or post-optimization techniques.\nAs far as we know, we are the ﬁrst to have anno-\ntated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window\nboundary and not to include other window parts. We\nbelieve that our consequent and precise annotation\nof windows helped our model training. We also\nhope it can beneﬁt the research community as a free\nresource for more precise window detection.\n7. FUTURE WORK\nIn this paper, we have got a sense of how to tune\nthe hyper-parameters effectively with our training\ndata. However, further work is recommended to\ninvestigate this with platforms, such as Wandb or\nOnepanel Jobs for easier coordination, synchroniza-\ntion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types\nwere the model has low accuracy.\nFurther work could also beneﬁt from training\nmore facade classes, such as building, balcony,\ndoors, chimney, to reduce false positives, false neg-\natives. The same goes for the window class, i.e.\nsplitting it into distinct window type classes, such\nas double-hung, transom etc. A model capable of\na more complete facade parsing we believe will\nalso make application easier as the model has a\nﬁner granularity of the facades properties. A caution\nis that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='is that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of\nevery facade structure.\nTo further reﬁne the results and perhaps mitigate\nsome of the typical limitations we experienced,\nsuch as dealing with occlusion, overconﬁdent false\npositives predictions, occasional mask overlaps, fu-\nture work should investigate both in pre- and post-\noptimization methods of previous approaches or\ntesting their novel approaches. Also, code refactor-\ning and optimization, e.g. rewriting some Python\ncode in TensorFlow to bring all the beneﬁts of the\nTensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS\nThis paper was the ﬁnal written report in the\ncourse DIT891 Project in Data Science. The course\nis offered within the Applied Data Science Master’s\nProgramme by the University of Gothenburg. We\nwould like to thank Shirin Tavara, teacher and the\ncourse responsible and Richard Johansson, teacher.\nA special thanks goes to our supervisors Alexan-\nder Hollberg, Sanjay Somanath and Yinan Yu for\ntheir congeniality and support.\n13\nREFERENCES\nREFERENCES\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='13\nREFERENCES\nREFERENCES\nREFERENCES\nAbdulla, Waleed (2017). “Mask R-CNN for object\ndetection and instance segmentation on Keras and\nTensorFlow https://github. com/matterport”. In:\nMask _ RCNN.\nAbdulla, Waleed (2018). Instance Segmentation\nwith Mask R-CNN and TensorFlow. https : / /\nengineering . matterport . com / splash - of - color -\ninstance- segmentation- with- mask- r- cnn- and-\ntensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia\nRagia (2020). “3D building façade reconstruction\nusing deep learning”. In: ISPRS International\nJournal of Geo-Information 9(5), p. 322.\nBadrinarayanan,\nVijay,\nAlex\nKendall,\nRoberto\nCipolla, et al. (2015). “1SegNet: A Deep Convo-\nlutional Encoder-Decoder Architecture for Image\nSegmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Segmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:\nInternational\nJournal\nof\nAdvanced\nResearch\nin Electronics and Communication Engineering\n(IJARECE) Volume 5, pp. 1448–1454.\nChen, Liang-Chieh et al. (2017). “Deeplab: Seman-\ntic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected\ncrfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.\nDutta, Abhishek and Andrew Zisserman (2019).\n“The VIA annotation software for images, audio\nand video”. In: Proceedings of the 27th ACM in-\nternational conference on multimedia, pp. 2276–\n2279.\nFathalla, Radwa and George Vogiatzis (2017). “A\ndeep learning pipeline for semantic facade seg-\nmentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer\nvision, pp. 1440–1448.\nHan, Feng and Song-Chun Zhu (2008). “Bottom-\nup/top-down image parsing with attribute gram-\nmar”. In: IEEE transactions on pattern analysis\nand machine intelligence 31(1), pp. 59–73.\nHe, Kaiming et al. (2017). “Mask r-cnn”. In: Pro-\nceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.\nHu, Han et al. (2020). “Fast and Regularized Re-\nconstruction of Building Fa\\c {c} ades from\nStreet-View Images using Binary Integer Pro-\ngramming”. In: arXiv preprint arXiv:2002.08549.\nKarmarkar, T (2018). “Regional Proposal network\n(RPN)—Backbone of Faster R-CNN”. In: Aug\n18, p. 6.\nKoch, David et al. (2018). “Visual estimation of\nbuilding condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='building condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on\nMultimedia for Real Estate Tech, pp. 12–17.\nKorc,\nFilip\nand\nWolfgang\nFörstner\n(2009).\n“eTRIMS\nImage\nDatabase\nfor\ninterpreting\nimages of man-made scenes”. In: Dept. of\nPhotogrammetry, University of Bonn, Tech. Rep.\nTR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep\nlearning approach to facade parsing”. In.\nLong, Jonathan, Evan Shelhamer, and Trevor Dar-\nrell (2015). “Fully convolutional networks for\nsemantic segmentation”. In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pp. 3431–3440.\nMa, Wenguang and Wei Ma (2020). “Deep window\ndetection in street scenes”. In: KSII Transactions\non Internet and Information Systems (TIIS) 14(2),\npp. 855–870.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='on Internet and Information Systems (TIIS) 14(2),\npp. 855–870.\nMathias, Markus et al. (2011). “Automatic architec-\ntural style recognition”. In: ISPRS-International\nArchives of the Photogrammetry, Remote Sensing\nand Spatial Information Sciences 3816, pp. 171–\n176.\nMüller, Pascal et al. (2007). “Image-based procedu-\nral modeling of facades”. In: ACM Trans. Graph.\n26(3), p. 85.\nNogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Nogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting\nconvolutional neural networks for remote sensing\nscene classiﬁcation”. In: Pattern Recognition 61,\npp. 539–556.\nParthasarathy,\nDhruv\n(2017).\n“A\nbrief\nhistory\nof CNNs in image segmentation: From R-\n14\nCNN to Mask R-CNN”. In: Available online:\nblog.\nathelas.\ncom/a-brief-history-of-cnns-in-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='blog.\nathelas.\ncom/a-brief-history-of-cnns-in-\nimage-segmentation-from-r-cnn-to-mask-rcnn-\n34ea83205de4 (accessed on 24 February 2019).\nPenatti, Otávio AB, Keiller Nogueira, and Jefersson\nA Dos Santos (2015). “Do deep features general-\nize from everyday objects to remote sensing and\naerial scenes domains?” In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition workshops, pp. 44–51.\nRahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der\nBundeswehr München.” In.\nRecky, Michal and Franz Leberl (2010). “Windows\ndetection using k-means in cie-lab color space”.\nIn: 2010 20th International Conference on Pat-\ntern Recognition. IEEE, pp. 356–359.\nRen, Shaoqing et al. (2016). “Faster R-CNN: to-\nwards real-time object detection with region pro-\nposal networks”. In: IEEE transactions on pat-\ntern analysis and machine intelligence 39(6),\npp. 1137–1149.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tern analysis and machine intelligence 39(6),\npp. 1137–1149.\nSchmitz, Matthias and Helmut Mayer (2016). “A\nconvolutional network for semantic facade seg-\nmentation and interpretation”. In: The Interna-\ntional Archives of Photogrammetry, Remote Sens-\ning and Spatial Information Sciences 41, p. 709.\nSultana, F, A Suﬁan, and P Dutta (2020). “Evolution\nof Image Segmentation using Deep Convolutional\nNeural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Neural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.\nTeboul, Olivier et al. (2010). “Segmentation of\nbuilding facades using procedural shape priors”.\nIn: 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition. IEEE,\npp. 3105–3112.\nTu, Zhuowen et al. (2005). “Image parsing: Unify-\ning segmentation, detection, and recognition”. In:\nInternational Journal of computer vision 63(2),\npp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst\nBischof (2010). “Unsupervised facade segmenta-\ntion using repetitive patterns”. In: Joint Pattern\nRecognition Symposium. Springer, pp. 51–60.\nZhao, Peng et al. (2010). “Rectilinear parsing of ar-\nchitecture in urban environment”. In: 2010 IEEE\nComputer Society Conference on Computer Vi-\nsion and Pattern Recognition. IEEE, pp. 342–349.\nZhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Zhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-\nject recognition by data driven Markov chain\nMonte Carlo”. In: Proceedings IEEE Conference\non Computer Vision and Pattern Recognition.\nCVPR 2000 (Cat. No. PR00662). Vol. 1. IEEE,\npp. 738–745.\n15', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc257ad0> 111
cuda:2
[Document(page_content='Window Detection In Facade Imagery:\nA Deep Learning Approach Using Mask R-CNN\nNils Nordmark, University of Gothenburg\nMola Ayenew, University of Gothenburg\nAbstract\nThe parsing of windows in building facades is a long-\ndesired but challenging task in computer vision. It is\ncrucial to urban analysis, semantic reconstruction, life\ncycle analysis, digital twins and scene parsing amongst\nother building-related tasks that require high-quality se-\nmantic data. In this article, we investigate the usage of the\nMask R-CNN framework to be used for window detection\nof facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='of facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to\nproduce instance segmentations of our new window class.\nExperimental results show that our suggested approach can\nwith a relatively small dataset train the network only with\ntransfer learning and augmentation achieve results on par\nwith prior state-of-the-art window detection approaches,\neven without post-optimization techniques.\n1. INTRODUCTION\nGathering semantic data regarding building fea-\ntures from images is desired as manual gathering\ncan often be a challenging, time consuming and\nresource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and\nbalconies are often not available, can change over\ntime and the data gathering is usually a manual\nprocess.\nMany different approaches have been applied to\nsolve the gathering of semantic data within many\ndifferent domains. Within building facade parsing\nthis task has mostly been treated as a process of\nsemantic segmentation of facade imagery into archi-\ntectural structural classes such as windows, doors,\nbalconies and so on. This semantic segmentation\napproach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='approach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the\nsame object as each pixel is classiﬁed to belong\nto a particular class. Speciﬁcally for parsing of\nwindows instances in building facades, we suggest\nan approach that can decouple segmentation and\nclassiﬁcation to detect windows in an image with\na bounding box and segmentation mask for each\nwindow instance.\nWith instance segmentation, we can analyze the\nwindows independently with detailed and compre-\nhensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='hensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,\nwe propose an approach based on the Mask R-CNN\nframework.\n2. RELATED WORK\nFacade parsing has been studied for a long time\nand there exists a large body of work on how to\ndeal with such a problem.\nEarlier approaches\nZhao et al. (2010) proposed parsing ground-\nlevel images into architectural units which could\nbe used for city modelling. Wendel, Donoser, and\nBischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-\nmeans clustering to tackle the same problem.\nMany others proposed a procedural shape gram-\nmar approach. This approach could at a high-level\nbe explained as a hand-crafted set of rules of basic\nshapes which represents structured geometries, i.e.\nthe structure of the object is encoded as a set of\nparametric rules. Models could then be generated\nby iteratively applying the rules on a starting shape.\nMüller et al. (2007) used a procedural modelling\npipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.\nHan and Zhu (2008) extended the work with an\nattribute grammar which used a more advanced\narXiv:2107.10006v1  [cs.CV]  21 Jul 2021\nbottom-up and top-down mechanism for recogniz-\ning objects. Their method was based on previous\nobject recognition work, such as Zhu, Zhang, and\nTu (2000) and Tu et al. (2005).\nTeboul et al. (2010) combined machine learning\nalgorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='algorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve\nthe works of Müller et al. (2007) and Han and\nZhu (2008), the learning process to some extent still\nwas based on hand-crafted rules.\nMathias et al. (2011) took another approach based\non architectural styles for automatic classiﬁcation.\nThey tried to extend Müller et al. (2007) work\nwith appropriate style information as they argued\nthat it could be used to select a more appropriate\nprocedural grammar for the task of building recon-\nstruction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='struction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on\nthe assumption that the building style was known\nand needed to be conducted as a manual task.\nInstead, they tried to automate the classiﬁcation of\narchitectural styles, by using facade images to train\ntheir classiﬁer to distinguish between three distinct\narchitectural styles. For classifying those speciﬁc\nstyles they were quite accurate but it also required\na rather speciﬁc set of circumstances to succeed.\nAs most of the earlier approaches used hand-\ncrafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more\ncomplex objects, i.e. could not handle deviations\nfrom these set of user-deﬁned rules well. They also\nsuffered in accuracy when the input image was\nmore complex, such as street-view images where the\nimage was not perfectly rectiﬁed. Another common\nlimitation was that they often suffered from heavy\ncomputational costs and slow running times (Bala\nand Kumar (2016), Koch et al. (2018)).\nConvolutional Neural Networks Approaches\nMore recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='More recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success\nof AlexNet in 2012, many semantic segmentation\napproaches which used to apply traditional ma-\nchine learning algorithms have instead been pri-\nmarily based on CNNs Bala and Kumar (2016);\nParthasarathy (2017).\nImproved models quickly followed AlexNet such\nas Segnet (Badrinarayanan, Kendall, Cipolla, et\nal. (2015)) which was one of the ﬁrst to tackle\nthe problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-\nhamer, and Darrell (2015) showed that a fully\nconvolutional network (FCN) trained end-to-end,\npixels-to-pixels could exceed then state-of-the-art\nsemantic image segmentation methods by retaining\nspatial information of an image. In more detail,\nthey used three pre-trained base models (AlexNet,\nVGGNet and GoogleNet) and transferred them from\nclassiﬁers by replacing fully connected layers with\nconvolutional layers. Since then a semantic seg-\nmentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.\nDutta (2020)).\nChen et al. (2017) use dilated convolution instead\nof plain convolution to set new state-of-art results\nwith their proposed DeepLab system.\nRecently, more automated methods and entirely\nbased on deep learning have achieved remarkable\nimprovements in image analysis, such as Penatti,\nNogueira, and Dos Santos (2015) and Nogueira,\nPenatti, and Dos Santos (2017).\nDeep Learning Approaches\nDeep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Deep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision\ntasks (classiﬁcation, object detection, semantic seg-\nmentation and instance segmentation) in a lot of\nbenchmarks and a variety of ﬁelds, e.g. medicine,\nsurveillance, economics, sociology. They have been\napplied to a diversity of tasks such as face detection,\nspeech recognition, autonomous vehicles and so on.\nDeep Learning Approaches Related to Facade\nParsing\nNaturally, deep learning was also applied to\nbuilding-related image analysis problems as well.\n2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing\nquite speciﬁcally and rather successfully.\nSchmitz and Mayer (2016) method for semantic\ninterpretation of facade images used a convolutional\nnetwork and a key insight was that even smaller\ndatasets could train the network when transfer learn-\ning could be employed. For validation, they used the\neTRIMS dataset to measure the overall accuracy,\nprecision and recall. The F1 score had an accuracy\nof 0.82± 0.09 for the facade categories, e.g. win-\ndows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='dows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of\nfacade parsing even further with their method Deep-\nFacade which combined both the learning capacity\nof deep convolutional neural networks with rules\nand a loss function based on symmetry found in\nbuilding facades structures. They trained an FCN-\n8s network with their novel loss function to obtain\nexperimental results on both the ECP dataset and\nthe eTRIMS dataset that signiﬁcantly outperformed\nprevious state-of-the-art methods. Pixel accuracies\non the eTRIMS dataset reached as high as 0.91 for\nwindows.\nFathalla and Vogiatzis (2017) used appearance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='windows.\nFathalla and Vogiatzis (2017) used appearance\nand layout cues combined with the VGG-model.\nTheir method only had a single object of focus and\noverall accuracy was on par with other methods,\nalthough supplementary materials had to be asked\nfor an in-depth understanding of the results.\nKoch et al. (2018) developed a multi-scale patch-\nbased pattern extraction that combined with a CNN\n(AlexNet), to automatically estimate the condition\nof buildings. They showed to some degree their\nmethod could serve as a proxy for condition esti-\nmates by appraisers.\nBacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Bacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with\na deep learning-based facade segmentation stage\nbased on generative adversarial networks (GANs).\nThe task was to reconstruct 3D models of build-\nings facades, in the urban environment for cultural\nheritage.\nHu et al. (2020) used the bounding boxes detected\nby YOLO architecture in real-time to guide facade\nreconstruction in an interactive environment. Results\nshowed accuracies above 0.82, which they deemed\nas sufﬁcient for their 3D reconstruction task.\nRahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of\nboth traditional and modern machine learning ap-\nproaches. His system was trained for windows delin-\neation segmentation, i.e. detecting window frames,\nmullions, transoms and so on. For small datasets\nit showed promising results, however, this method\nis not competitive compared to complete end-to-\nend state-of-the-art deep learning approaches such\nas Liu et al. (2017).\nWenguang Ma and Wei Ma (2020) used Faster\nR-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.\nThey called their method WD-Net which showed\n0.5 per cent improved accuracy compared with the\nbaseline Faster R-CNN.\n3. OUR APPROACH\nMask R-CNN Architecture\nIn this paper, we suggested the Mask R-CNN\nmodel, which is an intuitive extension of the Faster\nR-CNN model and outputs the predicting segmenta-\ntion masks on each Region of Interest (RoI), in par-\nallel with the existing branch for classiﬁcation and\nbounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='bounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed\nfor pixel-to-pixel alignment between network inputs\nand outputs. The segmentation mask in Mask R-\nCNN represents the pixel-level segmentation and\nalignment of each object that helps to extract each\ninstance of an object separately without background.\nMask R-CNN is a two-stage framework. The ﬁrst\nstage scans the image and generates proposals, the\nsecond stage classiﬁes the proposals and generates\nbounding boxes and masks Abdulla (2018); He\net al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='et al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal\nNetwork (RPN), ROI Classiﬁer & Bounding Box\nRegressor, and Segmentation Masks as shown in the\n(Figure 2) below.\n3\nFigure 1. Mask R-CNN framework\nNote. Image from (He et al. (2017))\nFigure 2. Mask R-CNN Internal Architecture\nNote. Figure 2 shows the steps until the instance\nsegmentation processes takes place from the ﬁrst\nstage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='stage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,\ncalled the backbone. It is a pre-trained standard con-\nvolutional neural network, which is either ResNet50\nor ResNet101. The backbone network serves as a\nfeature extractor over an entire image, at the early\nlayers the backbone detects low-level features like\nedges and corners, and later layers successively\ndetect higher-level features like the car, person,\nsky. When the image passes through the backbone\nnetwork, it is converted from 1024x1024px x 3\n(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.\nAnother more effective backbone, called a Fea-\nture Pyramid Network (FPN) applied on the top of\nthe ResNet backbone to improve the feature extrac-\ntion method. FPN uses a top-down architecture with\nlateral connections to build an in-network feature\npyramid from a single-scale input. It improves the\nstandard feature extraction pyramid by adding a\nsecond pyramid that takes the high-level features\nfrom the ﬁrst pyramid and passes them down to\nlower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='lower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level\nfeatures. Using a ResNet-FPN backbone for feature\nextraction with Mask R-CNN gives excellent gains\nin both accuracy and speed Abdulla (2018).\nRegion Proposal Network (RPN)\nA Region Proposal Network (RPN) was intro-\nduced by Ren et al. (2016) that takes an image as\ninput and outputs a set of rectangular object propos-\nals and ﬁnds areas that contain objects. To generate\nregion proposals, the author scans the images in\na sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='a sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×\nn spatial window of the input convolutional feature\nmap. Each sliding window is mapped to a lower-\ndimensional feature. The RPN doesn’t scan over\nthe image directly, instead, the RPN scans over the\nbackbone feature map. This allows the RPN to reuse\nthe extracted features efﬁciently and avoid duplicate\ncalculations.Then this feature is fed into two fully\nconnected layers, a box-regression layer, and a box-\nclassiﬁcation layer.\nRPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='RPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the\nRPN scans over are called anchors, which are boxes\ndistributed over the image area. The anchor is the\ncentral point of the sliding window. For the ZF\nModel which was an extension of AlexNet, the\ndimensions are 256-d and for VGG-16, it was 512-d.\nThe classiﬁer determines the probability of a pro-\nposal having the target object. Regression regresses\nthe coordinates of the proposals. For any image,\n4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN\nproposed by passing a sliding window over the\nCNN feature maps and at each window outputting\nk potential bounding boxes and scores for how\ngood each bounding box expected to be.\nscale and aspect-ratio are two important parameters.\nAspect ratio is width of image/height of the image,\nthe scale is the size of the image. (Figure 3) shows\nthat if 3 scales and 3 aspect-ratio are chosen, the\ntotal of 9 proposals are possible for each pixel,\nthis is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='this is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole\nimage, the number of anchors will be W*H*K,\nwhere W and H are the width and height of the\nimage respectively Karmarkar (2018).\nThe RPN generates two outputs for each anchor:\nAnchor Class:\nThe anchor class can be either\nforeground or background classes. The foreground\nclass implies that there is likely an object in that\nbox.\nBounding Box Reﬁnement: A foreground an-\nchor might not be centered perfectly over the object.\nSo the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='So the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN\npredictions, the model picks the top anchors that are\nlikely to contain objects and reﬁne their location and\nsize.If several anchors overlap too much, it keeps the\none with the highest foreground score and discards\nthe rest (Non-max Suppression). After getting the\nﬁnal proposals (regions of interest) then, it passes\nto the next stage.\nFigure 4. Example, a) Anchor boxes per object b)\nTop three anchor boxes pre object\n(a)\n(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and\naspect ratios and they overlap each other to cover\nthe object as much as possible. (b) shows that the\ntop three anchor boxes that are more likely to\ncontain an object, after some reﬁnement on their\nlocation and size\nROI Classiﬁer & Bounding Box Regressor\nThis\narchitecture\nwas\nintroduced\nby\n(Gir-\nshick (2015)), by removing the SVM classiﬁers for\ntraining to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='training to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)\nproposed by the RPN.\nFirst, the network processes all images with sev-\neral convolutional and max-pooling layers to pro-\nduce convolutional feature map. Then a region of\ninterest (RoI) pooling layer extracts a ﬁxed length\nfeature vector for each object proposal from the fea-\nture map. Then, each feature vectored into the fully\nconvolutional layers and produces softmax proba-\nbility over different object classes with background\nclass and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='class and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:\nClass: The class of the object in the ROI. Since this\nnetwork is deeper and has the capacity to classify\nregions to speciﬁc classes like person, car, chair,etc.\nIt can also generate a background class, which\ncauses the ROI to be discarded.\nBounding Box Reﬁnement: Further reﬁne the\nlocation and size of the bounding box to encapsulate\n5\nthe object.\nRoI pooling layer: Due to the bounding box\nreﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='reﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop\na part of a feature map and resize it to a ﬁxed size.\nThe RoI pooling layer uses max pooling to convert\nthe features inside any valid region of interest into\na small feature map with a ﬁxed spatial extent of\nH×W (e.g., 7 × 7), where H and W are layer hyper-\nparameters that are independent of any particular\nRoI Girshick (2015).\nSegmentation Masks\nThe mask network is the addition that the Mask\nR-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='R-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks\nfor them. The generated masks are low resolution:\n28x28 pixels. But they are soft masks, represented\nby ﬂoat numbers, so they hold more details than\nbinary masks. The small mask size helps to keep the\nmask branch light. During training, the ground-truth\nmasks. scale down to 28x28 to compute the loss,\nand during inference, the predicted masks scale up\nto the size of the ROI bounding box and that gives\nus the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='us the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted\nsum of different losses at different stages of the\nmodel. Five general losses occur during training the\nweight Abdulla (2018).\nRpn class loss: Occur when an improper clas-\nsiﬁcation of anchor boxes by the Region Proposal\nNetwork. This loss increases when the multiple\nobjects are not being detected by the model in the\nlast output. This loss determines how well the Re-\ngion Proposal Network separates background with\nobjects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='objects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being\ndetected but the bounding box is not properly ﬁt\nwith the position of the object.\nMrcnn class loss: Occurs when an improper clas-\nsiﬁcation of objects that are present in the region\nproposal. This is to be increased in case the object\nis being detected from the image, but misclassiﬁed.\nIt determines how well the Mask RCNN recognizes\neach class of object.\nMrcnn bbox loss: This loss occurs during the local-\nization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of\nthe object is done, but localization is not precise.\nMrcnn mask loss: This loss corresponds to masks\ncreated on the identiﬁed objects, which is related to\nhow well the Mask RCNN segment objects.\nPublic datasets\nAs for current dataset resources, the Ecole\nCentrale Paris (ECP) Facades dataset Teboul et\nal. (2010) and the eTRIMS Korc and Först-\nner (2009) database are the two most relevant public\nfacade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='facade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.\nThe ECP dataset consists of 104 annotated im-\nages which are rectiﬁed and cropped facades of\nHaussmannian style buildings in Paris. Images are\nannotated with seven classes (balcony, chimney,\ndoor, roof, shop, wall and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image\nthat consists of a colourmap (Fig. 5). The original\nannotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='annotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.\nThe eTRIMS consists of 60 annotated street\nview images but different from the ECP dataset\nthe images are not rectiﬁed and the facades are of\nvarious architectural styles. Images are annotated\nwith eight classes (sky, building, door, vegetation,\ncar, road, pavement and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image that\nconsists of an array and a color-map matrix (Fig.\n6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to\ndetect windows in real street view images because\n6\nFigure 5. Example from ECP dataset, a) the input\nimage b) the annotation\n(a)\n(b)\nNote. Image from (Teboul et al. (2010))\nFigure 6. Example from eTRIMS, (a) the input\nimage (b) the annotation\n(a)\n(b)\nNote. In the middle part of (b), the upper window\nboundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='boundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).\nof multiple reasons. The ECP dataset only contains\nfacades that are manually rectiﬁed and viewed in a\nfront-parallel direction, thus using it as training data\nwould not be wise since it is not highly representa-\ntive of real street view images, which is the imagery\nwe want to detect windows instances in. However,\nthe most important reasons were that we found both\nto be imprecise or even wrongly annotated regarding\nwhat exactly deﬁnes the window boundaries. For\nexample, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='example, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior\ncasing (See Figure 6).\nWe searched and found a few more public\ndatasets that contained window annotations but none\nwere of very high quality and many had the same\nissues as mentioned above. Therefore we had to take\na decision to build a quality dataset from scratch.\nOur dataset\nWe decided to use Google Street View images\n(Map data ©2020 Google) of facades mainly from\ncentral Gothenburg, Sweden. The cropped images\nwere manually collected with an intention to con-\ntain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.\nWe annotated them manually with the VGG\nImage Annotator (VIA) tool A. Dutta and Zisser-\nman (2019). There exists a lot of other tools to\nannotate images, e.g. LabelMe, RectLabel, Label-\nBox and COCO UI, but we chose VIA because\nof its efﬁciency and simplicity. For example, it\nruns in most modern web browsers so it does not\nrequire any other installation or setup. Regarding\nthe annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others\nstore them in XML-format, and so on. The VIA tool\nsaves its annotations in a JSON-format ﬁle where\neach mask is a set of polygon points (Figure 7).\nSince we tried to be consistent to follow each\nwindow’s exact window frame and not to include\nother window parts as window boundaries it made\nannotation a very time-consuming and complex\ntask, especially as speciﬁc decisions had to be taken\nfor each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='for each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user\ninterface and ﬁgured out most of the different win-\n7\nFigure 7. Two examples from images in our dataset\nannotated with the VIA tool\n(a)\n(b)\nNote. The yellow lines are the manually annotated\npolygons which deﬁne each window boundary.\ndow types and their frames, we were annotating one\nwindow about every 5-10 seconds. However, there\nwere a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='were a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended\nup with a collection of 100 images that contained\n1540 annotated windows (an average of 15,4 win-\ndows per image). We then divide our dataset into a\ntraining set (80 images) and a validation set (20 im-\nages), i.e. we split the generated JSON-ﬁle into two\ndifferent ﬁles (Figure 8) with their corresponding\nimages.\nOptimization Methods\nTo use the Mask R-CNN framework we used\nan open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='an open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by\nAbdulla (2017). Then we hosted the implementation\non the Google Colab platform for free GPU usage\nand easy of shareability amongst other positive\nfactors.\nAssume to start with the structure of the Mask\nR-CNN as described in the paper He et al. (2017)\nand the source code provided in Abdulla (2017), we\nneeded to investigate the similarities and differences\nbetween the Matterport implementation running on\nKeras and TensorFlow and the ofﬁcial paper imple-\nFigure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Figure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the\ntraining set and the right to the validation set.\nBoth are viewed with JSON Editor Online.\nmentation build on the Caffe deep learning frame-\nwork.\nFrom our understanding, the implementation fol-\nlowed the paper with only a few exceptions and\nthose were done mostly for code simplicity and\ngeneralization.\nFor example, to support training multiple images\nper batch, the Matterport implementation resizes all\nimages to the same size and preserve the aspect ratio\nby padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='by padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-\ntations they chose to generate bounding boxes in the\ncode on the ﬂy, thus making image augmentations\neasier to apply. The downside is a slight decrease in\nbounding box placement accuracy, about only 2%\nof bounding boxes differed by 1px or more.\nThe paper uses a more aggressive learning rate of\n0.02, but for the Matterport implementation that was\ntoo high and causes the weights to explode in Ten-\nsorFlow as it computes gradients differently from\nCaffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Caffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between\nMatterport’s and the paper’s features, parameters,\nhyper-parameters and so on, we could start to\noptimize our model. The optimization adjustments\nand modiﬁcations were updated continuously during\n8\nthe whole process, but we can perhaps give some\nintuition on how they were done by describing them\nas iteratively separate steps. For example, optimiza-\ntion of the network structure with its corresponding\nconﬁguration can be considered ’low level’, while\noptimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='optimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to\nadjust our result within the context of our research\nproblem or goal. Furthermore, each step often\nneeded distinct conﬁguration adjustment depending\non which model weights and dataset changes were\nused, i.e. training resumed with COCO-weights,\naugmentation ﬂip, augmentation afﬁne and so on,\nall needed different conﬁgurations to train properly.\nFigure 9. Examples of the many conﬁgurations\noptions in the Matterport implementation\nNote. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and\nFigure 12).\n4. EXPERIMENTS\nIn this section, we will ﬁrst present some metrics\nused in our experiments, then give insight into the\nthought process of some of our structure and con-\nﬁguration optimization, training optimization and\ninference optimization. Finally, we will present our\nexperimental settings and some of the best results\nwe achieved.\nMetrics\nTo use the networks inference results for detect-\ning windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ing windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly\ndetected windows. In other words, recall is a crucial\nvalue to improve. To evaluate the images overall\ndetection results, we ﬁrst use IoU (Intersection\nover Union) to measure how much our predicted\nboundary overlaps with the ground truth (the real\nobject boundary). A predeﬁned IoU threshold of 0.5\nor greater is used for classifying the prediction as a\ntrue positive otherwise a false positive.\nFigure 10. Examples of IoU.\nNote. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence\nscore and measured IoU.\nThen we use the typical precision and recall rate:\nPrecision =\nTP\nTP + FP\n(1)\nRecall =\nTP\nTP + FN\n(2)\nwhere, TP, FP, FN denotes the true positive, false\npositive and false negative, respectively.\nFinally, we use the mean Average Precision\n(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR\n(3)\nwhere the P represent the precision rate and and R\nthe recall rate.\nStructure and conﬁguration optimization\nOverall, we found it a challenging task to test\nand tune the many parameters, mainly due to the\n9\nadaptive process of such a large search space, the\ncontinuous updates to our training data, but also the\ntime it takes to train and do inference to calculate\nthe accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='the accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-\nurations of the neural network, for example, we\ninvestigated Mask R-CNN in combination with both\nthe ResNet50 and the ResNet101 Backbone as\ndescribed in He et al. (2017). Those experiments\nshowed that the latter often yielded better results.\nWe kept the base conﬁguration of input images of\nsize 1024x1024 px for best accuracy even if some\nof our images were a bit smaller since the model\nresized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='resized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.\nWe found the gradient clipping norm to be ap-\npropriately set a 5 with a learning rate at 0.001 and\nmomentum of 0.9 and so on.\nBy plotting each test we could observe that train-\ning and validation losses indicated overﬁtting, i.e.\nwe got an increasingly lower training loss but the\nvalidation loss at certain points started to ﬂuctuate\nor stagnate (Figure 11).\nFigure 11. Examples of plots from our training\nNote. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Note. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with\nweight decay and dropout amongst other things. Un-\nfortunately, GridSearch with cross-validation, to au-\ntomatically search for the optimal hyper-parameter\nvalues, required more computational power than we\nhad access to, but we believe a valid option given\nthe appropriate resources.\nAnother option and the most intuitive, to prevent\noverﬁtting is often to extend the dataset. However,\nthis was neither feasible given the remaining project\ntime and also did not ﬁt well with our interest in\ninvestigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='investigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is\na resampling procedure used to evaluate machine\nlearning models on a limited data sample, without\nany major improvements on our test set in terms of\naccuracy.\nTraining optimization\nFor the training optimization we assume the given\nMask-RCNN loss function as:\nL = αL1 + βL2 + γL3 + δL4 + ϵL5\n(4)\nwhere the ﬁrst loss is the RPN class loss and\nthe second the RPN bounding box prediction loss.\nThe other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='The other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization\ngoal is to ﬁnd a corresponding conﬁguration such\nthat it minimizes the overall loss. This is an adaptive\nprocess of matching the results of the loss function\nwith the given dataset and the parameters as a result\nof a trial process.\nGiven a loss Li and equation (4), optimize K =\n(α, β, γ, δ, ϵ) ∈ R5 and L1, L2, L3, L4, L5 loss func-\ntions so that the average precision of our model so\nthat the average precision of our model is improved\nwith a threshold of AP50 :\nmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='that the average precision of our model is improved\nwith a threshold of AP50 :\nmax\nK,L1,L2,L3,L4,L5 AP50\n(5)\nUltimately, the ﬁnal choice of parameters chosen\ndepends on the structure conﬁgurations discussed\nin the previous section and also on our input data\n. While technically for K any combination of real\nvalues can be combined:\n(α, β, γ, δ, ϵ) ∼ n.(α, β, γ, δ, ϵ), ∀n ∈ R\n(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10\non the loss gradient and not the loss value. Further-\nmore, several other conﬁguration parameters need\nto be tested and changed to improve accuracy. The\npractical approach for tuning loss weights was to\nstart with a default weight of 1 for each loss and\nevaluate the model on the validation set by inferring\nthe model performance image by image. We had to\nevaluate the true positives, false positives, and false\nnegatives combined with IoU of their masks and\ntheir score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='their score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In\nother words, we are using transfer learning, which\nmeans we don’t need to train a new model from\nscratch and that instead utilize a lot of the already\nlearned features from the COCO dataset, which\ncontains around 120K images. After the training for\n40 epochs with k-fold cross-validation have been\ncompleted, we save the model’s parameters and\nevaluate the new weights (one for each epoch is\ngenerated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='generated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-\ntating new images would take too long. Since the\npolygons are calculated internally this was easy to\nimplement with the imgaug library.\nFirst, we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith augmentation that ﬂip/mirror each image in\nthe training set horizontally right/left. Then we\nagain save the model’s parameters and evaluate the\nperformance of the new weights.\nThen we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Then we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the\ntraining set by rotation (-45, 45) and shearing (-16,\n16). Finally, we save the model’s parameters and\nevaluate the performance of the new weights with\nrunning inference on the weights that yielded the\nlowest training loss.\nInference optimization\nIn the process of optimizing the trained network,\nwe constantly adjusted and modiﬁed the training set\nand other parameters. Since the output of the infer-\nence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust\naccordingly with regards to the properties of the test\nset images and the overall accuracy. We found a\ndetection minimum conﬁdence of 0.9 to yield as\ngood trade-off of the precision and recall values for\nour windows.\n5. RESULTS\nWe perform our test result experiments on the\neTrims dataset and not on the ECP dataset. The\nreason being that the ECP dataset doesn’t contain\nstreet-view images and the images are all manually\nrectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='rectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.\nHowever, to properly use the eTRIMS dataset\nwith our model, we had to reﬁne their annotation\nto overcome inaccuracies and mistakes, and also\naccommodate our deﬁned window boundary which\nfollows the window frame. We, therefore, annotated\n10 randomly chosen images for our test set to\nevaluate our model’s performances. Therefore, any\ncomparisons should note these differences.\nBoth the training and inference was done on\nGoogle Colab with the free Tesla K80 GPU of about\n12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of\nthose we present the best three models with regards\nto the highest mAP on our test set (Table 1) and\nfor visualization we present the inference done on\nthe model achieving the highest mAP below (Figure\n12).\n11\nFigure 12.\nWindow detection results from a model trained on coco-weights, then resumed with ﬂiplr.\nImage\nGround truth\nPrediction\nOverlap\ngreen colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='green colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score\ncaptions: score/IoU\n12\nREFERENCES\nREFERENCES\nTable 1.\nA comparison of pixel accuracies on the eTRIMS\ndataset. Accuracies are shown in percentage.\nClass\n[1]\n[2]\n[3]\n[4]\n[5]\nOur*\nOur**\nOur***\nWindow[%] 75\n73\n71\n86', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Our***\nWindow[%] 75\n73\n71\n86\n90.91 91.93\n94.55\n94.76\nNote: Our result is only from 10 randomly chosen images from the eTRIMS\ndataset with reﬁned annotation.\nmodel trained on coco-weights then resumed with ﬂiplr and afﬁne using\nk-fold cross-validation (the resumed weights were chosen by the folds\nhighest mAP)\n** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)\n*** model trained on coco-weights then resumed with ﬂiplr (the resumed\nweights were chosen by the lowest training loss)\n6. CONCLUSIONS\nIn this paper, we propose to use the state-of-\nthe-art Mask R-CNN framework He et al. (2017)\nas an end-to-end network capable of instance seg-\nmentation without further machinery. Previous fa-\ncade parsing detection work often only produced\nsemantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='semantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is\ncapable of detecting and producing a bounding box\nand segmentation mask for each window instance.\nExperimental results show that our suggested\napproach can with only a relatively small but precise\ndataset train the network—only with transfer learn-\ning and augmentation—to produce instance segmen-\ntation of windows and a comparable result with\nprior state-of-the-art window detection approaches,\neven without pre- or post-optimization techniques.\nAs far as we know, we are the ﬁrst to have anno-\ntated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window\nboundary and not to include other window parts. We\nbelieve that our consequent and precise annotation\nof windows helped our model training. We also\nhope it can beneﬁt the research community as a free\nresource for more precise window detection.\n7. FUTURE WORK\nIn this paper, we have got a sense of how to tune\nthe hyper-parameters effectively with our training\ndata. However, further work is recommended to\ninvestigate this with platforms, such as Wandb or\nOnepanel Jobs for easier coordination, synchroniza-\ntion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types\nwere the model has low accuracy.\nFurther work could also beneﬁt from training\nmore facade classes, such as building, balcony,\ndoors, chimney, to reduce false positives, false neg-\natives. The same goes for the window class, i.e.\nsplitting it into distinct window type classes, such\nas double-hung, transom etc. A model capable of\na more complete facade parsing we believe will\nalso make application easier as the model has a\nﬁner granularity of the facades properties. A caution\nis that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='is that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of\nevery facade structure.\nTo further reﬁne the results and perhaps mitigate\nsome of the typical limitations we experienced,\nsuch as dealing with occlusion, overconﬁdent false\npositives predictions, occasional mask overlaps, fu-\nture work should investigate both in pre- and post-\noptimization methods of previous approaches or\ntesting their novel approaches. Also, code refactor-\ning and optimization, e.g. rewriting some Python\ncode in TensorFlow to bring all the beneﬁts of the\nTensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS\nThis paper was the ﬁnal written report in the\ncourse DIT891 Project in Data Science. The course\nis offered within the Applied Data Science Master’s\nProgramme by the University of Gothenburg. We\nwould like to thank Shirin Tavara, teacher and the\ncourse responsible and Richard Johansson, teacher.\nA special thanks goes to our supervisors Alexan-\nder Hollberg, Sanjay Somanath and Yinan Yu for\ntheir congeniality and support.\n13\nREFERENCES\nREFERENCES\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='13\nREFERENCES\nREFERENCES\nREFERENCES\nAbdulla, Waleed (2017). “Mask R-CNN for object\ndetection and instance segmentation on Keras and\nTensorFlow https://github. com/matterport”. In:\nMask _ RCNN.\nAbdulla, Waleed (2018). Instance Segmentation\nwith Mask R-CNN and TensorFlow. https : / /\nengineering . matterport . com / splash - of - color -\ninstance- segmentation- with- mask- r- cnn- and-\ntensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia\nRagia (2020). “3D building façade reconstruction\nusing deep learning”. In: ISPRS International\nJournal of Geo-Information 9(5), p. 322.\nBadrinarayanan,\nVijay,\nAlex\nKendall,\nRoberto\nCipolla, et al. (2015). “1SegNet: A Deep Convo-\nlutional Encoder-Decoder Architecture for Image\nSegmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Segmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:\nInternational\nJournal\nof\nAdvanced\nResearch\nin Electronics and Communication Engineering\n(IJARECE) Volume 5, pp. 1448–1454.\nChen, Liang-Chieh et al. (2017). “Deeplab: Seman-\ntic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected\ncrfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='crfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.\nDutta, Abhishek and Andrew Zisserman (2019).\n“The VIA annotation software for images, audio\nand video”. In: Proceedings of the 27th ACM in-\nternational conference on multimedia, pp. 2276–\n2279.\nFathalla, Radwa and George Vogiatzis (2017). “A\ndeep learning pipeline for semantic facade seg-\nmentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='mentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer\nvision, pp. 1440–1448.\nHan, Feng and Song-Chun Zhu (2008). “Bottom-\nup/top-down image parsing with attribute gram-\nmar”. In: IEEE transactions on pattern analysis\nand machine intelligence 31(1), pp. 59–73.\nHe, Kaiming et al. (2017). “Mask r-cnn”. In: Pro-\nceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='ceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.\nHu, Han et al. (2020). “Fast and Regularized Re-\nconstruction of Building Fa\\c {c} ades from\nStreet-View Images using Binary Integer Pro-\ngramming”. In: arXiv preprint arXiv:2002.08549.\nKarmarkar, T (2018). “Regional Proposal network\n(RPN)—Backbone of Faster R-CNN”. In: Aug\n18, p. 6.\nKoch, David et al. (2018). “Visual estimation of\nbuilding condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='building condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on\nMultimedia for Real Estate Tech, pp. 12–17.\nKorc,\nFilip\nand\nWolfgang\nFörstner\n(2009).\n“eTRIMS\nImage\nDatabase\nfor\ninterpreting\nimages of man-made scenes”. In: Dept. of\nPhotogrammetry, University of Bonn, Tech. Rep.\nTR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='TR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep\nlearning approach to facade parsing”. In.\nLong, Jonathan, Evan Shelhamer, and Trevor Dar-\nrell (2015). “Fully convolutional networks for\nsemantic segmentation”. In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pp. 3431–3440.\nMa, Wenguang and Wei Ma (2020). “Deep window\ndetection in street scenes”. In: KSII Transactions\non Internet and Information Systems (TIIS) 14(2),\npp. 855–870.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='on Internet and Information Systems (TIIS) 14(2),\npp. 855–870.\nMathias, Markus et al. (2011). “Automatic architec-\ntural style recognition”. In: ISPRS-International\nArchives of the Photogrammetry, Remote Sensing\nand Spatial Information Sciences 3816, pp. 171–\n176.\nMüller, Pascal et al. (2007). “Image-based procedu-\nral modeling of facades”. In: ACM Trans. Graph.\n26(3), p. 85.\nNogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Nogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting\nconvolutional neural networks for remote sensing\nscene classiﬁcation”. In: Pattern Recognition 61,\npp. 539–556.\nParthasarathy,\nDhruv\n(2017).\n“A\nbrief\nhistory\nof CNNs in image segmentation: From R-\n14\nCNN to Mask R-CNN”. In: Available online:\nblog.\nathelas.\ncom/a-brief-history-of-cnns-in-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='blog.\nathelas.\ncom/a-brief-history-of-cnns-in-\nimage-segmentation-from-r-cnn-to-mask-rcnn-\n34ea83205de4 (accessed on 24 February 2019).\nPenatti, Otávio AB, Keiller Nogueira, and Jefersson\nA Dos Santos (2015). “Do deep features general-\nize from everyday objects to remote sensing and\naerial scenes domains?” In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition workshops, pp. 44–51.\nRahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Rahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der\nBundeswehr München.” In.\nRecky, Michal and Franz Leberl (2010). “Windows\ndetection using k-means in cie-lab color space”.\nIn: 2010 20th International Conference on Pat-\ntern Recognition. IEEE, pp. 356–359.\nRen, Shaoqing et al. (2016). “Faster R-CNN: to-\nwards real-time object detection with region pro-\nposal networks”. In: IEEE transactions on pat-\ntern analysis and machine intelligence 39(6),\npp. 1137–1149.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='tern analysis and machine intelligence 39(6),\npp. 1137–1149.\nSchmitz, Matthias and Helmut Mayer (2016). “A\nconvolutional network for semantic facade seg-\nmentation and interpretation”. In: The Interna-\ntional Archives of Photogrammetry, Remote Sens-\ning and Spatial Information Sciences 41, p. 709.\nSultana, F, A Suﬁan, and P Dutta (2020). “Evolution\nof Image Segmentation using Deep Convolutional\nNeural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Neural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.\nTeboul, Olivier et al. (2010). “Segmentation of\nbuilding facades using procedural shape priors”.\nIn: 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition. IEEE,\npp. 3105–3112.\nTu, Zhuowen et al. (2005). “Image parsing: Unify-\ning segmentation, detection, and recognition”. In:\nInternational Journal of computer vision 63(2),\npp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='pp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst\nBischof (2010). “Unsupervised facade segmenta-\ntion using repetitive patterns”. In: Joint Pattern\nRecognition Symposium. Springer, pp. 51–60.\nZhao, Peng et al. (2010). “Rectilinear parsing of ar-\nchitecture in urban environment”. In: 2010 IEEE\nComputer Society Conference on Computer Vi-\nsion and Pattern Recognition. IEEE, pp. 342–349.\nZhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'}), Document(page_content='Zhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-\nject recognition by data driven Markov chain\nMonte Carlo”. In: Proceedings IEEE Conference\non Computer Vision and Pattern Recognition.\nCVPR 2000 (Cat. No. PR00662). Vol. 1. IEEE,\npp. 738–745.\n15', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpunkyn4bc/Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN.pdf'})]
cuda:2
[UploadFile(filename='000dac21-5129-46fb-b974-9e84bb9c7846.pdf', size=11976858, headers=Headers({'content-disposition': 'form-data; name="files"; filename="000dac21-5129-46fb-b974-9e84bb9c7846.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpksb47qki, tmpksb47qki
File: 000dac21-5129-46fb-b974-9e84bb9c7846.pdf, msg: 成功上传文件 000dac21-5129-46fb-b974-9e84bb9c7846.pdf, docs: [Document(page_content='Window Detection In Facade Imagery:\nA Deep Learning Approach Using Mask R-CNN\nNils Nordmark, University of Gothenburg\nMola Ayenew, University of Gothenburg\nAbstract\nThe parsing of windows in building facades is a long-\ndesired but challenging task in computer vision. It is\ncrucial to urban analysis, semantic reconstruction, life\ncycle analysis, digital twins and scene parsing amongst\nother building-related tasks that require high-quality se-\nmantic data. In this article, we investigate the usage of the\nMask R-CNN framework to be used for window detection\nof facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='of facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to\nproduce instance segmentations of our new window class.\nExperimental results show that our suggested approach can\nwith a relatively small dataset train the network only with\ntransfer learning and augmentation achieve results on par\nwith prior state-of-the-art window detection approaches,\neven without post-optimization techniques.\n1. INTRODUCTION\nGathering semantic data regarding building fea-\ntures from images is desired as manual gathering\ncan often be a challenging, time consuming and\nresource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='resource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and\nbalconies are often not available, can change over\ntime and the data gathering is usually a manual\nprocess.\nMany different approaches have been applied to\nsolve the gathering of semantic data within many\ndifferent domains. Within building facade parsing\nthis task has mostly been treated as a process of\nsemantic segmentation of facade imagery into archi-\ntectural structural classes such as windows, doors,\nbalconies and so on. This semantic segmentation\napproach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='approach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the\nsame object as each pixel is classiﬁed to belong\nto a particular class. Speciﬁcally for parsing of\nwindows instances in building facades, we suggest\nan approach that can decouple segmentation and\nclassiﬁcation to detect windows in an image with\na bounding box and segmentation mask for each\nwindow instance.\nWith instance segmentation, we can analyze the\nwindows independently with detailed and compre-\nhensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='hensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,\nwe propose an approach based on the Mask R-CNN\nframework.\n2. RELATED WORK\nFacade parsing has been studied for a long time\nand there exists a large body of work on how to\ndeal with such a problem.\nEarlier approaches\nZhao et al. (2010) proposed parsing ground-\nlevel images into architectural units which could\nbe used for city modelling. Wendel, Donoser, and\nBischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Bischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-\nmeans clustering to tackle the same problem.\nMany others proposed a procedural shape gram-\nmar approach. This approach could at a high-level\nbe explained as a hand-crafted set of rules of basic\nshapes which represents structured geometries, i.e.\nthe structure of the object is encoded as a set of\nparametric rules. Models could then be generated\nby iteratively applying the rules on a starting shape.\nMüller et al. (2007) used a procedural modelling\npipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='pipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.\nHan and Zhu (2008) extended the work with an\nattribute grammar which used a more advanced\narXiv:2107.10006v1  [cs.CV]  21 Jul 2021\nbottom-up and top-down mechanism for recogniz-\ning objects. Their method was based on previous\nobject recognition work, such as Zhu, Zhang, and\nTu (2000) and Tu et al. (2005).\nTeboul et al. (2010) combined machine learning\nalgorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='algorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve\nthe works of Müller et al. (2007) and Han and\nZhu (2008), the learning process to some extent still\nwas based on hand-crafted rules.\nMathias et al. (2011) took another approach based\non architectural styles for automatic classiﬁcation.\nThey tried to extend Müller et al. (2007) work\nwith appropriate style information as they argued\nthat it could be used to select a more appropriate\nprocedural grammar for the task of building recon-\nstruction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='struction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on\nthe assumption that the building style was known\nand needed to be conducted as a manual task.\nInstead, they tried to automate the classiﬁcation of\narchitectural styles, by using facade images to train\ntheir classiﬁer to distinguish between three distinct\narchitectural styles. For classifying those speciﬁc\nstyles they were quite accurate but it also required\na rather speciﬁc set of circumstances to succeed.\nAs most of the earlier approaches used hand-\ncrafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='crafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more\ncomplex objects, i.e. could not handle deviations\nfrom these set of user-deﬁned rules well. They also\nsuffered in accuracy when the input image was\nmore complex, such as street-view images where the\nimage was not perfectly rectiﬁed. Another common\nlimitation was that they often suffered from heavy\ncomputational costs and slow running times (Bala\nand Kumar (2016), Koch et al. (2018)).\nConvolutional Neural Networks Approaches\nMore recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='More recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success\nof AlexNet in 2012, many semantic segmentation\napproaches which used to apply traditional ma-\nchine learning algorithms have instead been pri-\nmarily based on CNNs Bala and Kumar (2016);\nParthasarathy (2017).\nImproved models quickly followed AlexNet such\nas Segnet (Badrinarayanan, Kendall, Cipolla, et\nal. (2015)) which was one of the ﬁrst to tackle\nthe problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='the problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-\nhamer, and Darrell (2015) showed that a fully\nconvolutional network (FCN) trained end-to-end,\npixels-to-pixels could exceed then state-of-the-art\nsemantic image segmentation methods by retaining\nspatial information of an image. In more detail,\nthey used three pre-trained base models (AlexNet,\nVGGNet and GoogleNet) and transferred them from\nclassiﬁers by replacing fully connected layers with\nconvolutional layers. Since then a semantic seg-\nmentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='mentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.\nDutta (2020)).\nChen et al. (2017) use dilated convolution instead\nof plain convolution to set new state-of-art results\nwith their proposed DeepLab system.\nRecently, more automated methods and entirely\nbased on deep learning have achieved remarkable\nimprovements in image analysis, such as Penatti,\nNogueira, and Dos Santos (2015) and Nogueira,\nPenatti, and Dos Santos (2017).\nDeep Learning Approaches\nDeep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Deep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision\ntasks (classiﬁcation, object detection, semantic seg-\nmentation and instance segmentation) in a lot of\nbenchmarks and a variety of ﬁelds, e.g. medicine,\nsurveillance, economics, sociology. They have been\napplied to a diversity of tasks such as face detection,\nspeech recognition, autonomous vehicles and so on.\nDeep Learning Approaches Related to Facade\nParsing\nNaturally, deep learning was also applied to\nbuilding-related image analysis problems as well.\n2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing\nquite speciﬁcally and rather successfully.\nSchmitz and Mayer (2016) method for semantic\ninterpretation of facade images used a convolutional\nnetwork and a key insight was that even smaller\ndatasets could train the network when transfer learn-\ning could be employed. For validation, they used the\neTRIMS dataset to measure the overall accuracy,\nprecision and recall. The F1 score had an accuracy\nof 0.82± 0.09 for the facade categories, e.g. win-\ndows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='dows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of\nfacade parsing even further with their method Deep-\nFacade which combined both the learning capacity\nof deep convolutional neural networks with rules\nand a loss function based on symmetry found in\nbuilding facades structures. They trained an FCN-\n8s network with their novel loss function to obtain\nexperimental results on both the ECP dataset and\nthe eTRIMS dataset that signiﬁcantly outperformed\nprevious state-of-the-art methods. Pixel accuracies\non the eTRIMS dataset reached as high as 0.91 for\nwindows.\nFathalla and Vogiatzis (2017) used appearance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='windows.\nFathalla and Vogiatzis (2017) used appearance\nand layout cues combined with the VGG-model.\nTheir method only had a single object of focus and\noverall accuracy was on par with other methods,\nalthough supplementary materials had to be asked\nfor an in-depth understanding of the results.\nKoch et al. (2018) developed a multi-scale patch-\nbased pattern extraction that combined with a CNN\n(AlexNet), to automatically estimate the condition\nof buildings. They showed to some degree their\nmethod could serve as a proxy for condition esti-\nmates by appraisers.\nBacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Bacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with\na deep learning-based facade segmentation stage\nbased on generative adversarial networks (GANs).\nThe task was to reconstruct 3D models of build-\nings facades, in the urban environment for cultural\nheritage.\nHu et al. (2020) used the bounding boxes detected\nby YOLO architecture in real-time to guide facade\nreconstruction in an interactive environment. Results\nshowed accuracies above 0.82, which they deemed\nas sufﬁcient for their 3D reconstruction task.\nRahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Rahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of\nboth traditional and modern machine learning ap-\nproaches. His system was trained for windows delin-\neation segmentation, i.e. detecting window frames,\nmullions, transoms and so on. For small datasets\nit showed promising results, however, this method\nis not competitive compared to complete end-to-\nend state-of-the-art deep learning approaches such\nas Liu et al. (2017).\nWenguang Ma and Wei Ma (2020) used Faster\nR-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='R-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.\nThey called their method WD-Net which showed\n0.5 per cent improved accuracy compared with the\nbaseline Faster R-CNN.\n3. OUR APPROACH\nMask R-CNN Architecture\nIn this paper, we suggested the Mask R-CNN\nmodel, which is an intuitive extension of the Faster\nR-CNN model and outputs the predicting segmenta-\ntion masks on each Region of Interest (RoI), in par-\nallel with the existing branch for classiﬁcation and\nbounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='bounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed\nfor pixel-to-pixel alignment between network inputs\nand outputs. The segmentation mask in Mask R-\nCNN represents the pixel-level segmentation and\nalignment of each object that helps to extract each\ninstance of an object separately without background.\nMask R-CNN is a two-stage framework. The ﬁrst\nstage scans the image and generates proposals, the\nsecond stage classiﬁes the proposals and generates\nbounding boxes and masks Abdulla (2018); He\net al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='et al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal\nNetwork (RPN), ROI Classiﬁer & Bounding Box\nRegressor, and Segmentation Masks as shown in the\n(Figure 2) below.\n3\nFigure 1. Mask R-CNN framework\nNote. Image from (He et al. (2017))\nFigure 2. Mask R-CNN Internal Architecture\nNote. Figure 2 shows the steps until the instance\nsegmentation processes takes place from the ﬁrst\nstage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='stage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,\ncalled the backbone. It is a pre-trained standard con-\nvolutional neural network, which is either ResNet50\nor ResNet101. The backbone network serves as a\nfeature extractor over an entire image, at the early\nlayers the backbone detects low-level features like\nedges and corners, and later layers successively\ndetect higher-level features like the car, person,\nsky. When the image passes through the backbone\nnetwork, it is converted from 1024x1024px x 3\n(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.\nAnother more effective backbone, called a Fea-\nture Pyramid Network (FPN) applied on the top of\nthe ResNet backbone to improve the feature extrac-\ntion method. FPN uses a top-down architecture with\nlateral connections to build an in-network feature\npyramid from a single-scale input. It improves the\nstandard feature extraction pyramid by adding a\nsecond pyramid that takes the high-level features\nfrom the ﬁrst pyramid and passes them down to\nlower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='lower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level\nfeatures. Using a ResNet-FPN backbone for feature\nextraction with Mask R-CNN gives excellent gains\nin both accuracy and speed Abdulla (2018).\nRegion Proposal Network (RPN)\nA Region Proposal Network (RPN) was intro-\nduced by Ren et al. (2016) that takes an image as\ninput and outputs a set of rectangular object propos-\nals and ﬁnds areas that contain objects. To generate\nregion proposals, the author scans the images in\na sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='a sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×\nn spatial window of the input convolutional feature\nmap. Each sliding window is mapped to a lower-\ndimensional feature. The RPN doesn’t scan over\nthe image directly, instead, the RPN scans over the\nbackbone feature map. This allows the RPN to reuse\nthe extracted features efﬁciently and avoid duplicate\ncalculations.Then this feature is fed into two fully\nconnected layers, a box-regression layer, and a box-\nclassiﬁcation layer.\nRPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='RPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the\nRPN scans over are called anchors, which are boxes\ndistributed over the image area. The anchor is the\ncentral point of the sliding window. For the ZF\nModel which was an extension of AlexNet, the\ndimensions are 256-d and for VGG-16, it was 512-d.\nThe classiﬁer determines the probability of a pro-\nposal having the target object. Regression regresses\nthe coordinates of the proposals. For any image,\n4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN\nproposed by passing a sliding window over the\nCNN feature maps and at each window outputting\nk potential bounding boxes and scores for how\ngood each bounding box expected to be.\nscale and aspect-ratio are two important parameters.\nAspect ratio is width of image/height of the image,\nthe scale is the size of the image. (Figure 3) shows\nthat if 3 scales and 3 aspect-ratio are chosen, the\ntotal of 9 proposals are possible for each pixel,\nthis is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='this is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole\nimage, the number of anchors will be W*H*K,\nwhere W and H are the width and height of the\nimage respectively Karmarkar (2018).\nThe RPN generates two outputs for each anchor:\nAnchor Class:\nThe anchor class can be either\nforeground or background classes. The foreground\nclass implies that there is likely an object in that\nbox.\nBounding Box Reﬁnement: A foreground an-\nchor might not be centered perfectly over the object.\nSo the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='So the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN\npredictions, the model picks the top anchors that are\nlikely to contain objects and reﬁne their location and\nsize.If several anchors overlap too much, it keeps the\none with the highest foreground score and discards\nthe rest (Non-max Suppression). After getting the\nﬁnal proposals (regions of interest) then, it passes\nto the next stage.\nFigure 4. Example, a) Anchor boxes per object b)\nTop three anchor boxes pre object\n(a)\n(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and\naspect ratios and they overlap each other to cover\nthe object as much as possible. (b) shows that the\ntop three anchor boxes that are more likely to\ncontain an object, after some reﬁnement on their\nlocation and size\nROI Classiﬁer & Bounding Box Regressor\nThis\narchitecture\nwas\nintroduced\nby\n(Gir-\nshick (2015)), by removing the SVM classiﬁers for\ntraining to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='training to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)\nproposed by the RPN.\nFirst, the network processes all images with sev-\neral convolutional and max-pooling layers to pro-\nduce convolutional feature map. Then a region of\ninterest (RoI) pooling layer extracts a ﬁxed length\nfeature vector for each object proposal from the fea-\nture map. Then, each feature vectored into the fully\nconvolutional layers and produces softmax proba-\nbility over different object classes with background\nclass and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='class and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:\nClass: The class of the object in the ROI. Since this\nnetwork is deeper and has the capacity to classify\nregions to speciﬁc classes like person, car, chair,etc.\nIt can also generate a background class, which\ncauses the ROI to be discarded.\nBounding Box Reﬁnement: Further reﬁne the\nlocation and size of the bounding box to encapsulate\n5\nthe object.\nRoI pooling layer: Due to the bounding box\nreﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='reﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop\na part of a feature map and resize it to a ﬁxed size.\nThe RoI pooling layer uses max pooling to convert\nthe features inside any valid region of interest into\na small feature map with a ﬁxed spatial extent of\nH×W (e.g., 7 × 7), where H and W are layer hyper-\nparameters that are independent of any particular\nRoI Girshick (2015).\nSegmentation Masks\nThe mask network is the addition that the Mask\nR-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='R-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks\nfor them. The generated masks are low resolution:\n28x28 pixels. But they are soft masks, represented\nby ﬂoat numbers, so they hold more details than\nbinary masks. The small mask size helps to keep the\nmask branch light. During training, the ground-truth\nmasks. scale down to 28x28 to compute the loss,\nand during inference, the predicted masks scale up\nto the size of the ROI bounding box and that gives\nus the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='us the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted\nsum of different losses at different stages of the\nmodel. Five general losses occur during training the\nweight Abdulla (2018).\nRpn class loss: Occur when an improper clas-\nsiﬁcation of anchor boxes by the Region Proposal\nNetwork. This loss increases when the multiple\nobjects are not being detected by the model in the\nlast output. This loss determines how well the Re-\ngion Proposal Network separates background with\nobjects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='objects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being\ndetected but the bounding box is not properly ﬁt\nwith the position of the object.\nMrcnn class loss: Occurs when an improper clas-\nsiﬁcation of objects that are present in the region\nproposal. This is to be increased in case the object\nis being detected from the image, but misclassiﬁed.\nIt determines how well the Mask RCNN recognizes\neach class of object.\nMrcnn bbox loss: This loss occurs during the local-\nization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of\nthe object is done, but localization is not precise.\nMrcnn mask loss: This loss corresponds to masks\ncreated on the identiﬁed objects, which is related to\nhow well the Mask RCNN segment objects.\nPublic datasets\nAs for current dataset resources, the Ecole\nCentrale Paris (ECP) Facades dataset Teboul et\nal. (2010) and the eTRIMS Korc and Först-\nner (2009) database are the two most relevant public\nfacade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='facade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.\nThe ECP dataset consists of 104 annotated im-\nages which are rectiﬁed and cropped facades of\nHaussmannian style buildings in Paris. Images are\nannotated with seven classes (balcony, chimney,\ndoor, roof, shop, wall and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image\nthat consists of a colourmap (Fig. 5). The original\nannotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='annotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.\nThe eTRIMS consists of 60 annotated street\nview images but different from the ECP dataset\nthe images are not rectiﬁed and the facades are of\nvarious architectural styles. Images are annotated\nwith eight classes (sky, building, door, vegetation,\ncar, road, pavement and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image that\nconsists of an array and a color-map matrix (Fig.\n6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to\ndetect windows in real street view images because\n6\nFigure 5. Example from ECP dataset, a) the input\nimage b) the annotation\n(a)\n(b)\nNote. Image from (Teboul et al. (2010))\nFigure 6. Example from eTRIMS, (a) the input\nimage (b) the annotation\n(a)\n(b)\nNote. In the middle part of (b), the upper window\nboundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='boundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).\nof multiple reasons. The ECP dataset only contains\nfacades that are manually rectiﬁed and viewed in a\nfront-parallel direction, thus using it as training data\nwould not be wise since it is not highly representa-\ntive of real street view images, which is the imagery\nwe want to detect windows instances in. However,\nthe most important reasons were that we found both\nto be imprecise or even wrongly annotated regarding\nwhat exactly deﬁnes the window boundaries. For\nexample, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='example, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior\ncasing (See Figure 6).\nWe searched and found a few more public\ndatasets that contained window annotations but none\nwere of very high quality and many had the same\nissues as mentioned above. Therefore we had to take\na decision to build a quality dataset from scratch.\nOur dataset\nWe decided to use Google Street View images\n(Map data ©2020 Google) of facades mainly from\ncentral Gothenburg, Sweden. The cropped images\nwere manually collected with an intention to con-\ntain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.\nWe annotated them manually with the VGG\nImage Annotator (VIA) tool A. Dutta and Zisser-\nman (2019). There exists a lot of other tools to\nannotate images, e.g. LabelMe, RectLabel, Label-\nBox and COCO UI, but we chose VIA because\nof its efﬁciency and simplicity. For example, it\nruns in most modern web browsers so it does not\nrequire any other installation or setup. Regarding\nthe annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='the annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others\nstore them in XML-format, and so on. The VIA tool\nsaves its annotations in a JSON-format ﬁle where\neach mask is a set of polygon points (Figure 7).\nSince we tried to be consistent to follow each\nwindow’s exact window frame and not to include\nother window parts as window boundaries it made\nannotation a very time-consuming and complex\ntask, especially as speciﬁc decisions had to be taken\nfor each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='for each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user\ninterface and ﬁgured out most of the different win-\n7\nFigure 7. Two examples from images in our dataset\nannotated with the VIA tool\n(a)\n(b)\nNote. The yellow lines are the manually annotated\npolygons which deﬁne each window boundary.\ndow types and their frames, we were annotating one\nwindow about every 5-10 seconds. However, there\nwere a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='were a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended\nup with a collection of 100 images that contained\n1540 annotated windows (an average of 15,4 win-\ndows per image). We then divide our dataset into a\ntraining set (80 images) and a validation set (20 im-\nages), i.e. we split the generated JSON-ﬁle into two\ndifferent ﬁles (Figure 8) with their corresponding\nimages.\nOptimization Methods\nTo use the Mask R-CNN framework we used\nan open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='an open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by\nAbdulla (2017). Then we hosted the implementation\non the Google Colab platform for free GPU usage\nand easy of shareability amongst other positive\nfactors.\nAssume to start with the structure of the Mask\nR-CNN as described in the paper He et al. (2017)\nand the source code provided in Abdulla (2017), we\nneeded to investigate the similarities and differences\nbetween the Matterport implementation running on\nKeras and TensorFlow and the ofﬁcial paper imple-\nFigure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Figure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the\ntraining set and the right to the validation set.\nBoth are viewed with JSON Editor Online.\nmentation build on the Caffe deep learning frame-\nwork.\nFrom our understanding, the implementation fol-\nlowed the paper with only a few exceptions and\nthose were done mostly for code simplicity and\ngeneralization.\nFor example, to support training multiple images\nper batch, the Matterport implementation resizes all\nimages to the same size and preserve the aspect ratio\nby padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='by padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-\ntations they chose to generate bounding boxes in the\ncode on the ﬂy, thus making image augmentations\neasier to apply. The downside is a slight decrease in\nbounding box placement accuracy, about only 2%\nof bounding boxes differed by 1px or more.\nThe paper uses a more aggressive learning rate of\n0.02, but for the Matterport implementation that was\ntoo high and causes the weights to explode in Ten-\nsorFlow as it computes gradients differently from\nCaffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Caffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between\nMatterport’s and the paper’s features, parameters,\nhyper-parameters and so on, we could start to\noptimize our model. The optimization adjustments\nand modiﬁcations were updated continuously during\n8\nthe whole process, but we can perhaps give some\nintuition on how they were done by describing them\nas iteratively separate steps. For example, optimiza-\ntion of the network structure with its corresponding\nconﬁguration can be considered ’low level’, while\noptimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='optimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to\nadjust our result within the context of our research\nproblem or goal. Furthermore, each step often\nneeded distinct conﬁguration adjustment depending\non which model weights and dataset changes were\nused, i.e. training resumed with COCO-weights,\naugmentation ﬂip, augmentation afﬁne and so on,\nall needed different conﬁgurations to train properly.\nFigure 9. Examples of the many conﬁgurations\noptions in the Matterport implementation\nNote. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Note. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and\nFigure 12).\n4. EXPERIMENTS\nIn this section, we will ﬁrst present some metrics\nused in our experiments, then give insight into the\nthought process of some of our structure and con-\nﬁguration optimization, training optimization and\ninference optimization. Finally, we will present our\nexperimental settings and some of the best results\nwe achieved.\nMetrics\nTo use the networks inference results for detect-\ning windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ing windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly\ndetected windows. In other words, recall is a crucial\nvalue to improve. To evaluate the images overall\ndetection results, we ﬁrst use IoU (Intersection\nover Union) to measure how much our predicted\nboundary overlaps with the ground truth (the real\nobject boundary). A predeﬁned IoU threshold of 0.5\nor greater is used for classifying the prediction as a\ntrue positive otherwise a false positive.\nFigure 10. Examples of IoU.\nNote. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Note. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence\nscore and measured IoU.\nThen we use the typical precision and recall rate:\nPrecision =\nTP\nTP + FP\n(1)\nRecall =\nTP\nTP + FN\n(2)\nwhere, TP, FP, FN denotes the true positive, false\npositive and false negative, respectively.\nFinally, we use the mean Average Precision\n(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR\n(3)\nwhere the P represent the precision rate and and R\nthe recall rate.\nStructure and conﬁguration optimization\nOverall, we found it a challenging task to test\nand tune the many parameters, mainly due to the\n9\nadaptive process of such a large search space, the\ncontinuous updates to our training data, but also the\ntime it takes to train and do inference to calculate\nthe accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='the accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-\nurations of the neural network, for example, we\ninvestigated Mask R-CNN in combination with both\nthe ResNet50 and the ResNet101 Backbone as\ndescribed in He et al. (2017). Those experiments\nshowed that the latter often yielded better results.\nWe kept the base conﬁguration of input images of\nsize 1024x1024 px for best accuracy even if some\nof our images were a bit smaller since the model\nresized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='resized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.\nWe found the gradient clipping norm to be ap-\npropriately set a 5 with a learning rate at 0.001 and\nmomentum of 0.9 and so on.\nBy plotting each test we could observe that train-\ning and validation losses indicated overﬁtting, i.e.\nwe got an increasingly lower training loss but the\nvalidation loss at certain points started to ﬂuctuate\nor stagnate (Figure 11).\nFigure 11. Examples of plots from our training\nNote. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Note. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with\nweight decay and dropout amongst other things. Un-\nfortunately, GridSearch with cross-validation, to au-\ntomatically search for the optimal hyper-parameter\nvalues, required more computational power than we\nhad access to, but we believe a valid option given\nthe appropriate resources.\nAnother option and the most intuitive, to prevent\noverﬁtting is often to extend the dataset. However,\nthis was neither feasible given the remaining project\ntime and also did not ﬁt well with our interest in\ninvestigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='investigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is\na resampling procedure used to evaluate machine\nlearning models on a limited data sample, without\nany major improvements on our test set in terms of\naccuracy.\nTraining optimization\nFor the training optimization we assume the given\nMask-RCNN loss function as:\nL = αL1 + βL2 + γL3 + δL4 + ϵL5\n(4)\nwhere the ﬁrst loss is the RPN class loss and\nthe second the RPN bounding box prediction loss.\nThe other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='The other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization\ngoal is to ﬁnd a corresponding conﬁguration such\nthat it minimizes the overall loss. This is an adaptive\nprocess of matching the results of the loss function\nwith the given dataset and the parameters as a result\nof a trial process.\nGiven a loss Li and equation (4), optimize K =\n(α, β, γ, δ, ϵ) ∈ R5 and L1, L2, L3, L4, L5 loss func-\ntions so that the average precision of our model so\nthat the average precision of our model is improved\nwith a threshold of AP50 :\nmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='that the average precision of our model is improved\nwith a threshold of AP50 :\nmax\nK,L1,L2,L3,L4,L5 AP50\n(5)\nUltimately, the ﬁnal choice of parameters chosen\ndepends on the structure conﬁgurations discussed\nin the previous section and also on our input data\n. While technically for K any combination of real\nvalues can be combined:\n(α, β, γ, δ, ϵ) ∼ n.(α, β, γ, δ, ϵ), ∀n ∈ R\n(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10\non the loss gradient and not the loss value. Further-\nmore, several other conﬁguration parameters need\nto be tested and changed to improve accuracy. The\npractical approach for tuning loss weights was to\nstart with a default weight of 1 for each loss and\nevaluate the model on the validation set by inferring\nthe model performance image by image. We had to\nevaluate the true positives, false positives, and false\nnegatives combined with IoU of their masks and\ntheir score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='their score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In\nother words, we are using transfer learning, which\nmeans we don’t need to train a new model from\nscratch and that instead utilize a lot of the already\nlearned features from the COCO dataset, which\ncontains around 120K images. After the training for\n40 epochs with k-fold cross-validation have been\ncompleted, we save the model’s parameters and\nevaluate the new weights (one for each epoch is\ngenerated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='generated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-\ntating new images would take too long. Since the\npolygons are calculated internally this was easy to\nimplement with the imgaug library.\nFirst, we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith augmentation that ﬂip/mirror each image in\nthe training set horizontally right/left. Then we\nagain save the model’s parameters and evaluate the\nperformance of the new weights.\nThen we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Then we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the\ntraining set by rotation (-45, 45) and shearing (-16,\n16). Finally, we save the model’s parameters and\nevaluate the performance of the new weights with\nrunning inference on the weights that yielded the\nlowest training loss.\nInference optimization\nIn the process of optimizing the trained network,\nwe constantly adjusted and modiﬁed the training set\nand other parameters. Since the output of the infer-\nence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust\naccordingly with regards to the properties of the test\nset images and the overall accuracy. We found a\ndetection minimum conﬁdence of 0.9 to yield as\ngood trade-off of the precision and recall values for\nour windows.\n5. RESULTS\nWe perform our test result experiments on the\neTrims dataset and not on the ECP dataset. The\nreason being that the ECP dataset doesn’t contain\nstreet-view images and the images are all manually\nrectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='rectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.\nHowever, to properly use the eTRIMS dataset\nwith our model, we had to reﬁne their annotation\nto overcome inaccuracies and mistakes, and also\naccommodate our deﬁned window boundary which\nfollows the window frame. We, therefore, annotated\n10 randomly chosen images for our test set to\nevaluate our model’s performances. Therefore, any\ncomparisons should note these differences.\nBoth the training and inference was done on\nGoogle Colab with the free Tesla K80 GPU of about\n12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of\nthose we present the best three models with regards\nto the highest mAP on our test set (Table 1) and\nfor visualization we present the inference done on\nthe model achieving the highest mAP below (Figure\n12).\n11\nFigure 12.\nWindow detection results from a model trained on coco-weights, then resumed with ﬂiplr.\nImage\nGround truth\nPrediction\nOverlap\ngreen colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='green colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score\ncaptions: score/IoU\n12\nREFERENCES\nREFERENCES\nTable 1.\nA comparison of pixel accuracies on the eTRIMS\ndataset. Accuracies are shown in percentage.\nClass\n[1]\n[2]\n[3]\n[4]\n[5]\nOur*\nOur**\nOur***\nWindow[%] 75\n73\n71\n86', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Our***\nWindow[%] 75\n73\n71\n86\n90.91 91.93\n94.55\n94.76\nNote: Our result is only from 10 randomly chosen images from the eTRIMS\ndataset with reﬁned annotation.\nmodel trained on coco-weights then resumed with ﬂiplr and afﬁne using\nk-fold cross-validation (the resumed weights were chosen by the folds\nhighest mAP)\n** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)\n*** model trained on coco-weights then resumed with ﬂiplr (the resumed\nweights were chosen by the lowest training loss)\n6. CONCLUSIONS\nIn this paper, we propose to use the state-of-\nthe-art Mask R-CNN framework He et al. (2017)\nas an end-to-end network capable of instance seg-\nmentation without further machinery. Previous fa-\ncade parsing detection work often only produced\nsemantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='semantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is\ncapable of detecting and producing a bounding box\nand segmentation mask for each window instance.\nExperimental results show that our suggested\napproach can with only a relatively small but precise\ndataset train the network—only with transfer learn-\ning and augmentation—to produce instance segmen-\ntation of windows and a comparable result with\nprior state-of-the-art window detection approaches,\neven without pre- or post-optimization techniques.\nAs far as we know, we are the ﬁrst to have anno-\ntated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window\nboundary and not to include other window parts. We\nbelieve that our consequent and precise annotation\nof windows helped our model training. We also\nhope it can beneﬁt the research community as a free\nresource for more precise window detection.\n7. FUTURE WORK\nIn this paper, we have got a sense of how to tune\nthe hyper-parameters effectively with our training\ndata. However, further work is recommended to\ninvestigate this with platforms, such as Wandb or\nOnepanel Jobs for easier coordination, synchroniza-\ntion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types\nwere the model has low accuracy.\nFurther work could also beneﬁt from training\nmore facade classes, such as building, balcony,\ndoors, chimney, to reduce false positives, false neg-\natives. The same goes for the window class, i.e.\nsplitting it into distinct window type classes, such\nas double-hung, transom etc. A model capable of\na more complete facade parsing we believe will\nalso make application easier as the model has a\nﬁner granularity of the facades properties. A caution\nis that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='is that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of\nevery facade structure.\nTo further reﬁne the results and perhaps mitigate\nsome of the typical limitations we experienced,\nsuch as dealing with occlusion, overconﬁdent false\npositives predictions, occasional mask overlaps, fu-\nture work should investigate both in pre- and post-\noptimization methods of previous approaches or\ntesting their novel approaches. Also, code refactor-\ning and optimization, e.g. rewriting some Python\ncode in TensorFlow to bring all the beneﬁts of the\nTensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='TensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS\nThis paper was the ﬁnal written report in the\ncourse DIT891 Project in Data Science. The course\nis offered within the Applied Data Science Master’s\nProgramme by the University of Gothenburg. We\nwould like to thank Shirin Tavara, teacher and the\ncourse responsible and Richard Johansson, teacher.\nA special thanks goes to our supervisors Alexan-\nder Hollberg, Sanjay Somanath and Yinan Yu for\ntheir congeniality and support.\n13\nREFERENCES\nREFERENCES\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='13\nREFERENCES\nREFERENCES\nREFERENCES\nAbdulla, Waleed (2017). “Mask R-CNN for object\ndetection and instance segmentation on Keras and\nTensorFlow https://github. com/matterport”. In:\nMask _ RCNN.\nAbdulla, Waleed (2018). Instance Segmentation\nwith Mask R-CNN and TensorFlow. https : / /\nengineering . matterport . com / splash - of - color -\ninstance- segmentation- with- mask- r- cnn- and-\ntensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia\nRagia (2020). “3D building façade reconstruction\nusing deep learning”. In: ISPRS International\nJournal of Geo-Information 9(5), p. 322.\nBadrinarayanan,\nVijay,\nAlex\nKendall,\nRoberto\nCipolla, et al. (2015). “1SegNet: A Deep Convo-\nlutional Encoder-Decoder Architecture for Image\nSegmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Segmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:\nInternational\nJournal\nof\nAdvanced\nResearch\nin Electronics and Communication Engineering\n(IJARECE) Volume 5, pp. 1448–1454.\nChen, Liang-Chieh et al. (2017). “Deeplab: Seman-\ntic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected\ncrfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='crfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.\nDutta, Abhishek and Andrew Zisserman (2019).\n“The VIA annotation software for images, audio\nand video”. In: Proceedings of the 27th ACM in-\nternational conference on multimedia, pp. 2276–\n2279.\nFathalla, Radwa and George Vogiatzis (2017). “A\ndeep learning pipeline for semantic facade seg-\nmentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='mentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer\nvision, pp. 1440–1448.\nHan, Feng and Song-Chun Zhu (2008). “Bottom-\nup/top-down image parsing with attribute gram-\nmar”. In: IEEE transactions on pattern analysis\nand machine intelligence 31(1), pp. 59–73.\nHe, Kaiming et al. (2017). “Mask r-cnn”. In: Pro-\nceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.\nHu, Han et al. (2020). “Fast and Regularized Re-\nconstruction of Building Fa\\c {c} ades from\nStreet-View Images using Binary Integer Pro-\ngramming”. In: arXiv preprint arXiv:2002.08549.\nKarmarkar, T (2018). “Regional Proposal network\n(RPN)—Backbone of Faster R-CNN”. In: Aug\n18, p. 6.\nKoch, David et al. (2018). “Visual estimation of\nbuilding condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='building condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on\nMultimedia for Real Estate Tech, pp. 12–17.\nKorc,\nFilip\nand\nWolfgang\nFörstner\n(2009).\n“eTRIMS\nImage\nDatabase\nfor\ninterpreting\nimages of man-made scenes”. In: Dept. of\nPhotogrammetry, University of Bonn, Tech. Rep.\nTR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='TR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep\nlearning approach to facade parsing”. In.\nLong, Jonathan, Evan Shelhamer, and Trevor Dar-\nrell (2015). “Fully convolutional networks for\nsemantic segmentation”. In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pp. 3431–3440.\nMa, Wenguang and Wei Ma (2020). “Deep window\ndetection in street scenes”. In: KSII Transactions\non Internet and Information Systems (TIIS) 14(2),\npp. 855–870.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='on Internet and Information Systems (TIIS) 14(2),\npp. 855–870.\nMathias, Markus et al. (2011). “Automatic architec-\ntural style recognition”. In: ISPRS-International\nArchives of the Photogrammetry, Remote Sensing\nand Spatial Information Sciences 3816, pp. 171–\n176.\nMüller, Pascal et al. (2007). “Image-based procedu-\nral modeling of facades”. In: ACM Trans. Graph.\n26(3), p. 85.\nNogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Nogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting\nconvolutional neural networks for remote sensing\nscene classiﬁcation”. In: Pattern Recognition 61,\npp. 539–556.\nParthasarathy,\nDhruv\n(2017).\n“A\nbrief\nhistory\nof CNNs in image segmentation: From R-\n14\nCNN to Mask R-CNN”. In: Available online:\nblog.\nathelas.\ncom/a-brief-history-of-cnns-in-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='blog.\nathelas.\ncom/a-brief-history-of-cnns-in-\nimage-segmentation-from-r-cnn-to-mask-rcnn-\n34ea83205de4 (accessed on 24 February 2019).\nPenatti, Otávio AB, Keiller Nogueira, and Jefersson\nA Dos Santos (2015). “Do deep features general-\nize from everyday objects to remote sensing and\naerial scenes domains?” In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition workshops, pp. 44–51.\nRahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Rahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der\nBundeswehr München.” In.\nRecky, Michal and Franz Leberl (2010). “Windows\ndetection using k-means in cie-lab color space”.\nIn: 2010 20th International Conference on Pat-\ntern Recognition. IEEE, pp. 356–359.\nRen, Shaoqing et al. (2016). “Faster R-CNN: to-\nwards real-time object detection with region pro-\nposal networks”. In: IEEE transactions on pat-\ntern analysis and machine intelligence 39(6),\npp. 1137–1149.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tern analysis and machine intelligence 39(6),\npp. 1137–1149.\nSchmitz, Matthias and Helmut Mayer (2016). “A\nconvolutional network for semantic facade seg-\nmentation and interpretation”. In: The Interna-\ntional Archives of Photogrammetry, Remote Sens-\ning and Spatial Information Sciences 41, p. 709.\nSultana, F, A Suﬁan, and P Dutta (2020). “Evolution\nof Image Segmentation using Deep Convolutional\nNeural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Neural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.\nTeboul, Olivier et al. (2010). “Segmentation of\nbuilding facades using procedural shape priors”.\nIn: 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition. IEEE,\npp. 3105–3112.\nTu, Zhuowen et al. (2005). “Image parsing: Unify-\ning segmentation, detection, and recognition”. In:\nInternational Journal of computer vision 63(2),\npp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='pp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst\nBischof (2010). “Unsupervised facade segmenta-\ntion using repetitive patterns”. In: Joint Pattern\nRecognition Symposium. Springer, pp. 51–60.\nZhao, Peng et al. (2010). “Rectilinear parsing of ar-\nchitecture in urban environment”. In: 2010 IEEE\nComputer Society Conference on Computer Vi-\nsion and Pattern Recognition. IEEE, pp. 342–349.\nZhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Zhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-\nject recognition by data driven Markov chain\nMonte Carlo”. In: Proceedings IEEE Conference\non Computer Vision and Pattern Recognition.\nCVPR 2000 (Cat. No. PR00662). Vol. 1. IEEE,\npp. 738–745.\n15', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa5795f50> 111
cuda:2
[Document(page_content='Window Detection In Facade Imagery:\nA Deep Learning Approach Using Mask R-CNN\nNils Nordmark, University of Gothenburg\nMola Ayenew, University of Gothenburg\nAbstract\nThe parsing of windows in building facades is a long-\ndesired but challenging task in computer vision. It is\ncrucial to urban analysis, semantic reconstruction, life\ncycle analysis, digital twins and scene parsing amongst\nother building-related tasks that require high-quality se-\nmantic data. In this article, we investigate the usage of the\nMask R-CNN framework to be used for window detection\nof facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='of facade imagery input. We utilize transfer learning to\ntrain our proposed method on COCO weights with our\nown collected dataset of street view images of facades to\nproduce instance segmentations of our new window class.\nExperimental results show that our suggested approach can\nwith a relatively small dataset train the network only with\ntransfer learning and augmentation achieve results on par\nwith prior state-of-the-art window detection approaches,\neven without post-optimization techniques.\n1. INTRODUCTION\nGathering semantic data regarding building fea-\ntures from images is desired as manual gathering\ncan often be a challenging, time consuming and\nresource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='resource intensive task. For example, information\nregarding location, shapes and sizes, materials of\nbuilding structures such as windows, doors and\nbalconies are often not available, can change over\ntime and the data gathering is usually a manual\nprocess.\nMany different approaches have been applied to\nsolve the gathering of semantic data within many\ndifferent domains. Within building facade parsing\nthis task has mostly been treated as a process of\nsemantic segmentation of facade imagery into archi-\ntectural structural classes such as windows, doors,\nbalconies and so on. This semantic segmentation\napproach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='approach provides information about facade classes\nas it assigns each pixel with a class label. This\nmeans that it does not differentiate instances of the\nsame object as each pixel is classiﬁed to belong\nto a particular class. Speciﬁcally for parsing of\nwindows instances in building facades, we suggest\nan approach that can decouple segmentation and\nclassiﬁcation to detect windows in an image with\na bounding box and segmentation mask for each\nwindow instance.\nWith instance segmentation, we can analyze the\nwindows independently with detailed and compre-\nhensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='hensive information for each pixel and speciﬁcally\nto which window it belongs to. To separate the\npixels of the same category into different instances,\nwe propose an approach based on the Mask R-CNN\nframework.\n2. RELATED WORK\nFacade parsing has been studied for a long time\nand there exists a large body of work on how to\ndeal with such a problem.\nEarlier approaches\nZhao et al. (2010) proposed parsing ground-\nlevel images into architectural units which could\nbe used for city modelling. Wendel, Donoser, and\nBischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Bischof (2010) used Scale–Invariant Feature Trans-\nform (SIFT) and Recky and Leberl (2010) used K-\nmeans clustering to tackle the same problem.\nMany others proposed a procedural shape gram-\nmar approach. This approach could at a high-level\nbe explained as a hand-crafted set of rules of basic\nshapes which represents structured geometries, i.e.\nthe structure of the object is encoded as a set of\nparametric rules. Models could then be generated\nby iteratively applying the rules on a starting shape.\nMüller et al. (2007) used a procedural modelling\npipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='pipeline of shape grammars combined with image\nanalysis to derive hierarchical facade subdivisions.\nHan and Zhu (2008) extended the work with an\nattribute grammar which used a more advanced\narXiv:2107.10006v1  [cs.CV]  21 Jul 2021\nbottom-up and top-down mechanism for recogniz-\ning objects. Their method was based on previous\nobject recognition work, such as Zhu, Zhang, and\nTu (2000) and Tu et al. (2005).\nTeboul et al. (2010) combined machine learning\nalgorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='algorithms such as Reinforcement Learning (RL)\nwith procedural modelling and shape grammar.\nAlthough their method could be said to improve\nthe works of Müller et al. (2007) and Han and\nZhu (2008), the learning process to some extent still\nwas based on hand-crafted rules.\nMathias et al. (2011) took another approach based\non architectural styles for automatic classiﬁcation.\nThey tried to extend Müller et al. (2007) work\nwith appropriate style information as they argued\nthat it could be used to select a more appropriate\nprocedural grammar for the task of building recon-\nstruction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='struction. Their approach was sort of an ‘inverse’\nprocedural modelling, i.e. an attempt to not rely on\nthe assumption that the building style was known\nand needed to be conducted as a manual task.\nInstead, they tried to automate the classiﬁcation of\narchitectural styles, by using facade images to train\ntheir classiﬁer to distinguish between three distinct\narchitectural styles. For classifying those speciﬁc\nstyles they were quite accurate but it also required\na rather speciﬁc set of circumstances to succeed.\nAs most of the earlier approaches used hand-\ncrafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='crafted features and traditional classiﬁers a common\nlimitation was that the required hand-crafted fea-\ntures often failed to represent the diversity of more\ncomplex objects, i.e. could not handle deviations\nfrom these set of user-deﬁned rules well. They also\nsuffered in accuracy when the input image was\nmore complex, such as street-view images where the\nimage was not perfectly rectiﬁed. Another common\nlimitation was that they often suffered from heavy\ncomputational costs and slow running times (Bala\nand Kumar (2016), Koch et al. (2018)).\nConvolutional Neural Networks Approaches\nMore recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='More recently, convolutional neural networks\n(CNNs) based models have gained in popularity\ndue to their good performances. With the success\nof AlexNet in 2012, many semantic segmentation\napproaches which used to apply traditional ma-\nchine learning algorithms have instead been pri-\nmarily based on CNNs Bala and Kumar (2016);\nParthasarathy (2017).\nImproved models quickly followed AlexNet such\nas Segnet (Badrinarayanan, Kendall, Cipolla, et\nal. (2015)) which was one of the ﬁrst to tackle\nthe problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='the problem of semantic segmentation of a whole\nimage.\nAnother major milestone came when Long, Shel-\nhamer, and Darrell (2015) showed that a fully\nconvolutional network (FCN) trained end-to-end,\npixels-to-pixels could exceed then state-of-the-art\nsemantic image segmentation methods by retaining\nspatial information of an image. In more detail,\nthey used three pre-trained base models (AlexNet,\nVGGNet and GoogleNet) and transferred them from\nclassiﬁers by replacing fully connected layers with\nconvolutional layers. Since then a semantic seg-\nmentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='mentation model trend based on FCNs followed\nwith improving results (Sultana, Suﬁan, and P.\nDutta (2020)).\nChen et al. (2017) use dilated convolution instead\nof plain convolution to set new state-of-art results\nwith their proposed DeepLab system.\nRecently, more automated methods and entirely\nbased on deep learning have achieved remarkable\nimprovements in image analysis, such as Penatti,\nNogueira, and Dos Santos (2015) and Nogueira,\nPenatti, and Dos Santos (2017).\nDeep Learning Approaches\nDeep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Deep learning and speciﬁcally deep convolu-\ntional neural networks have shown to outperform\nother image analysis methods for traditional vision\ntasks (classiﬁcation, object detection, semantic seg-\nmentation and instance segmentation) in a lot of\nbenchmarks and a variety of ﬁelds, e.g. medicine,\nsurveillance, economics, sociology. They have been\napplied to a diversity of tasks such as face detection,\nspeech recognition, autonomous vehicles and so on.\nDeep Learning Approaches Related to Facade\nParsing\nNaturally, deep learning was also applied to\nbuilding-related image analysis problems as well.\n2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='2\nRecently there are some examples of work where\ndeep learning has been applied for facade parsing\nquite speciﬁcally and rather successfully.\nSchmitz and Mayer (2016) method for semantic\ninterpretation of facade images used a convolutional\nnetwork and a key insight was that even smaller\ndatasets could train the network when transfer learn-\ning could be employed. For validation, they used the\neTRIMS dataset to measure the overall accuracy,\nprecision and recall. The F1 score had an accuracy\nof 0.82± 0.09 for the facade categories, e.g. win-\ndows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='dows 0.86 ± 0.05 0.67.\nLiu et al. (2017) pushed the development of\nfacade parsing even further with their method Deep-\nFacade which combined both the learning capacity\nof deep convolutional neural networks with rules\nand a loss function based on symmetry found in\nbuilding facades structures. They trained an FCN-\n8s network with their novel loss function to obtain\nexperimental results on both the ECP dataset and\nthe eTRIMS dataset that signiﬁcantly outperformed\nprevious state-of-the-art methods. Pixel accuracies\non the eTRIMS dataset reached as high as 0.91 for\nwindows.\nFathalla and Vogiatzis (2017) used appearance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='windows.\nFathalla and Vogiatzis (2017) used appearance\nand layout cues combined with the VGG-model.\nTheir method only had a single object of focus and\noverall accuracy was on par with other methods,\nalthough supplementary materials had to be asked\nfor an in-depth understanding of the results.\nKoch et al. (2018) developed a multi-scale patch-\nbased pattern extraction that combined with a CNN\n(AlexNet), to automatically estimate the condition\nof buildings. They showed to some degree their\nmethod could serve as a proxy for condition esti-\nmates by appraisers.\nBacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Bacharidis, Sarri, and Ragia (2020) used stereo-\nscopic image and tachometer data combined with\na deep learning-based facade segmentation stage\nbased on generative adversarial networks (GANs).\nThe task was to reconstruct 3D models of build-\nings facades, in the urban environment for cultural\nheritage.\nHu et al. (2020) used the bounding boxes detected\nby YOLO architecture in real-time to guide facade\nreconstruction in an interactive environment. Results\nshowed accuracies above 0.82, which they deemed\nas sufﬁcient for their 3D reconstruction task.\nRahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Rahmani and K. (2020) developed a hybrid\npipeline for his master thesis which consisted of\nboth traditional and modern machine learning ap-\nproaches. His system was trained for windows delin-\neation segmentation, i.e. detecting window frames,\nmullions, transoms and so on. For small datasets\nit showed promising results, however, this method\nis not competitive compared to complete end-to-\nend state-of-the-art deep learning approaches such\nas Liu et al. (2017).\nWenguang Ma and Wei Ma (2020) used Faster\nR-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='R-CNN architecture for window detection, they\nspeciﬁcally wanted to obtain the locations of the\nwindows in their images for 3D reconstruction.\nThey called their method WD-Net which showed\n0.5 per cent improved accuracy compared with the\nbaseline Faster R-CNN.\n3. OUR APPROACH\nMask R-CNN Architecture\nIn this paper, we suggested the Mask R-CNN\nmodel, which is an intuitive extension of the Faster\nR-CNN model and outputs the predicting segmenta-\ntion masks on each Region of Interest (RoI), in par-\nallel with the existing branch for classiﬁcation and\nbounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='bounding box regression. Faster R-CNN is a popular\nframework for object detection but it is not designed\nfor pixel-to-pixel alignment between network inputs\nand outputs. The segmentation mask in Mask R-\nCNN represents the pixel-level segmentation and\nalignment of each object that helps to extract each\ninstance of an object separately without background.\nMask R-CNN is a two-stage framework. The ﬁrst\nstage scans the image and generates proposals, the\nsecond stage classiﬁes the proposals and generates\nbounding boxes and masks Abdulla (2018); He\net al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='et al. (2017).\nMask R-CNN architecture consists of four high-\nlevel modules those are; Backbone, Region Proposal\nNetwork (RPN), ROI Classiﬁer & Bounding Box\nRegressor, and Segmentation Masks as shown in the\n(Figure 2) below.\n3\nFigure 1. Mask R-CNN framework\nNote. Image from (He et al. (2017))\nFigure 2. Mask R-CNN Internal Architecture\nNote. Figure 2 shows the steps until the instance\nsegmentation processes takes place from the ﬁrst\nstage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='stage to the last stage.\nBackbone\nThe input image is fed into the CNN architecture,\ncalled the backbone. It is a pre-trained standard con-\nvolutional neural network, which is either ResNet50\nor ResNet101. The backbone network serves as a\nfeature extractor over an entire image, at the early\nlayers the backbone detects low-level features like\nedges and corners, and later layers successively\ndetect higher-level features like the car, person,\nsky. When the image passes through the backbone\nnetwork, it is converted from 1024x1024px x 3\n(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(RGB) to a feature map of shape 32x32x2048. This\nfeature map becomes the input for the following\nstages.\nAnother more effective backbone, called a Fea-\nture Pyramid Network (FPN) applied on the top of\nthe ResNet backbone to improve the feature extrac-\ntion method. FPN uses a top-down architecture with\nlateral connections to build an in-network feature\npyramid from a single-scale input. It improves the\nstandard feature extraction pyramid by adding a\nsecond pyramid that takes the high-level features\nfrom the ﬁrst pyramid and passes them down to\nlower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='lower layers. By doing so, it allows features at every\nlevel to have access to both, lower and higher-level\nfeatures. Using a ResNet-FPN backbone for feature\nextraction with Mask R-CNN gives excellent gains\nin both accuracy and speed Abdulla (2018).\nRegion Proposal Network (RPN)\nA Region Proposal Network (RPN) was intro-\nduced by Ren et al. (2016) that takes an image as\ninput and outputs a set of rectangular object propos-\nals and ﬁnds areas that contain objects. To generate\nregion proposals, the author scans the images in\na sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='a sliding-window fashion over the convolutional\nfeature map received from the ResNet101 + FPN\nbackbone layers. The network takes as input an n ×\nn spatial window of the input convolutional feature\nmap. Each sliding window is mapped to a lower-\ndimensional feature. The RPN doesn’t scan over\nthe image directly, instead, the RPN scans over the\nbackbone feature map. This allows the RPN to reuse\nthe extracted features efﬁciently and avoid duplicate\ncalculations.Then this feature is fed into two fully\nconnected layers, a box-regression layer, and a box-\nclassiﬁcation layer.\nRPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='RPN has a classiﬁer(cls) and a regressor(reg)\nas shown in the (Figure 3). The regions that the\nRPN scans over are called anchors, which are boxes\ndistributed over the image area. The anchor is the\ncentral point of the sliding window. For the ZF\nModel which was an extension of AlexNet, the\ndimensions are 256-d and for VGG-16, it was 512-d.\nThe classiﬁer determines the probability of a pro-\nposal having the target object. Regression regresses\nthe coordinates of the proposals. For any image,\n4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='4\nFigure 3. Region Proposal Network (RPN)\nNote. Image from (ren2015faster ). The RPN\nproposed by passing a sliding window over the\nCNN feature maps and at each window outputting\nk potential bounding boxes and scores for how\ngood each bounding box expected to be.\nscale and aspect-ratio are two important parameters.\nAspect ratio is width of image/height of the image,\nthe scale is the size of the image. (Figure 3) shows\nthat if 3 scales and 3 aspect-ratio are chosen, the\ntotal of 9 proposals are possible for each pixel,\nthis is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='this is how the value of k is decided, K=9 for this\ncase, k being the number of anchors. For the whole\nimage, the number of anchors will be W*H*K,\nwhere W and H are the width and height of the\nimage respectively Karmarkar (2018).\nThe RPN generates two outputs for each anchor:\nAnchor Class:\nThe anchor class can be either\nforeground or background classes. The foreground\nclass implies that there is likely an object in that\nbox.\nBounding Box Reﬁnement: A foreground an-\nchor might not be centered perfectly over the object.\nSo the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='So the RPN estimates a delta change to reﬁne the\nanchor box to ﬁt the object better. Using the RPN\npredictions, the model picks the top anchors that are\nlikely to contain objects and reﬁne their location and\nsize.If several anchors overlap too much, it keeps the\none with the highest foreground score and discards\nthe rest (Non-max Suppression). After getting the\nﬁnal proposals (regions of interest) then, it passes\nto the next stage.\nFigure 4. Example, a) Anchor boxes per object b)\nTop three anchor boxes pre object\n(a)\n(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(b)\n(a) shows that there are about 200k anchors can\ncreated for each object with different sizes and\naspect ratios and they overlap each other to cover\nthe object as much as possible. (b) shows that the\ntop three anchor boxes that are more likely to\ncontain an object, after some reﬁnement on their\nlocation and size\nROI Classiﬁer & Bounding Box Regressor\nThis\narchitecture\nwas\nintroduced\nby\n(Gir-\nshick (2015)), by removing the SVM classiﬁers for\ntraining to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='training to a regression layer and classiﬁcation layer.\nThis stage runs on the regions of interest (ROIs)\nproposed by the RPN.\nFirst, the network processes all images with sev-\neral convolutional and max-pooling layers to pro-\nduce convolutional feature map. Then a region of\ninterest (RoI) pooling layer extracts a ﬁxed length\nfeature vector for each object proposal from the fea-\nture map. Then, each feature vectored into the fully\nconvolutional layers and produces softmax proba-\nbility over different object classes with background\nclass and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='class and output the probability of conﬁdence for\neach object.\nThis module generates two outputs for each ROI:\nClass: The class of the object in the ROI. Since this\nnetwork is deeper and has the capacity to classify\nregions to speciﬁc classes like person, car, chair,etc.\nIt can also generate a background class, which\ncauses the ROI to be discarded.\nBounding Box Reﬁnement: Further reﬁne the\nlocation and size of the bounding box to encapsulate\n5\nthe object.\nRoI pooling layer: Due to the bounding box\nreﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='reﬁnement step in the RPN, the ROI boxes can\nhave different sizes. ROI pooling is used to crop\na part of a feature map and resize it to a ﬁxed size.\nThe RoI pooling layer uses max pooling to convert\nthe features inside any valid region of interest into\na small feature map with a ﬁxed spatial extent of\nH×W (e.g., 7 × 7), where H and W are layer hyper-\nparameters that are independent of any particular\nRoI Girshick (2015).\nSegmentation Masks\nThe mask network is the addition that the Mask\nR-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='R-CNN introduces. The mask branch is a con-\nvolutional network that takes the positive regions\nselected by the ROI classiﬁer and generates masks\nfor them. The generated masks are low resolution:\n28x28 pixels. But they are soft masks, represented\nby ﬂoat numbers, so they hold more details than\nbinary masks. The small mask size helps to keep the\nmask branch light. During training, the ground-truth\nmasks. scale down to 28x28 to compute the loss,\nand during inference, the predicted masks scale up\nto the size of the ROI bounding box and that gives\nus the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='us the ﬁnal masks, one per object.\nMask R-CNN Losses\nThe loss function in Mask R-CNN is the weighted\nsum of different losses at different stages of the\nmodel. Five general losses occur during training the\nweight Abdulla (2018).\nRpn class loss: Occur when an improper clas-\nsiﬁcation of anchor boxes by the Region Proposal\nNetwork. This loss increases when the multiple\nobjects are not being detected by the model in the\nlast output. This loss determines how well the Re-\ngion Proposal Network separates background with\nobjects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='objects.\nRpn bbox loss: This loss occurs during the local-\nization accuracy of the RPN. The object is being\ndetected but the bounding box is not properly ﬁt\nwith the position of the object.\nMrcnn class loss: Occurs when an improper clas-\nsiﬁcation of objects that are present in the region\nproposal. This is to be increased in case the object\nis being detected from the image, but misclassiﬁed.\nIt determines how well the Mask RCNN recognizes\neach class of object.\nMrcnn bbox loss: This loss occurs during the local-\nization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ization of the bounding box of the identiﬁed class.\nThe loss increases when the correct classiﬁcation of\nthe object is done, but localization is not precise.\nMrcnn mask loss: This loss corresponds to masks\ncreated on the identiﬁed objects, which is related to\nhow well the Mask RCNN segment objects.\nPublic datasets\nAs for current dataset resources, the Ecole\nCentrale Paris (ECP) Facades dataset Teboul et\nal. (2010) and the eTRIMS Korc and Först-\nner (2009) database are the two most relevant public\nfacade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='facade datasets we are aware of as both have been\nused as a benchmark for facade parsing in previous\nliterature.\nThe ECP dataset consists of 104 annotated im-\nages which are rectiﬁed and cropped facades of\nHaussmannian style buildings in Paris. Images are\nannotated with seven classes (balcony, chimney,\ndoor, roof, shop, wall and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image\nthat consists of a colourmap (Fig. 5). The original\nannotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='annotations were improved by Mathias et al. (2011)\nto better ﬁt the ground truth.\nThe eTRIMS consists of 60 annotated street\nview images but different from the ECP dataset\nthe images are not rectiﬁed and the facades are of\nvarious architectural styles. Images are annotated\nwith eight classes (sky, building, door, vegetation,\ncar, road, pavement and window). Ground truth\nobject segmentation assigns each pixel to either\none of the annotated objects or background. The\nobject segmentation is represented as an image that\nconsists of an array and a color-map matrix (Fig.\n6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='6).\nAfter inspecting these dataset we found it unwise\nto use either datasets for training our model to\ndetect windows in real street view images because\n6\nFigure 5. Example from ECP dataset, a) the input\nimage b) the annotation\n(a)\n(b)\nNote. Image from (Teboul et al. (2010))\nFigure 6. Example from eTRIMS, (a) the input\nimage (b) the annotation\n(a)\n(b)\nNote. In the middle part of (b), the upper window\nboundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='boundaries include the ‘crown’ or ﬂashing over the\ncornice. Image from (Korc and Förstner (2009)).\nof multiple reasons. The ECP dataset only contains\nfacades that are manually rectiﬁed and viewed in a\nfront-parallel direction, thus using it as training data\nwould not be wise since it is not highly representa-\ntive of real street view images, which is the imagery\nwe want to detect windows instances in. However,\nthe most important reasons were that we found both\nto be imprecise or even wrongly annotated regarding\nwhat exactly deﬁnes the window boundaries. For\nexample, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='example, the eTRIMS dataset often annotated the\nwindow boundaries to include not only the window\nframe but even other details such as the exterior\ncasing (See Figure 6).\nWe searched and found a few more public\ndatasets that contained window annotations but none\nwere of very high quality and many had the same\nissues as mentioned above. Therefore we had to take\na decision to build a quality dataset from scratch.\nOur dataset\nWe decided to use Google Street View images\n(Map data ©2020 Google) of facades mainly from\ncentral Gothenburg, Sweden. The cropped images\nwere manually collected with an intention to con-\ntain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tain a diversity of typical Swedish urban building\nfacades that encompassed a large variety of window\ntypes.\nWe annotated them manually with the VGG\nImage Annotator (VIA) tool A. Dutta and Zisser-\nman (2019). There exists a lot of other tools to\nannotate images, e.g. LabelMe, RectLabel, Label-\nBox and COCO UI, but we chose VIA because\nof its efﬁciency and simplicity. For example, it\nruns in most modern web browsers so it does not\nrequire any other installation or setup. Regarding\nthe annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='the annotated segmentation masks there is not a\nuniversally standard data format to store them. Some\ndatasets save them as PNG images, while others\nstore them in XML-format, and so on. The VIA tool\nsaves its annotations in a JSON-format ﬁle where\neach mask is a set of polygon points (Figure 7).\nSince we tried to be consistent to follow each\nwindow’s exact window frame and not to include\nother window parts as window boundaries it made\nannotation a very time-consuming and complex\ntask, especially as speciﬁc decisions had to be taken\nfor each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='for each window type. This precision and attention\nto detail made annotating the earlier images very\ngruelling, but once we got used to both the user\ninterface and ﬁgured out most of the different win-\n7\nFigure 7. Two examples from images in our dataset\nannotated with the VIA tool\n(a)\n(b)\nNote. The yellow lines are the manually annotated\npolygons which deﬁne each window boundary.\ndow types and their frames, we were annotating one\nwindow about every 5-10 seconds. However, there\nwere a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='were a lot of windows in some pictures and we\nwanted to be as consistent and precise as possible.\nAfter gradually expanding our dataset we ended\nup with a collection of 100 images that contained\n1540 annotated windows (an average of 15,4 win-\ndows per image). We then divide our dataset into a\ntraining set (80 images) and a validation set (20 im-\nages), i.e. we split the generated JSON-ﬁle into two\ndifferent ﬁles (Figure 8) with their corresponding\nimages.\nOptimization Methods\nTo use the Mask R-CNN framework we used\nan open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='an open-source code implementation (Matterport) of\nit written on Python 3, Keras, and TensorFlow by\nAbdulla (2017). Then we hosted the implementation\non the Google Colab platform for free GPU usage\nand easy of shareability amongst other positive\nfactors.\nAssume to start with the structure of the Mask\nR-CNN as described in the paper He et al. (2017)\nand the source code provided in Abdulla (2017), we\nneeded to investigate the similarities and differences\nbetween the Matterport implementation running on\nKeras and TensorFlow and the ofﬁcial paper imple-\nFigure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Figure 8. Examples of the JSON-ﬁles generated\nwith the VIA tool\nNote. The left JSON-ﬁle corresponds to the\ntraining set and the right to the validation set.\nBoth are viewed with JSON Editor Online.\nmentation build on the Caffe deep learning frame-\nwork.\nFrom our understanding, the implementation fol-\nlowed the paper with only a few exceptions and\nthose were done mostly for code simplicity and\ngeneralization.\nFor example, to support training multiple images\nper batch, the Matterport implementation resizes all\nimages to the same size and preserve the aspect ratio\nby padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='by padding with zeros if the image is not a perfect\nsquare.\nTo support the training of different dataset’s anno-\ntations they chose to generate bounding boxes in the\ncode on the ﬂy, thus making image augmentations\neasier to apply. The downside is a slight decrease in\nbounding box placement accuracy, about only 2%\nof bounding boxes differed by 1px or more.\nThe paper uses a more aggressive learning rate of\n0.02, but for the Matterport implementation that was\ntoo high and causes the weights to explode in Ten-\nsorFlow as it computes gradients differently from\nCaffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Caffe. To mitigate this problem, gradient clipping\nwas implemented.\nWith an understanding of the differences between\nMatterport’s and the paper’s features, parameters,\nhyper-parameters and so on, we could start to\noptimize our model. The optimization adjustments\nand modiﬁcations were updated continuously during\n8\nthe whole process, but we can perhaps give some\nintuition on how they were done by describing them\nas iteratively separate steps. For example, optimiza-\ntion of the network structure with its corresponding\nconﬁguration can be considered ’low level’, while\noptimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='optimizing training was done with more regards to\nthe properties of our dataset. Finally, in inference\noptimization, we look at the outcome and try to\nadjust our result within the context of our research\nproblem or goal. Furthermore, each step often\nneeded distinct conﬁguration adjustment depending\non which model weights and dataset changes were\nused, i.e. training resumed with COCO-weights,\naugmentation ﬂip, augmentation afﬁne and so on,\nall needed different conﬁgurations to train properly.\nFigure 9. Examples of the many conﬁgurations\noptions in the Matterport implementation\nNote. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Note. This conﬁguration was used throughout the\ntraining steps for the model which achieved the\nhighest mAP on our testset (See Table 1 and\nFigure 12).\n4. EXPERIMENTS\nIn this section, we will ﬁrst present some metrics\nused in our experiments, then give insight into the\nthought process of some of our structure and con-\nﬁguration optimization, training optimization and\ninference optimization. Finally, we will present our\nexperimental settings and some of the best results\nwe achieved.\nMetrics\nTo use the networks inference results for detect-\ning windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ing windows and analysing the performance of our\nmodel, we are interested in detecting every existing\nwindow as well as reducing the number of wrongly\ndetected windows. In other words, recall is a crucial\nvalue to improve. To evaluate the images overall\ndetection results, we ﬁrst use IoU (Intersection\nover Union) to measure how much our predicted\nboundary overlaps with the ground truth (the real\nobject boundary). A predeﬁned IoU threshold of 0.5\nor greater is used for classifying the prediction as a\ntrue positive otherwise a false positive.\nFigure 10. Examples of IoU.\nNote. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Note. The ground truth is colored green and the\nprediction is colored red. Each detected window\ninstance has a captions with its own conﬁdence\nscore and measured IoU.\nThen we use the typical precision and recall rate:\nPrecision =\nTP\nTP + FP\n(1)\nRecall =\nTP\nTP + FN\n(2)\nwhere, TP, FP, FN denotes the true positive, false\npositive and false negative, respectively.\nFinally, we use the mean Average Precision\n(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(mAP):\nmAPI& =\nZ 1\n0\nP(R) dR\n(3)\nwhere the P represent the precision rate and and R\nthe recall rate.\nStructure and conﬁguration optimization\nOverall, we found it a challenging task to test\nand tune the many parameters, mainly due to the\n9\nadaptive process of such a large search space, the\ncontinuous updates to our training data, but also the\ntime it takes to train and do inference to calculate\nthe accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='the accuracy. However, we will try to summarize\nthe more important optimization decision we took.\nWe tested many different ‘lower-level’ conﬁg-\nurations of the neural network, for example, we\ninvestigated Mask R-CNN in combination with both\nthe ResNet50 and the ResNet101 Backbone as\ndescribed in He et al. (2017). Those experiments\nshowed that the latter often yielded better results.\nWe kept the base conﬁguration of input images of\nsize 1024x1024 px for best accuracy even if some\nof our images were a bit smaller since the model\nresized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='resized them automatically. Further testing could\nbeneﬁt separate tests with batches of different image\nsizes.\nWe found the gradient clipping norm to be ap-\npropriately set a 5 with a learning rate at 0.001 and\nmomentum of 0.9 and so on.\nBy plotting each test we could observe that train-\ning and validation losses indicated overﬁtting, i.e.\nwe got an increasingly lower training loss but the\nvalidation loss at certain points started to ﬂuctuate\nor stagnate (Figure 11).\nFigure 11. Examples of plots from our training\nNote. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Note. This particular plot was tested for one of\nour k-fold cross validation tests.\nTo overcome overﬁtting we experimented with\nweight decay and dropout amongst other things. Un-\nfortunately, GridSearch with cross-validation, to au-\ntomatically search for the optimal hyper-parameter\nvalues, required more computational power than we\nhad access to, but we believe a valid option given\nthe appropriate resources.\nAnother option and the most intuitive, to prevent\noverﬁtting is often to extend the dataset. However,\nthis was neither feasible given the remaining project\ntime and also did not ﬁt well with our interest in\ninvestigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='investigating the performance of a relatively small\ndataset.\nFinally, we used k-fold cross-validation, which is\na resampling procedure used to evaluate machine\nlearning models on a limited data sample, without\nany major improvements on our test set in terms of\naccuracy.\nTraining optimization\nFor the training optimization we assume the given\nMask-RCNN loss function as:\nL = αL1 + βL2 + γL3 + δL4 + ϵL5\n(4)\nwhere the ﬁrst loss is the RPN class loss and\nthe second the RPN bounding box prediction loss.\nThe other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='The other three losses correspond to those of the\nhead neural networks classes, bounding box and\nmask loss respectively. The training optimization\ngoal is to ﬁnd a corresponding conﬁguration such\nthat it minimizes the overall loss. This is an adaptive\nprocess of matching the results of the loss function\nwith the given dataset and the parameters as a result\nof a trial process.\nGiven a loss Li and equation (4), optimize K =\n(α, β, γ, δ, ϵ) ∈ R5 and L1, L2, L3, L4, L5 loss func-\ntions so that the average precision of our model so\nthat the average precision of our model is improved\nwith a threshold of AP50 :\nmax', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='that the average precision of our model is improved\nwith a threshold of AP50 :\nmax\nK,L1,L2,L3,L4,L5 AP50\n(5)\nUltimately, the ﬁnal choice of parameters chosen\ndepends on the structure conﬁgurations discussed\nin the previous section and also on our input data\n. While technically for K any combination of real\nvalues can be combined:\n(α, β, γ, δ, ϵ) ∼ n.(α, β, γ, δ, ϵ), ∀n ∈ R\n(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='(6)\nThis means that even though the total loss value\nchanges, our accuracy of the detection is dependent\n10\non the loss gradient and not the loss value. Further-\nmore, several other conﬁguration parameters need\nto be tested and changed to improve accuracy. The\npractical approach for tuning loss weights was to\nstart with a default weight of 1 for each loss and\nevaluate the model on the validation set by inferring\nthe model performance image by image. We had to\nevaluate the true positives, false positives, and false\nnegatives combined with IoU of their masks and\ntheir score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='their score.\nTransfer learning\nIn our approach, we specify that training should\nstart from weights pre-trained on COCO dataset. In\nother words, we are using transfer learning, which\nmeans we don’t need to train a new model from\nscratch and that instead utilize a lot of the already\nlearned features from the COCO dataset, which\ncontains around 120K images. After the training for\n40 epochs with k-fold cross-validation have been\ncompleted, we save the model’s parameters and\nevaluate the new weights (one for each epoch is\ngenerated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='generated).\nAugmentation\nTo train our model with more training data, we\nused augmentation on our existing data as anno-\ntating new images would take too long. Since the\npolygons are calculated internally this was easy to\nimplement with the imgaug library.\nFirst, we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith augmentation that ﬂip/mirror each image in\nthe training set horizontally right/left. Then we\nagain save the model’s parameters and evaluate the\nperformance of the new weights.\nThen we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Then we resume training, on the weights with\nthe lowest training loss from the previous training,\nwith afﬁne which transforms each image in the\ntraining set by rotation (-45, 45) and shearing (-16,\n16). Finally, we save the model’s parameters and\nevaluate the performance of the new weights with\nrunning inference on the weights that yielded the\nlowest training loss.\nInference optimization\nIn the process of optimizing the trained network,\nwe constantly adjusted and modiﬁed the training set\nand other parameters. Since the output of the infer-\nence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ence is a list of possible detection boxes and masks,\nwhose probabilities are larger than our set threshold\nof detection minimum conﬁdence, we need to adjust\naccordingly with regards to the properties of the test\nset images and the overall accuracy. We found a\ndetection minimum conﬁdence of 0.9 to yield as\ngood trade-off of the precision and recall values for\nour windows.\n5. RESULTS\nWe perform our test result experiments on the\neTrims dataset and not on the ECP dataset. The\nreason being that the ECP dataset doesn’t contain\nstreet-view images and the images are all manually\nrectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='rectiﬁed and viewed in a frontal direction, i.e. mak-\ning the window detection task relatively easy.\nHowever, to properly use the eTRIMS dataset\nwith our model, we had to reﬁne their annotation\nto overcome inaccuracies and mistakes, and also\naccommodate our deﬁned window boundary which\nfollows the window frame. We, therefore, annotated\n10 randomly chosen images for our test set to\nevaluate our model’s performances. Therefore, any\ncomparisons should note these differences.\nBoth the training and inference was done on\nGoogle Colab with the free Tesla K80 GPU of about\n12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='12GB. We have overall trained and evaluated more\nthan 60 models with different conﬁgurations. Out of\nthose we present the best three models with regards\nto the highest mAP on our test set (Table 1) and\nfor visualization we present the inference done on\nthe model achieving the highest mAP below (Figure\n12).\n11\nFigure 12.\nWindow detection results from a model trained on coco-weights, then resumed with ﬂiplr.\nImage\nGround truth\nPrediction\nOverlap\ngreen colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='green colors\nrandom colors\ngreen/red colors\ncaptions: class\ncaptions: class/score\ncaptions: score/IoU\n12\nREFERENCES\nREFERENCES\nTable 1.\nA comparison of pixel accuracies on the eTRIMS\ndataset. Accuracies are shown in percentage.\nClass\n[1]\n[2]\n[3]\n[4]\n[5]\nOur*\nOur**\nOur***\nWindow[%] 75\n73\n71\n86', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Our***\nWindow[%] 75\n73\n71\n86\n90.91 91.93\n94.55\n94.76\nNote: Our result is only from 10 randomly chosen images from the eTRIMS\ndataset with reﬁned annotation.\nmodel trained on coco-weights then resumed with ﬂiplr and afﬁne using\nk-fold cross-validation (the resumed weights were chosen by the folds\nhighest mAP)\n** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='** model trained on coco-weights then resumed with ﬂiplr using k-fold\ncross-validation (the resumed weights were chosen by the folds highest mAP)\n*** model trained on coco-weights then resumed with ﬂiplr (the resumed\nweights were chosen by the lowest training loss)\n6. CONCLUSIONS\nIn this paper, we propose to use the state-of-\nthe-art Mask R-CNN framework He et al. (2017)\nas an end-to-end network capable of instance seg-\nmentation without further machinery. Previous fa-\ncade parsing detection work often only produced\nsemantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='semantic segmentation of a categorical class (e.g.\ndoor, balcony, window etc. ), while Mask R-CNN is\ncapable of detecting and producing a bounding box\nand segmentation mask for each window instance.\nExperimental results show that our suggested\napproach can with only a relatively small but precise\ndataset train the network—only with transfer learn-\ning and augmentation—to produce instance segmen-\ntation of windows and a comparable result with\nprior state-of-the-art window detection approaches,\neven without pre- or post-optimization techniques.\nAs far as we know, we are the ﬁrst to have anno-\ntated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tated a street-view windows dataset by consistently\ntrying to follow the window’s frame as the window\nboundary and not to include other window parts. We\nbelieve that our consequent and precise annotation\nof windows helped our model training. We also\nhope it can beneﬁt the research community as a free\nresource for more precise window detection.\n7. FUTURE WORK\nIn this paper, we have got a sense of how to tune\nthe hyper-parameters effectively with our training\ndata. However, further work is recommended to\ninvestigate this with platforms, such as Wandb or\nOnepanel Jobs for easier coordination, synchroniza-\ntion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tion and tracking of conﬁgurations.\nAnother possibility is to extend the dataset with\nmore high-quality annotations of window types\nwere the model has low accuracy.\nFurther work could also beneﬁt from training\nmore facade classes, such as building, balcony,\ndoors, chimney, to reduce false positives, false neg-\natives. The same goes for the window class, i.e.\nsplitting it into distinct window type classes, such\nas double-hung, transom etc. A model capable of\na more complete facade parsing we believe will\nalso make application easier as the model has a\nﬁner granularity of the facades properties. A caution\nis that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='is that this would require well-reasoned and well-\ndeﬁned class boundaries for each corner case of\nevery facade structure.\nTo further reﬁne the results and perhaps mitigate\nsome of the typical limitations we experienced,\nsuch as dealing with occlusion, overconﬁdent false\npositives predictions, occasional mask overlaps, fu-\nture work should investigate both in pre- and post-\noptimization methods of previous approaches or\ntesting their novel approaches. Also, code refactor-\ning and optimization, e.g. rewriting some Python\ncode in TensorFlow to bring all the beneﬁts of the\nTensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='TensorFlow, Keras ecosystem to the model are de-\nsired. There is also always room for improvements.\nACKNOWLEDGMENTS\nThis paper was the ﬁnal written report in the\ncourse DIT891 Project in Data Science. The course\nis offered within the Applied Data Science Master’s\nProgramme by the University of Gothenburg. We\nwould like to thank Shirin Tavara, teacher and the\ncourse responsible and Richard Johansson, teacher.\nA special thanks goes to our supervisors Alexan-\nder Hollberg, Sanjay Somanath and Yinan Yu for\ntheir congeniality and support.\n13\nREFERENCES\nREFERENCES\nREFERENCES', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='13\nREFERENCES\nREFERENCES\nREFERENCES\nAbdulla, Waleed (2017). “Mask R-CNN for object\ndetection and instance segmentation on Keras and\nTensorFlow https://github. com/matterport”. In:\nMask _ RCNN.\nAbdulla, Waleed (2018). Instance Segmentation\nwith Mask R-CNN and TensorFlow. https : / /\nengineering . matterport . com / splash - of - color -\ninstance- segmentation- with- mask- r- cnn- and-\ntensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tensorﬂow-7c761e238b46.\nBacharidis, Konstantinos, Froso Sarri, and Lemonia\nRagia (2020). “3D building façade reconstruction\nusing deep learning”. In: ISPRS International\nJournal of Geo-Information 9(5), p. 322.\nBadrinarayanan,\nVijay,\nAlex\nKendall,\nRoberto\nCipolla, et al. (2015). “1SegNet: A Deep Convo-\nlutional Encoder-Decoder Architecture for Image\nSegmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Segmentation”. In.\nBala, Shashi and Ajaya Kumar (2016). “A brief\nreview of image segmentation techniques”. In:\nInternational\nJournal\nof\nAdvanced\nResearch\nin Electronics and Communication Engineering\n(IJARECE) Volume 5, pp. 1448–1454.\nChen, Liang-Chieh et al. (2017). “Deeplab: Seman-\ntic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected\ncrfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='crfs”. In: IEEE transactions on pattern analysis\nand machine intelligence 40(4), pp. 834–848.\nDutta, Abhishek and Andrew Zisserman (2019).\n“The VIA annotation software for images, audio\nand video”. In: Proceedings of the 27th ACM in-\nternational conference on multimedia, pp. 2276–\n2279.\nFathalla, Radwa and George Vogiatzis (2017). “A\ndeep learning pipeline for semantic facade seg-\nmentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='mentation”. In.\nGirshick, Ross (2015). “Fast r-cnn”. In: Proceedings\nof the IEEE international conference on computer\nvision, pp. 1440–1448.\nHan, Feng and Song-Chun Zhu (2008). “Bottom-\nup/top-down image parsing with attribute gram-\nmar”. In: IEEE transactions on pattern analysis\nand machine intelligence 31(1), pp. 59–73.\nHe, Kaiming et al. (2017). “Mask r-cnn”. In: Pro-\nceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='ceedings of the IEEE international conference on\ncomputer vision, pp. 2961–2969.\nHu, Han et al. (2020). “Fast and Regularized Re-\nconstruction of Building Fa\\c {c} ades from\nStreet-View Images using Binary Integer Pro-\ngramming”. In: arXiv preprint arXiv:2002.08549.\nKarmarkar, T (2018). “Regional Proposal network\n(RPN)—Backbone of Faster R-CNN”. In: Aug\n18, p. 6.\nKoch, David et al. (2018). “Visual estimation of\nbuilding condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='building condition with patch-level ConvNets”.\nIn: Proceedings of the 2018 ACM Workshop on\nMultimedia for Real Estate Tech, pp. 12–17.\nKorc,\nFilip\nand\nWolfgang\nFörstner\n(2009).\n“eTRIMS\nImage\nDatabase\nfor\ninterpreting\nimages of man-made scenes”. In: Dept. of\nPhotogrammetry, University of Bonn, Tech. Rep.\nTR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='TR-IGG-P-2009-01.\nLiu, Hantang et al. (2017). “Deepfacade: A deep\nlearning approach to facade parsing”. In.\nLong, Jonathan, Evan Shelhamer, and Trevor Dar-\nrell (2015). “Fully convolutional networks for\nsemantic segmentation”. In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pp. 3431–3440.\nMa, Wenguang and Wei Ma (2020). “Deep window\ndetection in street scenes”. In: KSII Transactions\non Internet and Information Systems (TIIS) 14(2),\npp. 855–870.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='on Internet and Information Systems (TIIS) 14(2),\npp. 855–870.\nMathias, Markus et al. (2011). “Automatic architec-\ntural style recognition”. In: ISPRS-International\nArchives of the Photogrammetry, Remote Sensing\nand Spatial Information Sciences 3816, pp. 171–\n176.\nMüller, Pascal et al. (2007). “Image-based procedu-\nral modeling of facades”. In: ACM Trans. Graph.\n26(3), p. 85.\nNogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Nogueira, Keiller, Otávio AB Penatti, and Jefersson\nA Dos Santos (2017). “Towards better exploiting\nconvolutional neural networks for remote sensing\nscene classiﬁcation”. In: Pattern Recognition 61,\npp. 539–556.\nParthasarathy,\nDhruv\n(2017).\n“A\nbrief\nhistory\nof CNNs in image segmentation: From R-\n14\nCNN to Mask R-CNN”. In: Available online:\nblog.\nathelas.\ncom/a-brief-history-of-cnns-in-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='blog.\nathelas.\ncom/a-brief-history-of-cnns-in-\nimage-segmentation-from-r-cnn-to-mask-rcnn-\n34ea83205de4 (accessed on 24 February 2019).\nPenatti, Otávio AB, Keiller Nogueira, and Jefersson\nA Dos Santos (2015). “Do deep features general-\nize from everyday objects to remote sensing and\naerial scenes domains?” In: Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition workshops, pp. 44–51.\nRahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Rahmani and K. (2020). “Facade Interpretation from\nImages. Fakultät für Informatik at Universität der\nBundeswehr München.” In.\nRecky, Michal and Franz Leberl (2010). “Windows\ndetection using k-means in cie-lab color space”.\nIn: 2010 20th International Conference on Pat-\ntern Recognition. IEEE, pp. 356–359.\nRen, Shaoqing et al. (2016). “Faster R-CNN: to-\nwards real-time object detection with region pro-\nposal networks”. In: IEEE transactions on pat-\ntern analysis and machine intelligence 39(6),\npp. 1137–1149.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='tern analysis and machine intelligence 39(6),\npp. 1137–1149.\nSchmitz, Matthias and Helmut Mayer (2016). “A\nconvolutional network for semantic facade seg-\nmentation and interpretation”. In: The Interna-\ntional Archives of Photogrammetry, Remote Sens-\ning and Spatial Information Sciences 41, p. 709.\nSultana, F, A Suﬁan, and P Dutta (2020). “Evolution\nof Image Segmentation using Deep Convolutional\nNeural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Neural Network: A Survey. arXiv 2020”. In:\narXiv preprint arXiv:2001.04074.\nTeboul, Olivier et al. (2010). “Segmentation of\nbuilding facades using procedural shape priors”.\nIn: 2010 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition. IEEE,\npp. 3105–3112.\nTu, Zhuowen et al. (2005). “Image parsing: Unify-\ning segmentation, detection, and recognition”. In:\nInternational Journal of computer vision 63(2),\npp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='pp. 113–140.\nWendel, Andreas, Michael Donoser, and Horst\nBischof (2010). “Unsupervised facade segmenta-\ntion using repetitive patterns”. In: Joint Pattern\nRecognition Symposium. Springer, pp. 51–60.\nZhao, Peng et al. (2010). “Rectilinear parsing of ar-\nchitecture in urban environment”. In: 2010 IEEE\nComputer Society Conference on Computer Vi-\nsion and Pattern Recognition. IEEE, pp. 342–349.\nZhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'}), Document(page_content='Zhu, Song-Chun, Rong Zhang, and Zhuowen Tu\n(2000). “Integrating bottom-up/top-down for ob-\nject recognition by data driven Markov chain\nMonte Carlo”. In: Proceedings IEEE Conference\non Computer Vision and Pattern Recognition.\nCVPR 2000 (Cat. No. PR00662). Vol. 1. IEEE,\npp. 738–745.\n15', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpksb47qki/000dac21-5129-46fb-b974-9e84bb9c7846.pdf'})]
cuda:2
请讲述研究现状部分
 tmpksb47qki
[]
cuda:2
[0.9762645, 0.99457926, 1.0130118, 1.0131525, 1.014944, 1.0223485]
请讲讲这篇论文解决的问题
 tmpksb47qki
[]
cuda:2
[1.0252235, 1.0910449, 1.1115847, 1.1259103, 1.1438899, 1.1468027]
请讲讲这篇论文解决的问题
 tmpksb47qki
[]
cuda:2
[1.0252235, 1.0910449, 1.1115847, 1.1259103, 1.1438899, 1.1468027]
请讲讲这篇论文实验得到的结果
 tmpksb47qki
[]
cuda:2
[1.0737593, 1.0798584, 1.1155242, 1.1269408, 1.1441091, 1.144616]
请讲讲这篇论文得出的结论
 tmpksb47qki
[]
cuda:2
[1.0245891, 1.0843207, 1.0917091, 1.1035992, 1.113192, 1.1274189]
[UploadFile(filename='05473f62-adf9-4adf-9196-f589d9c96780.txt', size=5283, headers=Headers({'content-disposition': 'form-data; name="files"; filename="05473f62-adf9-4adf-9196-f589d9c96780.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpx3kmjdn2, tmpx3kmjdn2
File: 05473f62-adf9-4adf-9196-f589d9c96780.txt, msg: 成功上传文件 05473f62-adf9-4adf-9196-f589d9c96780.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5d34d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpx3kmjdn2/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
cuda:2
[UploadFile(filename='高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf', size=1920751, headers=Headers({'content-disposition': 'form-data; name="files"; filename="é«\x98æ\x80§è\x83½è®¡ç®\x97 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpt3ajqo7n, tmpt3ajqo7n
File: 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, msg: 成功上传文件 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, docs: [Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa5797c90> 111
cuda:2
[Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt3ajqo7n/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
cuda:2
你好 tmpt3ajqo7n
[]
cuda:2
[1.3490074, 1.3502891, 1.362247, 1.3773541, 1.4059615, 1.4112672]
你好 tmpt3ajqo7n
[]
cuda:2
[1.3490074, 1.3502891, 1.362247, 1.3773541, 1.4059615, 1.4112672]
[UploadFile(filename='05473f62-adf9-4adf-9196-f589d9c96780.txt', size=5283, headers=Headers({'content-disposition': 'form-data; name="files"; filename="05473f62-adf9-4adf-9196-f589d9c96780.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmphwhfiqln, tmphwhfiqln
File: 05473f62-adf9-4adf-9196-f589d9c96780.txt, msg: 成功上传文件 05473f62-adf9-4adf-9196-f589d9c96780.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa5640790> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmphwhfiqln/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
cuda:2
[UploadFile(filename='05473f62-adf9-4adf-9196-f589d9c96780.txt', size=5283, headers=Headers({'content-disposition': 'form-data; name="files"; filename="05473f62-adf9-4adf-9196-f589d9c96780.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpthpminqi, tmpthpminqi
File: 05473f62-adf9-4adf-9196-f589d9c96780.txt, msg: 成功上传文件 05473f62-adf9-4adf-9196-f589d9c96780.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa56400d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpthpminqi/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp6squ5ej9, tmp6squ5ej9
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa550fad0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp6squ5ej9/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp5xqhbm7w, tmp5xqhbm7w
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc25fad0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp5xqhbm7w/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpkiueeaai, tmpkiueeaai
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5b4e50> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpkiueeaai/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmptflhf6gi, tmptflhf6gi
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5c8990> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptflhf6gi/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='南大笔试复习.pdf', size=264799, headers=Headers({'content-disposition': 'form-data; name="files"; filename="å\x8d\x97å¤§ç¬\x94è¯\x95å¤\x8dä¹\xa0.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpq30conxx, tmpq30conxx
File: 南大笔试复习.pdf, msg: 成功上传文件 南大笔试复习.pdf, docs: [Document(page_content='计算机网络', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='1. A类地址1-126，B类地址128-191，C类地址192-223，127是自回环 2. TCP当前序号为x，发送y长度数据，对方回复的Ack=x+y 3. 网段地址（或称为网络地址）是通过将 IP 地址与子网掩码进行逻辑 AND 操作得到的 4. 数据报校验的协议位于哪个层次：传输层，网络层，链路层（循环冗余校验与帧校验序列） 5. 拥塞控制 6. 高优先级包优先发送：帧间隔发送时间 7. 以太网，传输速率100Mbps，信号传播速度200000km/s，如果最小数据帧的长度增大90bits，则 1. 电缆的最大长度能够增加多少？2. 信道利用率是上升还是下降？', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='2. 信道利用率是上升还是下降？ 1. （最小帧长/数据传输速率）* 信号传播速度=2*最大传输距离；即此处电缆的最大长度能够增 加90km； 2. 先根据传输速率算出传输时间 = 2 * 单向时延（即在一个数据帧传输的时间内，上一个数据帧 应该至少走过了两倍的全程），算出单向时延，通过信号传播速度算出最长长度。 8. 以太网速度快的原因：串行通信简单，非常适合长距离传输；串行通信由于控制简单、抗干扰能力 强、成本低等特点，得到了广泛的应用。 数据结构', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='1. m叉树的节点计算：按边算（会省略根节点），按点算（会省略叶子节点）\n2. 图的邻接矩阵：i\n>j，adj(i,j)=1\n3. 拓扑排序（有向无环图）：\n1. 将入度为0的点入栈\n2. 如果栈非空，出栈，将该点的出度邻接点入度\n1，回到步骤1\n4. 各种排序：\n1. 冒泡排序（Bubble Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：不稳定（可以通过适当的调整实现稳定性）\n特点：不受输入数据的影响，但是性能较差\n3. 插入排序（Insertion Sort）\n时间复杂度：平均 O(n^2)，最好情况 O(n^2)（已经排序的输入数据）\n稳定性：稳定\n特点：在接近排序状态的数据上表现良好，适合小规模数据排序\n4. 希尔排序（Shell Sort）\n时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定\n特点：改进版的插入排序，适用于中等大小的数据集\n5. 归并排序（Merge Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：稳定\n特点：非常适合大数据量排序，需要额外的内存开销\n6. 快速排序（Quick Sort）\n时间复杂度：平均 O(nlog n)O(nlogn)，最坏 O(n2)O(n2)\n稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：不稳定\n构造大根堆，将根节点（最大）和末尾节点互换，调整大根堆，迭代\n特点：利用堆数据结构，适合于大数据量排序，无需额外空间\n8. 计数排序（Counting Sort）\n时间复杂度：O(n+k)O(n+k)（其中 kk 是数据的范围）\n稳定性：稳定\n特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）\n时间复杂度：平均 O(n+k)O(n+k)，最坏 O(n2)O(n2)\n稳定性：稳定\n特点：适用于数据分布均匀的情况，需要额外空间\n10. 基数排序（Radix Sort）\n时间复杂度：O(nk)O(nk)（其中 kk 是键长）\n稳定性：稳定\n特点：非比较排序，适用于位数较少的整数或字符串排序，需要较大的空间开销\n5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序\n6. 希尔排序流程：从length/2组开始，对每组分别排序，变为局部有序。后续每次划分组都除以2，\n直至组数为1，排序完毕。\n计组01\n1. 流水线结构中，branch指令发生跳转，需要插入多少个空操作（bubble）或者阻塞多少个周期？\n在F\nD级处理阻塞的话，最多需要空两个操作。\n2. 中断与异常：\n1. 中断：软件中断，syscall调用内核函数；硬件中断，IO交互，时钟中断\n2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止\n3. 中断又分为内中断和外中断：\n1. 内中断（也称异常、例外、陷入）信号来源是CPU内部，与当前执行的指令有关；\n2. 外中断（狭义的中断）信号的来源是CPU外部，与当前执行的指令无关。\n操作系统', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='SIGBUS / SIGEMT/ SIGIOT 指示一个实现定义的硬件故障,非法地址，包括内存地址对齐出错。 SIGFPE( floating-point exception)，此信号表示一个算术运算异常，例如除以0，浮点溢出等。 SIGILL(illegal instruction ), 此信号指示进程已执行一条非法硬件指令。 SIGHUP：hang up 挂断 SIGTRAP：由断点指令或其他trap指令产生。 SIGPIPE：管道破裂。 SIGSEGV：试图访问未分配给自己的内存, 或试图往没有写权限的内存地址写数据.。 SIGPROF 当setitimer( 2 )函数设置的梗概统计间隔时间已经超过时产生此信号 SIGQUIT 当用户在终端上按退出键（一般采用Ctrl - \\）时，产生此信号，并送至前台进程组中的所有进 程（见图9 - 8）。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='此信号不仅终止前台进程组（如 SIGINT所做的那样），同时产生一个core文件。 SIGINFO 这是一种4 . 3 + BSD信号，当用户按状态键（一般采用Ctrl - T）时，终端驱动程序产生此信号 并送至前台进程组中的每一个进程。此信号通常造成在终端上显示前台进程组中各进程的状态信息。 SIGINT 当用户按中断键（一般采用DELETE或Ctrl - C）时，终端驱动程序产生此信号并送至前台进程组 中的每一个进程。当一个进程在运行时失控，特别是它正在屏幕上产生大量不需要的输出时，常用此信 号终止它。 1. 进程：是进程实体运行的过程，它是系统进行资源分配和调度的一个独立单位。 线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。 2. 磁盘调度算法 FCFS先来先服务 First Come First Service SSTF最短寻道时间      Shortest Seek Time First选择寻找时间最短的访问者调度【会产生"饥饿现 象"】 电梯调度算法（SCAN） 磁道编号是从外到里的，即由内到外磁道号越来越小。 从当前所在磁道号，从外向里运动，再从里 向外运动，或反之。这样就避免了饥饿现象，由于这种移臂调度规律颇似电梯的运动，因而称为电梯算 法。 循环扫描算法（CSCAN） 为了减少这种延迟，规定磁头单向读/写运动 写运动 (如只能由内向外)，完成读写后 ，立即返到最小/大 磁道号的位置 (构成循环 )，再进行扫描。即 CSCAN算法。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='即 CSCAN算法。 3. 页面置换算法 1. FIFO[first in first out]先进先出页面置换算法 2. 最优算法 3. LRU 4. 第二机会/时钟算法 4. 死锁必要条件：互斥；占有并等待；非抢占；循环等待 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa5794790> 111
cuda:2
[Document(page_content='计算机网络', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='1. A类地址1-126，B类地址128-191，C类地址192-223，127是自回环 2. TCP当前序号为x，发送y长度数据，对方回复的Ack=x+y 3. 网段地址（或称为网络地址）是通过将 IP 地址与子网掩码进行逻辑 AND 操作得到的 4. 数据报校验的协议位于哪个层次：传输层，网络层，链路层（循环冗余校验与帧校验序列） 5. 拥塞控制 6. 高优先级包优先发送：帧间隔发送时间 7. 以太网，传输速率100Mbps，信号传播速度200000km/s，如果最小数据帧的长度增大90bits，则 1. 电缆的最大长度能够增加多少？2. 信道利用率是上升还是下降？', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='2. 信道利用率是上升还是下降？ 1. （最小帧长/数据传输速率）* 信号传播速度=2*最大传输距离；即此处电缆的最大长度能够增 加90km； 2. 先根据传输速率算出传输时间 = 2 * 单向时延（即在一个数据帧传输的时间内，上一个数据帧 应该至少走过了两倍的全程），算出单向时延，通过信号传播速度算出最长长度。 8. 以太网速度快的原因：串行通信简单，非常适合长距离传输；串行通信由于控制简单、抗干扰能力 强、成本低等特点，得到了广泛的应用。 数据结构', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='1. m叉树的节点计算：按边算（会省略根节点），按点算（会省略叶子节点）\n2. 图的邻接矩阵：i\n>j，adj(i,j)=1\n3. 拓扑排序（有向无环图）：\n1. 将入度为0的点入栈\n2. 如果栈非空，出栈，将该点的出度邻接点入度\n1，回到步骤1\n4. 各种排序：\n1. 冒泡排序（Bubble Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='稳定性：稳定\n特点：实现简单，性能较差，适用于小数据量排序\n2. 选择排序（Selection Sort）\n时间复杂度：平均和最坏情况均为 O(n^2)\n稳定性：不稳定（可以通过适当的调整实现稳定性）\n特点：不受输入数据的影响，但是性能较差\n3. 插入排序（Insertion Sort）\n时间复杂度：平均 O(n^2)，最好情况 O(n^2)（已经排序的输入数据）\n稳定性：稳定\n特点：在接近排序状态的数据上表现良好，适合小规模数据排序\n4. 希尔排序（Shell Sort）\n时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='时间复杂度：取决于增量序列，平均可能为 O(nlog n) 或更差\n稳定性：不稳定\n特点：改进版的插入排序，适用于中等大小的数据集\n5. 归并排序（Merge Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：稳定\n特点：非常适合大数据量排序，需要额外的内存开销\n6. 快速排序（Quick Sort）\n时间复杂度：平均 O(nlog n)O(nlogn)，最坏 O(n2)O(n2)\n稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='稳定性：不稳定\n特点：广泛使用的排序算法，适合大规模数据排序，但在最坏情况下性能不佳\n7. 堆排序（Heap Sort）\n时间复杂度：平均和最坏情况均为 O(nlog n)O(nlogn)\n稳定性：不稳定\n构造大根堆，将根节点（最大）和末尾节点互换，调整大根堆，迭代\n特点：利用堆数据结构，适合于大数据量排序，无需额外空间\n8. 计数排序（Counting Sort）\n时间复杂度：O(n+k)O(n+k)（其中 kk 是数据的范围）\n稳定性：稳定\n特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='特点：非比较排序，适合小范围内的整数排序，非常快，但需要额外空间\n9. 桶排序（Bucket Sort）\n时间复杂度：平均 O(n+k)O(n+k)，最坏 O(n2)O(n2)\n稳定性：稳定\n特点：适用于数据分布均匀的情况，需要额外空间\n10. 基数排序（Radix Sort）\n时间复杂度：O(nk)O(nk)（其中 kk 是键长）\n稳定性：稳定\n特点：非比较排序，适用于位数较少的整数或字符串排序，需要较大的空间开销\n5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='5. 不稳定排序：选择排序，希尔排序，快速排序，堆排序\n6. 希尔排序流程：从length/2组开始，对每组分别排序，变为局部有序。后续每次划分组都除以2，\n直至组数为1，排序完毕。\n计组01\n1. 流水线结构中，branch指令发生跳转，需要插入多少个空操作（bubble）或者阻塞多少个周期？\n在F\nD级处理阻塞的话，最多需要空两个操作。\n2. 中断与异常：\n1. 中断：软件中断，syscall调用内核函数；硬件中断，IO交互，时钟中断\n2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='2. 异常：故障（缺页异常，计算异常等），可执行相应的恢复程序纠正；trap，调试用；终止\n3. 中断又分为内中断和外中断：\n1. 内中断（也称异常、例外、陷入）信号来源是CPU内部，与当前执行的指令有关；\n2. 外中断（狭义的中断）信号的来源是CPU外部，与当前执行的指令无关。\n操作系统', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='SIGBUS / SIGEMT/ SIGIOT 指示一个实现定义的硬件故障,非法地址，包括内存地址对齐出错。 SIGFPE( floating-point exception)，此信号表示一个算术运算异常，例如除以0，浮点溢出等。 SIGILL(illegal instruction ), 此信号指示进程已执行一条非法硬件指令。 SIGHUP：hang up 挂断 SIGTRAP：由断点指令或其他trap指令产生。 SIGPIPE：管道破裂。 SIGSEGV：试图访问未分配给自己的内存, 或试图往没有写权限的内存地址写数据.。 SIGPROF 当setitimer( 2 )函数设置的梗概统计间隔时间已经超过时产生此信号 SIGQUIT 当用户在终端上按退出键（一般采用Ctrl - \\）时，产生此信号，并送至前台进程组中的所有进 程（见图9 - 8）。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='此信号不仅终止前台进程组（如 SIGINT所做的那样），同时产生一个core文件。 SIGINFO 这是一种4 . 3 + BSD信号，当用户按状态键（一般采用Ctrl - T）时，终端驱动程序产生此信号 并送至前台进程组中的每一个进程。此信号通常造成在终端上显示前台进程组中各进程的状态信息。 SIGINT 当用户按中断键（一般采用DELETE或Ctrl - C）时，终端驱动程序产生此信号并送至前台进程组 中的每一个进程。当一个进程在运行时失控，特别是它正在屏幕上产生大量不需要的输出时，常用此信 号终止它。 1. 进程：是进程实体运行的过程，它是系统进行资源分配和调度的一个独立单位。 线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='线程：是操作系统能够进行运算调度的最小单位，是被包含在进程之中的，是进程中的实际运作单 位。 2. 磁盘调度算法 FCFS先来先服务 First Come First Service SSTF最短寻道时间      Shortest Seek Time First选择寻找时间最短的访问者调度【会产生"饥饿现 象"】 电梯调度算法（SCAN） 磁道编号是从外到里的，即由内到外磁道号越来越小。 从当前所在磁道号，从外向里运动，再从里 向外运动，或反之。这样就避免了饥饿现象，由于这种移臂调度规律颇似电梯的运动，因而称为电梯算 法。 循环扫描算法（CSCAN） 为了减少这种延迟，规定磁头单向读/写运动 写运动 (如只能由内向外)，完成读写后 ，立即返到最小/大 磁道号的位置 (构成循环 )，再进行扫描。即 CSCAN算法。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'}), Document(page_content='即 CSCAN算法。 3. 页面置换算法 1. FIFO[first in first out]先进先出页面置换算法 2. 最优算法 3. LRU 4. 第二机会/时钟算法 4. 死锁必要条件：互斥；占有并等待；非抢占；循环等待 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpq30conxx/南大笔试复习.pdf'})]
cuda:2
[UploadFile(filename='杨博文为何是北航校草.pdf', size=151520, headers=Headers({'content-disposition': 'form-data; name="files"; filename="æ\x9d¨å\x8d\x9aæ\x96\x87ä¸ºä½\x95æ\x98¯å\x8c\x97è\x88ªæ\xa0¡è\x8d\x89.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp048pz4ga, tmp048pz4ga
File: 杨博文为何是北航校草.pdf, msg: 成功上传文件 杨博文为何是北航校草.pdf, docs: [Document(page_content='杨博文为什么是北航校草\n引言\n在北京航空航天大学计算机学院，杨博文以其出众的外貌和优异的学术成绩广受关注。在一个竞争激烈\n的学术环境中，杨博文不仅在学术上取得了显著的成就，而且在课外活动和社会交往中也表现出色，自\n然而然地成为了北航的校草。本文将探讨杨博文获此殊荣的几个关键因素。\n主体\n首先，杨博文的外表无疑是他成为校草的一个重要因素。他的容貌俊朗，总是给人留下良好的第一印\n象。然而，仅凭外表的吸引力是不足以让他在北航这样以工程和技术见长的学校中脱颖而出的。他的个\n人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp048pz4ga/杨博文为何是北航校草.pdf'}), Document(page_content='人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理\n论和技术概念的深刻理解。他的能力不仅限于理论学习，更在实践中得到了验证。杨博文参与并带领了\n多个工程项目，包括开发软件应用和参与硬件设计，这些项目不仅成功实施，还在学术和技术会议上获\n得了认可。\n例如，他曾领导一个团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n此外，他在操作系统的课程设计中，独立完成了一个微型操作系统的设计，该系统能有效管理硬件资源\n并支持基本的多任务处理，这一成就显示了他在软件和硬件接口方面的高水平能力。\n除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp048pz4ga/杨博文为何是北航校草.pdf'}), Document(page_content='除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与\n各种校园活动，展现了他的领导能力和社交技巧。此外，他还参与志愿服务工作，帮助社区中的弱势群\n体，体现了他的社会责任感。\n结论\n综上所述，杨博文之所以被誉为北航校草，不仅仅是因为他俊朗的外表，更重要的是他在学术、领导力\n和社会责任方面的全面表现。他在计算机科学的各个方面的深厚造诣，尤其是在计算机组成和操作系统\n等专业课程中的杰出表现，以及他在实际工程项目中的实际应用能力，使他在北航获得了师生的广泛认\n可和尊敬。杨博文的故事激励着每一个北航学子，不仅要追求学术上的卓越，更要在个人成长和社会贡\n献上不断努力。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp048pz4ga/杨博文为何是北航校草.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc22ab50> 111
cuda:2
[Document(page_content='杨博文为什么是北航校草\n引言\n在北京航空航天大学计算机学院，杨博文以其出众的外貌和优异的学术成绩广受关注。在一个竞争激烈\n的学术环境中，杨博文不仅在学术上取得了显著的成就，而且在课外活动和社会交往中也表现出色，自\n然而然地成为了北航的校草。本文将探讨杨博文获此殊荣的几个关键因素。\n主体\n首先，杨博文的外表无疑是他成为校草的一个重要因素。他的容貌俊朗，总是给人留下良好的第一印\n象。然而，仅凭外表的吸引力是不足以让他在北航这样以工程和技术见长的学校中脱颖而出的。他的个\n人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp048pz4ga/杨博文为何是北航校草.pdf'}), Document(page_content='人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理\n论和技术概念的深刻理解。他的能力不仅限于理论学习，更在实践中得到了验证。杨博文参与并带领了\n多个工程项目，包括开发软件应用和参与硬件设计，这些项目不仅成功实施，还在学术和技术会议上获\n得了认可。\n例如，他曾领导一个团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n此外，他在操作系统的课程设计中，独立完成了一个微型操作系统的设计，该系统能有效管理硬件资源\n并支持基本的多任务处理，这一成就显示了他在软件和硬件接口方面的高水平能力。\n除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp048pz4ga/杨博文为何是北航校草.pdf'}), Document(page_content='除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与\n各种校园活动，展现了他的领导能力和社交技巧。此外，他还参与志愿服务工作，帮助社区中的弱势群\n体，体现了他的社会责任感。\n结论\n综上所述，杨博文之所以被誉为北航校草，不仅仅是因为他俊朗的外表，更重要的是他在学术、领导力\n和社会责任方面的全面表现。他在计算机科学的各个方面的深厚造诣，尤其是在计算机组成和操作系统\n等专业课程中的杰出表现，以及他在实际工程项目中的实际应用能力，使他在北航获得了师生的广泛认\n可和尊敬。杨博文的故事激励着每一个北航学子，不仅要追求学术上的卓越，更要在个人成长和社会贡\n献上不断努力。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp048pz4ga/杨博文为何是北航校草.pdf'})]
cuda:2
如何评价杨博文 tmp048pz4ga
[History(role='user', content='杨博文最爱的人是？'), History(role='assistant', content='根据已知信息，没有关于杨博文最爱的人的相关信息。')]
cuda:2
[0.7798513]
如何评价杨博文 tmp048pz4ga
[History(role='user', content='杨博文最爱的人是？'), History(role='assistant', content='根据已知信息，没有关于杨博文最爱的人的相关信息。')]
cuda:2
[0.7798513]
[UploadFile(filename='杨博文为何是北航校草.pdf', size=151520, headers=Headers({'content-disposition': 'form-data; name="files"; filename="æ\x9d¨å\x8d\x9aæ\x96\x87ä¸ºä½\x95æ\x98¯å\x8c\x97è\x88ªæ\xa0¡è\x8d\x89.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpyqr2z7fv, tmpyqr2z7fv
File: 杨博文为何是北航校草.pdf, msg: 成功上传文件 杨博文为何是北航校草.pdf, docs: [Document(page_content='杨博文为什么是北航校草\n引言\n在北京航空航天大学计算机学院，杨博文以其出众的外貌和优异的学术成绩广受关注。在一个竞争激烈\n的学术环境中，杨博文不仅在学术上取得了显著的成就，而且在课外活动和社会交往中也表现出色，自\n然而然地成为了北航的校草。本文将探讨杨博文获此殊荣的几个关键因素。\n主体\n首先，杨博文的外表无疑是他成为校草的一个重要因素。他的容貌俊朗，总是给人留下良好的第一印\n象。然而，仅凭外表的吸引力是不足以让他在北航这样以工程和技术见长的学校中脱颖而出的。他的个\n人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyqr2z7fv/杨博文为何是北航校草.pdf'}), Document(page_content='人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理\n论和技术概念的深刻理解。他的能力不仅限于理论学习，更在实践中得到了验证。杨博文参与并带领了\n多个工程项目，包括开发软件应用和参与硬件设计，这些项目不仅成功实施，还在学术和技术会议上获\n得了认可。\n例如，他曾领导一个团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n此外，他在操作系统的课程设计中，独立完成了一个微型操作系统的设计，该系统能有效管理硬件资源\n并支持基本的多任务处理，这一成就显示了他在软件和硬件接口方面的高水平能力。\n除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyqr2z7fv/杨博文为何是北航校草.pdf'}), Document(page_content='除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与\n各种校园活动，展现了他的领导能力和社交技巧。此外，他还参与志愿服务工作，帮助社区中的弱势群\n体，体现了他的社会责任感。\n结论\n综上所述，杨博文之所以被誉为北航校草，不仅仅是因为他俊朗的外表，更重要的是他在学术、领导力\n和社会责任方面的全面表现。他在计算机科学的各个方面的深厚造诣，尤其是在计算机组成和操作系统\n等专业课程中的杰出表现，以及他在实际工程项目中的实际应用能力，使他在北航获得了师生的广泛认\n可和尊敬。杨博文的故事激励着每一个北航学子，不仅要追求学术上的卓越，更要在个人成长和社会贡\n献上不断努力。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyqr2z7fv/杨博文为何是北航校草.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa578e2d0> 111
cuda:2
[Document(page_content='杨博文为什么是北航校草\n引言\n在北京航空航天大学计算机学院，杨博文以其出众的外貌和优异的学术成绩广受关注。在一个竞争激烈\n的学术环境中，杨博文不仅在学术上取得了显著的成就，而且在课外活动和社会交往中也表现出色，自\n然而然地成为了北航的校草。本文将探讨杨博文获此殊荣的几个关键因素。\n主体\n首先，杨博文的外表无疑是他成为校草的一个重要因素。他的容貌俊朗，总是给人留下良好的第一印\n象。然而，仅凭外表的吸引力是不足以让他在北航这样以工程和技术见长的学校中脱颖而出的。他的个\n人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyqr2z7fv/杨博文为何是北航校草.pdf'}), Document(page_content='人才华和能力才是更加重要的因素。\n在学业上，杨博文在计算机组成、操作系统等专业课程中不仅取得了优异的成绩，而且展示了对复杂理\n论和技术概念的深刻理解。他的能力不仅限于理论学习，更在实践中得到了验证。杨博文参与并带领了\n多个工程项目，包括开发软件应用和参与硬件设计，这些项目不仅成功实施，还在学术和技术会议上获\n得了认可。\n例如，他曾领导一个团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n此外，他在操作系统的课程设计中，独立完成了一个微型操作系统的设计，该系统能有效管理硬件资源\n并支持基本的多任务处理，这一成就显示了他在软件和硬件接口方面的高水平能力。\n除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyqr2z7fv/杨博文为何是北航校草.pdf'}), Document(page_content='除了学术上的成就，杨博文还积极参与校园生活和社会活动。他是学生会的积极成员，经常组织和参与\n各种校园活动，展现了他的领导能力和社交技巧。此外，他还参与志愿服务工作，帮助社区中的弱势群\n体，体现了他的社会责任感。\n结论\n综上所述，杨博文之所以被誉为北航校草，不仅仅是因为他俊朗的外表，更重要的是他在学术、领导力\n和社会责任方面的全面表现。他在计算机科学的各个方面的深厚造诣，尤其是在计算机组成和操作系统\n等专业课程中的杰出表现，以及他在实际工程项目中的实际应用能力，使他在北航获得了师生的广泛认\n可和尊敬。杨博文的故事激励着每一个北航学子，不仅要追求学术上的卓越，更要在个人成长和社会贡\n献上不断努力。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpyqr2z7fv/杨博文为何是北航校草.pdf'})]
cuda:2
请讲述研究现状部分
 tmpyqr2z7fv
[]
cuda:2
[1.2417551]
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpdf500l5g, tmpdf500l5g
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc236190> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpdf500l5g/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
请讲讲这篇论文解决的问题
 tmpyqr2z7fv
[]
cuda:2
[1.27719]
请讲讲这篇论文解决的问题
 tmpyqr2z7fv
[]
cuda:2
[1.27719]
请讲讲这篇论文实验得到的结果
 tmpyqr2z7fv
[]
cuda:2
[1.2761463]
请讲讲这篇论文得出的结论
 tmpyqr2z7fv
[]
cuda:2
[1.1955752]
告诉我更多 tmp048pz4ga
[History(role='user', content='杨博文最爱的人是？'), History(role='assistant', content='根据已知信息，没有关于杨博文最爱的人的相关信息。'), History(role='user', content='如何评价杨博文'), History(role='assistant', content='杨博文是一位具有高度才华和能力的人才。他在计算机组成、操作系统等专业课程中表现出色，不仅具有扎实的理论基础，还能将所学知识运用到实践中，展现出强烈的问题解决能力。杨博文曾参与并领导多个工程项目，包括软件应用开发和硬件设计，这些项目都取得了成功，并获得了学术和技术会议的认可。\n\n在学术方面，杨博文具有很高的成就。他独立完成了一个微型操作系统的设计，显示出在软件和硬件接口方面的高水平能力。此外，他还领导团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n\n除了学术上的成就，杨博文还积极参与校园生活和社会活动，表现出极高的综合素质。他是学生会的积极成员，经常组织和参与各种活动，充分体现了他的团队精神和组织能力。\n\n综上所述，杨博文是一位具备优秀学术成绩、实践能力和社会活动能力的优秀人才，值得高度评价。')]
cuda:2
[1.3776584]
告诉我更多 tmp048pz4ga
[History(role='user', content='杨博文最爱的人是？'), History(role='assistant', content='根据已知信息，没有关于杨博文最爱的人的相关信息。'), History(role='user', content='如何评价杨博文'), History(role='assistant', content='杨博文是一位具有高度才华和能力的人才。他在计算机组成、操作系统等专业课程中表现出色，不仅具有扎实的理论基础，还能将所学知识运用到实践中，展现出强烈的问题解决能力。杨博文曾参与并领导多个工程项目，包括软件应用开发和硬件设计，这些项目都取得了成功，并获得了学术和技术会议的认可。\n\n在学术方面，杨博文具有很高的成就。他独立完成了一个微型操作系统的设计，显示出在软件和硬件接口方面的高水平能力。此外，他还领导团队开发了一个基于AI的图像处理软件，该项目在国内外技术展览中获得了奖项。\n\n除了学术上的成就，杨博文还积极参与校园生活和社会活动，表现出极高的综合素质。他是学生会的积极成员，经常组织和参与各种活动，充分体现了他的团队精神和组织能力。\n\n综上所述，杨博文是一位具备优秀学术成绩、实践能力和社会活动能力的优秀人才，值得高度评价。')]
cuda:2
[1.3776584]
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmprj2sdjx8, tmprj2sdjx8
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5a3f10> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprj2sdjx8/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
cuda:2
[UploadFile(filename='None.txt', size=7872, headers=Headers({'content-disposition': 'form-data; name="files"; filename="None.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp1_c_qfon, tmp1_c_qfon
File: None.txt, msg: 成功上传文件 None.txt, docs: [Document(page_content='Fast and Robust Non-Rigid Registration Using Accelerated  Majorization-Minimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Non-rigid 3D registration, which deforms a source 3D shape in a non-rigid wayto align with a target 3D shape, is a classical problem in computer vision.Such problems can be challenging because of imperfect data (noise, outliers andpartial overlap) and high degrees of freedom. Existing methods typically adoptthe $\\ell_p$ type robust norm to measure the alignment error and regularize thesmoothness of deformation, and use a proximal algorithm to solve the resultingnon-smooth optimization problem. However, the slow convergence of suchalgorithms limits their wide applications. In this paper, we propose aformulation for robust non-rigid registration based on a globally smooth robustnorm for alignment and regularization, which can effectively handle outliersand partial overlaps. The problem is solved using the majorization-minimizationalgorithm, which reduces each iteration to a convex quadratic problem with aclosed-form solution.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='The problem is solved using the majorization-minimizationalgorithm, which reduces each iteration to a convex quadratic problem with aclosed-form solution. We further apply Anderson acceleration to speed up theconvergence of the solver, enabling the solver to run efficiently on deviceswith limited compute capability. Extensive experiments demonstrate theeffectiveness of our method for non-rigid alignment between two shapes withoutliers and partial overlaps, with quantitative evaluation showing that itoutperforms state-of-the-art methods in terms of registration accuracy andcomputational speed. The source code is available athttps://github.com/yaoyx689/AMM_NRR.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='HALSIE: Hybrid Approach to Learning Segmentation by Simultaneously  Exploiting Image and Event Modalities', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="Event cameras detect changes in per-pixel intensity to generate asynchronous`event streams'. They offer great potential for accurate semantic map retrievalin real-time autonomous systems owing to their much higher temporal resolutionand high dynamic range (HDR) compared to conventional cameras. However,existing implementations for event-based segmentation suffer from sub-optimalperformance since these temporally dense events only measure the varyingcomponent of a visual signal, limiting their ability to encode dense spatialcontext compared to frames. To address this issue, we propose a hybridend-to-end learning framework HALSIE, utilizing three key concepts to reduceinference cost by up to $20\\times$ versus prior art while retaining similarperformance: First, a simple and efficient cross-domain learning scheme toextract complementary spatio-temporal embeddings from both frames and events.Second, a specially designed dual-encoder scheme with Spiking Neural Network(SNN) and Artificial Neural Network (ANN) branches to minimize latency whileretaining cross-domain feature aggregation.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Third, a multi-scale cue mixer tomodel rich representations of the fused embeddings. These qualities of HALSIEallow for a very lightweight architecture achieving state-of-the-artsegmentation performance on DDD-17, MVSEC, and DSEC-Semantic datasets with upto $33\\times$ higher parameter efficiency and favorable inference cost (17.9mJper cycle).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Our ablation study also brings new insights into effective designchoices that can prove beneficial for research across other vision tasks.\nJM3D & JM3D-LLM: Elevating 3D Understanding with Joint Multi-modal Cues', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='The rising importance of 3D understanding, pivotal in computer vision,autonomous driving, and robotics, is evident. However, a prevailing trend,which straightforwardly resorted to transferring 2D alignment strategies to the3D domain, encounters three distinct challenges: (1) Information Degradation:This arises from the alignment of 3D data with mere single-view 2D images andgeneric texts, neglecting the need for multi-view images and detailedsubcategory texts. (2) Insufficient Synergy: These strategies align 3Drepresentations to image and text features individually, hampering the overalloptimization for 3D models. (3) Underutilization: The fine-grained informationinherent in the learned representations is often not fully exploited,indicating a potential loss in detail. To address these issues, we introduceJM3D, a comprehensive approach integrating point cloud, text, and image.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="To address these issues, we introduceJM3D, a comprehensive approach integrating point cloud, text, and image. Keycontributions include the Structured Multimodal Organizer (SMO), enrichingvision-language representation with multiple views and hierarchical text, andthe Joint Multi-modal Alignment (JMA), combining language understanding withvisual representation. Our advanced model, JM3D-LLM, marries 3D representationwith large language models via efficient fine-tuning. Evaluations on ModelNet40and ScanObjectNN establish JM3D's superiority. The superior performance ofJM3D-LLM further underscores the effectiveness of our representation transferapproach.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Our code and models are available at https://github.com/Mr-Neko/JM3D.\nRAGO: Recurrent Graph Optimizer For Multiple Rotation Averaging', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="This paper proposes a deep recurrent Rotation Averaging Graph Optimizer(RAGO) for Multiple Rotation Averaging (MRA). Conventional optimization-basedmethods usually fail to produce accurate results due to corrupted and noisyrelative measurements. Recent learning-based approaches regard MRA as aregression problem, while these methods are sensitive to initialization due tothe gauge freedom problem. To handle these problems, we propose a learnableiterative graph optimizer minimizing a gauge-invariant cost function with anedge rectification strategy to mitigate the effect of inaccurate measurements.Our graph optimizer iteratively refines the global camera rotations byminimizing each node's single rotation objective function. Besides, ourapproach iteratively rectifies relative rotations to make them more consistentwith the current camera orientations and observed relative rotations.Furthermore, we employ a gated recurrent unit to improve the result by tracingthe temporal information of the cost graph.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Our framework is a real-timelearning-to-optimize rotation averaging graph optimizer with a tiny sizedeployed for real-world applications. RAGO outperforms previous traditional anddeep methods on real-world and synthetic datasets. The code is available athttps://github.com/sfu-gruvi-3dv/RAGO', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='NeuroGen: activation optimized image synthesis for discovery  neuroscience', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="Functional MRI (fMRI) is a powerful technique that has allowed us tocharacterize visual cortex responses to stimuli, yet such experiments are bynature constructed based on a priori hypotheses, limited to the set of imagespresented to the individual while they are in the scanner, are subject to noisein the observed brain responses, and may vary widely across individuals. Inthis work, we propose a novel computational strategy, which we call NeuroGen,to overcome these limitations and develop a powerful tool for human visionneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding modelof human vision with a deep generative network to synthesize images predictedto achieve a target pattern of macro-scale brain activation. We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience. By using only a smallnumber of synthetic images created by NeuroGen, we demonstrate that we candetect and amplify differences in regional and individual human brain responsepatterns to visual stimuli. We then verify that these discoveries are reflectedin the several thousand observed image responses measured with fMRI.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='We furtherdemonstrate that NeuroGen can create synthetic images predicted to achieveregional response patterns not achievable by the best-matching natural images.The NeuroGen framework extends the utility of brain encoding models and opensup a new avenue for exploring, and possibly precisely controlling, the humanvisual system.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa57c3810> 111
cuda:2
[Document(page_content='Fast and Robust Non-Rigid Registration Using Accelerated  Majorization-Minimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Non-rigid 3D registration, which deforms a source 3D shape in a non-rigid wayto align with a target 3D shape, is a classical problem in computer vision.Such problems can be challenging because of imperfect data (noise, outliers andpartial overlap) and high degrees of freedom. Existing methods typically adoptthe $\\ell_p$ type robust norm to measure the alignment error and regularize thesmoothness of deformation, and use a proximal algorithm to solve the resultingnon-smooth optimization problem. However, the slow convergence of suchalgorithms limits their wide applications. In this paper, we propose aformulation for robust non-rigid registration based on a globally smooth robustnorm for alignment and regularization, which can effectively handle outliersand partial overlaps. The problem is solved using the majorization-minimizationalgorithm, which reduces each iteration to a convex quadratic problem with aclosed-form solution.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='The problem is solved using the majorization-minimizationalgorithm, which reduces each iteration to a convex quadratic problem with aclosed-form solution. We further apply Anderson acceleration to speed up theconvergence of the solver, enabling the solver to run efficiently on deviceswith limited compute capability. Extensive experiments demonstrate theeffectiveness of our method for non-rigid alignment between two shapes withoutliers and partial overlaps, with quantitative evaluation showing that itoutperforms state-of-the-art methods in terms of registration accuracy andcomputational speed. The source code is available athttps://github.com/yaoyx689/AMM_NRR.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='HALSIE: Hybrid Approach to Learning Segmentation by Simultaneously  Exploiting Image and Event Modalities', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="Event cameras detect changes in per-pixel intensity to generate asynchronous`event streams'. They offer great potential for accurate semantic map retrievalin real-time autonomous systems owing to their much higher temporal resolutionand high dynamic range (HDR) compared to conventional cameras. However,existing implementations for event-based segmentation suffer from sub-optimalperformance since these temporally dense events only measure the varyingcomponent of a visual signal, limiting their ability to encode dense spatialcontext compared to frames. To address this issue, we propose a hybridend-to-end learning framework HALSIE, utilizing three key concepts to reduceinference cost by up to $20\\times$ versus prior art while retaining similarperformance: First, a simple and efficient cross-domain learning scheme toextract complementary spatio-temporal embeddings from both frames and events.Second, a specially designed dual-encoder scheme with Spiking Neural Network(SNN) and Artificial Neural Network (ANN) branches to minimize latency whileretaining cross-domain feature aggregation.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Third, a multi-scale cue mixer tomodel rich representations of the fused embeddings. These qualities of HALSIEallow for a very lightweight architecture achieving state-of-the-artsegmentation performance on DDD-17, MVSEC, and DSEC-Semantic datasets with upto $33\\times$ higher parameter efficiency and favorable inference cost (17.9mJper cycle).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Our ablation study also brings new insights into effective designchoices that can prove beneficial for research across other vision tasks.\nJM3D & JM3D-LLM: Elevating 3D Understanding with Joint Multi-modal Cues', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='The rising importance of 3D understanding, pivotal in computer vision,autonomous driving, and robotics, is evident. However, a prevailing trend,which straightforwardly resorted to transferring 2D alignment strategies to the3D domain, encounters three distinct challenges: (1) Information Degradation:This arises from the alignment of 3D data with mere single-view 2D images andgeneric texts, neglecting the need for multi-view images and detailedsubcategory texts. (2) Insufficient Synergy: These strategies align 3Drepresentations to image and text features individually, hampering the overalloptimization for 3D models. (3) Underutilization: The fine-grained informationinherent in the learned representations is often not fully exploited,indicating a potential loss in detail. To address these issues, we introduceJM3D, a comprehensive approach integrating point cloud, text, and image.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="To address these issues, we introduceJM3D, a comprehensive approach integrating point cloud, text, and image. Keycontributions include the Structured Multimodal Organizer (SMO), enrichingvision-language representation with multiple views and hierarchical text, andthe Joint Multi-modal Alignment (JMA), combining language understanding withvisual representation. Our advanced model, JM3D-LLM, marries 3D representationwith large language models via efficient fine-tuning. Evaluations on ModelNet40and ScanObjectNN establish JM3D's superiority. The superior performance ofJM3D-LLM further underscores the effectiveness of our representation transferapproach.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Our code and models are available at https://github.com/Mr-Neko/JM3D.\nRAGO: Recurrent Graph Optimizer For Multiple Rotation Averaging', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="This paper proposes a deep recurrent Rotation Averaging Graph Optimizer(RAGO) for Multiple Rotation Averaging (MRA). Conventional optimization-basedmethods usually fail to produce accurate results due to corrupted and noisyrelative measurements. Recent learning-based approaches regard MRA as aregression problem, while these methods are sensitive to initialization due tothe gauge freedom problem. To handle these problems, we propose a learnableiterative graph optimizer minimizing a gauge-invariant cost function with anedge rectification strategy to mitigate the effect of inaccurate measurements.Our graph optimizer iteratively refines the global camera rotations byminimizing each node's single rotation objective function. Besides, ourapproach iteratively rectifies relative rotations to make them more consistentwith the current camera orientations and observed relative rotations.Furthermore, we employ a gated recurrent unit to improve the result by tracingthe temporal information of the cost graph.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='Our framework is a real-timelearning-to-optimize rotation averaging graph optimizer with a tiny sizedeployed for real-world applications. RAGO outperforms previous traditional anddeep methods on real-world and synthetic datasets. The code is available athttps://github.com/sfu-gruvi-3dv/RAGO', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='NeuroGen: activation optimized image synthesis for discovery  neuroscience', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="Functional MRI (fMRI) is a powerful technique that has allowed us tocharacterize visual cortex responses to stimuli, yet such experiments are bynature constructed based on a priori hypotheses, limited to the set of imagespresented to the individual while they are in the scanner, are subject to noisein the observed brain responses, and may vary widely across individuals. Inthis work, we propose a novel computational strategy, which we call NeuroGen,to overcome these limitations and develop a powerful tool for human visionneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding modelof human vision with a deep generative network to synthesize images predictedto achieve a target pattern of macro-scale brain activation. We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content="We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience. By using only a smallnumber of synthetic images created by NeuroGen, we demonstrate that we candetect and amplify differences in regional and individual human brain responsepatterns to visual stimuli. We then verify that these discoveries are reflectedin the several thousand observed image responses measured with fMRI.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'}), Document(page_content='We furtherdemonstrate that NeuroGen can create synthetic images predicted to achieveregional response patterns not achievable by the best-matching natural images.The NeuroGen framework extends the utility of brain encoding models and opensup a new avenue for exploring, and possibly precisely controlling, the humanvisual system.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1_c_qfon/None.txt'})]
cuda:2
[UploadFile(filename='06-计算机学院-计算机科学与技术专业.pdf', size=1264788, headers=Headers({'content-disposition': 'form-data; name="files"; filename="06-è®¡ç®\x97æ\x9cºå\xad¦é\x99¢-è®¡ç®\x97æ\x9cºç§\x91å\xad¦ä¸\x8eæ\x8a\x80æ\x9c¯ä¸\x93ä¸\x9a.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpshmre8uw, tmpshmre8uw
File: 06-计算机学院-计算机科学与技术专业.pdf, msg: 成功上传文件 06-计算机学院-计算机科学与技术专业.pdf, docs: [Document(page_content='北京航空航天大学\n计算机学院\n2021 级本科培养方案\n（计算机科学与技术专业）\n计算机学院·计算机科学与技术专业\n1\n计算机学院\n学院简介\n北京航空航天大学 1958 年建立计算机专业，1978 年成立计算机科学与工程系，2002 年组\n建计算机学院。学院遵循“面向国家需求，瞄准学科前沿，汇聚优秀人才，争创国际知名”的发展\n战略，形成了人才培养与科学研究相互促进、协调发展的格局。\n学院现有教职工 163 人，其中中国科学院院士 3 人，中国工程院院士 1 人，英国皇家学会院\n士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家\n工程技术中心、１个国家级国际科技合作基地等十个省部级以上科研教学基地；获国家自然科学二\n等奖 1 项，国家科技进步一等奖 2 项，其他国家级科技奖项 17 项；计算机科学与技术、软件工程\n在 2017 年全国一级学科评估分别为 A 和 A+，双获国家“双一流”建设学科。\n学院设置计算机科学与技术、虚拟现实技术两个本科专业；获国家教学成果一等奖 1 项、二等\n奖 4 项，首届全国优秀教材二等奖 1 项，国家级教学团队 1 个，国家级教学名师 1 人，国家级精\n品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036\n人，博士 598 人，培养质量受到社会广泛好评。\n院      长 ：王蕴红\n专业负责人 ：王蕴红\n教学副院长 ：高小鹏\n教 学 秘 书 ：丁映中、纵绮\n北京航空航天大学本科培养方案\n2\n计算机科学与技术专业\n一、专业简介\n本专业始终坚持立德树人根本任务，落实“五育”并举，践行“三全育人”要求，遵循学校“厚\n植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型\n与工程型相结合”的宽口径人才，秉承“寓教于研”办学理念并依托学科优势构建高质量课程体系，\n重视数学及学科基础理论、专业核心能力以及高水平人才所应具备的人文素养。主要培养特点如下：\n1. 重视核心能力培养。专业必修课分为基础理论、系统能力、软件能力三大模块，普遍具有\n大课重课特性，培养学生具备专业核心能力，为后续课程学习奠定坚实基础。\n2. 自主规划专业选修。本专业实施完全学分制培养模式，引导学生根据自我规划和学习兴趣\n制定个性化的专业课程选修方案，鼓励学生在某一专业方向上的系统性学习。\n3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要\n素，增强学生公民意识及责任感、提升思辨力，让学生建立工程伦理并了解技术、社会与\n自然三者间的作用关系。2～3 年级设置连贯性的科技实践与成果表达训练环节，培养创\n新意识及有效表达能力。\n二、培养目标和毕业要求\n（一）培养目标\n以学校“培养引领和支撑国家重大战略需求的领军领导人才”的人才培养战略总目标为指导，\n培养具有良好人文素养、强烈的事业心、使命感及担当精神，具有创新精神、全球化视野、终身学\n习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机\n领域复杂工程问题的能力，具有团队合作与组织管理能力，能参与国际竞争的计算机专业高水平人\n才。\n学生毕业 5 年后：\n计算机学院·计算机科学与技术专业\n3\n1. 能就专业相关的工程问题，综合考虑技术、经济、法律、伦理等因素，分析、制定解决方\n案，并管理项目的实施；\n2. 能在职业发展中具有担当精神、行动力、感染力和领导力；\n3. 能与国内外同行、客户和公众有效沟通；\n4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求\n1. 工程知识：具备较扎实的数学、自然科学知识，系统掌握计算机领域的工程基础和专业知识，\n能够将这些知识用于解决计算机领域复杂工程问题。\n2. 问题分析：能够应用数学、自然科学基本原理与专业知识，并通过文献研究，识别、表达、\n分析计算机领域复杂工程问题，以获得有效结论。\n3. 设计/开发解决方案：能够设计针对计算机领域复杂工程问题的解决方案，设计满足特定需\n求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。\n4. 研究：能够基于专业知识对计算机领域复杂工程问题进行研究，包括文献调研、设计实验、\n分析与解释数据、并通过信息综合得到合理有效的结论。\n5. 使用现代工具：能够在本专业工程实践中，选择与使用合理有效的技术、软硬件资源、软硬\n件开发工具和信息技术工具，并了解其局限性。\n6. 工程与社会：具有追求创新的态度和意识，掌握基本的创新方法，以及综合运用理论和技术\n手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。\n7. 环境和可持续发展：了解与本专业相关的职业和行业的生产、设计、研究与开发、环境保护\n和可持续发展等方面的方针、政策和法津、法规；能够正确认识本专业工程实践对环境和社会可持\n续发展的影响，合理评价本专业工程实践解决方案对社会、健康、安全、法律及文化的影响。\n北京航空航天大学本科培养方案\n4\n8. 职业规范：具有良好的人文素养、社会责任感，能够在本专业工程实践中理解并遵守工程职\n业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。\n10. 沟通：能够就复杂计算机工程问题与业界同行及社会公众进行有效沟通与交流，包括撰写\n报告和设计文稿、陈述发言、清晰表达个人见解等，并具备一定的国际视野，能够在跨文化背景下\n进行沟通和交流。\n11. 项目管理：具有一定的组织与工程管理能力、表达与人际交往能力，能够在多学科背景下\n的团队中发挥重要作用。\n12. 终身学习：具有自主学习和终身学习的意识，具有不断学习和适应本领域快速发展的能力；\n具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图\n毕业要求\n主要专业必修课\n1)\n工程知识\n2)\n问题分析\n3)\n解决方案\n4)\n研究\n5)\n工具\n6)\n工程与社\n会\n7)\n环境与可\n持续发展\n8)\n职业规范\n9)\n个人与团', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='持续发展\n8)\n职业规范\n9)\n个人与团\n队\n10)\n沟通\n11)\n项目管理\n12)\n终身学习\n专业\n导论\n导论(信息类)\n√\n√\n√\n√\n√\n√\n基础\n理论\n离散数学(信息类)\n√\n√\n离散数学(2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='理论\n离散数学(信息类)\n√\n√\n离散数学(2)\n√\n√\n系统\n能力\n计算机组成\n√\n√\n√\n√\n√\n√\n操作系统\n√\n√\n√\n√\n√\n√\n√\n√\n编译技术\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n编译技术\n√\n√\n√\n√\n√\n√\n√\n√\n计算机网络\n√\n√\n√\n√\n√\n√\n√\n√\n软件\n能力\n数据结构与程序设\n计(信息类)\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计(信息类)\n√\n√\n√\n√\n√\n面向对象设计与构\n造\n√\n√\n√\n√\n√\n√\n√\n√\n√\n算法设计与分析\n√\n√\n√\n√\n√\n数据库系统原理\n√\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n√\n√\n√\n√\n√\n√\n软件工程\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n北京航空航天大学本科培养方案\n6\n三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合\n格，达到学校毕业要求的，准予毕业，学校颁发毕业证书；符合学士学位授予条件的，授予学士学\n位。\n毕业总学分：155\n授予学位类型：计算机科学与技术学士学位\n计算机科学与技术专业本科指导性最低学分框架表\n课程模块\n序列\n课程类别\n最低学分要求\n1 年级\n2-4 年级\n学分小计\nI\n基础课程\nA\n数学与自然科学类\n22', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='I\n基础课程\nA\n数学与自然科学类\n22\n7\n46\nB\n工程基础类\n9\n0\nC\n外语类\n4\n4\nII\n通修课程\nD\n思政类\n8.5\n10.5\n41\n军理类\n0\n4\nE\n体育类\n1\n2.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n4\nE\n体育类\n1\n2.5\nK\n素质教育理论必修课\n0\n2.5\nH\n素质教育实践必修课\n0.5\n1.5\nF/G\n素质教育通识限修课\n2.5\n7.5\nIII\n专业课程\nI\n核心专业类\n0\n48\n68\nJ\n一般专业类\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n48\n68\nJ\n一般专业类\n0\n20\n学分小计\n47.5\n107.5\n155\n毕业最低总学分\n155\n注：外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家\n安全、素质教育实践必修课等修读要求见相关文件，其中：\n①劳动教育课程要求：至少选修劳动教育必修课或劳动教育模块学时总数≥32 学时及参加劳动月等活\n动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业\n7\n②素质教育通识限修课：大类概论课、航空航天概论 B 为必修，至少还应包含 3 门人文类通识课（每门\n课程学分不得低于 2 学分；“法律、科技与社会”为必修）。\n③创新创业课程要求：至少选修 3 学分，详见每学期创新创业课程清单，修读要求见相应创新创业学分\n认定办法。\n④全英文课程要求：至少选修 2 学分全英文课程（外语类课程除外）。鼓励学生积极申报学校出国留学\n计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保\n学习成效，所选修课程总数不得超过 2 门。\n⑥一般专业类：至少需要从计算机学院为本专业设置的方向基础类课程清单中选修不少于 3 门课程。\n北京航空航天大学本科培养方案\n8\n四、课程设置与学分分布表\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n基\n础\n课\n程\n数\n学\n与\n自\n然\n科', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='数\n学\n与\n自\n然\n科\n学\n类\nB1A09104A 工科数学分析(1)\nMathematical Analysis for Engineering\n(1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09106A 工科高等代数\nAdvanced Algebra for Engineering (1)\n6\n96\n96', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Advanced Algebra for Engineering (1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09105A 工科数学分析(2)\nMathematical Analysis for Engineering\n(2)\n6\n96\n96\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB1A19104A 基础物理学(信息类)\nFundamental Physics(Information class)\n4\n64\n64\n0\n0\n一\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考试\n全汉语\nB1A19201B 工科大学物理(2)\nEngineering College physics(2)\n4\n64\n64\n0\n0\n二\n秋\n必修\n考试\n全汉语\n工\n程\n基\n础\n类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2\n48\n16\n32\n0\n一\n秋\n必修\n考试\n全汉语\nB1B021150\n电子设计基础训练\nBasic training in electronic design\n2\n56\n8\n48\n0\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考查\n全汉语\nB1B061060\n离散数学(信息类)\nDiscrete Mathematics (Information\nclass)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1B061100\n数据结构与程序设计(信息类)\nData Structures and Programming\n(Information class)\n3\n64', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Data Structures and Programming\n(Information class)\n3\n64\n32\n32\n0\n一\n春\n必修\n考试\n全汉语\n外\n语\n类\nB1C12107A 大学英语 A(1)\nCollege English A (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n秋\n必修\n考试\n全英文\nB1C12108A 大学英语 A(2)\nCollege English A (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207A 大学英语 A(3)\nCollege English A (3)\n2\n32\n32\n0\n0\n二', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208A 大学英语 A(4)\nCollege English A (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试\n全英文\nB1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文\nB1C12108B 大学英语 B(2)\nCollege English B (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207B 大学英语 B(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全英文\nB1C12207B 大学英语 B(3)\nCollege English B (3)\n2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208B 大学英语 B(4)\nCollege English B (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n二\n春\n必修\n考试\n全英文\n通\n思\nB2D281050\n思想道德与法治\nEthic Thought and Rule of Law\n3\n48\n48\n0\n0\n一\n秋\n必修\n考试\n全汉语\n计算机学院·计算机科学与技术专业\n9\n课程\n模块\n课程\n类别', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='9\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content="方式\n授课\n语言\n学年\n学期\n修\n课\n程\n政\n类\nB2D282060\n习近平新时代中国特色社会主\n义思想概论\nIntroduction to Xi Jinping's Thoughts on\nSocialism with Chinese Characteristics\nfor a New Era\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2D281060", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='秋\n必修\n考试\n全汉语\nB2D281060\n中国近现代史纲要\nOutline of Modern Chinese History\n3\n48\n48\n0\n0\n一\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n思\n政\n类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (1)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2D282090\n毛泽东思想和中国特色社会主\n义理论体系概论(2)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Mao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)\n2\n80\n0\n0\n80\n二\n寒假\n必修\n考查\n全汉语\nB2D282070\n马克思主义基本原理\nFundamental Principles of Marxism\n3\n48\n48\n0\n0\n二\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n二\n春\n必修\n考试\n全汉语\nB2D281110\n形势与政策(1)\nSituation and Policy (1)\n0.2\n8\n4\n0\n4\n一\n秋\n必修\n考查\n全汉语\nB2D281120\n形势与政策(2)\nSituation and Policy (2)\n0.3\n8\n4\n0\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n8\n4\n0\n4\n一\n春\n必修\n考查\n全汉语\nB2D282110\n形势与政策(3)\nSituation and Policy (3)\n0.2\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)\n0.3\n8\n8\n0\n0\n二\n春\n必修\n考查\n全汉语\nB2D283110\n形势与政策(5)\nSituation and Policy (5)\n0.2\n8\n8\n0\n0\n三\n秋\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考查\n全汉语\nB2D283120\n形势与政策(6)\nSituation and Policy (6)\n0.3\n8\n8\n0\n0\n三\n春\n必修\n考查\n全汉语\nB2D284110\n形势与政策(7)\nSituation and Policy (7)\n0.2\n8\n8\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.2\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB2D284120\n形势与政策(8)\nSituation and Policy (8)\n0.3\n8\n8\n0\n0\n四\n春\n必修\n考查\n全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n限修≥1\n学分\n考试\n全汉语\nB2D280120\n新中国史\nThe History of the People’s Republic of\nChina\n1\n16\n16\n0\n0\n一至', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280130\n改革开放史\nThe History of the Reform and Opening-\nup\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\n北京航空航天大学本科培养方案\n10\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n军理\n类\nB2D511040\n军事理论\nMilitary Theory\n2\n36', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2D511040\n军事理论\nMilitary Theory\n2\n36\n32\n0\n4\n二\n春\n必修\n考试\n全汉语\nB2D511030\n军事技能\nMilitary Skills\n2\n112\n0\n0\n112\n一\n夏\n必修\n考查\n全汉语\n体\n育\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\n体\n育\n类\nB2E331030\n体育(1)\nPhysical Education (1)\n0.5\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2E331040\n体育(2)\nPhysical Education (2)\n0.5\n32\n32\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.5\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB2E332050\n体育(3)\nPhysical Education (3)\n0.5\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2E332060\n体育(4)\nPhysical Education (4)\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2E332060\n体育(4)\nPhysical Education (4)\n0.5\n32\n32\n0\n0\n二\n春\n必修\n考试\n全汉语\nB2E333070\n体育(5)\nPhysical Education (5)\n0.5\n16\n16\n0\n0\n三\n秋\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考试\n全汉语\nB2E333080\n体育(6)\nPhysical Education (6)\n0.5\n16\n16\n0\n0\n三\n春\n必修\n考试\n全汉语\nB2E334030\n体质健康标准测试\n0.5\n0\n0\n0\n0\n四\n秋', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n0\n0\n四\n秋\n必修\n考试\n全汉语\n通\n修\n课\n程\n素\n质\n教\n育\n实\n践\n必\n修\n课\nB2H511110\n素质教育(博雅课程)(1)\nComprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Comprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16\n4\n0\n12\n一\n秋\n必修\n考查\n全汉语\nB2H511120\n素质教育(博雅课程)(2)\nComprehensive Development\nEducation(Liberal Arts Course )(2)\n0.3\n16\n4\n0\n12\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='12\n一\n春\n必修\n考查\n全汉语\nB2H511130\n素质教育(博雅课程)(3)\nComprehensive Development\nEducation(Liberal Arts Course )(3)\n0.2\n16\n4\n0\n12\n二\n秋\n必修\n考查\n全汉语\nB2H511140\n素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3\n16\n4\n0\n12\n二\n春\n必修\n考查\n全汉语\nB2H511150\n素质教育(博雅课程)(5)\nComprehensive Development\nEducation(Liberal Arts Course )(5)\n0.2\n16\n4\n0\n12\n三\n秋\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4\n0\n12\n三\n秋\n必修\n考查\n全汉语\nB2H511160\n素质教育(博雅课程)(6)\nComprehensive Development\nEducation(Liberal Arts Course )(6)\n0.3\n16\n4\n0\n12\n三\n春\n必修\n考查\n全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development\nEducation(Liberal Arts Course )(7)\n0.2\n16\n4\n0\n12\n四\n秋\n必修\n考查\n全汉语\nB2H511180\n素质教育(博雅课程)(8)\nComprehensive Development\nEducation(Liberal Arts Course )(8)\n0.3\n16\n4\n0\n12\n四', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n16\n4\n0\n12\n四\n春\n必修\n考查\n全汉语\n素质\n教育\n理论\n美育类课程（至少 1.5 学分），各类课程见各学期开课清单\n1.5\n必修\n劳动教育课程（至少 32 学时），劳动教育必修课或劳动教育模块，详见每学期劳动\n教育课程清单\n32\n一至\n四\n秋/春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='一至\n四\n秋/春\n必修\n考查\n全汉语\n计算机学院·计算机科学与技术专业\n11\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n必修\n课\nB2K141010\n国家安全\nNational Security Education\n1\n16\n14\n0\n2\n一至\n三\n秋/春\n必修\n考查\n全汉语\n素', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n秋/春\n必修\n考查\n全汉语\n素\n质\n教\n育\n通\n识\n限\n修\n课\n概论课(大类内部)以下七门：\nB2F020390\n电子信息工程导论\nIntroduction to Electronic Information\nEngineering\n1.5\n24\n24\n0\n0\n一\n秋\n限修，', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='24\n0\n0\n一\n秋\n限修，\n≥1.5 学\n分\n考查\n全汉语\nB2F030360\n自动化科学与电气工程导论\nIntroduction of automation Science and\nelectrical engineering major\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and\nComputer Ethics\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F170210\n仪器科学概览\nOverview of Instrument Science\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n一\n秋\n考查\n全汉语\nB2F210120\n走进软件\nIntroduction to Software\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F390110\n网络空间安全导论\nIntroduction to Cyberspace Security\n1.5\n24\n24\n0\n0\n一', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F491110\n集成电路导论\nIntroduction to Integrated Circuit\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n素质\n教育\n通识\n限修\n课\n人文、经典、社科、科技文明 4 类素质教育课\n1\n秋/春\n限修，\n≥1 学分', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n秋/春\n限修，\n≥1 学分\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n3 门人文类核心通识课（法律、科技与社会为必选）\n2\n32\n32\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n专', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n春\n必修\n考查\n全汉语\n专\n业\n课\n程\n核\n心\n专\n业\n类\nB3J063270\n科研课堂\nScientific Research Training\n2\n32\n0\n0\n32\n二\n秋\n必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)\nInternship\n5\n320\n0\n0\n320\n三\n夏\n必修\n考查\n全汉语\nB3I064410\n毕业设计\nGraduation Project\n8\n640\n0\n0\n640\n四\n春\n必修\n考查', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n640\n四\n春\n必修\n考查\n全汉语\nB3I061142\n离散数学 2\nDiscrete Mathematics (2)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB3I062260\n计算机组成\nPrinciples of Computer Organization\n5.5\n112\n64\n48', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Principles of Computer Organization\n5.5\n112\n64\n48\n0\n二\n秋\n必修\n考试\n全汉语\n北京航空航天大学本科培养方案\n12\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\nB3I062270\n操作系统\nOperating Systems\n4.5\n96\n48\n48\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='48\n0\n二\n春\n必修\n考试\n全汉语\nB3I063120\n编译技术\nCompiler Technology\n4.5\n96\n48\n48\n0\n三\n秋\n必修\n考试\n全汉语\nB3I062180\n面向对象设计与构造\nObject-oriented Design and\nConstruction\n2.5\n48\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Object-oriented Design and\nConstruction\n2.5\n48\n32\n16\n0\n二\n春\n必修\n考试\n全汉语\nB3I063320\n算法设计与分析\nThe Design and Analysis of Algorithm\n2\n32\n32\n0\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063180\n数据库系统原理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB3I063180\n数据库系统原理\nPrinciples of Database Systems\n4\n80\n48\n32\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063130\n软件工程\nSoftware Engineering\n2\n32\n32\n0\n0\n三\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063210\n计算机网络\nComputer Network\n2\n32\n32\n0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063220\n计算机网络实验\nComputer Network Experiment\n1\n32\n0\n32\n0\n三', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n32\n0\n32\n0\n三\n春\n必修\n考试\n全汉语\nB3I064120\n计算机科学方法论\nMethodology of Computer Science\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n一\n般\n专\n业\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\n一\n般\n专\n业\n类\nB3J062610\n职业规划与选择讲座\nProfession Planning and Choice\n0.5\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB3J063610\n学科技术前沿讲座\nSeminars on the Development of\nComputer Science\n1\n16\n16\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Computer Science\n1\n16\n16\n0\n0\n三\n春\n必修\n考查\n全汉语\nB3J064610\n求职辅导系列讲座\nSerial Lectures on How to Seek\nEmployment\n0.5\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB3J062410\n实践与展示(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3J062410\n实践与展示(1)\nPractice and Presentation(Ⅰ)\n1\n32\n0\n0\n32\n二\n春\n必修\n考查\n全汉语\nB3J062420\n实践与展示(2)\nPractice and Presentation (2)\n1\n32\n0\n0\n32\n三\n春\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n32\n三\n春\n必修\n考查\n全汉语\n共计选修 16 学分的专业选修课。\n16\n计算机学院·计算机科学与技术专业\n13\n五、核心课程先修逻辑关系图\n六、专业准入办法一览表\n为更好的体现学生结合自身学习特点和兴趣以更好的选择专业，本专业分别在 1 年级春季学\n期、2 年级秋季学期和 2 年级春季学期即将结束时转入转出申请。\n对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业\n学习的候选者，同时结合候选者其他相关课程的学习决定其是否同级转专业还是降级转专业。\n关于具体准入条件，请参考《北京航空航天大学计算机学院转专业管理规定》。该文件可以在\n学校转专业相关网页查阅到。\n七、毕业生未来发展图\n除了升学深造外，由于计算机专业社会需求广泛，因此本专业毕业生具有广泛的就业空间及发\n展可能。本培养方案仅给出部分可能的发展规划，具体内容参见下表。\n主分类\n次分类\n描述\n就业\nIT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='IT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主\n北京航空航天大学本科培养方案\n14\n自主创业\n升学\n国内深造\n软件与理论方向：大学、中科院计算所、中科院软件所\n体系结构方向：大学、中科院计算所、国防系统研究所\n计算机应用方向：大学、中科院相关院所、国防系统研究所\n出国深造\n国外大学攻读硕士学位、博士学位', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc146cd0> 111
cuda:2
[Document(page_content='北京航空航天大学\n计算机学院\n2021 级本科培养方案\n（计算机科学与技术专业）\n计算机学院·计算机科学与技术专业\n1\n计算机学院\n学院简介\n北京航空航天大学 1958 年建立计算机专业，1978 年成立计算机科学与工程系，2002 年组\n建计算机学院。学院遵循“面向国家需求，瞄准学科前沿，汇聚优秀人才，争创国际知名”的发展\n战略，形成了人才培养与科学研究相互促进、协调发展的格局。\n学院现有教职工 163 人，其中中国科学院院士 3 人，中国工程院院士 1 人，英国皇家学会院\n士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家\n工程技术中心、１个国家级国际科技合作基地等十个省部级以上科研教学基地；获国家自然科学二\n等奖 1 项，国家科技进步一等奖 2 项，其他国家级科技奖项 17 项；计算机科学与技术、软件工程\n在 2017 年全国一级学科评估分别为 A 和 A+，双获国家“双一流”建设学科。\n学院设置计算机科学与技术、虚拟现实技术两个本科专业；获国家教学成果一等奖 1 项、二等\n奖 4 项，首届全国优秀教材二等奖 1 项，国家级教学团队 1 个，国家级教学名师 1 人，国家级精\n品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036\n人，博士 598 人，培养质量受到社会广泛好评。\n院      长 ：王蕴红\n专业负责人 ：王蕴红\n教学副院长 ：高小鹏\n教 学 秘 书 ：丁映中、纵绮\n北京航空航天大学本科培养方案\n2\n计算机科学与技术专业\n一、专业简介\n本专业始终坚持立德树人根本任务，落实“五育”并举，践行“三全育人”要求，遵循学校“厚\n植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型\n与工程型相结合”的宽口径人才，秉承“寓教于研”办学理念并依托学科优势构建高质量课程体系，\n重视数学及学科基础理论、专业核心能力以及高水平人才所应具备的人文素养。主要培养特点如下：\n1. 重视核心能力培养。专业必修课分为基础理论、系统能力、软件能力三大模块，普遍具有\n大课重课特性，培养学生具备专业核心能力，为后续课程学习奠定坚实基础。\n2. 自主规划专业选修。本专业实施完全学分制培养模式，引导学生根据自我规划和学习兴趣\n制定个性化的专业课程选修方案，鼓励学生在某一专业方向上的系统性学习。\n3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要\n素，增强学生公民意识及责任感、提升思辨力，让学生建立工程伦理并了解技术、社会与\n自然三者间的作用关系。2～3 年级设置连贯性的科技实践与成果表达训练环节，培养创\n新意识及有效表达能力。\n二、培养目标和毕业要求\n（一）培养目标\n以学校“培养引领和支撑国家重大战略需求的领军领导人才”的人才培养战略总目标为指导，\n培养具有良好人文素养、强烈的事业心、使命感及担当精神，具有创新精神、全球化视野、终身学\n习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机\n领域复杂工程问题的能力，具有团队合作与组织管理能力，能参与国际竞争的计算机专业高水平人\n才。\n学生毕业 5 年后：\n计算机学院·计算机科学与技术专业\n3\n1. 能就专业相关的工程问题，综合考虑技术、经济、法律、伦理等因素，分析、制定解决方\n案，并管理项目的实施；\n2. 能在职业发展中具有担当精神、行动力、感染力和领导力；\n3. 能与国内外同行、客户和公众有效沟通；\n4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求\n1. 工程知识：具备较扎实的数学、自然科学知识，系统掌握计算机领域的工程基础和专业知识，\n能够将这些知识用于解决计算机领域复杂工程问题。\n2. 问题分析：能够应用数学、自然科学基本原理与专业知识，并通过文献研究，识别、表达、\n分析计算机领域复杂工程问题，以获得有效结论。\n3. 设计/开发解决方案：能够设计针对计算机领域复杂工程问题的解决方案，设计满足特定需\n求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。\n4. 研究：能够基于专业知识对计算机领域复杂工程问题进行研究，包括文献调研、设计实验、\n分析与解释数据、并通过信息综合得到合理有效的结论。\n5. 使用现代工具：能够在本专业工程实践中，选择与使用合理有效的技术、软硬件资源、软硬\n件开发工具和信息技术工具，并了解其局限性。\n6. 工程与社会：具有追求创新的态度和意识，掌握基本的创新方法，以及综合运用理论和技术\n手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。\n7. 环境和可持续发展：了解与本专业相关的职业和行业的生产、设计、研究与开发、环境保护\n和可持续发展等方面的方针、政策和法津、法规；能够正确认识本专业工程实践对环境和社会可持\n续发展的影响，合理评价本专业工程实践解决方案对社会、健康、安全、法律及文化的影响。\n北京航空航天大学本科培养方案\n4\n8. 职业规范：具有良好的人文素养、社会责任感，能够在本专业工程实践中理解并遵守工程职\n业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。\n10. 沟通：能够就复杂计算机工程问题与业界同行及社会公众进行有效沟通与交流，包括撰写\n报告和设计文稿、陈述发言、清晰表达个人见解等，并具备一定的国际视野，能够在跨文化背景下\n进行沟通和交流。\n11. 项目管理：具有一定的组织与工程管理能力、表达与人际交往能力，能够在多学科背景下\n的团队中发挥重要作用。\n12. 终身学习：具有自主学习和终身学习的意识，具有不断学习和适应本领域快速发展的能力；\n具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图\n毕业要求\n主要专业必修课\n1)\n工程知识\n2)\n问题分析\n3)\n解决方案\n4)\n研究\n5)\n工具\n6)\n工程与社\n会\n7)\n环境与可\n持续发展\n8)\n职业规范\n9)\n个人与团', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='持续发展\n8)\n职业规范\n9)\n个人与团\n队\n10)\n沟通\n11)\n项目管理\n12)\n终身学习\n专业\n导论\n导论(信息类)\n√\n√\n√\n√\n√\n√\n基础\n理论\n离散数学(信息类)\n√\n√\n离散数学(2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='理论\n离散数学(信息类)\n√\n√\n离散数学(2)\n√\n√\n系统\n能力\n计算机组成\n√\n√\n√\n√\n√\n√\n操作系统\n√\n√\n√\n√\n√\n√\n√\n√\n编译技术\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n编译技术\n√\n√\n√\n√\n√\n√\n√\n√\n计算机网络\n√\n√\n√\n√\n√\n√\n√\n√\n软件\n能力\n数据结构与程序设\n计(信息类)\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计(信息类)\n√\n√\n√\n√\n√\n面向对象设计与构\n造\n√\n√\n√\n√\n√\n√\n√\n√\n√\n算法设计与分析\n√\n√\n√\n√\n√\n数据库系统原理\n√\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n√\n√\n√\n√\n√\n√\n软件工程\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n北京航空航天大学本科培养方案\n6\n三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合\n格，达到学校毕业要求的，准予毕业，学校颁发毕业证书；符合学士学位授予条件的，授予学士学\n位。\n毕业总学分：155\n授予学位类型：计算机科学与技术学士学位\n计算机科学与技术专业本科指导性最低学分框架表\n课程模块\n序列\n课程类别\n最低学分要求\n1 年级\n2-4 年级\n学分小计\nI\n基础课程\nA\n数学与自然科学类\n22', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='I\n基础课程\nA\n数学与自然科学类\n22\n7\n46\nB\n工程基础类\n9\n0\nC\n外语类\n4\n4\nII\n通修课程\nD\n思政类\n8.5\n10.5\n41\n军理类\n0\n4\nE\n体育类\n1\n2.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n4\nE\n体育类\n1\n2.5\nK\n素质教育理论必修课\n0\n2.5\nH\n素质教育实践必修课\n0.5\n1.5\nF/G\n素质教育通识限修课\n2.5\n7.5\nIII\n专业课程\nI\n核心专业类\n0\n48\n68\nJ\n一般专业类\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n48\n68\nJ\n一般专业类\n0\n20\n学分小计\n47.5\n107.5\n155\n毕业最低总学分\n155\n注：外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家\n安全、素质教育实践必修课等修读要求见相关文件，其中：\n①劳动教育课程要求：至少选修劳动教育必修课或劳动教育模块学时总数≥32 学时及参加劳动月等活\n动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业\n7\n②素质教育通识限修课：大类概论课、航空航天概论 B 为必修，至少还应包含 3 门人文类通识课（每门\n课程学分不得低于 2 学分；“法律、科技与社会”为必修）。\n③创新创业课程要求：至少选修 3 学分，详见每学期创新创业课程清单，修读要求见相应创新创业学分\n认定办法。\n④全英文课程要求：至少选修 2 学分全英文课程（外语类课程除外）。鼓励学生积极申报学校出国留学\n计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保\n学习成效，所选修课程总数不得超过 2 门。\n⑥一般专业类：至少需要从计算机学院为本专业设置的方向基础类课程清单中选修不少于 3 门课程。\n北京航空航天大学本科培养方案\n8\n四、课程设置与学分分布表\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n基\n础\n课\n程\n数\n学\n与\n自\n然\n科', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='数\n学\n与\n自\n然\n科\n学\n类\nB1A09104A 工科数学分析(1)\nMathematical Analysis for Engineering\n(1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09106A 工科高等代数\nAdvanced Algebra for Engineering (1)\n6\n96\n96', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Advanced Algebra for Engineering (1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09105A 工科数学分析(2)\nMathematical Analysis for Engineering\n(2)\n6\n96\n96\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB1A19104A 基础物理学(信息类)\nFundamental Physics(Information class)\n4\n64\n64\n0\n0\n一\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考试\n全汉语\nB1A19201B 工科大学物理(2)\nEngineering College physics(2)\n4\n64\n64\n0\n0\n二\n秋\n必修\n考试\n全汉语\n工\n程\n基\n础\n类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2\n48\n16\n32\n0\n一\n秋\n必修\n考试\n全汉语\nB1B021150\n电子设计基础训练\nBasic training in electronic design\n2\n56\n8\n48\n0\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考查\n全汉语\nB1B061060\n离散数学(信息类)\nDiscrete Mathematics (Information\nclass)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1B061100\n数据结构与程序设计(信息类)\nData Structures and Programming\n(Information class)\n3\n64', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Data Structures and Programming\n(Information class)\n3\n64\n32\n32\n0\n一\n春\n必修\n考试\n全汉语\n外\n语\n类\nB1C12107A 大学英语 A(1)\nCollege English A (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n秋\n必修\n考试\n全英文\nB1C12108A 大学英语 A(2)\nCollege English A (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207A 大学英语 A(3)\nCollege English A (3)\n2\n32\n32\n0\n0\n二', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208A 大学英语 A(4)\nCollege English A (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试\n全英文\nB1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文\nB1C12108B 大学英语 B(2)\nCollege English B (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207B 大学英语 B(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全英文\nB1C12207B 大学英语 B(3)\nCollege English B (3)\n2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208B 大学英语 B(4)\nCollege English B (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n二\n春\n必修\n考试\n全英文\n通\n思\nB2D281050\n思想道德与法治\nEthic Thought and Rule of Law\n3\n48\n48\n0\n0\n一\n秋\n必修\n考试\n全汉语\n计算机学院·计算机科学与技术专业\n9\n课程\n模块\n课程\n类别', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='9\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content="方式\n授课\n语言\n学年\n学期\n修\n课\n程\n政\n类\nB2D282060\n习近平新时代中国特色社会主\n义思想概论\nIntroduction to Xi Jinping's Thoughts on\nSocialism with Chinese Characteristics\nfor a New Era\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2D281060", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='秋\n必修\n考试\n全汉语\nB2D281060\n中国近现代史纲要\nOutline of Modern Chinese History\n3\n48\n48\n0\n0\n一\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n思\n政\n类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (1)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2D282090\n毛泽东思想和中国特色社会主\n义理论体系概论(2)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Mao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)\n2\n80\n0\n0\n80\n二\n寒假\n必修\n考查\n全汉语\nB2D282070\n马克思主义基本原理\nFundamental Principles of Marxism\n3\n48\n48\n0\n0\n二\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n二\n春\n必修\n考试\n全汉语\nB2D281110\n形势与政策(1)\nSituation and Policy (1)\n0.2\n8\n4\n0\n4\n一\n秋\n必修\n考查\n全汉语\nB2D281120\n形势与政策(2)\nSituation and Policy (2)\n0.3\n8\n4\n0\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n8\n4\n0\n4\n一\n春\n必修\n考查\n全汉语\nB2D282110\n形势与政策(3)\nSituation and Policy (3)\n0.2\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)\n0.3\n8\n8\n0\n0\n二\n春\n必修\n考查\n全汉语\nB2D283110\n形势与政策(5)\nSituation and Policy (5)\n0.2\n8\n8\n0\n0\n三\n秋\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考查\n全汉语\nB2D283120\n形势与政策(6)\nSituation and Policy (6)\n0.3\n8\n8\n0\n0\n三\n春\n必修\n考查\n全汉语\nB2D284110\n形势与政策(7)\nSituation and Policy (7)\n0.2\n8\n8\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.2\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB2D284120\n形势与政策(8)\nSituation and Policy (8)\n0.3\n8\n8\n0\n0\n四\n春\n必修\n考查\n全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n限修≥1\n学分\n考试\n全汉语\nB2D280120\n新中国史\nThe History of the People’s Republic of\nChina\n1\n16\n16\n0\n0\n一至', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280130\n改革开放史\nThe History of the Reform and Opening-\nup\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\n北京航空航天大学本科培养方案\n10\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n军理\n类\nB2D511040\n军事理论\nMilitary Theory\n2\n36', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2D511040\n军事理论\nMilitary Theory\n2\n36\n32\n0\n4\n二\n春\n必修\n考试\n全汉语\nB2D511030\n军事技能\nMilitary Skills\n2\n112\n0\n0\n112\n一\n夏\n必修\n考查\n全汉语\n体\n育\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\n体\n育\n类\nB2E331030\n体育(1)\nPhysical Education (1)\n0.5\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2E331040\n体育(2)\nPhysical Education (2)\n0.5\n32\n32\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.5\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB2E332050\n体育(3)\nPhysical Education (3)\n0.5\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2E332060\n体育(4)\nPhysical Education (4)\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2E332060\n体育(4)\nPhysical Education (4)\n0.5\n32\n32\n0\n0\n二\n春\n必修\n考试\n全汉语\nB2E333070\n体育(5)\nPhysical Education (5)\n0.5\n16\n16\n0\n0\n三\n秋\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考试\n全汉语\nB2E333080\n体育(6)\nPhysical Education (6)\n0.5\n16\n16\n0\n0\n三\n春\n必修\n考试\n全汉语\nB2E334030\n体质健康标准测试\n0.5\n0\n0\n0\n0\n四\n秋', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n0\n0\n四\n秋\n必修\n考试\n全汉语\n通\n修\n课\n程\n素\n质\n教\n育\n实\n践\n必\n修\n课\nB2H511110\n素质教育(博雅课程)(1)\nComprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Comprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16\n4\n0\n12\n一\n秋\n必修\n考查\n全汉语\nB2H511120\n素质教育(博雅课程)(2)\nComprehensive Development\nEducation(Liberal Arts Course )(2)\n0.3\n16\n4\n0\n12\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='12\n一\n春\n必修\n考查\n全汉语\nB2H511130\n素质教育(博雅课程)(3)\nComprehensive Development\nEducation(Liberal Arts Course )(3)\n0.2\n16\n4\n0\n12\n二\n秋\n必修\n考查\n全汉语\nB2H511140\n素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3\n16\n4\n0\n12\n二\n春\n必修\n考查\n全汉语\nB2H511150\n素质教育(博雅课程)(5)\nComprehensive Development\nEducation(Liberal Arts Course )(5)\n0.2\n16\n4\n0\n12\n三\n秋\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4\n0\n12\n三\n秋\n必修\n考查\n全汉语\nB2H511160\n素质教育(博雅课程)(6)\nComprehensive Development\nEducation(Liberal Arts Course )(6)\n0.3\n16\n4\n0\n12\n三\n春\n必修\n考查\n全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development\nEducation(Liberal Arts Course )(7)\n0.2\n16\n4\n0\n12\n四\n秋\n必修\n考查\n全汉语\nB2H511180\n素质教育(博雅课程)(8)\nComprehensive Development\nEducation(Liberal Arts Course )(8)\n0.3\n16\n4\n0\n12\n四', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n16\n4\n0\n12\n四\n春\n必修\n考查\n全汉语\n素质\n教育\n理论\n美育类课程（至少 1.5 学分），各类课程见各学期开课清单\n1.5\n必修\n劳动教育课程（至少 32 学时），劳动教育必修课或劳动教育模块，详见每学期劳动\n教育课程清单\n32\n一至\n四\n秋/春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='一至\n四\n秋/春\n必修\n考查\n全汉语\n计算机学院·计算机科学与技术专业\n11\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n必修\n课\nB2K141010\n国家安全\nNational Security Education\n1\n16\n14\n0\n2\n一至\n三\n秋/春\n必修\n考查\n全汉语\n素', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n秋/春\n必修\n考查\n全汉语\n素\n质\n教\n育\n通\n识\n限\n修\n课\n概论课(大类内部)以下七门：\nB2F020390\n电子信息工程导论\nIntroduction to Electronic Information\nEngineering\n1.5\n24\n24\n0\n0\n一\n秋\n限修，', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='24\n0\n0\n一\n秋\n限修，\n≥1.5 学\n分\n考查\n全汉语\nB2F030360\n自动化科学与电气工程导论\nIntroduction of automation Science and\nelectrical engineering major\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and\nComputer Ethics\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F170210\n仪器科学概览\nOverview of Instrument Science\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n一\n秋\n考查\n全汉语\nB2F210120\n走进软件\nIntroduction to Software\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F390110\n网络空间安全导论\nIntroduction to Cyberspace Security\n1.5\n24\n24\n0\n0\n一', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F491110\n集成电路导论\nIntroduction to Integrated Circuit\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n素质\n教育\n通识\n限修\n课\n人文、经典、社科、科技文明 4 类素质教育课\n1\n秋/春\n限修，\n≥1 学分', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n秋/春\n限修，\n≥1 学分\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n3 门人文类核心通识课（法律、科技与社会为必选）\n2\n32\n32\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n专', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n春\n必修\n考查\n全汉语\n专\n业\n课\n程\n核\n心\n专\n业\n类\nB3J063270\n科研课堂\nScientific Research Training\n2\n32\n0\n0\n32\n二\n秋\n必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)\nInternship\n5\n320\n0\n0\n320\n三\n夏\n必修\n考查\n全汉语\nB3I064410\n毕业设计\nGraduation Project\n8\n640\n0\n0\n640\n四\n春\n必修\n考查', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n640\n四\n春\n必修\n考查\n全汉语\nB3I061142\n离散数学 2\nDiscrete Mathematics (2)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB3I062260\n计算机组成\nPrinciples of Computer Organization\n5.5\n112\n64\n48', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Principles of Computer Organization\n5.5\n112\n64\n48\n0\n二\n秋\n必修\n考试\n全汉语\n北京航空航天大学本科培养方案\n12\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\nB3I062270\n操作系统\nOperating Systems\n4.5\n96\n48\n48\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='48\n0\n二\n春\n必修\n考试\n全汉语\nB3I063120\n编译技术\nCompiler Technology\n4.5\n96\n48\n48\n0\n三\n秋\n必修\n考试\n全汉语\nB3I062180\n面向对象设计与构造\nObject-oriented Design and\nConstruction\n2.5\n48\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Object-oriented Design and\nConstruction\n2.5\n48\n32\n16\n0\n二\n春\n必修\n考试\n全汉语\nB3I063320\n算法设计与分析\nThe Design and Analysis of Algorithm\n2\n32\n32\n0\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063180\n数据库系统原理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB3I063180\n数据库系统原理\nPrinciples of Database Systems\n4\n80\n48\n32\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063130\n软件工程\nSoftware Engineering\n2\n32\n32\n0\n0\n三\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063210\n计算机网络\nComputer Network\n2\n32\n32\n0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063220\n计算机网络实验\nComputer Network Experiment\n1\n32\n0\n32\n0\n三', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n32\n0\n32\n0\n三\n春\n必修\n考试\n全汉语\nB3I064120\n计算机科学方法论\nMethodology of Computer Science\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n一\n般\n专\n业\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\n一\n般\n专\n业\n类\nB3J062610\n职业规划与选择讲座\nProfession Planning and Choice\n0.5\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB3J063610\n学科技术前沿讲座\nSeminars on the Development of\nComputer Science\n1\n16\n16\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Computer Science\n1\n16\n16\n0\n0\n三\n春\n必修\n考查\n全汉语\nB3J064610\n求职辅导系列讲座\nSerial Lectures on How to Seek\nEmployment\n0.5\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB3J062410\n实践与展示(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3J062410\n实践与展示(1)\nPractice and Presentation(Ⅰ)\n1\n32\n0\n0\n32\n二\n春\n必修\n考查\n全汉语\nB3J062420\n实践与展示(2)\nPractice and Presentation (2)\n1\n32\n0\n0\n32\n三\n春\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n32\n三\n春\n必修\n考查\n全汉语\n共计选修 16 学分的专业选修课。\n16\n计算机学院·计算机科学与技术专业\n13\n五、核心课程先修逻辑关系图\n六、专业准入办法一览表\n为更好的体现学生结合自身学习特点和兴趣以更好的选择专业，本专业分别在 1 年级春季学\n期、2 年级秋季学期和 2 年级春季学期即将结束时转入转出申请。\n对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业\n学习的候选者，同时结合候选者其他相关课程的学习决定其是否同级转专业还是降级转专业。\n关于具体准入条件，请参考《北京航空航天大学计算机学院转专业管理规定》。该文件可以在\n学校转专业相关网页查阅到。\n七、毕业生未来发展图\n除了升学深造外，由于计算机专业社会需求广泛，因此本专业毕业生具有广泛的就业空间及发\n展可能。本培养方案仅给出部分可能的发展规划，具体内容参见下表。\n主分类\n次分类\n描述\n就业\nIT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='IT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主\n北京航空航天大学本科培养方案\n14\n自主创业\n升学\n国内深造\n软件与理论方向：大学、中科院计算所、中科院软件所\n体系结构方向：大学、中科院计算所、国防系统研究所\n计算机应用方向：大学、中科院相关院所、国防系统研究所\n出国深造\n国外大学攻读硕士学位、博士学位', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpshmre8uw/06-计算机学院-计算机科学与技术专业.pdf'})]
cuda:2
计算机毕业要修几学分 tmpshmre8uw
[]
cuda:2
[0.5711255, 0.5929657, 0.61670065, 0.63521945, 0.68208724, 0.6832893]
计算机毕业要修几学分 tmpshmre8uw
[]
cuda:2
[0.5711255, 0.5929657, 0.61670065, 0.63521945, 0.68208724, 0.6832893]
[UploadFile(filename='06-计算机学院-计算机科学与技术专业.pdf', size=1264788, headers=Headers({'content-disposition': 'form-data; name="files"; filename="06-è®¡ç®\x97æ\x9cºå\xad¦é\x99¢-è®¡ç®\x97æ\x9cºç§\x91å\xad¦ä¸\x8eæ\x8a\x80æ\x9c¯ä¸\x93ä¸\x9a.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpoppk3ku_, tmpoppk3ku_
File: 06-计算机学院-计算机科学与技术专业.pdf, msg: 成功上传文件 06-计算机学院-计算机科学与技术专业.pdf, docs: [Document(page_content='北京航空航天大学\n计算机学院\n2021 级本科培养方案\n（计算机科学与技术专业）\n计算机学院·计算机科学与技术专业\n1\n计算机学院\n学院简介\n北京航空航天大学 1958 年建立计算机专业，1978 年成立计算机科学与工程系，2002 年组\n建计算机学院。学院遵循“面向国家需求，瞄准学科前沿，汇聚优秀人才，争创国际知名”的发展\n战略，形成了人才培养与科学研究相互促进、协调发展的格局。\n学院现有教职工 163 人，其中中国科学院院士 3 人，中国工程院院士 1 人，英国皇家学会院\n士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家\n工程技术中心、１个国家级国际科技合作基地等十个省部级以上科研教学基地；获国家自然科学二\n等奖 1 项，国家科技进步一等奖 2 项，其他国家级科技奖项 17 项；计算机科学与技术、软件工程\n在 2017 年全国一级学科评估分别为 A 和 A+，双获国家“双一流”建设学科。\n学院设置计算机科学与技术、虚拟现实技术两个本科专业；获国家教学成果一等奖 1 项、二等\n奖 4 项，首届全国优秀教材二等奖 1 项，国家级教学团队 1 个，国家级教学名师 1 人，国家级精\n品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036\n人，博士 598 人，培养质量受到社会广泛好评。\n院      长 ：王蕴红\n专业负责人 ：王蕴红\n教学副院长 ：高小鹏\n教 学 秘 书 ：丁映中、纵绮\n北京航空航天大学本科培养方案\n2\n计算机科学与技术专业\n一、专业简介\n本专业始终坚持立德树人根本任务，落实“五育”并举，践行“三全育人”要求，遵循学校“厚\n植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型\n与工程型相结合”的宽口径人才，秉承“寓教于研”办学理念并依托学科优势构建高质量课程体系，\n重视数学及学科基础理论、专业核心能力以及高水平人才所应具备的人文素养。主要培养特点如下：\n1. 重视核心能力培养。专业必修课分为基础理论、系统能力、软件能力三大模块，普遍具有\n大课重课特性，培养学生具备专业核心能力，为后续课程学习奠定坚实基础。\n2. 自主规划专业选修。本专业实施完全学分制培养模式，引导学生根据自我规划和学习兴趣\n制定个性化的专业课程选修方案，鼓励学生在某一专业方向上的系统性学习。\n3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要\n素，增强学生公民意识及责任感、提升思辨力，让学生建立工程伦理并了解技术、社会与\n自然三者间的作用关系。2～3 年级设置连贯性的科技实践与成果表达训练环节，培养创\n新意识及有效表达能力。\n二、培养目标和毕业要求\n（一）培养目标\n以学校“培养引领和支撑国家重大战略需求的领军领导人才”的人才培养战略总目标为指导，\n培养具有良好人文素养、强烈的事业心、使命感及担当精神，具有创新精神、全球化视野、终身学\n习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机\n领域复杂工程问题的能力，具有团队合作与组织管理能力，能参与国际竞争的计算机专业高水平人\n才。\n学生毕业 5 年后：\n计算机学院·计算机科学与技术专业\n3\n1. 能就专业相关的工程问题，综合考虑技术、经济、法律、伦理等因素，分析、制定解决方\n案，并管理项目的实施；\n2. 能在职业发展中具有担当精神、行动力、感染力和领导力；\n3. 能与国内外同行、客户和公众有效沟通；\n4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求\n1. 工程知识：具备较扎实的数学、自然科学知识，系统掌握计算机领域的工程基础和专业知识，\n能够将这些知识用于解决计算机领域复杂工程问题。\n2. 问题分析：能够应用数学、自然科学基本原理与专业知识，并通过文献研究，识别、表达、\n分析计算机领域复杂工程问题，以获得有效结论。\n3. 设计/开发解决方案：能够设计针对计算机领域复杂工程问题的解决方案，设计满足特定需\n求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。\n4. 研究：能够基于专业知识对计算机领域复杂工程问题进行研究，包括文献调研、设计实验、\n分析与解释数据、并通过信息综合得到合理有效的结论。\n5. 使用现代工具：能够在本专业工程实践中，选择与使用合理有效的技术、软硬件资源、软硬\n件开发工具和信息技术工具，并了解其局限性。\n6. 工程与社会：具有追求创新的态度和意识，掌握基本的创新方法，以及综合运用理论和技术\n手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。\n7. 环境和可持续发展：了解与本专业相关的职业和行业的生产、设计、研究与开发、环境保护\n和可持续发展等方面的方针、政策和法津、法规；能够正确认识本专业工程实践对环境和社会可持\n续发展的影响，合理评价本专业工程实践解决方案对社会、健康、安全、法律及文化的影响。\n北京航空航天大学本科培养方案\n4\n8. 职业规范：具有良好的人文素养、社会责任感，能够在本专业工程实践中理解并遵守工程职\n业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。\n10. 沟通：能够就复杂计算机工程问题与业界同行及社会公众进行有效沟通与交流，包括撰写\n报告和设计文稿、陈述发言、清晰表达个人见解等，并具备一定的国际视野，能够在跨文化背景下\n进行沟通和交流。\n11. 项目管理：具有一定的组织与工程管理能力、表达与人际交往能力，能够在多学科背景下\n的团队中发挥重要作用。\n12. 终身学习：具有自主学习和终身学习的意识，具有不断学习和适应本领域快速发展的能力；\n具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图\n毕业要求\n主要专业必修课\n1)\n工程知识\n2)\n问题分析\n3)\n解决方案\n4)\n研究\n5)\n工具\n6)\n工程与社\n会\n7)\n环境与可\n持续发展\n8)\n职业规范\n9)\n个人与团', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='持续发展\n8)\n职业规范\n9)\n个人与团\n队\n10)\n沟通\n11)\n项目管理\n12)\n终身学习\n专业\n导论\n导论(信息类)\n√\n√\n√\n√\n√\n√\n基础\n理论\n离散数学(信息类)\n√\n√\n离散数学(2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='理论\n离散数学(信息类)\n√\n√\n离散数学(2)\n√\n√\n系统\n能力\n计算机组成\n√\n√\n√\n√\n√\n√\n操作系统\n√\n√\n√\n√\n√\n√\n√\n√\n编译技术\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n编译技术\n√\n√\n√\n√\n√\n√\n√\n√\n计算机网络\n√\n√\n√\n√\n√\n√\n√\n√\n软件\n能力\n数据结构与程序设\n计(信息类)\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计(信息类)\n√\n√\n√\n√\n√\n面向对象设计与构\n造\n√\n√\n√\n√\n√\n√\n√\n√\n√\n算法设计与分析\n√\n√\n√\n√\n√\n数据库系统原理\n√\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n√\n√\n√\n√\n√\n√\n软件工程\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n北京航空航天大学本科培养方案\n6\n三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合\n格，达到学校毕业要求的，准予毕业，学校颁发毕业证书；符合学士学位授予条件的，授予学士学\n位。\n毕业总学分：155\n授予学位类型：计算机科学与技术学士学位\n计算机科学与技术专业本科指导性最低学分框架表\n课程模块\n序列\n课程类别\n最低学分要求\n1 年级\n2-4 年级\n学分小计\nI\n基础课程\nA\n数学与自然科学类\n22', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='I\n基础课程\nA\n数学与自然科学类\n22\n7\n46\nB\n工程基础类\n9\n0\nC\n外语类\n4\n4\nII\n通修课程\nD\n思政类\n8.5\n10.5\n41\n军理类\n0\n4\nE\n体育类\n1\n2.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n4\nE\n体育类\n1\n2.5\nK\n素质教育理论必修课\n0\n2.5\nH\n素质教育实践必修课\n0.5\n1.5\nF/G\n素质教育通识限修课\n2.5\n7.5\nIII\n专业课程\nI\n核心专业类\n0\n48\n68\nJ\n一般专业类\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n48\n68\nJ\n一般专业类\n0\n20\n学分小计\n47.5\n107.5\n155\n毕业最低总学分\n155\n注：外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家\n安全、素质教育实践必修课等修读要求见相关文件，其中：\n①劳动教育课程要求：至少选修劳动教育必修课或劳动教育模块学时总数≥32 学时及参加劳动月等活\n动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业\n7\n②素质教育通识限修课：大类概论课、航空航天概论 B 为必修，至少还应包含 3 门人文类通识课（每门\n课程学分不得低于 2 学分；“法律、科技与社会”为必修）。\n③创新创业课程要求：至少选修 3 学分，详见每学期创新创业课程清单，修读要求见相应创新创业学分\n认定办法。\n④全英文课程要求：至少选修 2 学分全英文课程（外语类课程除外）。鼓励学生积极申报学校出国留学\n计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保\n学习成效，所选修课程总数不得超过 2 门。\n⑥一般专业类：至少需要从计算机学院为本专业设置的方向基础类课程清单中选修不少于 3 门课程。\n北京航空航天大学本科培养方案\n8\n四、课程设置与学分分布表\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n基\n础\n课\n程\n数\n学\n与\n自\n然\n科', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='数\n学\n与\n自\n然\n科\n学\n类\nB1A09104A 工科数学分析(1)\nMathematical Analysis for Engineering\n(1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09106A 工科高等代数\nAdvanced Algebra for Engineering (1)\n6\n96\n96', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Advanced Algebra for Engineering (1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09105A 工科数学分析(2)\nMathematical Analysis for Engineering\n(2)\n6\n96\n96\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB1A19104A 基础物理学(信息类)\nFundamental Physics(Information class)\n4\n64\n64\n0\n0\n一\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考试\n全汉语\nB1A19201B 工科大学物理(2)\nEngineering College physics(2)\n4\n64\n64\n0\n0\n二\n秋\n必修\n考试\n全汉语\n工\n程\n基\n础\n类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2\n48\n16\n32\n0\n一\n秋\n必修\n考试\n全汉语\nB1B021150\n电子设计基础训练\nBasic training in electronic design\n2\n56\n8\n48\n0\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考查\n全汉语\nB1B061060\n离散数学(信息类)\nDiscrete Mathematics (Information\nclass)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1B061100\n数据结构与程序设计(信息类)\nData Structures and Programming\n(Information class)\n3\n64', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Data Structures and Programming\n(Information class)\n3\n64\n32\n32\n0\n一\n春\n必修\n考试\n全汉语\n外\n语\n类\nB1C12107A 大学英语 A(1)\nCollege English A (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n秋\n必修\n考试\n全英文\nB1C12108A 大学英语 A(2)\nCollege English A (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207A 大学英语 A(3)\nCollege English A (3)\n2\n32\n32\n0\n0\n二', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208A 大学英语 A(4)\nCollege English A (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试\n全英文\nB1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文\nB1C12108B 大学英语 B(2)\nCollege English B (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207B 大学英语 B(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全英文\nB1C12207B 大学英语 B(3)\nCollege English B (3)\n2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208B 大学英语 B(4)\nCollege English B (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n二\n春\n必修\n考试\n全英文\n通\n思\nB2D281050\n思想道德与法治\nEthic Thought and Rule of Law\n3\n48\n48\n0\n0\n一\n秋\n必修\n考试\n全汉语\n计算机学院·计算机科学与技术专业\n9\n课程\n模块\n课程\n类别', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='9\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content="方式\n授课\n语言\n学年\n学期\n修\n课\n程\n政\n类\nB2D282060\n习近平新时代中国特色社会主\n义思想概论\nIntroduction to Xi Jinping's Thoughts on\nSocialism with Chinese Characteristics\nfor a New Era\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2D281060", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='秋\n必修\n考试\n全汉语\nB2D281060\n中国近现代史纲要\nOutline of Modern Chinese History\n3\n48\n48\n0\n0\n一\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n思\n政\n类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (1)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2D282090\n毛泽东思想和中国特色社会主\n义理论体系概论(2)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Mao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)\n2\n80\n0\n0\n80\n二\n寒假\n必修\n考查\n全汉语\nB2D282070\n马克思主义基本原理\nFundamental Principles of Marxism\n3\n48\n48\n0\n0\n二\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n二\n春\n必修\n考试\n全汉语\nB2D281110\n形势与政策(1)\nSituation and Policy (1)\n0.2\n8\n4\n0\n4\n一\n秋\n必修\n考查\n全汉语\nB2D281120\n形势与政策(2)\nSituation and Policy (2)\n0.3\n8\n4\n0\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n8\n4\n0\n4\n一\n春\n必修\n考查\n全汉语\nB2D282110\n形势与政策(3)\nSituation and Policy (3)\n0.2\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)\n0.3\n8\n8\n0\n0\n二\n春\n必修\n考查\n全汉语\nB2D283110\n形势与政策(5)\nSituation and Policy (5)\n0.2\n8\n8\n0\n0\n三\n秋\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考查\n全汉语\nB2D283120\n形势与政策(6)\nSituation and Policy (6)\n0.3\n8\n8\n0\n0\n三\n春\n必修\n考查\n全汉语\nB2D284110\n形势与政策(7)\nSituation and Policy (7)\n0.2\n8\n8\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.2\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB2D284120\n形势与政策(8)\nSituation and Policy (8)\n0.3\n8\n8\n0\n0\n四\n春\n必修\n考查\n全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n限修≥1\n学分\n考试\n全汉语\nB2D280120\n新中国史\nThe History of the People’s Republic of\nChina\n1\n16\n16\n0\n0\n一至', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280130\n改革开放史\nThe History of the Reform and Opening-\nup\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\n北京航空航天大学本科培养方案\n10\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n军理\n类\nB2D511040\n军事理论\nMilitary Theory\n2\n36', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2D511040\n军事理论\nMilitary Theory\n2\n36\n32\n0\n4\n二\n春\n必修\n考试\n全汉语\nB2D511030\n军事技能\nMilitary Skills\n2\n112\n0\n0\n112\n一\n夏\n必修\n考查\n全汉语\n体\n育\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\n体\n育\n类\nB2E331030\n体育(1)\nPhysical Education (1)\n0.5\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2E331040\n体育(2)\nPhysical Education (2)\n0.5\n32\n32\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.5\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB2E332050\n体育(3)\nPhysical Education (3)\n0.5\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2E332060\n体育(4)\nPhysical Education (4)\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2E332060\n体育(4)\nPhysical Education (4)\n0.5\n32\n32\n0\n0\n二\n春\n必修\n考试\n全汉语\nB2E333070\n体育(5)\nPhysical Education (5)\n0.5\n16\n16\n0\n0\n三\n秋\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考试\n全汉语\nB2E333080\n体育(6)\nPhysical Education (6)\n0.5\n16\n16\n0\n0\n三\n春\n必修\n考试\n全汉语\nB2E334030\n体质健康标准测试\n0.5\n0\n0\n0\n0\n四\n秋', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n0\n0\n四\n秋\n必修\n考试\n全汉语\n通\n修\n课\n程\n素\n质\n教\n育\n实\n践\n必\n修\n课\nB2H511110\n素质教育(博雅课程)(1)\nComprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Comprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16\n4\n0\n12\n一\n秋\n必修\n考查\n全汉语\nB2H511120\n素质教育(博雅课程)(2)\nComprehensive Development\nEducation(Liberal Arts Course )(2)\n0.3\n16\n4\n0\n12\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='12\n一\n春\n必修\n考查\n全汉语\nB2H511130\n素质教育(博雅课程)(3)\nComprehensive Development\nEducation(Liberal Arts Course )(3)\n0.2\n16\n4\n0\n12\n二\n秋\n必修\n考查\n全汉语\nB2H511140\n素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3\n16\n4\n0\n12\n二\n春\n必修\n考查\n全汉语\nB2H511150\n素质教育(博雅课程)(5)\nComprehensive Development\nEducation(Liberal Arts Course )(5)\n0.2\n16\n4\n0\n12\n三\n秋\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4\n0\n12\n三\n秋\n必修\n考查\n全汉语\nB2H511160\n素质教育(博雅课程)(6)\nComprehensive Development\nEducation(Liberal Arts Course )(6)\n0.3\n16\n4\n0\n12\n三\n春\n必修\n考查\n全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development\nEducation(Liberal Arts Course )(7)\n0.2\n16\n4\n0\n12\n四\n秋\n必修\n考查\n全汉语\nB2H511180\n素质教育(博雅课程)(8)\nComprehensive Development\nEducation(Liberal Arts Course )(8)\n0.3\n16\n4\n0\n12\n四', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n16\n4\n0\n12\n四\n春\n必修\n考查\n全汉语\n素质\n教育\n理论\n美育类课程（至少 1.5 学分），各类课程见各学期开课清单\n1.5\n必修\n劳动教育课程（至少 32 学时），劳动教育必修课或劳动教育模块，详见每学期劳动\n教育课程清单\n32\n一至\n四\n秋/春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='一至\n四\n秋/春\n必修\n考查\n全汉语\n计算机学院·计算机科学与技术专业\n11\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n必修\n课\nB2K141010\n国家安全\nNational Security Education\n1\n16\n14\n0\n2\n一至\n三\n秋/春\n必修\n考查\n全汉语\n素', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n秋/春\n必修\n考查\n全汉语\n素\n质\n教\n育\n通\n识\n限\n修\n课\n概论课(大类内部)以下七门：\nB2F020390\n电子信息工程导论\nIntroduction to Electronic Information\nEngineering\n1.5\n24\n24\n0\n0\n一\n秋\n限修，', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='24\n0\n0\n一\n秋\n限修，\n≥1.5 学\n分\n考查\n全汉语\nB2F030360\n自动化科学与电气工程导论\nIntroduction of automation Science and\nelectrical engineering major\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and\nComputer Ethics\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F170210\n仪器科学概览\nOverview of Instrument Science\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n一\n秋\n考查\n全汉语\nB2F210120\n走进软件\nIntroduction to Software\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F390110\n网络空间安全导论\nIntroduction to Cyberspace Security\n1.5\n24\n24\n0\n0\n一', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F491110\n集成电路导论\nIntroduction to Integrated Circuit\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n素质\n教育\n通识\n限修\n课\n人文、经典、社科、科技文明 4 类素质教育课\n1\n秋/春\n限修，\n≥1 学分', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n秋/春\n限修，\n≥1 学分\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n3 门人文类核心通识课（法律、科技与社会为必选）\n2\n32\n32\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n专', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n春\n必修\n考查\n全汉语\n专\n业\n课\n程\n核\n心\n专\n业\n类\nB3J063270\n科研课堂\nScientific Research Training\n2\n32\n0\n0\n32\n二\n秋\n必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)\nInternship\n5\n320\n0\n0\n320\n三\n夏\n必修\n考查\n全汉语\nB3I064410\n毕业设计\nGraduation Project\n8\n640\n0\n0\n640\n四\n春\n必修\n考查', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n640\n四\n春\n必修\n考查\n全汉语\nB3I061142\n离散数学 2\nDiscrete Mathematics (2)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB3I062260\n计算机组成\nPrinciples of Computer Organization\n5.5\n112\n64\n48', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Principles of Computer Organization\n5.5\n112\n64\n48\n0\n二\n秋\n必修\n考试\n全汉语\n北京航空航天大学本科培养方案\n12\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\nB3I062270\n操作系统\nOperating Systems\n4.5\n96\n48\n48\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='48\n0\n二\n春\n必修\n考试\n全汉语\nB3I063120\n编译技术\nCompiler Technology\n4.5\n96\n48\n48\n0\n三\n秋\n必修\n考试\n全汉语\nB3I062180\n面向对象设计与构造\nObject-oriented Design and\nConstruction\n2.5\n48\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Object-oriented Design and\nConstruction\n2.5\n48\n32\n16\n0\n二\n春\n必修\n考试\n全汉语\nB3I063320\n算法设计与分析\nThe Design and Analysis of Algorithm\n2\n32\n32\n0\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063180\n数据库系统原理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB3I063180\n数据库系统原理\nPrinciples of Database Systems\n4\n80\n48\n32\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063130\n软件工程\nSoftware Engineering\n2\n32\n32\n0\n0\n三\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063210\n计算机网络\nComputer Network\n2\n32\n32\n0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063220\n计算机网络实验\nComputer Network Experiment\n1\n32\n0\n32\n0\n三', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n32\n0\n32\n0\n三\n春\n必修\n考试\n全汉语\nB3I064120\n计算机科学方法论\nMethodology of Computer Science\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n一\n般\n专\n业\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\n一\n般\n专\n业\n类\nB3J062610\n职业规划与选择讲座\nProfession Planning and Choice\n0.5\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB3J063610\n学科技术前沿讲座\nSeminars on the Development of\nComputer Science\n1\n16\n16\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Computer Science\n1\n16\n16\n0\n0\n三\n春\n必修\n考查\n全汉语\nB3J064610\n求职辅导系列讲座\nSerial Lectures on How to Seek\nEmployment\n0.5\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB3J062410\n实践与展示(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3J062410\n实践与展示(1)\nPractice and Presentation(Ⅰ)\n1\n32\n0\n0\n32\n二\n春\n必修\n考查\n全汉语\nB3J062420\n实践与展示(2)\nPractice and Presentation (2)\n1\n32\n0\n0\n32\n三\n春\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n32\n三\n春\n必修\n考查\n全汉语\n共计选修 16 学分的专业选修课。\n16\n计算机学院·计算机科学与技术专业\n13\n五、核心课程先修逻辑关系图\n六、专业准入办法一览表\n为更好的体现学生结合自身学习特点和兴趣以更好的选择专业，本专业分别在 1 年级春季学\n期、2 年级秋季学期和 2 年级春季学期即将结束时转入转出申请。\n对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业\n学习的候选者，同时结合候选者其他相关课程的学习决定其是否同级转专业还是降级转专业。\n关于具体准入条件，请参考《北京航空航天大学计算机学院转专业管理规定》。该文件可以在\n学校转专业相关网页查阅到。\n七、毕业生未来发展图\n除了升学深造外，由于计算机专业社会需求广泛，因此本专业毕业生具有广泛的就业空间及发\n展可能。本培养方案仅给出部分可能的发展规划，具体内容参见下表。\n主分类\n次分类\n描述\n就业\nIT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='IT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主\n北京航空航天大学本科培养方案\n14\n自主创业\n升学\n国内深造\n软件与理论方向：大学、中科院计算所、中科院软件所\n体系结构方向：大学、中科院计算所、国防系统研究所\n计算机应用方向：大学、中科院相关院所、国防系统研究所\n出国深造\n国外大学攻读硕士学位、博士学位', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa5500b10> 111
cuda:2
[Document(page_content='北京航空航天大学\n计算机学院\n2021 级本科培养方案\n（计算机科学与技术专业）\n计算机学院·计算机科学与技术专业\n1\n计算机学院\n学院简介\n北京航空航天大学 1958 年建立计算机专业，1978 年成立计算机科学与工程系，2002 年组\n建计算机学院。学院遵循“面向国家需求，瞄准学科前沿，汇聚优秀人才，争创国际知名”的发展\n战略，形成了人才培养与科学研究相互促进、协调发展的格局。\n学院现有教职工 163 人，其中中国科学院院士 3 人，中国工程院院士 1 人，英国皇家学会院\n士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家\n工程技术中心、１个国家级国际科技合作基地等十个省部级以上科研教学基地；获国家自然科学二\n等奖 1 项，国家科技进步一等奖 2 项，其他国家级科技奖项 17 项；计算机科学与技术、软件工程\n在 2017 年全国一级学科评估分别为 A 和 A+，双获国家“双一流”建设学科。\n学院设置计算机科学与技术、虚拟现实技术两个本科专业；获国家教学成果一等奖 1 项、二等\n奖 4 项，首届全国优秀教材二等奖 1 项，国家级教学团队 1 个，国家级教学名师 1 人，国家级精\n品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036\n人，博士 598 人，培养质量受到社会广泛好评。\n院      长 ：王蕴红\n专业负责人 ：王蕴红\n教学副院长 ：高小鹏\n教 学 秘 书 ：丁映中、纵绮\n北京航空航天大学本科培养方案\n2\n计算机科学与技术专业\n一、专业简介\n本专业始终坚持立德树人根本任务，落实“五育”并举，践行“三全育人”要求，遵循学校“厚\n植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型\n与工程型相结合”的宽口径人才，秉承“寓教于研”办学理念并依托学科优势构建高质量课程体系，\n重视数学及学科基础理论、专业核心能力以及高水平人才所应具备的人文素养。主要培养特点如下：\n1. 重视核心能力培养。专业必修课分为基础理论、系统能力、软件能力三大模块，普遍具有\n大课重课特性，培养学生具备专业核心能力，为后续课程学习奠定坚实基础。\n2. 自主规划专业选修。本专业实施完全学分制培养模式，引导学生根据自我规划和学习兴趣\n制定个性化的专业课程选修方案，鼓励学生在某一专业方向上的系统性学习。\n3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要\n素，增强学生公民意识及责任感、提升思辨力，让学生建立工程伦理并了解技术、社会与\n自然三者间的作用关系。2～3 年级设置连贯性的科技实践与成果表达训练环节，培养创\n新意识及有效表达能力。\n二、培养目标和毕业要求\n（一）培养目标\n以学校“培养引领和支撑国家重大战略需求的领军领导人才”的人才培养战略总目标为指导，\n培养具有良好人文素养、强烈的事业心、使命感及担当精神，具有创新精神、全球化视野、终身学\n习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机\n领域复杂工程问题的能力，具有团队合作与组织管理能力，能参与国际竞争的计算机专业高水平人\n才。\n学生毕业 5 年后：\n计算机学院·计算机科学与技术专业\n3\n1. 能就专业相关的工程问题，综合考虑技术、经济、法律、伦理等因素，分析、制定解决方\n案，并管理项目的实施；\n2. 能在职业发展中具有担当精神、行动力、感染力和领导力；\n3. 能与国内外同行、客户和公众有效沟通；\n4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求\n1. 工程知识：具备较扎实的数学、自然科学知识，系统掌握计算机领域的工程基础和专业知识，\n能够将这些知识用于解决计算机领域复杂工程问题。\n2. 问题分析：能够应用数学、自然科学基本原理与专业知识，并通过文献研究，识别、表达、\n分析计算机领域复杂工程问题，以获得有效结论。\n3. 设计/开发解决方案：能够设计针对计算机领域复杂工程问题的解决方案，设计满足特定需\n求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。\n4. 研究：能够基于专业知识对计算机领域复杂工程问题进行研究，包括文献调研、设计实验、\n分析与解释数据、并通过信息综合得到合理有效的结论。\n5. 使用现代工具：能够在本专业工程实践中，选择与使用合理有效的技术、软硬件资源、软硬\n件开发工具和信息技术工具，并了解其局限性。\n6. 工程与社会：具有追求创新的态度和意识，掌握基本的创新方法，以及综合运用理论和技术\n手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。\n7. 环境和可持续发展：了解与本专业相关的职业和行业的生产、设计、研究与开发、环境保护\n和可持续发展等方面的方针、政策和法津、法规；能够正确认识本专业工程实践对环境和社会可持\n续发展的影响，合理评价本专业工程实践解决方案对社会、健康、安全、法律及文化的影响。\n北京航空航天大学本科培养方案\n4\n8. 职业规范：具有良好的人文素养、社会责任感，能够在本专业工程实践中理解并遵守工程职\n业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。\n10. 沟通：能够就复杂计算机工程问题与业界同行及社会公众进行有效沟通与交流，包括撰写\n报告和设计文稿、陈述发言、清晰表达个人见解等，并具备一定的国际视野，能够在跨文化背景下\n进行沟通和交流。\n11. 项目管理：具有一定的组织与工程管理能力、表达与人际交往能力，能够在多学科背景下\n的团队中发挥重要作用。\n12. 终身学习：具有自主学习和终身学习的意识，具有不断学习和适应本领域快速发展的能力；\n具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图\n毕业要求\n主要专业必修课\n1)\n工程知识\n2)\n问题分析\n3)\n解决方案\n4)\n研究\n5)\n工具\n6)\n工程与社\n会\n7)\n环境与可\n持续发展\n8)\n职业规范\n9)\n个人与团', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='持续发展\n8)\n职业规范\n9)\n个人与团\n队\n10)\n沟通\n11)\n项目管理\n12)\n终身学习\n专业\n导论\n导论(信息类)\n√\n√\n√\n√\n√\n√\n基础\n理论\n离散数学(信息类)\n√\n√\n离散数学(2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='理论\n离散数学(信息类)\n√\n√\n离散数学(2)\n√\n√\n系统\n能力\n计算机组成\n√\n√\n√\n√\n√\n√\n操作系统\n√\n√\n√\n√\n√\n√\n√\n√\n编译技术\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n编译技术\n√\n√\n√\n√\n√\n√\n√\n√\n计算机网络\n√\n√\n√\n√\n√\n√\n√\n√\n软件\n能力\n数据结构与程序设\n计(信息类)\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计(信息类)\n√\n√\n√\n√\n√\n面向对象设计与构\n造\n√\n√\n√\n√\n√\n√\n√\n√\n√\n算法设计与分析\n√\n√\n√\n√\n√\n数据库系统原理\n√\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n√\n√\n√\n√\n√\n√\n软件工程\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n北京航空航天大学本科培养方案\n6\n三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合\n格，达到学校毕业要求的，准予毕业，学校颁发毕业证书；符合学士学位授予条件的，授予学士学\n位。\n毕业总学分：155\n授予学位类型：计算机科学与技术学士学位\n计算机科学与技术专业本科指导性最低学分框架表\n课程模块\n序列\n课程类别\n最低学分要求\n1 年级\n2-4 年级\n学分小计\nI\n基础课程\nA\n数学与自然科学类\n22', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='I\n基础课程\nA\n数学与自然科学类\n22\n7\n46\nB\n工程基础类\n9\n0\nC\n外语类\n4\n4\nII\n通修课程\nD\n思政类\n8.5\n10.5\n41\n军理类\n0\n4\nE\n体育类\n1\n2.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n4\nE\n体育类\n1\n2.5\nK\n素质教育理论必修课\n0\n2.5\nH\n素质教育实践必修课\n0.5\n1.5\nF/G\n素质教育通识限修课\n2.5\n7.5\nIII\n专业课程\nI\n核心专业类\n0\n48\n68\nJ\n一般专业类\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n48\n68\nJ\n一般专业类\n0\n20\n学分小计\n47.5\n107.5\n155\n毕业最低总学分\n155\n注：外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家\n安全、素质教育实践必修课等修读要求见相关文件，其中：\n①劳动教育课程要求：至少选修劳动教育必修课或劳动教育模块学时总数≥32 学时及参加劳动月等活\n动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业\n7\n②素质教育通识限修课：大类概论课、航空航天概论 B 为必修，至少还应包含 3 门人文类通识课（每门\n课程学分不得低于 2 学分；“法律、科技与社会”为必修）。\n③创新创业课程要求：至少选修 3 学分，详见每学期创新创业课程清单，修读要求见相应创新创业学分\n认定办法。\n④全英文课程要求：至少选修 2 学分全英文课程（外语类课程除外）。鼓励学生积极申报学校出国留学\n计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保\n学习成效，所选修课程总数不得超过 2 门。\n⑥一般专业类：至少需要从计算机学院为本专业设置的方向基础类课程清单中选修不少于 3 门课程。\n北京航空航天大学本科培养方案\n8\n四、课程设置与学分分布表\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n基\n础\n课\n程\n数\n学\n与\n自\n然\n科', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='数\n学\n与\n自\n然\n科\n学\n类\nB1A09104A 工科数学分析(1)\nMathematical Analysis for Engineering\n(1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09106A 工科高等代数\nAdvanced Algebra for Engineering (1)\n6\n96\n96', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Advanced Algebra for Engineering (1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09105A 工科数学分析(2)\nMathematical Analysis for Engineering\n(2)\n6\n96\n96\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB1A19104A 基础物理学(信息类)\nFundamental Physics(Information class)\n4\n64\n64\n0\n0\n一\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考试\n全汉语\nB1A19201B 工科大学物理(2)\nEngineering College physics(2)\n4\n64\n64\n0\n0\n二\n秋\n必修\n考试\n全汉语\n工\n程\n基\n础\n类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2\n48\n16\n32\n0\n一\n秋\n必修\n考试\n全汉语\nB1B021150\n电子设计基础训练\nBasic training in electronic design\n2\n56\n8\n48\n0\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考查\n全汉语\nB1B061060\n离散数学(信息类)\nDiscrete Mathematics (Information\nclass)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1B061100\n数据结构与程序设计(信息类)\nData Structures and Programming\n(Information class)\n3\n64', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Data Structures and Programming\n(Information class)\n3\n64\n32\n32\n0\n一\n春\n必修\n考试\n全汉语\n外\n语\n类\nB1C12107A 大学英语 A(1)\nCollege English A (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n秋\n必修\n考试\n全英文\nB1C12108A 大学英语 A(2)\nCollege English A (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207A 大学英语 A(3)\nCollege English A (3)\n2\n32\n32\n0\n0\n二', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208A 大学英语 A(4)\nCollege English A (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试\n全英文\nB1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文\nB1C12108B 大学英语 B(2)\nCollege English B (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207B 大学英语 B(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全英文\nB1C12207B 大学英语 B(3)\nCollege English B (3)\n2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208B 大学英语 B(4)\nCollege English B (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n二\n春\n必修\n考试\n全英文\n通\n思\nB2D281050\n思想道德与法治\nEthic Thought and Rule of Law\n3\n48\n48\n0\n0\n一\n秋\n必修\n考试\n全汉语\n计算机学院·计算机科学与技术专业\n9\n课程\n模块\n课程\n类别', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='9\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content="方式\n授课\n语言\n学年\n学期\n修\n课\n程\n政\n类\nB2D282060\n习近平新时代中国特色社会主\n义思想概论\nIntroduction to Xi Jinping's Thoughts on\nSocialism with Chinese Characteristics\nfor a New Era\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2D281060", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='秋\n必修\n考试\n全汉语\nB2D281060\n中国近现代史纲要\nOutline of Modern Chinese History\n3\n48\n48\n0\n0\n一\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n思\n政\n类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (1)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2D282090\n毛泽东思想和中国特色社会主\n义理论体系概论(2)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Mao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)\n2\n80\n0\n0\n80\n二\n寒假\n必修\n考查\n全汉语\nB2D282070\n马克思主义基本原理\nFundamental Principles of Marxism\n3\n48\n48\n0\n0\n二\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n二\n春\n必修\n考试\n全汉语\nB2D281110\n形势与政策(1)\nSituation and Policy (1)\n0.2\n8\n4\n0\n4\n一\n秋\n必修\n考查\n全汉语\nB2D281120\n形势与政策(2)\nSituation and Policy (2)\n0.3\n8\n4\n0\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n8\n4\n0\n4\n一\n春\n必修\n考查\n全汉语\nB2D282110\n形势与政策(3)\nSituation and Policy (3)\n0.2\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)\n0.3\n8\n8\n0\n0\n二\n春\n必修\n考查\n全汉语\nB2D283110\n形势与政策(5)\nSituation and Policy (5)\n0.2\n8\n8\n0\n0\n三\n秋\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考查\n全汉语\nB2D283120\n形势与政策(6)\nSituation and Policy (6)\n0.3\n8\n8\n0\n0\n三\n春\n必修\n考查\n全汉语\nB2D284110\n形势与政策(7)\nSituation and Policy (7)\n0.2\n8\n8\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.2\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB2D284120\n形势与政策(8)\nSituation and Policy (8)\n0.3\n8\n8\n0\n0\n四\n春\n必修\n考查\n全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n限修≥1\n学分\n考试\n全汉语\nB2D280120\n新中国史\nThe History of the People’s Republic of\nChina\n1\n16\n16\n0\n0\n一至', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280130\n改革开放史\nThe History of the Reform and Opening-\nup\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\n北京航空航天大学本科培养方案\n10\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n军理\n类\nB2D511040\n军事理论\nMilitary Theory\n2\n36', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2D511040\n军事理论\nMilitary Theory\n2\n36\n32\n0\n4\n二\n春\n必修\n考试\n全汉语\nB2D511030\n军事技能\nMilitary Skills\n2\n112\n0\n0\n112\n一\n夏\n必修\n考查\n全汉语\n体\n育\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\n体\n育\n类\nB2E331030\n体育(1)\nPhysical Education (1)\n0.5\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2E331040\n体育(2)\nPhysical Education (2)\n0.5\n32\n32\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.5\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB2E332050\n体育(3)\nPhysical Education (3)\n0.5\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2E332060\n体育(4)\nPhysical Education (4)\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2E332060\n体育(4)\nPhysical Education (4)\n0.5\n32\n32\n0\n0\n二\n春\n必修\n考试\n全汉语\nB2E333070\n体育(5)\nPhysical Education (5)\n0.5\n16\n16\n0\n0\n三\n秋\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考试\n全汉语\nB2E333080\n体育(6)\nPhysical Education (6)\n0.5\n16\n16\n0\n0\n三\n春\n必修\n考试\n全汉语\nB2E334030\n体质健康标准测试\n0.5\n0\n0\n0\n0\n四\n秋', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n0\n0\n四\n秋\n必修\n考试\n全汉语\n通\n修\n课\n程\n素\n质\n教\n育\n实\n践\n必\n修\n课\nB2H511110\n素质教育(博雅课程)(1)\nComprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Comprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16\n4\n0\n12\n一\n秋\n必修\n考查\n全汉语\nB2H511120\n素质教育(博雅课程)(2)\nComprehensive Development\nEducation(Liberal Arts Course )(2)\n0.3\n16\n4\n0\n12\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='12\n一\n春\n必修\n考查\n全汉语\nB2H511130\n素质教育(博雅课程)(3)\nComprehensive Development\nEducation(Liberal Arts Course )(3)\n0.2\n16\n4\n0\n12\n二\n秋\n必修\n考查\n全汉语\nB2H511140\n素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3\n16\n4\n0\n12\n二\n春\n必修\n考查\n全汉语\nB2H511150\n素质教育(博雅课程)(5)\nComprehensive Development\nEducation(Liberal Arts Course )(5)\n0.2\n16\n4\n0\n12\n三\n秋\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4\n0\n12\n三\n秋\n必修\n考查\n全汉语\nB2H511160\n素质教育(博雅课程)(6)\nComprehensive Development\nEducation(Liberal Arts Course )(6)\n0.3\n16\n4\n0\n12\n三\n春\n必修\n考查\n全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development\nEducation(Liberal Arts Course )(7)\n0.2\n16\n4\n0\n12\n四\n秋\n必修\n考查\n全汉语\nB2H511180\n素质教育(博雅课程)(8)\nComprehensive Development\nEducation(Liberal Arts Course )(8)\n0.3\n16\n4\n0\n12\n四', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n16\n4\n0\n12\n四\n春\n必修\n考查\n全汉语\n素质\n教育\n理论\n美育类课程（至少 1.5 学分），各类课程见各学期开课清单\n1.5\n必修\n劳动教育课程（至少 32 学时），劳动教育必修课或劳动教育模块，详见每学期劳动\n教育课程清单\n32\n一至\n四\n秋/春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='一至\n四\n秋/春\n必修\n考查\n全汉语\n计算机学院·计算机科学与技术专业\n11\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n必修\n课\nB2K141010\n国家安全\nNational Security Education\n1\n16\n14\n0\n2\n一至\n三\n秋/春\n必修\n考查\n全汉语\n素', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n秋/春\n必修\n考查\n全汉语\n素\n质\n教\n育\n通\n识\n限\n修\n课\n概论课(大类内部)以下七门：\nB2F020390\n电子信息工程导论\nIntroduction to Electronic Information\nEngineering\n1.5\n24\n24\n0\n0\n一\n秋\n限修，', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='24\n0\n0\n一\n秋\n限修，\n≥1.5 学\n分\n考查\n全汉语\nB2F030360\n自动化科学与电气工程导论\nIntroduction of automation Science and\nelectrical engineering major\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and\nComputer Ethics\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F170210\n仪器科学概览\nOverview of Instrument Science\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n一\n秋\n考查\n全汉语\nB2F210120\n走进软件\nIntroduction to Software\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F390110\n网络空间安全导论\nIntroduction to Cyberspace Security\n1.5\n24\n24\n0\n0\n一', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F491110\n集成电路导论\nIntroduction to Integrated Circuit\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n素质\n教育\n通识\n限修\n课\n人文、经典、社科、科技文明 4 类素质教育课\n1\n秋/春\n限修，\n≥1 学分', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n秋/春\n限修，\n≥1 学分\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n3 门人文类核心通识课（法律、科技与社会为必选）\n2\n32\n32\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n专', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n春\n必修\n考查\n全汉语\n专\n业\n课\n程\n核\n心\n专\n业\n类\nB3J063270\n科研课堂\nScientific Research Training\n2\n32\n0\n0\n32\n二\n秋\n必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)\nInternship\n5\n320\n0\n0\n320\n三\n夏\n必修\n考查\n全汉语\nB3I064410\n毕业设计\nGraduation Project\n8\n640\n0\n0\n640\n四\n春\n必修\n考查', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n640\n四\n春\n必修\n考查\n全汉语\nB3I061142\n离散数学 2\nDiscrete Mathematics (2)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB3I062260\n计算机组成\nPrinciples of Computer Organization\n5.5\n112\n64\n48', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Principles of Computer Organization\n5.5\n112\n64\n48\n0\n二\n秋\n必修\n考试\n全汉语\n北京航空航天大学本科培养方案\n12\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\nB3I062270\n操作系统\nOperating Systems\n4.5\n96\n48\n48\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='48\n0\n二\n春\n必修\n考试\n全汉语\nB3I063120\n编译技术\nCompiler Technology\n4.5\n96\n48\n48\n0\n三\n秋\n必修\n考试\n全汉语\nB3I062180\n面向对象设计与构造\nObject-oriented Design and\nConstruction\n2.5\n48\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Object-oriented Design and\nConstruction\n2.5\n48\n32\n16\n0\n二\n春\n必修\n考试\n全汉语\nB3I063320\n算法设计与分析\nThe Design and Analysis of Algorithm\n2\n32\n32\n0\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063180\n数据库系统原理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB3I063180\n数据库系统原理\nPrinciples of Database Systems\n4\n80\n48\n32\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063130\n软件工程\nSoftware Engineering\n2\n32\n32\n0\n0\n三\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063210\n计算机网络\nComputer Network\n2\n32\n32\n0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063220\n计算机网络实验\nComputer Network Experiment\n1\n32\n0\n32\n0\n三', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n32\n0\n32\n0\n三\n春\n必修\n考试\n全汉语\nB3I064120\n计算机科学方法论\nMethodology of Computer Science\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n一\n般\n专\n业\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\n一\n般\n专\n业\n类\nB3J062610\n职业规划与选择讲座\nProfession Planning and Choice\n0.5\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB3J063610\n学科技术前沿讲座\nSeminars on the Development of\nComputer Science\n1\n16\n16\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Computer Science\n1\n16\n16\n0\n0\n三\n春\n必修\n考查\n全汉语\nB3J064610\n求职辅导系列讲座\nSerial Lectures on How to Seek\nEmployment\n0.5\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB3J062410\n实践与展示(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3J062410\n实践与展示(1)\nPractice and Presentation(Ⅰ)\n1\n32\n0\n0\n32\n二\n春\n必修\n考查\n全汉语\nB3J062420\n实践与展示(2)\nPractice and Presentation (2)\n1\n32\n0\n0\n32\n三\n春\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n32\n三\n春\n必修\n考查\n全汉语\n共计选修 16 学分的专业选修课。\n16\n计算机学院·计算机科学与技术专业\n13\n五、核心课程先修逻辑关系图\n六、专业准入办法一览表\n为更好的体现学生结合自身学习特点和兴趣以更好的选择专业，本专业分别在 1 年级春季学\n期、2 年级秋季学期和 2 年级春季学期即将结束时转入转出申请。\n对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业\n学习的候选者，同时结合候选者其他相关课程的学习决定其是否同级转专业还是降级转专业。\n关于具体准入条件，请参考《北京航空航天大学计算机学院转专业管理规定》。该文件可以在\n学校转专业相关网页查阅到。\n七、毕业生未来发展图\n除了升学深造外，由于计算机专业社会需求广泛，因此本专业毕业生具有广泛的就业空间及发\n展可能。本培养方案仅给出部分可能的发展规划，具体内容参见下表。\n主分类\n次分类\n描述\n就业\nIT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='IT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主\n北京航空航天大学本科培养方案\n14\n自主创业\n升学\n国内深造\n软件与理论方向：大学、中科院计算所、中科院软件所\n体系结构方向：大学、中科院计算所、国防系统研究所\n计算机应用方向：大学、中科院相关院所、国防系统研究所\n出国深造\n国外大学攻读硕士学位、博士学位', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpoppk3ku_/06-计算机学院-计算机科学与技术专业.pdf'})]
cuda:2
院长是谁 tmpoppk3ku_
[History(role='user', content='计算机毕业要修几学分'), History(role='assistant', content='根据已知信息，计算机科学与技术专业的基本学制为4年，毕业总学分要求为155分。在课程模块中，包括基础课程、一般专业类课程、外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等。具体的课程学分要求如下：\n\n- 基础课程：22学分\n- 一般专业类课程：20学分\n-外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等：0-15学分\n\n因此，计算机毕业要修的总学分大于等于155分，具体取决于你选择了哪些课程。')]
cuda:2
[1.165998, 1.2160115, 1.2274582, 1.2476966, 1.2643951, 1.2885139]
院长是谁 tmpoppk3ku_
[History(role='user', content='计算机毕业要修几学分'), History(role='assistant', content='根据已知信息，计算机科学与技术专业的基本学制为4年，毕业总学分要求为155分。在课程模块中，包括基础课程、一般专业类课程、外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等。具体的课程学分要求如下：\n\n- 基础课程：22学分\n- 一般专业类课程：20学分\n-外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等：0-15学分\n\n因此，计算机毕业要修的总学分大于等于155分，具体取决于你选择了哪些课程。')]
cuda:2
[1.165998, 1.2160115, 1.2274582, 1.2476966, 1.2643951, 1.2885139]
高小鹏呢 tmpoppk3ku_
[History(role='user', content='计算机毕业要修几学分'), History(role='assistant', content='根据已知信息，计算机科学与技术专业的基本学制为4年，毕业总学分要求为155分。在课程模块中，包括基础课程、一般专业类课程、外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等。具体的课程学分要求如下：\n\n- 基础课程：22学分\n- 一般专业类课程：20学分\n-外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等：0-15学分\n\n因此，计算机毕业要修的总学分大于等于155分，具体取决于你选择了哪些课程。'), History(role='user', content='院长是谁'), History(role='assistant', content='院长是王蕴红。')]
cuda:2
[1.2560002, 1.2832595, 1.2935587, 1.3017001, 1.3099164, 1.3176091]
高小鹏呢 tmpoppk3ku_
[History(role='user', content='计算机毕业要修几学分'), History(role='assistant', content='根据已知信息，计算机科学与技术专业的基本学制为4年，毕业总学分要求为155分。在课程模块中，包括基础课程、一般专业类课程、外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等。具体的课程学分要求如下：\n\n- 基础课程：22学分\n- 一般专业类课程：20学分\n-外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家安全、素质教育实践必修课等：0-15学分\n\n因此，计算机毕业要修的总学分大于等于155分，具体取决于你选择了哪些课程。'), History(role='user', content='院长是谁'), History(role='assistant', content='院长是王蕴红。')]
cuda:2
[1.2560002, 1.2832595, 1.2935587, 1.3017001, 1.3099164, 1.3176091]
[UploadFile(filename='端口镜像实验.pdf', size=665217, headers=Headers({'content-disposition': 'form-data; name="files"; filename="ç«¯å\x8f£é\x95\x9cå\x83\x8få®\x9eéª\x8c.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp91lugvue, tmp91lugvue
File: 端口镜像实验.pdf, msg: 成功上传文件 端口镜像实验.pdf, docs: [Document(page_content='端口镜像（port Mirroring）实验\n1 端口镜像简介\n端口镜像（port Mirroring）功能是指在交换机或路由器上，将一个或多个源\n端口的数据流量转发到某一个指定端口来实现对网络的监听，指定端口称之为\n“镜像端口”或“目的端口”，在不严重影响源端口正常吞吐流量的情况下，可以通\n过镜像端口对网络的流量进行监控分析。\n端口镜像功能简单地说就是将被监控流量镜像到监控端口，以便对被监控流\n量进行故障定位、流量分析、流量备份等，监控端口一般直接与监控主机等相连。\n例如，为了方便对一个或多个网络接口的流量进行分析，可以通过配置交换', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='例如，为了方便对一个或多个网络接口的流量进行分析，可以通过配置交换\n机或路由器来把一个或多个端口的数据转发到某一个端口，即镜像端口，将入侵\n检测 IDS 产品、网络分析仪等设备与镜像端口连接，来实现对网络的监听。\n端口镜像功能是对网络流量监控的一个有效的手段，对监控流量的分析既可\n以进行安全性的检查，同时也能及时地在网络发生故障时进行准确的定位。\n2 基本概念\n交换机把某一个端口接收或发送的数据帧完全相同的复制给另一个端口；其\n中被复制的端口称为镜像源端口，复制的端口称为镜像目的端口。\n2.1 镜像源', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='中被复制的端口称为镜像源端口，复制的端口称为镜像目的端口。\n2.1 镜像源\n镜像源是指被监控的对象，该对象为端口，我们将之称为源端口。经由被监\n控的对象收发的报文会被复制一份到与数据监测设备相连的端口，用户就可以对\n这些报文（称为镜像报文）进行监控和分析了。镜像源所在的设备就称为源设备。\n2.2 镜像目的\n镜像目的是指镜像报文所要到达的目的地，即与数据监测设备相连的那个端\n口，我们称之为目的端口，与目的端口相连的设备就称为目的设备。目的端口会\n将镜像报文转发给与之相连的数据监测目的设备。\n2.3 镜像方向', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='将镜像报文转发给与之相连的数据监测目的设备。\n2.3 镜像方向\n镜像方向是指在镜像源上可复制哪些方向的报文：\n入方向：是指仅复制镜像源收到的报文。\n出方向：是指仅复制镜像源发出的报文。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='双向：是指对镜像源收到和发出的报文都进行复制。 2.4 镜像组 镜像组是一个逻辑上的概念，镜像源和镜像目的都要属于某一个镜像组。根 据具体的实现方式不同，镜像组可分为本地镜像组、远程源镜像组和远程目的镜 像组三类，本实验只涉及本地镜像组。 远程源镜像组、远程目的镜像组、反射端口、出端口和远程镜像 VLAN 等相 关概念，请大家参考华为、华三、思科的相关技术文档资料。 3 端口镜像的分类和实现方式 根据镜像源与镜像目的是否位于同一台设备上，可以将端口镜像分为本地端 口镜像和远程端口镜像两大类。本实验重点介绍最基础的本地端口镜像。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='本实验重点介绍最基础的本地端口镜像。 3.1 本地端口镜像 当源设备与数据监测设备直接相连时，源设备可以同时作为目的设备，即由 本设备将镜像报文转发至数据检测设备，这种方式实现的端口镜像称为本地端口 镜像。对于本地端口镜像，镜像源和镜像目的属于同一台设备上的同一个镜像组， 该镜像组称为本地镜像组。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='图 1 本地端口镜像示意图 如图 1 所示，现在需要设备将进入端口 GigabitEthernet1/0/1 的报文复制 一份，从端口 GigabitEthernet1/0/2 将报文转发给数据监测设备。为满足该需求， 可以配置本地镜像组，其中源端口为 GigabitEthernet1/0/1，镜像方向为入方向， 目的端口为 GigabitEthernet1/0/2。\n4 本地端口镜像配置实验 4.1 组网需求 交换机 Device 通过端口 GigabitEthernet1/0/1 和 GigabitEthernet1/0/2 分别 连接市场部和技术部，并通过端口 GigabitEthernet1/0/3 连接监控服务器 Server。 通过配置源端口方式的本地端口镜像，使 Server 可以监控所有进、出市场 部和技术部的报文。 4.2 组网图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='图 2 本地端口镜像配置组网图 4.3 配置步骤 # 创建本地镜像组 1。 <Device> system-view [Device] mirroring-group 1 local # 配置本地镜像组 1 的源端口为 GigabitEthernet1/0/1 和 GigabitEthernet1/0/2，对源端口收 发的报文都进行镜像，目的端口为 GigabitEthernet1/0/3。 [Device] mirroring-group 1 mirroring-port gigabitethernet 1/0/1 gigabitethernet 1/0/2 both [Device] mirroring-group 1 monitor-port gigabitethernet 1/0/3 # 在目的端口 GigabitEthernet1/0/3 上关闭生成树协议。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='[Device] interface gigabitethernet 1/0/3 [Device-GigabitEthernet1/0/3] undo stp enable [Device-GigabitEthernet1/0/3] quit 4.4 验证配置 # 显示所有镜像组的配置信息。 [Device] display mirroring-group all Mirroring group 1: Type: Local Status: Active', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='Mirroring port: GigabitEthernet1/0/1 Both GigabitEthernet1/0/2 Both Monitor port: GigabitEthernet1/0/3 所有主机启动 Wireshark 软件截获报文，PCA ping PCB，查看 Server PCC 截获 的报文，验证 Server PCC 是否能够监控所有进、出市场部和技术部的报文。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc7b7690> 111
cuda:2
[Document(page_content='端口镜像（port Mirroring）实验\n1 端口镜像简介\n端口镜像（port Mirroring）功能是指在交换机或路由器上，将一个或多个源\n端口的数据流量转发到某一个指定端口来实现对网络的监听，指定端口称之为\n“镜像端口”或“目的端口”，在不严重影响源端口正常吞吐流量的情况下，可以通\n过镜像端口对网络的流量进行监控分析。\n端口镜像功能简单地说就是将被监控流量镜像到监控端口，以便对被监控流\n量进行故障定位、流量分析、流量备份等，监控端口一般直接与监控主机等相连。\n例如，为了方便对一个或多个网络接口的流量进行分析，可以通过配置交换', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='例如，为了方便对一个或多个网络接口的流量进行分析，可以通过配置交换\n机或路由器来把一个或多个端口的数据转发到某一个端口，即镜像端口，将入侵\n检测 IDS 产品、网络分析仪等设备与镜像端口连接，来实现对网络的监听。\n端口镜像功能是对网络流量监控的一个有效的手段，对监控流量的分析既可\n以进行安全性的检查，同时也能及时地在网络发生故障时进行准确的定位。\n2 基本概念\n交换机把某一个端口接收或发送的数据帧完全相同的复制给另一个端口；其\n中被复制的端口称为镜像源端口，复制的端口称为镜像目的端口。\n2.1 镜像源', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='中被复制的端口称为镜像源端口，复制的端口称为镜像目的端口。\n2.1 镜像源\n镜像源是指被监控的对象，该对象为端口，我们将之称为源端口。经由被监\n控的对象收发的报文会被复制一份到与数据监测设备相连的端口，用户就可以对\n这些报文（称为镜像报文）进行监控和分析了。镜像源所在的设备就称为源设备。\n2.2 镜像目的\n镜像目的是指镜像报文所要到达的目的地，即与数据监测设备相连的那个端\n口，我们称之为目的端口，与目的端口相连的设备就称为目的设备。目的端口会\n将镜像报文转发给与之相连的数据监测目的设备。\n2.3 镜像方向', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='将镜像报文转发给与之相连的数据监测目的设备。\n2.3 镜像方向\n镜像方向是指在镜像源上可复制哪些方向的报文：\n入方向：是指仅复制镜像源收到的报文。\n出方向：是指仅复制镜像源发出的报文。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='双向：是指对镜像源收到和发出的报文都进行复制。 2.4 镜像组 镜像组是一个逻辑上的概念，镜像源和镜像目的都要属于某一个镜像组。根 据具体的实现方式不同，镜像组可分为本地镜像组、远程源镜像组和远程目的镜 像组三类，本实验只涉及本地镜像组。 远程源镜像组、远程目的镜像组、反射端口、出端口和远程镜像 VLAN 等相 关概念，请大家参考华为、华三、思科的相关技术文档资料。 3 端口镜像的分类和实现方式 根据镜像源与镜像目的是否位于同一台设备上，可以将端口镜像分为本地端 口镜像和远程端口镜像两大类。本实验重点介绍最基础的本地端口镜像。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='本实验重点介绍最基础的本地端口镜像。 3.1 本地端口镜像 当源设备与数据监测设备直接相连时，源设备可以同时作为目的设备，即由 本设备将镜像报文转发至数据检测设备，这种方式实现的端口镜像称为本地端口 镜像。对于本地端口镜像，镜像源和镜像目的属于同一台设备上的同一个镜像组， 该镜像组称为本地镜像组。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='图 1 本地端口镜像示意图 如图 1 所示，现在需要设备将进入端口 GigabitEthernet1/0/1 的报文复制 一份，从端口 GigabitEthernet1/0/2 将报文转发给数据监测设备。为满足该需求， 可以配置本地镜像组，其中源端口为 GigabitEthernet1/0/1，镜像方向为入方向， 目的端口为 GigabitEthernet1/0/2。\n4 本地端口镜像配置实验 4.1 组网需求 交换机 Device 通过端口 GigabitEthernet1/0/1 和 GigabitEthernet1/0/2 分别 连接市场部和技术部，并通过端口 GigabitEthernet1/0/3 连接监控服务器 Server。 通过配置源端口方式的本地端口镜像，使 Server 可以监控所有进、出市场 部和技术部的报文。 4.2 组网图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='图 2 本地端口镜像配置组网图 4.3 配置步骤 # 创建本地镜像组 1。 <Device> system-view [Device] mirroring-group 1 local # 配置本地镜像组 1 的源端口为 GigabitEthernet1/0/1 和 GigabitEthernet1/0/2，对源端口收 发的报文都进行镜像，目的端口为 GigabitEthernet1/0/3。 [Device] mirroring-group 1 mirroring-port gigabitethernet 1/0/1 gigabitethernet 1/0/2 both [Device] mirroring-group 1 monitor-port gigabitethernet 1/0/3 # 在目的端口 GigabitEthernet1/0/3 上关闭生成树协议。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='[Device] interface gigabitethernet 1/0/3 [Device-GigabitEthernet1/0/3] undo stp enable [Device-GigabitEthernet1/0/3] quit 4.4 验证配置 # 显示所有镜像组的配置信息。 [Device] display mirroring-group all Mirroring group 1: Type: Local Status: Active', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'}), Document(page_content='Mirroring port: GigabitEthernet1/0/1 Both GigabitEthernet1/0/2 Both Monitor port: GigabitEthernet1/0/3 所有主机启动 Wireshark 软件截获报文，PCA ping PCB，查看 Server PCC 截获 的报文，验证 Server PCC 是否能够监控所有进、出市场部和技术部的报文。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp91lugvue/端口镜像实验.pdf'})]
cuda:2
[UploadFile(filename='7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt', size=6915, headers=Headers({'content-disposition': 'form-data; name="files"; filename="7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpk4808tz9, tmpk4808tz9
File: 7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt, msg: 成功上传文件 7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt, docs: [Document(page_content='NeuroGen: activation optimized image synthesis for discovery  neuroscience', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Functional MRI (fMRI) is a powerful technique that has allowed us tocharacterize visual cortex responses to stimuli, yet such experiments are bynature constructed based on a priori hypotheses, limited to the set of imagespresented to the individual while they are in the scanner, are subject to noisein the observed brain responses, and may vary widely across individuals. Inthis work, we propose a novel computational strategy, which we call NeuroGen,to overcome these limitations and develop a powerful tool for human visionneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding modelof human vision with a deep generative network to synthesize images predictedto achieve a target pattern of macro-scale brain activation. We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience. By using only a smallnumber of synthetic images created by NeuroGen, we demonstrate that we candetect and amplify differences in regional and individual human brain responsepatterns to visual stimuli. We then verify that these discoveries are reflectedin the several thousand observed image responses measured with fMRI.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We furtherdemonstrate that NeuroGen can create synthetic images predicted to achieveregional response patterns not achievable by the best-matching natural images.The NeuroGen framework extends the utility of brain encoding models and opensup a new avenue for exploring, and possibly precisely controlling, the humanvisual system.\nClassification of jujube fruit based on several pricing factors using  machine learning methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Jujube is a fruit mainly cultivated in India, China and Iran and has manyhealth benefits. It is sold both fresh and dried. There are several factors injujube pricing such as weight, wrinkles and defections. Some jujube farmerssell their product all at once, without any proper sorting or classification,for an average price. Our studies and experiences show that their profit canincrease significantly if their product is sold after the sorting process.There are some traditional sorting methods for dried jujube fruit but they arecostly, time consuming and can be inaccurate due to human error. Nowadays,computer vision combined with machine learning methods, is used increasingly infood industry for sorting and classification purposes and solve many of thetraditional sorting methods' problems. In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers. In this method we firstacquire several images from different samples and then extract their visualfeatures such as color features, shape and size features, texture features,defection and wrinkle features and then we select the most useful featuresusing feature selection algorithms like PCA and CFS.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='A feature vector isobtained for each sample and we use these vectors to train our classifiers tobe able to specify the corresponding pre-defined group for each of the samples.We used different classifiers and training methods in order to obtain the bestresult and by using decision tree we could reach 98.8% accuracy of theclassification.\nDetecting Humans in RGB-D Data with CNNs\nWe address the problem of people detection in RGB-D data where we leveragedepth information to develop a region-of-interest (ROI) selection method thatprovides proposals to two color and depth CNNs. To combine the detectionsproduced by the two CNNs, we propose a novel fusion approach based on thecharacteristics of depth images. We also present a new depth-encoding scheme,which not only encodes depth images into three channels but also enhances theinformation for classification. We conduct experiments on a publicly availableRGB-D people dataset and show that our approach outperforms the baseline modelsthat only use RGB data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Aligning Large Multimodal Models with Factually Augmented RLHF', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Large Multimodal Models (LMM) are built across modalities and themisalignment between two modalities can result in "hallucination", generatingtextual outputs that are not grounded by the multimodal information in context.To address the multimodal misalignment issue, we adapt the ReinforcementLearning from Human Feedback (RLHF) from the text domain to the task ofvision-language alignment, where human annotators are asked to compare tworesponses and pinpoint the more hallucinated one, and the vision-language modelis trained to maximize the simulated human rewards. We propose a new alignmentalgorithm called Factually Augmented RLHF that augments the reward model withadditional factual information such as image captions and ground-truthmulti-choice options, which alleviates the reward hacking phenomenon in RLHFand further improves the performance. We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel. To evaluate the proposed approach in real-world scenarios, we develop anew evaluation benchmark MMHAL-BENCH with a special focus on penalizinghallucinations. As the first LMM trained with RLHF, our approach achievesremarkable improvement on the LLaVA-Bench dataset with the 94% performancelevel of the text-only GPT-4 (while previous best methods can only achieve the87% level), and an improvement by 60% on MMHAL-BENCH over other baselines.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Weopensource our code, model, data at https://llava-rlhf.github.io.\nReinforcement Learning with Videos: Combining Offline Observations with  Interaction', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Reinforcement Learning with Videos: Combining Offline Observations with  Interaction\nReinforcement learning is a powerful framework for robots to acquire skillsfrom experience, but often requires a substantial amount of online datacollection. As a result, it is difficult to collect sufficiently diverseexperiences that are needed for robots to generalize broadly. Videos of humans,on the other hand, are a readily available source of broad and interestingexperiences. In this paper, we consider the question: can we performreinforcement learning directly on experience collected by humans? This problemis particularly difficult, as such videos are not annotated with actions andexhibit substantial visual domain shift relative to the robot's embodiment. Toaddress these challenges, we propose a framework for reinforcement learningwith videos (RLV). RLV learns a policy and value function using experiencecollected by humans in combination with data collected by robots. In ourexperiments, we find that RLV is able to leverage such videos to learnchallenging vision-based skills with less than half as many samples as RLmethods that learn from scratch.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc3da350> 111
cuda:2
[Document(page_content='NeuroGen: activation optimized image synthesis for discovery  neuroscience', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Functional MRI (fMRI) is a powerful technique that has allowed us tocharacterize visual cortex responses to stimuli, yet such experiments are bynature constructed based on a priori hypotheses, limited to the set of imagespresented to the individual while they are in the scanner, are subject to noisein the observed brain responses, and may vary widely across individuals. Inthis work, we propose a novel computational strategy, which we call NeuroGen,to overcome these limitations and develop a powerful tool for human visionneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding modelof human vision with a deep generative network to synthesize images predictedto achieve a target pattern of macro-scale brain activation. We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience. By using only a smallnumber of synthetic images created by NeuroGen, we demonstrate that we candetect and amplify differences in regional and individual human brain responsepatterns to visual stimuli. We then verify that these discoveries are reflectedin the several thousand observed image responses measured with fMRI.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We furtherdemonstrate that NeuroGen can create synthetic images predicted to achieveregional response patterns not achievable by the best-matching natural images.The NeuroGen framework extends the utility of brain encoding models and opensup a new avenue for exploring, and possibly precisely controlling, the humanvisual system.\nClassification of jujube fruit based on several pricing factors using  machine learning methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Jujube is a fruit mainly cultivated in India, China and Iran and has manyhealth benefits. It is sold both fresh and dried. There are several factors injujube pricing such as weight, wrinkles and defections. Some jujube farmerssell their product all at once, without any proper sorting or classification,for an average price. Our studies and experiences show that their profit canincrease significantly if their product is sold after the sorting process.There are some traditional sorting methods for dried jujube fruit but they arecostly, time consuming and can be inaccurate due to human error. Nowadays,computer vision combined with machine learning methods, is used increasingly infood industry for sorting and classification purposes and solve many of thetraditional sorting methods' problems. In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers. In this method we firstacquire several images from different samples and then extract their visualfeatures such as color features, shape and size features, texture features,defection and wrinkle features and then we select the most useful featuresusing feature selection algorithms like PCA and CFS.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='A feature vector isobtained for each sample and we use these vectors to train our classifiers tobe able to specify the corresponding pre-defined group for each of the samples.We used different classifiers and training methods in order to obtain the bestresult and by using decision tree we could reach 98.8% accuracy of theclassification.\nDetecting Humans in RGB-D Data with CNNs\nWe address the problem of people detection in RGB-D data where we leveragedepth information to develop a region-of-interest (ROI) selection method thatprovides proposals to two color and depth CNNs. To combine the detectionsproduced by the two CNNs, we propose a novel fusion approach based on thecharacteristics of depth images. We also present a new depth-encoding scheme,which not only encodes depth images into three channels but also enhances theinformation for classification. We conduct experiments on a publicly availableRGB-D people dataset and show that our approach outperforms the baseline modelsthat only use RGB data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Aligning Large Multimodal Models with Factually Augmented RLHF', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Large Multimodal Models (LMM) are built across modalities and themisalignment between two modalities can result in "hallucination", generatingtextual outputs that are not grounded by the multimodal information in context.To address the multimodal misalignment issue, we adapt the ReinforcementLearning from Human Feedback (RLHF) from the text domain to the task ofvision-language alignment, where human annotators are asked to compare tworesponses and pinpoint the more hallucinated one, and the vision-language modelis trained to maximize the simulated human rewards. We propose a new alignmentalgorithm called Factually Augmented RLHF that augments the reward model withadditional factual information such as image captions and ground-truthmulti-choice options, which alleviates the reward hacking phenomenon in RLHFand further improves the performance. We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel. To evaluate the proposed approach in real-world scenarios, we develop anew evaluation benchmark MMHAL-BENCH with a special focus on penalizinghallucinations. As the first LMM trained with RLHF, our approach achievesremarkable improvement on the LLaVA-Bench dataset with the 94% performancelevel of the text-only GPT-4 (while previous best methods can only achieve the87% level), and an improvement by 60% on MMHAL-BENCH over other baselines.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Weopensource our code, model, data at https://llava-rlhf.github.io.\nReinforcement Learning with Videos: Combining Offline Observations with  Interaction', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Reinforcement Learning with Videos: Combining Offline Observations with  Interaction\nReinforcement learning is a powerful framework for robots to acquire skillsfrom experience, but often requires a substantial amount of online datacollection. As a result, it is difficult to collect sufficiently diverseexperiences that are needed for robots to generalize broadly. Videos of humans,on the other hand, are a readily available source of broad and interestingexperiences. In this paper, we consider the question: can we performreinforcement learning directly on experience collected by humans? This problemis particularly difficult, as such videos are not annotated with actions andexhibit substantial visual domain shift relative to the robot's embodiment. Toaddress these challenges, we propose a framework for reinforcement learningwith videos (RLV). RLV learns a policy and value function using experiencecollected by humans in combination with data collected by robots. In ourexperiments, we find that RLV is able to leverage such videos to learnchallenging vision-based skills with less than half as many samples as RLmethods that learn from scratch.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpk4808tz9/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'})]
cuda:2
[UploadFile(filename='7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt', size=6915, headers=Headers({'content-disposition': 'form-data; name="files"; filename="7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpcicxu6z6, tmpcicxu6z6
File: 7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt, msg: 成功上传文件 7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt, docs: [Document(page_content='NeuroGen: activation optimized image synthesis for discovery  neuroscience', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Functional MRI (fMRI) is a powerful technique that has allowed us tocharacterize visual cortex responses to stimuli, yet such experiments are bynature constructed based on a priori hypotheses, limited to the set of imagespresented to the individual while they are in the scanner, are subject to noisein the observed brain responses, and may vary widely across individuals. Inthis work, we propose a novel computational strategy, which we call NeuroGen,to overcome these limitations and develop a powerful tool for human visionneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding modelof human vision with a deep generative network to synthesize images predictedto achieve a target pattern of macro-scale brain activation. We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience. By using only a smallnumber of synthetic images created by NeuroGen, we demonstrate that we candetect and amplify differences in regional and individual human brain responsepatterns to visual stimuli. We then verify that these discoveries are reflectedin the several thousand observed image responses measured with fMRI.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We furtherdemonstrate that NeuroGen can create synthetic images predicted to achieveregional response patterns not achievable by the best-matching natural images.The NeuroGen framework extends the utility of brain encoding models and opensup a new avenue for exploring, and possibly precisely controlling, the humanvisual system.\nClassification of jujube fruit based on several pricing factors using  machine learning methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Jujube is a fruit mainly cultivated in India, China and Iran and has manyhealth benefits. It is sold both fresh and dried. There are several factors injujube pricing such as weight, wrinkles and defections. Some jujube farmerssell their product all at once, without any proper sorting or classification,for an average price. Our studies and experiences show that their profit canincrease significantly if their product is sold after the sorting process.There are some traditional sorting methods for dried jujube fruit but they arecostly, time consuming and can be inaccurate due to human error. Nowadays,computer vision combined with machine learning methods, is used increasingly infood industry for sorting and classification purposes and solve many of thetraditional sorting methods' problems. In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers. In this method we firstacquire several images from different samples and then extract their visualfeatures such as color features, shape and size features, texture features,defection and wrinkle features and then we select the most useful featuresusing feature selection algorithms like PCA and CFS.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='A feature vector isobtained for each sample and we use these vectors to train our classifiers tobe able to specify the corresponding pre-defined group for each of the samples.We used different classifiers and training methods in order to obtain the bestresult and by using decision tree we could reach 98.8% accuracy of theclassification.\nDetecting Humans in RGB-D Data with CNNs\nWe address the problem of people detection in RGB-D data where we leveragedepth information to develop a region-of-interest (ROI) selection method thatprovides proposals to two color and depth CNNs. To combine the detectionsproduced by the two CNNs, we propose a novel fusion approach based on thecharacteristics of depth images. We also present a new depth-encoding scheme,which not only encodes depth images into three channels but also enhances theinformation for classification. We conduct experiments on a publicly availableRGB-D people dataset and show that our approach outperforms the baseline modelsthat only use RGB data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Aligning Large Multimodal Models with Factually Augmented RLHF', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Large Multimodal Models (LMM) are built across modalities and themisalignment between two modalities can result in "hallucination", generatingtextual outputs that are not grounded by the multimodal information in context.To address the multimodal misalignment issue, we adapt the ReinforcementLearning from Human Feedback (RLHF) from the text domain to the task ofvision-language alignment, where human annotators are asked to compare tworesponses and pinpoint the more hallucinated one, and the vision-language modelis trained to maximize the simulated human rewards. We propose a new alignmentalgorithm called Factually Augmented RLHF that augments the reward model withadditional factual information such as image captions and ground-truthmulti-choice options, which alleviates the reward hacking phenomenon in RLHFand further improves the performance. We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel. To evaluate the proposed approach in real-world scenarios, we develop anew evaluation benchmark MMHAL-BENCH with a special focus on penalizinghallucinations. As the first LMM trained with RLHF, our approach achievesremarkable improvement on the LLaVA-Bench dataset with the 94% performancelevel of the text-only GPT-4 (while previous best methods can only achieve the87% level), and an improvement by 60% on MMHAL-BENCH over other baselines.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Weopensource our code, model, data at https://llava-rlhf.github.io.\nReinforcement Learning with Videos: Combining Offline Observations with  Interaction', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Reinforcement Learning with Videos: Combining Offline Observations with  Interaction\nReinforcement learning is a powerful framework for robots to acquire skillsfrom experience, but often requires a substantial amount of online datacollection. As a result, it is difficult to collect sufficiently diverseexperiences that are needed for robots to generalize broadly. Videos of humans,on the other hand, are a readily available source of broad and interestingexperiences. In this paper, we consider the question: can we performreinforcement learning directly on experience collected by humans? This problemis particularly difficult, as such videos are not annotated with actions andexhibit substantial visual domain shift relative to the robot's embodiment. Toaddress these challenges, we propose a framework for reinforcement learningwith videos (RLV). RLV learns a policy and value function using experiencecollected by humans in combination with data collected by robots. In ourexperiments, we find that RLV is able to leverage such videos to learnchallenging vision-based skills with less than half as many samples as RLmethods that learn from scratch.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa5757d90> 111
cuda:2
[Document(page_content='NeuroGen: activation optimized image synthesis for discovery  neuroscience', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Functional MRI (fMRI) is a powerful technique that has allowed us tocharacterize visual cortex responses to stimuli, yet such experiments are bynature constructed based on a priori hypotheses, limited to the set of imagespresented to the individual while they are in the scanner, are subject to noisein the observed brain responses, and may vary widely across individuals. Inthis work, we propose a novel computational strategy, which we call NeuroGen,to overcome these limitations and develop a powerful tool for human visionneuroscience discovery. NeuroGen combines an fMRI-trained neural encoding modelof human vision with a deep generative network to synthesize images predictedto achieve a target pattern of macro-scale brain activation. We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="We demonstratethat the reduction of noise that the encoding model provides, coupled with thegenerative network's ability to produce images of high fidelity, results in arobust discovery architecture for visual neuroscience. By using only a smallnumber of synthetic images created by NeuroGen, we demonstrate that we candetect and amplify differences in regional and individual human brain responsepatterns to visual stimuli. We then verify that these discoveries are reflectedin the several thousand observed image responses measured with fMRI.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We furtherdemonstrate that NeuroGen can create synthetic images predicted to achieveregional response patterns not achievable by the best-matching natural images.The NeuroGen framework extends the utility of brain encoding models and opensup a new avenue for exploring, and possibly precisely controlling, the humanvisual system.\nClassification of jujube fruit based on several pricing factors using  machine learning methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Jujube is a fruit mainly cultivated in India, China and Iran and has manyhealth benefits. It is sold both fresh and dried. There are several factors injujube pricing such as weight, wrinkles and defections. Some jujube farmerssell their product all at once, without any proper sorting or classification,for an average price. Our studies and experiences show that their profit canincrease significantly if their product is sold after the sorting process.There are some traditional sorting methods for dried jujube fruit but they arecostly, time consuming and can be inaccurate due to human error. Nowadays,computer vision combined with machine learning methods, is used increasingly infood industry for sorting and classification purposes and solve many of thetraditional sorting methods' problems. In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='In this paper we are proposing acomputer vision-based method for grading jujube fruits using machine learningtechniques which will take most of the important pricing factors into accountand can be used to increase the profit of farmers. In this method we firstacquire several images from different samples and then extract their visualfeatures such as color features, shape and size features, texture features,defection and wrinkle features and then we select the most useful featuresusing feature selection algorithms like PCA and CFS.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='A feature vector isobtained for each sample and we use these vectors to train our classifiers tobe able to specify the corresponding pre-defined group for each of the samples.We used different classifiers and training methods in order to obtain the bestresult and by using decision tree we could reach 98.8% accuracy of theclassification.\nDetecting Humans in RGB-D Data with CNNs\nWe address the problem of people detection in RGB-D data where we leveragedepth information to develop a region-of-interest (ROI) selection method thatprovides proposals to two color and depth CNNs. To combine the detectionsproduced by the two CNNs, we propose a novel fusion approach based on thecharacteristics of depth images. We also present a new depth-encoding scheme,which not only encodes depth images into three channels but also enhances theinformation for classification. We conduct experiments on a publicly availableRGB-D people dataset and show that our approach outperforms the baseline modelsthat only use RGB data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Aligning Large Multimodal Models with Factually Augmented RLHF', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Large Multimodal Models (LMM) are built across modalities and themisalignment between two modalities can result in "hallucination", generatingtextual outputs that are not grounded by the multimodal information in context.To address the multimodal misalignment issue, we adapt the ReinforcementLearning from Human Feedback (RLHF) from the text domain to the task ofvision-language alignment, where human annotators are asked to compare tworesponses and pinpoint the more hallucinated one, and the vision-language modelis trained to maximize the simulated human rewards. We propose a new alignmentalgorithm called Factually Augmented RLHF that augments the reward model withadditional factual information such as image captions and ground-truthmulti-choice options, which alleviates the reward hacking phenomenon in RLHFand further improves the performance. We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='We also enhance the GPT-4-generatedtraining data (for vision instruction tuning) with previously availablehuman-written image-text pairs to improve the general capabilities of ourmodel. To evaluate the proposed approach in real-world scenarios, we develop anew evaluation benchmark MMHAL-BENCH with a special focus on penalizinghallucinations. As the first LMM trained with RLHF, our approach achievesremarkable improvement on the LLaVA-Bench dataset with the 94% performancelevel of the text-only GPT-4 (while previous best methods can only achieve the87% level), and an improvement by 60% on MMHAL-BENCH over other baselines.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content='Weopensource our code, model, data at https://llava-rlhf.github.io.\nReinforcement Learning with Videos: Combining Offline Observations with  Interaction', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'}), Document(page_content="Reinforcement Learning with Videos: Combining Offline Observations with  Interaction\nReinforcement learning is a powerful framework for robots to acquire skillsfrom experience, but often requires a substantial amount of online datacollection. As a result, it is difficult to collect sufficiently diverseexperiences that are needed for robots to generalize broadly. Videos of humans,on the other hand, are a readily available source of broad and interestingexperiences. In this paper, we consider the question: can we performreinforcement learning directly on experience collected by humans? This problemis particularly difficult, as such videos are not annotated with actions andexhibit substantial visual domain shift relative to the robot's embodiment. Toaddress these challenges, we propose a framework for reinforcement learningwith videos (RLV). RLV learns a policy and value function using experiencecollected by humans in combination with data collected by robots. In ourexperiments, we find that RLV is able to leverage such videos to learnchallenging vision-based skills with less than half as many samples as RLmethods that learn from scratch.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcicxu6z6/7bc42d2b-161b-4ab7-93df-1f8d6f282c85.txt'})]
cuda:2
[UploadFile(filename='06-计算机学院-计算机科学与技术专业.pdf', size=1264788, headers=Headers({'content-disposition': 'form-data; name="files"; filename="06-è®¡ç®\x97æ\x9cºå\xad¦é\x99¢-è®¡ç®\x97æ\x9cºç§\x91å\xad¦ä¸\x8eæ\x8a\x80æ\x9c¯ä¸\x93ä¸\x9a.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpcepy4xh4, tmpcepy4xh4
File: 06-计算机学院-计算机科学与技术专业.pdf, msg: 成功上传文件 06-计算机学院-计算机科学与技术专业.pdf, docs: [Document(page_content='北京航空航天大学\n计算机学院\n2021 级本科培养方案\n（计算机科学与技术专业）\n计算机学院·计算机科学与技术专业\n1\n计算机学院\n学院简介\n北京航空航天大学 1958 年建立计算机专业，1978 年成立计算机科学与工程系，2002 年组\n建计算机学院。学院遵循“面向国家需求，瞄准学科前沿，汇聚优秀人才，争创国际知名”的发展\n战略，形成了人才培养与科学研究相互促进、协调发展的格局。\n学院现有教职工 163 人，其中中国科学院院士 3 人，中国工程院院士 1 人，英国皇家学会院\n士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家\n工程技术中心、１个国家级国际科技合作基地等十个省部级以上科研教学基地；获国家自然科学二\n等奖 1 项，国家科技进步一等奖 2 项，其他国家级科技奖项 17 项；计算机科学与技术、软件工程\n在 2017 年全国一级学科评估分别为 A 和 A+，双获国家“双一流”建设学科。\n学院设置计算机科学与技术、虚拟现实技术两个本科专业；获国家教学成果一等奖 1 项、二等\n奖 4 项，首届全国优秀教材二等奖 1 项，国家级教学团队 1 个，国家级教学名师 1 人，国家级精\n品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036\n人，博士 598 人，培养质量受到社会广泛好评。\n院      长 ：王蕴红\n专业负责人 ：王蕴红\n教学副院长 ：高小鹏\n教 学 秘 书 ：丁映中、纵绮\n北京航空航天大学本科培养方案\n2\n计算机科学与技术专业\n一、专业简介\n本专业始终坚持立德树人根本任务，落实“五育”并举，践行“三全育人”要求，遵循学校“厚\n植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型\n与工程型相结合”的宽口径人才，秉承“寓教于研”办学理念并依托学科优势构建高质量课程体系，\n重视数学及学科基础理论、专业核心能力以及高水平人才所应具备的人文素养。主要培养特点如下：\n1. 重视核心能力培养。专业必修课分为基础理论、系统能力、软件能力三大模块，普遍具有\n大课重课特性，培养学生具备专业核心能力，为后续课程学习奠定坚实基础。\n2. 自主规划专业选修。本专业实施完全学分制培养模式，引导学生根据自我规划和学习兴趣\n制定个性化的专业课程选修方案，鼓励学生在某一专业方向上的系统性学习。\n3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要\n素，增强学生公民意识及责任感、提升思辨力，让学生建立工程伦理并了解技术、社会与\n自然三者间的作用关系。2～3 年级设置连贯性的科技实践与成果表达训练环节，培养创\n新意识及有效表达能力。\n二、培养目标和毕业要求\n（一）培养目标\n以学校“培养引领和支撑国家重大战略需求的领军领导人才”的人才培养战略总目标为指导，\n培养具有良好人文素养、强烈的事业心、使命感及担当精神，具有创新精神、全球化视野、终身学\n习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机\n领域复杂工程问题的能力，具有团队合作与组织管理能力，能参与国际竞争的计算机专业高水平人\n才。\n学生毕业 5 年后：\n计算机学院·计算机科学与技术专业\n3\n1. 能就专业相关的工程问题，综合考虑技术、经济、法律、伦理等因素，分析、制定解决方\n案，并管理项目的实施；\n2. 能在职业发展中具有担当精神、行动力、感染力和领导力；\n3. 能与国内外同行、客户和公众有效沟通；\n4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求\n1. 工程知识：具备较扎实的数学、自然科学知识，系统掌握计算机领域的工程基础和专业知识，\n能够将这些知识用于解决计算机领域复杂工程问题。\n2. 问题分析：能够应用数学、自然科学基本原理与专业知识，并通过文献研究，识别、表达、\n分析计算机领域复杂工程问题，以获得有效结论。\n3. 设计/开发解决方案：能够设计针对计算机领域复杂工程问题的解决方案，设计满足特定需\n求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。\n4. 研究：能够基于专业知识对计算机领域复杂工程问题进行研究，包括文献调研、设计实验、\n分析与解释数据、并通过信息综合得到合理有效的结论。\n5. 使用现代工具：能够在本专业工程实践中，选择与使用合理有效的技术、软硬件资源、软硬\n件开发工具和信息技术工具，并了解其局限性。\n6. 工程与社会：具有追求创新的态度和意识，掌握基本的创新方法，以及综合运用理论和技术\n手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。\n7. 环境和可持续发展：了解与本专业相关的职业和行业的生产、设计、研究与开发、环境保护\n和可持续发展等方面的方针、政策和法津、法规；能够正确认识本专业工程实践对环境和社会可持\n续发展的影响，合理评价本专业工程实践解决方案对社会、健康、安全、法律及文化的影响。\n北京航空航天大学本科培养方案\n4\n8. 职业规范：具有良好的人文素养、社会责任感，能够在本专业工程实践中理解并遵守工程职\n业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。\n10. 沟通：能够就复杂计算机工程问题与业界同行及社会公众进行有效沟通与交流，包括撰写\n报告和设计文稿、陈述发言、清晰表达个人见解等，并具备一定的国际视野，能够在跨文化背景下\n进行沟通和交流。\n11. 项目管理：具有一定的组织与工程管理能力、表达与人际交往能力，能够在多学科背景下\n的团队中发挥重要作用。\n12. 终身学习：具有自主学习和终身学习的意识，具有不断学习和适应本领域快速发展的能力；\n具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图\n毕业要求\n主要专业必修课\n1)\n工程知识\n2)\n问题分析\n3)\n解决方案\n4)\n研究\n5)\n工具\n6)\n工程与社\n会\n7)\n环境与可\n持续发展\n8)\n职业规范\n9)\n个人与团', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='持续发展\n8)\n职业规范\n9)\n个人与团\n队\n10)\n沟通\n11)\n项目管理\n12)\n终身学习\n专业\n导论\n导论(信息类)\n√\n√\n√\n√\n√\n√\n基础\n理论\n离散数学(信息类)\n√\n√\n离散数学(2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='理论\n离散数学(信息类)\n√\n√\n离散数学(2)\n√\n√\n系统\n能力\n计算机组成\n√\n√\n√\n√\n√\n√\n操作系统\n√\n√\n√\n√\n√\n√\n√\n√\n编译技术\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n编译技术\n√\n√\n√\n√\n√\n√\n√\n√\n计算机网络\n√\n√\n√\n√\n√\n√\n√\n√\n软件\n能力\n数据结构与程序设\n计(信息类)\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计(信息类)\n√\n√\n√\n√\n√\n面向对象设计与构\n造\n√\n√\n√\n√\n√\n√\n√\n√\n√\n算法设计与分析\n√\n√\n√\n√\n√\n数据库系统原理\n√\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n√\n√\n√\n√\n√\n√\n软件工程\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n北京航空航天大学本科培养方案\n6\n三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合\n格，达到学校毕业要求的，准予毕业，学校颁发毕业证书；符合学士学位授予条件的，授予学士学\n位。\n毕业总学分：155\n授予学位类型：计算机科学与技术学士学位\n计算机科学与技术专业本科指导性最低学分框架表\n课程模块\n序列\n课程类别\n最低学分要求\n1 年级\n2-4 年级\n学分小计\nI\n基础课程\nA\n数学与自然科学类\n22', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='I\n基础课程\nA\n数学与自然科学类\n22\n7\n46\nB\n工程基础类\n9\n0\nC\n外语类\n4\n4\nII\n通修课程\nD\n思政类\n8.5\n10.5\n41\n军理类\n0\n4\nE\n体育类\n1\n2.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n4\nE\n体育类\n1\n2.5\nK\n素质教育理论必修课\n0\n2.5\nH\n素质教育实践必修课\n0.5\n1.5\nF/G\n素质教育通识限修课\n2.5\n7.5\nIII\n专业课程\nI\n核心专业类\n0\n48\n68\nJ\n一般专业类\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n48\n68\nJ\n一般专业类\n0\n20\n学分小计\n47.5\n107.5\n155\n毕业最低总学分\n155\n注：外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家\n安全、素质教育实践必修课等修读要求见相关文件，其中：\n①劳动教育课程要求：至少选修劳动教育必修课或劳动教育模块学时总数≥32 学时及参加劳动月等活\n动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业\n7\n②素质教育通识限修课：大类概论课、航空航天概论 B 为必修，至少还应包含 3 门人文类通识课（每门\n课程学分不得低于 2 学分；“法律、科技与社会”为必修）。\n③创新创业课程要求：至少选修 3 学分，详见每学期创新创业课程清单，修读要求见相应创新创业学分\n认定办法。\n④全英文课程要求：至少选修 2 学分全英文课程（外语类课程除外）。鼓励学生积极申报学校出国留学\n计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保\n学习成效，所选修课程总数不得超过 2 门。\n⑥一般专业类：至少需要从计算机学院为本专业设置的方向基础类课程清单中选修不少于 3 门课程。\n北京航空航天大学本科培养方案\n8\n四、课程设置与学分分布表\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n基\n础\n课\n程\n数\n学\n与\n自\n然\n科', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='数\n学\n与\n自\n然\n科\n学\n类\nB1A09104A 工科数学分析(1)\nMathematical Analysis for Engineering\n(1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09106A 工科高等代数\nAdvanced Algebra for Engineering (1)\n6\n96\n96', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Advanced Algebra for Engineering (1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09105A 工科数学分析(2)\nMathematical Analysis for Engineering\n(2)\n6\n96\n96\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB1A19104A 基础物理学(信息类)\nFundamental Physics(Information class)\n4\n64\n64\n0\n0\n一\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考试\n全汉语\nB1A19201B 工科大学物理(2)\nEngineering College physics(2)\n4\n64\n64\n0\n0\n二\n秋\n必修\n考试\n全汉语\n工\n程\n基\n础\n类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2\n48\n16\n32\n0\n一\n秋\n必修\n考试\n全汉语\nB1B021150\n电子设计基础训练\nBasic training in electronic design\n2\n56\n8\n48\n0\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考查\n全汉语\nB1B061060\n离散数学(信息类)\nDiscrete Mathematics (Information\nclass)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1B061100\n数据结构与程序设计(信息类)\nData Structures and Programming\n(Information class)\n3\n64', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Data Structures and Programming\n(Information class)\n3\n64\n32\n32\n0\n一\n春\n必修\n考试\n全汉语\n外\n语\n类\nB1C12107A 大学英语 A(1)\nCollege English A (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n秋\n必修\n考试\n全英文\nB1C12108A 大学英语 A(2)\nCollege English A (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207A 大学英语 A(3)\nCollege English A (3)\n2\n32\n32\n0\n0\n二', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208A 大学英语 A(4)\nCollege English A (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试\n全英文\nB1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文\nB1C12108B 大学英语 B(2)\nCollege English B (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207B 大学英语 B(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全英文\nB1C12207B 大学英语 B(3)\nCollege English B (3)\n2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208B 大学英语 B(4)\nCollege English B (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n二\n春\n必修\n考试\n全英文\n通\n思\nB2D281050\n思想道德与法治\nEthic Thought and Rule of Law\n3\n48\n48\n0\n0\n一\n秋\n必修\n考试\n全汉语\n计算机学院·计算机科学与技术专业\n9\n课程\n模块\n课程\n类别', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='9\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content="方式\n授课\n语言\n学年\n学期\n修\n课\n程\n政\n类\nB2D282060\n习近平新时代中国特色社会主\n义思想概论\nIntroduction to Xi Jinping's Thoughts on\nSocialism with Chinese Characteristics\nfor a New Era\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2D281060", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='秋\n必修\n考试\n全汉语\nB2D281060\n中国近现代史纲要\nOutline of Modern Chinese History\n3\n48\n48\n0\n0\n一\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n思\n政\n类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (1)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2D282090\n毛泽东思想和中国特色社会主\n义理论体系概论(2)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Mao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)\n2\n80\n0\n0\n80\n二\n寒假\n必修\n考查\n全汉语\nB2D282070\n马克思主义基本原理\nFundamental Principles of Marxism\n3\n48\n48\n0\n0\n二\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n二\n春\n必修\n考试\n全汉语\nB2D281110\n形势与政策(1)\nSituation and Policy (1)\n0.2\n8\n4\n0\n4\n一\n秋\n必修\n考查\n全汉语\nB2D281120\n形势与政策(2)\nSituation and Policy (2)\n0.3\n8\n4\n0\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n8\n4\n0\n4\n一\n春\n必修\n考查\n全汉语\nB2D282110\n形势与政策(3)\nSituation and Policy (3)\n0.2\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)\n0.3\n8\n8\n0\n0\n二\n春\n必修\n考查\n全汉语\nB2D283110\n形势与政策(5)\nSituation and Policy (5)\n0.2\n8\n8\n0\n0\n三\n秋\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考查\n全汉语\nB2D283120\n形势与政策(6)\nSituation and Policy (6)\n0.3\n8\n8\n0\n0\n三\n春\n必修\n考查\n全汉语\nB2D284110\n形势与政策(7)\nSituation and Policy (7)\n0.2\n8\n8\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.2\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB2D284120\n形势与政策(8)\nSituation and Policy (8)\n0.3\n8\n8\n0\n0\n四\n春\n必修\n考查\n全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n限修≥1\n学分\n考试\n全汉语\nB2D280120\n新中国史\nThe History of the People’s Republic of\nChina\n1\n16\n16\n0\n0\n一至', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280130\n改革开放史\nThe History of the Reform and Opening-\nup\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\n北京航空航天大学本科培养方案\n10\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n军理\n类\nB2D511040\n军事理论\nMilitary Theory\n2\n36', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2D511040\n军事理论\nMilitary Theory\n2\n36\n32\n0\n4\n二\n春\n必修\n考试\n全汉语\nB2D511030\n军事技能\nMilitary Skills\n2\n112\n0\n0\n112\n一\n夏\n必修\n考查\n全汉语\n体\n育\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\n体\n育\n类\nB2E331030\n体育(1)\nPhysical Education (1)\n0.5\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2E331040\n体育(2)\nPhysical Education (2)\n0.5\n32\n32\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.5\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB2E332050\n体育(3)\nPhysical Education (3)\n0.5\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2E332060\n体育(4)\nPhysical Education (4)\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2E332060\n体育(4)\nPhysical Education (4)\n0.5\n32\n32\n0\n0\n二\n春\n必修\n考试\n全汉语\nB2E333070\n体育(5)\nPhysical Education (5)\n0.5\n16\n16\n0\n0\n三\n秋\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考试\n全汉语\nB2E333080\n体育(6)\nPhysical Education (6)\n0.5\n16\n16\n0\n0\n三\n春\n必修\n考试\n全汉语\nB2E334030\n体质健康标准测试\n0.5\n0\n0\n0\n0\n四\n秋', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n0\n0\n四\n秋\n必修\n考试\n全汉语\n通\n修\n课\n程\n素\n质\n教\n育\n实\n践\n必\n修\n课\nB2H511110\n素质教育(博雅课程)(1)\nComprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Comprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16\n4\n0\n12\n一\n秋\n必修\n考查\n全汉语\nB2H511120\n素质教育(博雅课程)(2)\nComprehensive Development\nEducation(Liberal Arts Course )(2)\n0.3\n16\n4\n0\n12\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='12\n一\n春\n必修\n考查\n全汉语\nB2H511130\n素质教育(博雅课程)(3)\nComprehensive Development\nEducation(Liberal Arts Course )(3)\n0.2\n16\n4\n0\n12\n二\n秋\n必修\n考查\n全汉语\nB2H511140\n素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3\n16\n4\n0\n12\n二\n春\n必修\n考查\n全汉语\nB2H511150\n素质教育(博雅课程)(5)\nComprehensive Development\nEducation(Liberal Arts Course )(5)\n0.2\n16\n4\n0\n12\n三\n秋\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4\n0\n12\n三\n秋\n必修\n考查\n全汉语\nB2H511160\n素质教育(博雅课程)(6)\nComprehensive Development\nEducation(Liberal Arts Course )(6)\n0.3\n16\n4\n0\n12\n三\n春\n必修\n考查\n全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development\nEducation(Liberal Arts Course )(7)\n0.2\n16\n4\n0\n12\n四\n秋\n必修\n考查\n全汉语\nB2H511180\n素质教育(博雅课程)(8)\nComprehensive Development\nEducation(Liberal Arts Course )(8)\n0.3\n16\n4\n0\n12\n四', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n16\n4\n0\n12\n四\n春\n必修\n考查\n全汉语\n素质\n教育\n理论\n美育类课程（至少 1.5 学分），各类课程见各学期开课清单\n1.5\n必修\n劳动教育课程（至少 32 学时），劳动教育必修课或劳动教育模块，详见每学期劳动\n教育课程清单\n32\n一至\n四\n秋/春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='一至\n四\n秋/春\n必修\n考查\n全汉语\n计算机学院·计算机科学与技术专业\n11\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n必修\n课\nB2K141010\n国家安全\nNational Security Education\n1\n16\n14\n0\n2\n一至\n三\n秋/春\n必修\n考查\n全汉语\n素', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n秋/春\n必修\n考查\n全汉语\n素\n质\n教\n育\n通\n识\n限\n修\n课\n概论课(大类内部)以下七门：\nB2F020390\n电子信息工程导论\nIntroduction to Electronic Information\nEngineering\n1.5\n24\n24\n0\n0\n一\n秋\n限修，', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='24\n0\n0\n一\n秋\n限修，\n≥1.5 学\n分\n考查\n全汉语\nB2F030360\n自动化科学与电气工程导论\nIntroduction of automation Science and\nelectrical engineering major\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and\nComputer Ethics\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F170210\n仪器科学概览\nOverview of Instrument Science\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n一\n秋\n考查\n全汉语\nB2F210120\n走进软件\nIntroduction to Software\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F390110\n网络空间安全导论\nIntroduction to Cyberspace Security\n1.5\n24\n24\n0\n0\n一', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F491110\n集成电路导论\nIntroduction to Integrated Circuit\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n素质\n教育\n通识\n限修\n课\n人文、经典、社科、科技文明 4 类素质教育课\n1\n秋/春\n限修，\n≥1 学分', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n秋/春\n限修，\n≥1 学分\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n3 门人文类核心通识课（法律、科技与社会为必选）\n2\n32\n32\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n专', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n春\n必修\n考查\n全汉语\n专\n业\n课\n程\n核\n心\n专\n业\n类\nB3J063270\n科研课堂\nScientific Research Training\n2\n32\n0\n0\n32\n二\n秋\n必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)\nInternship\n5\n320\n0\n0\n320\n三\n夏\n必修\n考查\n全汉语\nB3I064410\n毕业设计\nGraduation Project\n8\n640\n0\n0\n640\n四\n春\n必修\n考查', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n640\n四\n春\n必修\n考查\n全汉语\nB3I061142\n离散数学 2\nDiscrete Mathematics (2)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB3I062260\n计算机组成\nPrinciples of Computer Organization\n5.5\n112\n64\n48', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Principles of Computer Organization\n5.5\n112\n64\n48\n0\n二\n秋\n必修\n考试\n全汉语\n北京航空航天大学本科培养方案\n12\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\nB3I062270\n操作系统\nOperating Systems\n4.5\n96\n48\n48\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='48\n0\n二\n春\n必修\n考试\n全汉语\nB3I063120\n编译技术\nCompiler Technology\n4.5\n96\n48\n48\n0\n三\n秋\n必修\n考试\n全汉语\nB3I062180\n面向对象设计与构造\nObject-oriented Design and\nConstruction\n2.5\n48\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Object-oriented Design and\nConstruction\n2.5\n48\n32\n16\n0\n二\n春\n必修\n考试\n全汉语\nB3I063320\n算法设计与分析\nThe Design and Analysis of Algorithm\n2\n32\n32\n0\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063180\n数据库系统原理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB3I063180\n数据库系统原理\nPrinciples of Database Systems\n4\n80\n48\n32\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063130\n软件工程\nSoftware Engineering\n2\n32\n32\n0\n0\n三\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063210\n计算机网络\nComputer Network\n2\n32\n32\n0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063220\n计算机网络实验\nComputer Network Experiment\n1\n32\n0\n32\n0\n三', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n32\n0\n32\n0\n三\n春\n必修\n考试\n全汉语\nB3I064120\n计算机科学方法论\nMethodology of Computer Science\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n一\n般\n专\n业\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\n一\n般\n专\n业\n类\nB3J062610\n职业规划与选择讲座\nProfession Planning and Choice\n0.5\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB3J063610\n学科技术前沿讲座\nSeminars on the Development of\nComputer Science\n1\n16\n16\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Computer Science\n1\n16\n16\n0\n0\n三\n春\n必修\n考查\n全汉语\nB3J064610\n求职辅导系列讲座\nSerial Lectures on How to Seek\nEmployment\n0.5\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB3J062410\n实践与展示(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3J062410\n实践与展示(1)\nPractice and Presentation(Ⅰ)\n1\n32\n0\n0\n32\n二\n春\n必修\n考查\n全汉语\nB3J062420\n实践与展示(2)\nPractice and Presentation (2)\n1\n32\n0\n0\n32\n三\n春\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n32\n三\n春\n必修\n考查\n全汉语\n共计选修 16 学分的专业选修课。\n16\n计算机学院·计算机科学与技术专业\n13\n五、核心课程先修逻辑关系图\n六、专业准入办法一览表\n为更好的体现学生结合自身学习特点和兴趣以更好的选择专业，本专业分别在 1 年级春季学\n期、2 年级秋季学期和 2 年级春季学期即将结束时转入转出申请。\n对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业\n学习的候选者，同时结合候选者其他相关课程的学习决定其是否同级转专业还是降级转专业。\n关于具体准入条件，请参考《北京航空航天大学计算机学院转专业管理规定》。该文件可以在\n学校转专业相关网页查阅到。\n七、毕业生未来发展图\n除了升学深造外，由于计算机专业社会需求广泛，因此本专业毕业生具有广泛的就业空间及发\n展可能。本培养方案仅给出部分可能的发展规划，具体内容参见下表。\n主分类\n次分类\n描述\n就业\nIT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='IT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主\n北京航空航天大学本科培养方案\n14\n自主创业\n升学\n国内深造\n软件与理论方向：大学、中科院计算所、中科院软件所\n体系结构方向：大学、中科院计算所、国防系统研究所\n计算机应用方向：大学、中科院相关院所、国防系统研究所\n出国深造\n国外大学攻读硕士学位、博士学位', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa574add0> 111
cuda:2
[Document(page_content='北京航空航天大学\n计算机学院\n2021 级本科培养方案\n（计算机科学与技术专业）\n计算机学院·计算机科学与技术专业\n1\n计算机学院\n学院简介\n北京航空航天大学 1958 年建立计算机专业，1978 年成立计算机科学与工程系，2002 年组\n建计算机学院。学院遵循“面向国家需求，瞄准学科前沿，汇聚优秀人才，争创国际知名”的发展\n战略，形成了人才培养与科学研究相互促进、协调发展的格局。\n学院现有教职工 163 人，其中中国科学院院士 3 人，中国工程院院士 1 人，英国皇家学会院\n士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='士 1 人，教授 52 人，副教授 56 人；建有 2 个国家重点实验室、１个国家工程实验室、1 个国家\n工程技术中心、１个国家级国际科技合作基地等十个省部级以上科研教学基地；获国家自然科学二\n等奖 1 项，国家科技进步一等奖 2 项，其他国家级科技奖项 17 项；计算机科学与技术、软件工程\n在 2017 年全国一级学科评估分别为 A 和 A+，双获国家“双一流”建设学科。\n学院设置计算机科学与技术、虚拟现实技术两个本科专业；获国家教学成果一等奖 1 项、二等\n奖 4 项，首届全国优秀教材二等奖 1 项，国家级教学团队 1 个，国家级教学名师 1 人，国家级精\n品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='品课程 2 门，拥有首个国家级计算机实验教学示范中心；累计培养本科毕业生 5796 人，硕士 4036\n人，博士 598 人，培养质量受到社会广泛好评。\n院      长 ：王蕴红\n专业负责人 ：王蕴红\n教学副院长 ：高小鹏\n教 学 秘 书 ：丁映中、纵绮\n北京航空航天大学本科培养方案\n2\n计算机科学与技术专业\n一、专业简介\n本专业始终坚持立德树人根本任务，落实“五育”并举，践行“三全育人”要求，遵循学校“厚\n植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='植情怀、强化基础、突出实践、科教融通”培养方针，按计算机科学与技术一级学科培养“科学型\n与工程型相结合”的宽口径人才，秉承“寓教于研”办学理念并依托学科优势构建高质量课程体系，\n重视数学及学科基础理论、专业核心能力以及高水平人才所应具备的人文素养。主要培养特点如下：\n1. 重视核心能力培养。专业必修课分为基础理论、系统能力、软件能力三大模块，普遍具有\n大课重课特性，培养学生具备专业核心能力，为后续课程学习奠定坚实基础。\n2. 自主规划专业选修。本专业实施完全学分制培养模式，引导学生根据自我规划和学习兴趣\n制定个性化的专业课程选修方案，鼓励学生在某一专业方向上的系统性学习。\n3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='3. 注重非技术能力培养。设置 3 门核心通识课程，并在所有专业课程中都增加课程思政要\n素，增强学生公民意识及责任感、提升思辨力，让学生建立工程伦理并了解技术、社会与\n自然三者间的作用关系。2～3 年级设置连贯性的科技实践与成果表达训练环节，培养创\n新意识及有效表达能力。\n二、培养目标和毕业要求\n（一）培养目标\n以学校“培养引领和支撑国家重大战略需求的领军领导人才”的人才培养战略总目标为指导，\n培养具有良好人文素养、强烈的事业心、使命感及担当精神，具有创新精神、全球化视野、终身学\n习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='习能力，具有较扎实的数理基础，系统掌握本专业的基础理论和专业技能，具有提出和解决计算机\n领域复杂工程问题的能力，具有团队合作与组织管理能力，能参与国际竞争的计算机专业高水平人\n才。\n学生毕业 5 年后：\n计算机学院·计算机科学与技术专业\n3\n1. 能就专业相关的工程问题，综合考虑技术、经济、法律、伦理等因素，分析、制定解决方\n案，并管理项目的实施；\n2. 能在职业发展中具有担当精神、行动力、感染力和领导力；\n3. 能与国内外同行、客户和公众有效沟通；\n4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4. 能始终坚持学习和自我完善，能紧跟技术发展趋势，并具有对新兴技术与应用的敏锐性和\n洞察力。\n（二）毕业要求\n1. 工程知识：具备较扎实的数学、自然科学知识，系统掌握计算机领域的工程基础和专业知识，\n能够将这些知识用于解决计算机领域复杂工程问题。\n2. 问题分析：能够应用数学、自然科学基本原理与专业知识，并通过文献研究，识别、表达、\n分析计算机领域复杂工程问题，以获得有效结论。\n3. 设计/开发解决方案：能够设计针对计算机领域复杂工程问题的解决方案，设计满足特定需\n求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='求的计算机系统、关键算法及应用程序，并能够在设计环节中体现创新意识，考虑法律、健康、安\n全、文化、社会以及环境等因素。\n4. 研究：能够基于专业知识对计算机领域复杂工程问题进行研究，包括文献调研、设计实验、\n分析与解释数据、并通过信息综合得到合理有效的结论。\n5. 使用现代工具：能够在本专业工程实践中，选择与使用合理有效的技术、软硬件资源、软硬\n件开发工具和信息技术工具，并了解其局限性。\n6. 工程与社会：具有追求创新的态度和意识，掌握基本的创新方法，以及综合运用理论和技术\n手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='手段设计复杂计算机系统与应用的能力；设计过程中能够综合考虑社会、经济、文化、环境、法律、\n安全、健康、伦理等制约因素。\n7. 环境和可持续发展：了解与本专业相关的职业和行业的生产、设计、研究与开发、环境保护\n和可持续发展等方面的方针、政策和法津、法规；能够正确认识本专业工程实践对环境和社会可持\n续发展的影响，合理评价本专业工程实践解决方案对社会、健康、安全、法律及文化的影响。\n北京航空航天大学本科培养方案\n4\n8. 职业规范：具有良好的人文素养、社会责任感，能够在本专业工程实践中理解并遵守工程职\n业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='业道德和规范，履行责任。\n9. 个人和团队：能够在多学科背景下的团队中承担个体、团队成员以及负责人的角色。\n10. 沟通：能够就复杂计算机工程问题与业界同行及社会公众进行有效沟通与交流，包括撰写\n报告和设计文稿、陈述发言、清晰表达个人见解等，并具备一定的国际视野，能够在跨文化背景下\n进行沟通和交流。\n11. 项目管理：具有一定的组织与工程管理能力、表达与人际交往能力，能够在多学科背景下\n的团队中发挥重要作用。\n12. 终身学习：具有自主学习和终身学习的意识，具有不断学习和适应本领域快速发展的能力；\n具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='具有坚持锻炼身体的良好习惯。\n计算机学院·计算机科学与技术专业\n5\n（三）核心课程与毕业要求关联图\n毕业要求\n主要专业必修课\n1)\n工程知识\n2)\n问题分析\n3)\n解决方案\n4)\n研究\n5)\n工具\n6)\n工程与社\n会\n7)\n环境与可\n持续发展\n8)\n职业规范\n9)\n个人与团', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='持续发展\n8)\n职业规范\n9)\n个人与团\n队\n10)\n沟通\n11)\n项目管理\n12)\n终身学习\n专业\n导论\n导论(信息类)\n√\n√\n√\n√\n√\n√\n基础\n理论\n离散数学(信息类)\n√\n√\n离散数学(2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='理论\n离散数学(信息类)\n√\n√\n离散数学(2)\n√\n√\n系统\n能力\n计算机组成\n√\n√\n√\n√\n√\n√\n操作系统\n√\n√\n√\n√\n√\n√\n√\n√\n编译技术\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n编译技术\n√\n√\n√\n√\n√\n√\n√\n√\n计算机网络\n√\n√\n√\n√\n√\n√\n√\n√\n软件\n能力\n数据结构与程序设\n计(信息类)\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计(信息类)\n√\n√\n√\n√\n√\n面向对象设计与构\n造\n√\n√\n√\n√\n√\n√\n√\n√\n√\n算法设计与分析\n√\n√\n√\n√\n√\n数据库系统原理\n√\n√\n√\n√\n√\n√', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='√\n√\n√\n√\n√\n√\n√\n√\n软件工程\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n√\n北京航空航天大学本科培养方案\n6\n三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三、学制、授予学位、最低毕业学分框架表\n本专业基本学制为 4 年，学生在学校规定的学习年限内，修完培养方案规定的内容，成绩合\n格，达到学校毕业要求的，准予毕业，学校颁发毕业证书；符合学士学位授予条件的，授予学士学\n位。\n毕业总学分：155\n授予学位类型：计算机科学与技术学士学位\n计算机科学与技术专业本科指导性最低学分框架表\n课程模块\n序列\n课程类别\n最低学分要求\n1 年级\n2-4 年级\n学分小计\nI\n基础课程\nA\n数学与自然科学类\n22', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='I\n基础课程\nA\n数学与自然科学类\n22\n7\n46\nB\n工程基础类\n9\n0\nC\n外语类\n4\n4\nII\n通修课程\nD\n思政类\n8.5\n10.5\n41\n军理类\n0\n4\nE\n体育类\n1\n2.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n4\nE\n体育类\n1\n2.5\nK\n素质教育理论必修课\n0\n2.5\nH\n素质教育实践必修课\n0.5\n1.5\nF/G\n素质教育通识限修课\n2.5\n7.5\nIII\n专业课程\nI\n核心专业类\n0\n48\n68\nJ\n一般专业类\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n48\n68\nJ\n一般专业类\n0\n20\n学分小计\n47.5\n107.5\n155\n毕业最低总学分\n155\n注：外语类课程、思政类课程、军理类课程、体育类课程、美育课程、劳动教育课程、心理健康、国家\n安全、素质教育实践必修课等修读要求见相关文件，其中：\n①劳动教育课程要求：至少选修劳动教育必修课或劳动教育模块学时总数≥32 学时及参加劳动月等活\n动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='动，详见每学期劳动教育课程清单。\n计算机学院·计算机科学与技术专业\n7\n②素质教育通识限修课：大类概论课、航空航天概论 B 为必修，至少还应包含 3 门人文类通识课（每门\n课程学分不得低于 2 学分；“法律、科技与社会”为必修）。\n③创新创业课程要求：至少选修 3 学分，详见每学期创新创业课程清单，修读要求见相应创新创业学分\n认定办法。\n④全英文课程要求：至少选修 2 学分全英文课程（外语类课程除外）。鼓励学生积极申报学校出国留学\n计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='计划，本培养方案提供课程学分置换。\n⑤对于已获得推免研究生的学生，可以选修研究生阶段的课程，学分计入研究生阶段培养方案。为确保\n学习成效，所选修课程总数不得超过 2 门。\n⑥一般专业类：至少需要从计算机学院为本专业设置的方向基础类课程清单中选修不少于 3 门课程。\n北京航空航天大学本科培养方案\n8\n四、课程设置与学分分布表\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n基\n础\n课\n程\n数\n学\n与\n自\n然\n科', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='数\n学\n与\n自\n然\n科\n学\n类\nB1A09104A 工科数学分析(1)\nMathematical Analysis for Engineering\n(1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09106A 工科高等代数\nAdvanced Algebra for Engineering (1)\n6\n96\n96', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Advanced Algebra for Engineering (1)\n6\n96\n96\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB1A09105A 工科数学分析(2)\nMathematical Analysis for Engineering\n(2)\n6\n96\n96\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB1A23204A 概率统计 A\nProbability Statistics\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB1A19104A 基础物理学(信息类)\nFundamental Physics(Information class)\n4\n64\n64\n0\n0\n一\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考试\n全汉语\nB1A19201B 工科大学物理(2)\nEngineering College physics(2)\n4\n64\n64\n0\n0\n二\n秋\n必修\n考试\n全汉语\n工\n程\n基\n础\n类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB1B061200\n程序设计基础\nFundamentals of Programming\n2\n48\n16\n32\n0\n一\n秋\n必修\n考试\n全汉语\nB1B021150\n电子设计基础训练\nBasic training in electronic design\n2\n56\n8\n48\n0\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n春\n必修\n考查\n全汉语\nB1B061060\n离散数学(信息类)\nDiscrete Mathematics (Information\nclass)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB1B061100\n数据结构与程序设计(信息类)\nData Structures and Programming\n(Information class)\n3\n64', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Data Structures and Programming\n(Information class)\n3\n64\n32\n32\n0\n一\n春\n必修\n考试\n全汉语\n外\n语\n类\nB1C12107A 大学英语 A(1)\nCollege English A (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n一\n秋\n必修\n考试\n全英文\nB1C12108A 大学英语 A(2)\nCollege English A (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207A 大学英语 A(3)\nCollege English A (3)\n2\n32\n32\n0\n0\n二', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208A 大学英语 A(4)\nCollege English A (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试\n全英文\nB1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B1C12107B 大学英语 B(1)\nCollege English B (1)\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全英文\nB1C12108B 大学英语 B(2)\nCollege English B (2)\n2\n32\n32\n0\n0\n一\n春\n必修\n考试\n全英文\nB1C12207B 大学英语 B(3)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全英文\nB1C12207B 大学英语 B(3)\nCollege English B (3)\n2\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全英文\nB1C12208B 大学英语 B(4)\nCollege English B (4)\n2\n32\n32\n0\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n二\n春\n必修\n考试\n全英文\n通\n思\nB2D281050\n思想道德与法治\nEthic Thought and Rule of Law\n3\n48\n48\n0\n0\n一\n秋\n必修\n考试\n全汉语\n计算机学院·计算机科学与技术专业\n9\n课程\n模块\n课程\n类别', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='9\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content="方式\n授课\n语言\n学年\n学期\n修\n课\n程\n政\n类\nB2D282060\n习近平新时代中国特色社会主\n义思想概论\nIntroduction to Xi Jinping's Thoughts on\nSocialism with Chinese Characteristics\nfor a New Era\n2\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2D281060", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='秋\n必修\n考试\n全汉语\nB2D281060\n中国近现代史纲要\nOutline of Modern Chinese History\n3\n48\n48\n0\n0\n一\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n思\n政\n类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='类\nB2D282080\n毛泽东思想和中国特色社会主\n义理论体系概论(1)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (1)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2D282090\n毛泽东思想和中国特色社会主\n义理论体系概论(2)\nMao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Mao Zedong Thought and the\nTheoretical System of Socialism with\nChinese Characteristics (2)\n2\n80\n0\n0\n80\n二\n寒假\n必修\n考查\n全汉语\nB2D282070\n马克思主义基本原理\nFundamental Principles of Marxism\n3\n48\n48\n0\n0\n二\n春\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n二\n春\n必修\n考试\n全汉语\nB2D281110\n形势与政策(1)\nSituation and Policy (1)\n0.2\n8\n4\n0\n4\n一\n秋\n必修\n考查\n全汉语\nB2D281120\n形势与政策(2)\nSituation and Policy (2)\n0.3\n8\n4\n0\n4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n8\n4\n0\n4\n一\n春\n必修\n考查\n全汉语\nB2D282110\n形势与政策(3)\nSituation and Policy (3)\n0.2\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D282120\n形势与政策(4)\nSituation and Policy (4)\n0.3\n8\n8\n0\n0\n二\n春\n必修\n考查\n全汉语\nB2D283110\n形势与政策(5)\nSituation and Policy (5)\n0.2\n8\n8\n0\n0\n三\n秋\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考查\n全汉语\nB2D283120\n形势与政策(6)\nSituation and Policy (6)\n0.3\n8\n8\n0\n0\n三\n春\n必修\n考查\n全汉语\nB2D284110\n形势与政策(7)\nSituation and Policy (7)\n0.2\n8\n8\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.2\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB2D284120\n形势与政策(8)\nSituation and Policy (8)\n0.3\n8\n8\n0\n0\n四\n春\n必修\n考查\n全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280110\n中国共产党历史\nThe History of the Chinese Communist\nParty\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n限修≥1\n学分\n考试\n全汉语\nB2D280120\n新中国史\nThe History of the People’s Republic of\nChina\n1\n16\n16\n0\n0\n一至', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280130\n改革开放史\nThe History of the Reform and Opening-\nup\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2D280140\n社会主义发展史\nThe History of Socialism Evolvement\n1\n16\n16\n0\n0\n一至\n四\n秋/春\n考试\n全汉语\n北京航空航天大学本科培养方案\n10\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n军理\n类\nB2D511040\n军事理论\nMilitary Theory\n2\n36', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2D511040\n军事理论\nMilitary Theory\n2\n36\n32\n0\n4\n二\n春\n必修\n考试\n全汉语\nB2D511030\n军事技能\nMilitary Skills\n2\n112\n0\n0\n112\n一\n夏\n必修\n考查\n全汉语\n体\n育\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\n体\n育\n类\nB2E331030\n体育(1)\nPhysical Education (1)\n0.5\n32\n32\n0\n0\n一\n秋\n必修\n考试\n全汉语\nB2E331040\n体育(2)\nPhysical Education (2)\n0.5\n32\n32\n0\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.5\n32\n32\n0\n0\n一\n春\n必修\n考试\n全汉语\nB2E332050\n体育(3)\nPhysical Education (3)\n0.5\n32\n32\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB2E332060\n体育(4)\nPhysical Education (4)\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='B2E332060\n体育(4)\nPhysical Education (4)\n0.5\n32\n32\n0\n0\n二\n春\n必修\n考试\n全汉语\nB2E333070\n体育(5)\nPhysical Education (5)\n0.5\n16\n16\n0\n0\n三\n秋\n必修\n考试\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n三\n秋\n必修\n考试\n全汉语\nB2E333080\n体育(6)\nPhysical Education (6)\n0.5\n16\n16\n0\n0\n三\n春\n必修\n考试\n全汉语\nB2E334030\n体质健康标准测试\n0.5\n0\n0\n0\n0\n四\n秋', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n0\n0\n四\n秋\n必修\n考试\n全汉语\n通\n修\n课\n程\n素\n质\n教\n育\n实\n践\n必\n修\n课\nB2H511110\n素质教育(博雅课程)(1)\nComprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Comprehensive Development\nEducation(Liberal Arts Course )(1)\n0.2\n16\n4\n0\n12\n一\n秋\n必修\n考查\n全汉语\nB2H511120\n素质教育(博雅课程)(2)\nComprehensive Development\nEducation(Liberal Arts Course )(2)\n0.3\n16\n4\n0\n12\n一\n春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='12\n一\n春\n必修\n考查\n全汉语\nB2H511130\n素质教育(博雅课程)(3)\nComprehensive Development\nEducation(Liberal Arts Course )(3)\n0.2\n16\n4\n0\n12\n二\n秋\n必修\n考查\n全汉语\nB2H511140\n素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='素质教育(博雅课程)(4)\nComprehensive Development\nEducation(Liberal Arts Course )(4)\n0.3\n16\n4\n0\n12\n二\n春\n必修\n考查\n全汉语\nB2H511150\n素质教育(博雅课程)(5)\nComprehensive Development\nEducation(Liberal Arts Course )(5)\n0.2\n16\n4\n0\n12\n三\n秋\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='4\n0\n12\n三\n秋\n必修\n考查\n全汉语\nB2H511160\n素质教育(博雅课程)(6)\nComprehensive Development\nEducation(Liberal Arts Course )(6)\n0.3\n16\n4\n0\n12\n三\n春\n必修\n考查\n全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2H511170\n素质教育(博雅课程)(7)\nComprehensive Development\nEducation(Liberal Arts Course )(7)\n0.2\n16\n4\n0\n12\n四\n秋\n必修\n考查\n全汉语\nB2H511180\n素质教育(博雅课程)(8)\nComprehensive Development\nEducation(Liberal Arts Course )(8)\n0.3\n16\n4\n0\n12\n四', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0.3\n16\n4\n0\n12\n四\n春\n必修\n考查\n全汉语\n素质\n教育\n理论\n美育类课程（至少 1.5 学分），各类课程见各学期开课清单\n1.5\n必修\n劳动教育课程（至少 32 学时），劳动教育必修课或劳动教育模块，详见每学期劳动\n教育课程清单\n32\n一至\n四\n秋/春\n必修\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='一至\n四\n秋/春\n必修\n考查\n全汉语\n计算机学院·计算机科学与技术专业\n11\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\n必修\n课\nB2K141010\n国家安全\nNational Security Education\n1\n16\n14\n0\n2\n一至\n三\n秋/春\n必修\n考查\n全汉语\n素', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n秋/春\n必修\n考查\n全汉语\n素\n质\n教\n育\n通\n识\n限\n修\n课\n概论课(大类内部)以下七门：\nB2F020390\n电子信息工程导论\nIntroduction to Electronic Information\nEngineering\n1.5\n24\n24\n0\n0\n一\n秋\n限修，', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='24\n0\n0\n一\n秋\n限修，\n≥1.5 学\n分\n考查\n全汉语\nB2F030360\n自动化科学与电气工程导论\nIntroduction of automation Science and\nelectrical engineering major\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\nB2F060110\n计算机导论与伦理学\nIntroduction to Computer Science and\nComputer Ethics\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F170210\n仪器科学概览\nOverview of Instrument Science\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n一\n秋\n考查\n全汉语\nB2F210120\n走进软件\nIntroduction to Software\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F390110\n网络空间安全导论\nIntroduction to Cyberspace Security\n1.5\n24\n24\n0\n0\n一', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F491110\n集成电路导论\nIntroduction to Integrated Circuit\n1.5\n24\n24\n0\n0\n一\n秋\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n通\n修\n课\n程\n素质\n教育\n通识\n限修\n课\n人文、经典、社科、科技文明 4 类素质教育课\n1\n秋/春\n限修，\n≥1 学分', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n秋/春\n限修，\n≥1 学分\n考查\n全汉语\nB2F050410\n航空航天概论 B\nIntroduction to Aeronautics and\nAstronautics B\n1.5\n24\n18\n6\n0\n二\n春\n必修\n考试\n全汉语\n3 门人文类核心通识课（法律、科技与社会为必选）\n2\n32\n32\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='2\n32\n32\n0\n0\n二\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n秋\n必修\n考查\n全汉语\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n专', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='三\n春\n必修\n考查\n全汉语\n专\n业\n课\n程\n核\n心\n专\n业\n类\nB3J063270\n科研课堂\nScientific Research Training\n2\n32\n0\n0\n32\n二\n秋\n必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3I063420\n社会课堂(生产实习)\nInternship\n5\n320\n0\n0\n320\n三\n夏\n必修\n考查\n全汉语\nB3I064410\n毕业设计\nGraduation Project\n8\n640\n0\n0\n640\n四\n春\n必修\n考查', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n640\n四\n春\n必修\n考查\n全汉语\nB3I061142\n离散数学 2\nDiscrete Mathematics (2)\n3\n48\n48\n0\n0\n二\n秋\n必修\n考试\n全汉语\nB3I062260\n计算机组成\nPrinciples of Computer Organization\n5.5\n112\n64\n48', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Principles of Computer Organization\n5.5\n112\n64\n48\n0\n二\n秋\n必修\n考试\n全汉语\n北京航空航天大学本科培养方案\n12\n课程\n模块\n课程\n类别\n课程代码\n中文课程名称\n英文课程名称\n总学\n分\n总学\n时\n理论\n学时\n实验\n学时\n实践', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='时\n理论\n学时\n实验\n学时\n实践\n学时\n开课学期\n课程性\n质及学\n习要求\n考核\n方式\n授课\n语言\n学年\n学期\nB3I062270\n操作系统\nOperating Systems\n4.5\n96\n48\n48\n0\n二\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='48\n0\n二\n春\n必修\n考试\n全汉语\nB3I063120\n编译技术\nCompiler Technology\n4.5\n96\n48\n48\n0\n三\n秋\n必修\n考试\n全汉语\nB3I062180\n面向对象设计与构造\nObject-oriented Design and\nConstruction\n2.5\n48\n32', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Object-oriented Design and\nConstruction\n2.5\n48\n32\n16\n0\n二\n春\n必修\n考试\n全汉语\nB3I063320\n算法设计与分析\nThe Design and Analysis of Algorithm\n2\n32\n32\n0\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063180\n数据库系统原理', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考试\n全汉语\nB3I063180\n数据库系统原理\nPrinciples of Database Systems\n4\n80\n48\n32\n0\n三\n秋\n必修\n考试\n全汉语\nB3I063130\n软件工程\nSoftware Engineering\n2\n32\n32\n0\n0\n三\n春\n必修\n考试', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063210\n计算机网络\nComputer Network\n2\n32\n32\n0\n0\n三\n春\n必修\n考试\n全汉语\nB3I063220\n计算机网络实验\nComputer Network Experiment\n1\n32\n0\n32\n0\n三', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='1\n32\n0\n32\n0\n三\n春\n必修\n考试\n全汉语\nB3I064120\n计算机科学方法论\nMethodology of Computer Science\n2\n32\n32\n0\n0\n三\n春\n必修\n考查\n全汉语\n一\n般\n专\n业\n类', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='全汉语\n一\n般\n专\n业\n类\nB3J062610\n职业规划与选择讲座\nProfession Planning and Choice\n0.5\n8\n8\n0\n0\n二\n秋\n必修\n考查\n全汉语\nB3J063610\n学科技术前沿讲座\nSeminars on the Development of\nComputer Science\n1\n16\n16\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='Computer Science\n1\n16\n16\n0\n0\n三\n春\n必修\n考查\n全汉语\nB3J064610\n求职辅导系列讲座\nSerial Lectures on How to Seek\nEmployment\n0.5\n8\n8\n0\n0\n四\n秋\n必修\n考查\n全汉语\nB3J062410\n实践与展示(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='必修\n考查\n全汉语\nB3J062410\n实践与展示(1)\nPractice and Presentation(Ⅰ)\n1\n32\n0\n0\n32\n二\n春\n必修\n考查\n全汉语\nB3J062420\n实践与展示(2)\nPractice and Presentation (2)\n1\n32\n0\n0\n32\n三\n春\n必修', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='0\n0\n32\n三\n春\n必修\n考查\n全汉语\n共计选修 16 学分的专业选修课。\n16\n计算机学院·计算机科学与技术专业\n13\n五、核心课程先修逻辑关系图\n六、专业准入办法一览表\n为更好的体现学生结合自身学习特点和兴趣以更好的选择专业，本专业分别在 1 年级春季学\n期、2 年级秋季学期和 2 年级春季学期即将结束时转入转出申请。\n对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='对于申请转入者，本专业将对所有达到要求的申请者进行综合面试从中选择适合进入本专业\n学习的候选者，同时结合候选者其他相关课程的学习决定其是否同级转专业还是降级转专业。\n关于具体准入条件，请参考《北京航空航天大学计算机学院转专业管理规定》。该文件可以在\n学校转专业相关网页查阅到。\n七、毕业生未来发展图\n除了升学深造外，由于计算机专业社会需求广泛，因此本专业毕业生具有广泛的就业空间及发\n展可能。本培养方案仅给出部分可能的发展规划，具体内容参见下表。\n主分类\n次分类\n描述\n就业\nIT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'}), Document(page_content='IT 企业\n以计算机软硬件开发、测试、系统维护为主\n非 IT 企业\n以领域应用软件开发、系统维护为主\n北京航空航天大学本科培养方案\n14\n自主创业\n升学\n国内深造\n软件与理论方向：大学、中科院计算所、中科院软件所\n体系结构方向：大学、中科院计算所、国防系统研究所\n计算机应用方向：大学、中科院相关院所、国防系统研究所\n出国深造\n国外大学攻读硕士学位、博士学位', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpcepy4xh4/06-计算机学院-计算机科学与技术专业.pdf'})]
cuda:2
[UploadFile(filename='2403.10825v1.pdf', size=515347, headers=Headers({'content-disposition': 'form-data; name="files"; filename="2403.10825v1.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpuheaomt1, tmpuheaomt1
File: 2403.10825v1.pdf, msg: 成功上传文件 2403.10825v1.pdf, docs: [Document(page_content='Affective Behaviour Analysis via Integrating Multi-Modal Knowledge\nWei Zhang1,∗, Feng Qiu1,∗, Chen Liu1,2, Lincheng Li1, †, Heming Du2, Tiancheng Guo2, Xin Yu2\n1 Netease Fuxi AI Lab\n2 The University of Queensland\n{zhangwei05, qiufeng, lilincheng}@corp.netease.com, chen.liu7@uqconnect.edu.au,\n{Heming.du, xin.yu}@uq.edu.au, alan5gtc@gmail.com\nAbstract\nAffective Behavior Analysis aims to facilitate technol-\nogy emotionally smart, creating a world where devices can\nunderstand and react to our emotions as humans do. To', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Affective Behavior Analysis aims to facilitate technol-\nogy emotionally smart, creating a world where devices can\nunderstand and react to our emotions as humans do. To\ncomprehensively evaluate the authenticity and applicabil-\nity of emotional behavior analysis techniques in natural\nenvironments, the 6th competition on Affective Behavior\nAnalysis in-the-wild (ABAW) utilizes the Aff-Wild2, Hume-\nVidmimic2, and C-EXPR-DB datasets to set up five com-\npetitive tracks, i.e., Valence-Arousal (VA) Estimation, Ex-\npression (EXPR) Recognition, Action Unit (AU) Detection,\nCompound Expression (CE) Recognition, and Emotional\nMimicry Intensity (EMI) Estimation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Compound Expression (CE) Recognition, and Emotional\nMimicry Intensity (EMI) Estimation.\nIn this paper, we\npresent our method designs for the five tasks. Specifically,\nour design mainly includes three aspects: 1) Utilizing a\ntransformer-based feature fusion module to fully integrate\nemotional information provided by audio signals, visual im-\nages, and transcripts, offering high-quality expression fea-\ntures for the downstream tasks. 2) To achieve high-quality\nfacial feature representations, we employ Masked-Auto En-\ncoder as the visual features extraction model and fine-tune it\nwith our facial dataset. 3) Considering the complexity of the\nvideo collection scenes, we conduct a more detailed dataset\ndivision based on scene characteristics and train the classi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='with our facial dataset. 3) Considering the complexity of the\nvideo collection scenes, we conduct a more detailed dataset\ndivision based on scene characteristics and train the classi-\nfier for each scene. Extensive experiments demonstrate the\nsuperiority of our designs.\n1. Introduction\nAffective Behavior Analysis is dedicated to enhancing the\nemotional intelligence of artificial intelligence systems by\nanalyzing and understanding human emotional behavior\n[30, 32–39, 54, 58, 75, 77, 80].\nIt involves identifying\nand interpreting the emotions and feelings people express\nthrough facial expressions, voice, body language, etc. The\nEqual contribution\n†Corresponding author\ngoal is to enable computers and robots to better under-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Equal contribution\n†Corresponding author\ngoal is to enable computers and robots to better under-\nstand human emotional states for more natural and effec-\ntive human-machine interactions, support mental monitor-\ning, and improve applications in education, entertainment,\nand social interactions [17, 19, 56, 63, 64].\nThe\n6th\nAffective\nBehavior\nAnalysis\ncompetition\n(ABAW6) has set up the following five tasks to analyze\nvarious aspects of human emotions and expressions. Ac-\ntion Unit (AU) Detection aims to identify facial action\ntypes from the Facia Action Coding System based on fa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='tion Unit (AU) Detection aims to identify facial action\ntypes from the Facia Action Coding System based on fa-\ncial muscle movements [1, 37, 46, 65].\nCompound Ex-\npression (CE) Recognition requires recognizing complex\nexpressions that combine two or more basic expressions\n[12, 23, 60, 69]. Emotional Mimicry Intensity (EMI) Es-\ntimation evaluates the intensity of an individual’s emotional\nmimicry [18, 24, 41, 70]. Expression Recognition (EXPR)\nidentifies basic emotional expressions like happiness, sad-\nness, and anger [16, 44, 57, 69, 84]. Valence-arousal (VA)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ness, and anger [16, 44, 57, 69, 84]. Valence-arousal (VA)\nestimation determines people’s emotional states on contin-\nuous emotional dimensions, where “valence” refers to the\npositivity or negativity of the emotion, and “arousal” refers\nto the level of emotional activation [27, 31, 37, 47, 55].\nTo enhance the applicability of affective behavior anal-\nysis techniques in the real world, ABAW6 assesses the\nmethod performance on Aff-Wild2 [29], C-EXPR-DB [28],\nand Hume-Vidmimic2 [39], in which videos are captured in\nuncontrolled natural environments. Specifically, Aff-Wild2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='and Hume-Vidmimic2 [39], in which videos are captured in\nuncontrolled natural environments. Specifically, Aff-Wild2\nshowcases individuals of different skin tones, ages, and gen-\nders, under varied lighting, with assorted backgrounds and\nhead poses, thereby enriching its diversity and applicabil-\nity. C-EXPR-DB is designed to analyze multiple emotions\nthat occur simultaneously on the face. It consists of videos\nsourced from YouTube, which feature naturally occurring\nemotions and expressions. Hume-Vidmimic2 emphasizes\ncapturing and analyzing the complexity of human emotions\nin a manner that closely mirrors natural human interactions.\nIt bridges the gap between the controlled environment of\nmost emotion recognition datasets and the unpredictability\nand richness of the natural world.\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='It bridges the gap between the controlled environment of\nmost emotion recognition datasets and the unpredictability\nand richness of the natural world.\n1\narXiv:2403.10825v1  [cs.CV]  16 Mar 2024\nBased on the characteristics of the above datasets, we es-\ntablish our objectives to fully utilize the emotional informa-\ntion provided in multimodal data and to enhance the appli-\ncability of our method in real-world scenarios. In this paper,\nwe detail our method designs in three aspects. Firstly, to ob-\ntain high-quality image features. we integrate a large-scale\nfacial image dataset and utilize the self-supervised model\nMasked Auto Encoder (MAE) [22, 79] to learn deep feature', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='facial image dataset and utilize the self-supervised model\nMasked Auto Encoder (MAE) [22, 79] to learn deep feature\nrepresentations from these emotional data, enhancing the\nperformance of downstream tasks. Moreover, we leverage\na transformer-based model to fuse the multi-modal infor-\nmation. This architecture facilitates the interactions across\nmodalities (i.e., audio, visual, text) and provides scalable,\nefficient, and effective solutions for integrating multimodal\ninformation [71]. Lastly, we adopt an ensemble learning\nstrategy to improve the applicability of our method in var-\nious scenes. In this strategy, we divide the whole dataset\ninto multiple sub-datasets according to their distinct back-\nground characteristics and assign these sub-datasets to dif-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='into multiple sub-datasets according to their distinct back-\nground characteristics and assign these sub-datasets to dif-\nferent classifiers. After that, we integrate the outputs of\nthese classifiers to obtain the final prediction results.\nExperiments conducted on the three datasets demon-\nstrate the effectiveness of our design choices. Overall, our\ncontributions are three-fold:\nWe integrate a large-scale facial expression dataset and\nfine-tune MAE on it to obtain an effective facial ex-\npression feature extractor, enhancing the performance for\ndownstream tasks.\nWe employ a transformer-based multi-modal integration\nmodel to facilitate the interactions of multi-modalities,\nenriching the expression features extracted from multi-\nmodal data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='model to facilitate the interactions of multi-modalities,\nenriching the expression features extracted from multi-\nmodal data.\nWe adopt an ensemble learning strategy, which trains\nmultiple classifiers on sub-datasets with different back-\nground characteristics and ensemble the results of these\nclassifiers to attain the final results. This strategy enables\nour method to generalize better in various environments.\n2. Related Work\n2.1. Action Unit Detection\nDetecting Action Units (AU) in the wild is a challenging\nyet crucial advancement task in facial expression analysis,\npushing the boundaries of applicability from controlled lab-\noratory settings to real-world environments [1, 46, 65]. This\nendeavor addresses the inherent variability in lighting, pose,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='oratory settings to real-world environments [1, 46, 65]. This\nendeavor addresses the inherent variability in lighting, pose,\nocclusion, and emotional context encountered in natural en-\nvironments [37]. Recent works highlight the effectiveness\nof multi-task frameworks in leveraging extra regularization,\nsuch as the extra label constraint, to enhance detection per-\nformance. Zhang et al. [78] introduce a streaming model\nto concurrently execute AU detection, expression, recogni-\ntion, and Valence-Arousal (VA) regression. Cui et al. [8]\npresent a biomechanics-guided AU detection approach to\nexplicitly incorporate facial biomechanics for AU detection.\nMoreover, to achieve robust and generalized AU detection,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='present a biomechanics-guided AU detection approach to\nexplicitly incorporate facial biomechanics for AU detection.\nMoreover, to achieve robust and generalized AU detection,\nsome works take generic knowledge (i.e. static spatial mus-\ncle relationships) into account [7], while others consider in-\ntegrating multi-modal knowledge to obtain rich expression\nfeatures [82].\n2.2. Compound Expression Recognition\nCompound Expression Recognition (CER) gains attention\nfor identifying complex facial expressions that convey a\ncombination of basic emotions, reflecting more nuanced hu-\nman emotional states [12, 23]. Typical methods focus on\nrecognizing basic emotional expressions with deep learning\nmethods, paving the way for more advanced methods ca-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='recognizing basic emotional expressions with deep learning\nmethods, paving the way for more advanced methods ca-\npable of deciphering compound expressions [4, 20, 25, 76].\nNotable efforts in this area include leveraging convolutional\nneural networks for feature extraction and employing re-\ncurrent neural networks or attention mechanisms to capture\nthe subtleties and dynamics of facial expressions over time.\nResearchers have also explored multi-task learning frame-\nworks to simultaneously recognize basic expressions more\naccurately and robustly [21, 44, 68]. Due to the complex-\nity of human emotions in the real world, detecting a single\nexpression is not suitable for real-life scenarios. Therefore,\nDimitrios [28] curates a Multi-Label Compound Expression', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='expression is not suitable for real-life scenarios. Therefore,\nDimitrios [28] curates a Multi-Label Compound Expression\ndataset, C-EXPR. Besides, he also proposes C-EXPR-NET,\nwhich addresses both CER and AU detection tasks simulta-\nneously, achieving improved results in recognizing multiple\nexpressions [28].\n2.3. Emotional Mimicry Intensity Estimation\nEmotional Mimicry Intensity (EMI) Estimation delves into\nthe nuanced realm of how individuals replicate and respond\nto the emotional expressions of others [41, 70]. It aims to\nquantify the degree of mimicry and its emotional impact.\nTraditionally, facial mimicry has been quantified through\nthe activation of facial muscles, either measured by elec-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Traditionally, facial mimicry has been quantified through\nthe activation of facial muscles, either measured by elec-\ntromyography (EMG) or analyzed through the frequency\nand intensity of facial muscle movements via the Facial\nAction Coding System (FACS) [15]. However, these tech-\nniques are either invasive or require significant time and ef-\nfort. Recent advancements [11, 11, 66] leverage computer\nvision and statistical methods to estimate facial expressions,\npostures, and emotions from video recordings, enabling the\nidentification of facial and behavioral mimicry. Despite be-\ning currently less precise than physiological signal-based\nmeasurements, this video-based approach is non-invasive,\nautomatable, and applicable to multimodal contexts, mak-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='measurements, this video-based approach is non-invasive,\nautomatable, and applicable to multimodal contexts, mak-\ning it scalable for real-time, real-world uses, such as in\nhuman-agent social interactions.\n2\nI am very happy to attend…\nInput Frames\nK\nAudio Signal\nText Description\nOnly for EMI\nImage\nEncoder\nAudio\nEncoder\nText\nEncoder\nConcat\nTransformer\nEncoder𝑻𝟏\nDownstream Tasks\nAU\nCE\nEMI', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Encoder𝑻𝟏\nDownstream Tasks\nAU\nCE\nEMI\nEXPR\nVA\nTransformer\nEncoder 𝑻𝟐\nTransformer\nEncoder 𝑻𝟑\n…\nResults\nEnsemble\nEnsemble Learning\n𝐹𝐼\n𝐹𝐴\n𝐹𝑇\nFigure 1. The overview of our proposed method. We first utilize the images in the facial image datasets to train the Image Encoder in a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='𝐹𝑇\nFigure 1. The overview of our proposed method. We first utilize the images in the facial image datasets to train the Image Encoder in a\nself-supervised manner, thus obtaining the visual feature FI. Then we leverage the pre-trained audio encoder and text encoder to attain the\naudio feature FA and text feature FT . Note that we only devise the text encoder for the EMI task. Subsequently, we concat these features\nand feed them into the Transformer Encoders. Here, we train these encoders on subsets divided based on background characteristics.\nFinally, we employ a voting strategy to attain the final results.\n2.4. Expression Recognition\nExpression Recognition has witnessed substantial growth,\ndriven by the integration of psychological insights and ad-\nvanced deep learning techniques [44, 57, 69]. Recently, the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='driven by the integration of psychological insights and ad-\nvanced deep learning techniques [44, 57, 69]. Recently, the\nadaptation of transformer-based models from natural lan-\nguage processing (NLP) [67] to computer vision tasks [13]\nhas led to their application in extracting spatial and tem-\nporal features from video sequences for emotion recogni-\ntion.\nNotably, Zhao et al.\n[83] introduce a transformer\nmodel specifically for dynamic facial expression recogni-\ntion, the Former-DFER, which includes CSFormer [73] and\nT-Former [73] modules to learn spatial and temporal fea-\ntures, respectively.\nMa et al.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='T-Former [73] modules to learn spatial and temporal fea-\ntures, respectively.\nMa et al.\n[51] developed a Spatio-\nTemporal Transformer (STT) that captures both spatial and\ntemporal information through a transformer-based encoder.\nAdditionally, Li et al. [43] proposed the NR-DFERNet,\ndesigned to minimize the influence of noisy frames within\nvideo sequences. While these advancements represent sig-\nnificant progress in addressing the challenges of dynamic\nfacial expression recognition (DFER) with discrete labels,\nthey overlook the interference from the background in im-\nages. To address this, we incorporate ensemble learning\ninto our method.\n2.5. Valence-arousal Estimation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ages. To address this, we incorporate ensemble learning\ninto our method.\n2.5. Valence-arousal Estimation\nValence-arousal estimation focuses on mapping emotional\nstates onto a two-dimensional space, where valence repre-\nsents the positivity or negativity of emotion, and arousal in-\ndicates its intensity or activation level [27, 31, 37]. Conven-\ntional approaches mainly relied on physiological signals,\nsuch as heart rate or skin conductance, to estimate these di-\nmensions [2, 40, 42]. However, with advancements in deep\nlearning, researchers shift towards leveraging visual and au-\nditory cues from facial expressions, voice tones, and body\nlanguage. Notably, convolutional neural networks and re-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ditory cues from facial expressions, voice tones, and body\nlanguage. Notably, convolutional neural networks and re-\ncurrent neural networks have been extensively applied to\ncapture the nuanced and dynamic aspects of emotions from\nimages, videos, and audio data [3, 52, 72].\nRecent studies introduce transformer models to better\nhandle the sequential and contextual nature of emotional\nexpressions in multi-modal data [5, 26, 61]. These improve-\nments have not only improved the accuracy and efficiency\nof valence-arousal estimation but also broadened its appli-\ncability in real-world scenarios, such as human-computer\ninteraction and mental health assessment [14, 50, 62]. De-\nspite progress, challenges remain in capturing the complex', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='interaction and mental health assessment [14, 50, 62]. De-\nspite progress, challenges remain in capturing the complex\nand subjective nature of emotions, necessitating further re-\nsearch into model interpretability and the integration of di-\nverse data sources.\n3. Method\nIn this section, we describe our method for analyzing hu-\nman affective behavior. The architecture flow is illustrated\nin Fig. 1. The proposed approach addresses two critical\nproblems: 1) the emotional information in the multimodal\ndata is not fully explored and 2) the model has poor gener-\nalization ability for videos with complex backgrounds. For\na clear exposition, we first introduce how we utilize the en-\ncoders to extract features from multi-modal data in Sec. 3.1.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='a clear exposition, we first introduce how we utilize the en-\ncoders to extract features from multi-modal data in Sec. 3.1.\nThen we detail the transformer-based multi-modal feature\nfusion method in Sec. 3.2. Finally, in Sec. 3.3, we present\nthe ensemble learning strategy that is leveraged to enhance\nthe model generalization ability.\n3\n3.1. Feature Extraction Encoder\nImage Encoder. In this work, we employ MAE as the im-\nage encoder since its self-supervised training manner en-\nables the extracted features more generalizable.\nTo fur-\nther attain powerful and expressive features, we construct\na large-scale facial image dataset which consists of Affect-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='To fur-\nther attain powerful and expressive features, we construct\na large-scale facial image dataset which consists of Affect-\nNet [53], CASIA-WebFace [74], CelebA [48], IMDB-WIKI\n[59], and WebFace260M [85].\nThe total number of our\nintegrated dataset is 262M. Based on the integrated facial\ndataset, we finetune MAE through facial image reconstruc-\ntion.\nSpecifically, in the pre-training phase, our method\nadopts the “mask-then-reconstruct” strategy. Here images\nare dissected into multiple patches (measuring 16×16 pix-\nels), with a random selection of 75% being obscured. These', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='are dissected into multiple patches (measuring 16×16 pix-\nels), with a random selection of 75% being obscured. These\nmasked images are then input into the encoder, while the de-\ncoder restores them to the corresponding original. We adopt\nthe pixel-wise L2 loss to optimize the model, ensuring the\nreconstructed facial images closely mirror the originals.\nAfter the pre-training, we modify the model for specific\ndownstream tasks by detaching the MAE decoder and incor-\nporating a fully connected layer to the end of the encoder.\nThis alternation facilitates the model to better adapt to the\ndownstream tasks.\nAudio Encoder. Considering that the tone and intonation of\nthe speech can also reflect certain emotional information,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='downstream tasks.\nAudio Encoder. Considering that the tone and intonation of\nthe speech can also reflect certain emotional information,\nwe leverage VGGish [6] as our audio encoder to generate\nthe audio representation. Given that VGGish is trained on\nthe large-scale dataset VGGSound and can capture a wide\nrange of audio features, we directly utilize it as the feature\nextractor without training on our dataset.\nText Encoder. Compared to other tracks, EMI not only pro-\nvides audio and visual frames but also includes a transcript\nfor each video. Here, we employ the large off-the-shelf\nmodel LoRA [10] to extract features from the transcript.\n3.2. Transformer-based Multi-modal Fusion\nWe fuse features across different modalities to obtain more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='3.2. Transformer-based Multi-modal Fusion\nWe fuse features across different modalities to obtain more\nreliable emotional features and utilize the fused feature for\ndownstream tasks. By combining information from vari-\nous modalities such as visual, audio, and text, we achieve a\nmore comprehensive and accurate emotion representation.\nTo align the three modalities at the temporal dimension,\nwe trim each video into multiple clips with k frames. For\neach frame, we employ our image encoder to extract the\nvisual feature fI. In this fashion, we attain the visual fea-\nture F K×d\nI\nfor the whole clip. Here, d represents the fea-\nture dimension. Meanwhile, we employ the audio and text\nencoders to generate the features for the whole clip, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ture dimension. Meanwhile, we employ the audio and text\nencoders to generate the features for the whole clip, and\nthe features are expressed by F 1×d\nA\nand F 1×d\nT\n, respectively.\nSubsequently, we concat these features and input them into\nthe Transformer Encoder. Specifically, our transformer en-\ncoder consists of four encoder layers with a dropout rate of\n0.3. The output is then fed into a fully connected layer to\nadjust the final output dimension according to the task re-\nquirements. Note that, at the feature fused stage, the image\nencoder, audio encoder, and text encoder are all fixed, while\nonly the transformer encoder as well as the fully connected', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='encoder, audio encoder, and text encoder are all fixed, while\nonly the transformer encoder as well as the fully connected\nlayer are trainable.\n3.3. Ensemble Learning\nTo improve the applicability of affective behavior analysis\nmethods, the 6th ABAW leverages the datasets collected\nfrom the real world as the test data. Given the complex\nbackgrounds in the videos, we adopt the ensemble learn-\ning strategy to enable our method robust against complex\nenvironments.\nSpecifically, we first partition the dataset\ninto multiple subsets according to the background charac-\nteristics, ensuring each subset contains images with similar\nbackground properties. Next, we separately train the classi-\nfiers for each subset to effectively capture emotional infor-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='teristics, ensuring each subset contains images with similar\nbackground properties. Next, we separately train the classi-\nfiers for each subset to effectively capture emotional infor-\nmation within the images.\nDuring the inference stage, we integrate predictions from\nclassifiers on each subset via a voting method. Specifically,\nfor each sample, we allow classifiers from each subset to\nclassify it and record the predictions from each classifier.\nFinally, we employ a voting mechanism based on these pre-\ndictions to determine the ultimate label. Here, we select the\nlabel with the highest number of votes as the final classi-\nfication result. Our voting method effectively reduces er-\nrors caused by biases in classifiers from individual subsets,\nthereby enhancing overall classification performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fication result. Our voting method effectively reduces er-\nrors caused by biases in classifiers from individual subsets,\nthereby enhancing overall classification performance.\n3.4. Training Objectives\nObjectives for Image Encoder. To enhance the adaptabil-\nity of the Image Encoder across various tasks, we fine-tune\nit for each downstream task. Specifically, when dealing with\nAU and EXPR, we optimize the model via cross-entropy\nloss LAU CE and LEXP R−CE, respectively. They are de-\nfined as follows:\nLAU−CE = − 1\n12\n12\nX\nj=1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='LAU−CE = − 1\n12\n12\nX\nj=1\nWauj [yj log ˆyj + (1 − yj) log (1 − ˆyj)] ,\n(1)\nLEXP R−CE = −1\n8\n8\nX\nj=1\nWexp−jzj log ˆzj,\n(2)\nwhere ˆy and ˆz represent the predicted results for the action\nunit and expression category respectively, whereas y and z\ndenote the ground truth values for the action unit and ex-\npression category.\nIn the VA task, to better capture the correlation between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='denote the ground truth values for the action unit and ex-\npression category.\nIn the VA task, to better capture the correlation between\nvalence and arousal and thus improve the accuracy of emo-\ntion recognition, we leverage the consistency correlation co-\n4\nefficient as the model optimization function, defined as:\nCCC(X, ˆ\nX) =\n2ρX ˆ\nX δX δ ˆ\nX\nδ2\nX + δ2\nˆ\nX +\nTable 1. The AU F1 scores (in %) of models that are trained and tested on different folds (including the original training/validation set of\nAff-Wild2 dataset).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Aff-Wild2 dataset).\nVal Set\nAU1\nAU2\nAU4\nAU6\nAU7\nAU10\nAU12\nAU15\nAU23\nAU24\nAU25\nAU26\nAvg.\nOfficial\n55.29\n51.40\n65.81\n68.61\n76.08\n75.00\n75.24\n37.65\n18.89', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='75.00\n75.24\n37.65\n18.89\n30.89\n83.41\n44.98\n56.94\nfold-1\n62.61\n46.20\n71.22\n77.71\n67.44\n69.69\n74.62\n36.32\n29.43\n21.75\n81.56\n40.73', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='29.43\n21.75\n81.56\n40.73\n56.61\nfold-2\n64.23\n54.35\n73.85\n77.33\n77.49\n76.70\n80.74\n29.05\n28.96\n18.47\n87.71\n43.63\n59.37\nfold-3\n58.55', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='43.63\n59.37\nfold-3\n58.55\n48.37\n60.05\n71.22\n72.43\n74.29\n75.43\n29.81\n19.52\n32.86\n83.37\n47.63\n56.13\nfold-4\n53.34\n39.34\n66.26\n70.67', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='53.34\n39.34\n66.26\n70.67\n66.51\n69.39\n71.76\n39.49\n25.17\n32.40\n82.27\n40.05\n54.72\nfold-5\n53.50\n44.68\n63.45\n72.02\n69.72\n74.00\n78.24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='72.02\n69.72\n74.00\n78.24\n38.81\n23.67\n7.56\n81.24\n43.67\n54.22\nvideos of around 2.7M frames that are annotated in terms of\nthe 6 basic expressions (i.e., anger, disgust, fear, happiness,\nsadness, surprise), plus the neutral state, plus a category\n‘other’ that denotes expressions/affective states other than\nthe 6 basic ones. The performance measure is the average\nF1 Score across all 8 categories. The VA estimation track', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the 6 basic ones. The performance measure is the average\nF1 Score across all 8 categories. The VA estimation track\nutilizes 594 videos of around 3M frames of 584 subjects an-\nnotated in terms of valence and arousal. The performance\nmeasure is the mean Concordance Correlation Coefficient\n(CCC) of valence and arousal.\nThe fourth track of ABAW6 utilizes 56 videos which\nare from the C-EXPR-DB database.\nThe complete C-\nEXPR-DB dataset contains 400 videos totaling approxi-\nmately 200,000 frames, with each frame annotated for 12\ncompound expressions. For this track, the task is to pre-\ndict 7 compound expressions for each frame in a subset of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='compound expressions. For this track, the task is to pre-\ndict 7 compound expressions for each frame in a subset of\nthe C-EXPR-DB videos. Specifically, the 7 compound ex-\npressions are Fearfully Surprised, Happily Surprised, Sadly\nSurprised, Disgustedly Surprised, Angrily Surprised, Sadly\nFearful, and Sadly Angry. The evaluation metric for this\ntrack is the average F1 Score across all 7 categories.\nThe fifth track of ABAW6 is based on the multimodal\nHume-Vidmimic2 dataset which consists of more than\n15,000 videos totaling over 25 hours. Each subject of this\ndataset needs to imitate a ‘seed’ video, replicating the spe-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='15,000 videos totaling over 25 hours. Each subject of this\ndataset needs to imitate a ‘seed’ video, replicating the spe-\ncific emotion displayed by the individual in the video. Then\nthe imitators are asked to annotate the emotional intensity of\nthe seed video using a range of predefined emotional cate-\ngories (Admiration, Amusement, Determination, Empathic\nPain, Excitement, and Joy). A normalized score from 0 to 1\nis provided as a ground truth value for each seed video and\neach performance video of the imitator. The evaluation met-\nric for this track is the average Pearson’s correlation across\nthe 6 emotion dimensions.\nIn addition to the official datasets mentioned above, we\nalso used some additional data from the open-source and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the 6 emotion dimensions.\nIn addition to the official datasets mentioned above, we\nalso used some additional data from the open-source and\nprivate datasets. For the AU detection track, we use the ex-\ntra dataset BP4D [81] to supplement some of the limited\nAU categories in Aff-wild2. For the expression recognition\ntrack, we use the extra dataset RAF-DB [45] and Affect-\nNet [53] to supplement the Anger, Disgust, and Fear data.\nFor the fourth track, we utilize our private video dataset and\nannotated these videos based on the rules of 7 compound\nexpressions for training and testing.\n4.3. Implementatal Setting\nWe utilize retinaface [9] to detect faces for each frame and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='expressions for training and testing.\n4.3. Implementatal Setting\nWe utilize retinaface [9] to detect faces for each frame and\nnormalize them to a size of 224x224.\nWe pre-train an\nMAE on a large facial images dataset that consists of sev-\neral open-source face images datasets (i.e., AffectNet [53],\nCASIA-WebFace [74], CelebA [48] and IMDB-WIKI [59],\nWebface260M [85]). We use this MAE as the basic feature\nextractor to capture the visual information for facial images\nin each track. The pre-training process is trained for 800\nepochs with a batch size of 4096 on 8 NVIDIA A30 GPUs,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='in each track. The pre-training process is trained for 800\nepochs with a batch size of 4096 on 8 NVIDIA A30 GPUs,\nusing the AdamW optimizer [49]. For the tasks of AU de-\ntection, expression recognition and VA estimation, we in-\ncorporate the temporal, audio, and other information to fur-\nther improve the performance. At this stage, the training\ndata consists of continuous video clips of 100 frames. The\nlearning rate is set as 0.0001 using the AdamW optimizer.\nTo reduce the gap caused by data division, we conduct five-\nfold cross-validation for all the tracks.\n4.4. Results for AU Detection\nIn this section, we show our final results for the task of AU', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fold cross-validation for all the tracks.\n4.4. Results for AU Detection\nIn this section, we show our final results for the task of AU\ndetection. The model is evaluated by the average F1 score\nfor 12 AUs. Table 1 presents the F1 results on the official\nvalidation set and five-fold cross-validation set.\n4.5. Results for Expression Recognition\nIn this section, we show our final results for the task of ex-\npression recognition. The model is evaluated by the average\nF1 score for 8 categories. Table 2 presents the F1 results on\nthe official validation set and five-fold cross-validation set.\n4.6. Results for VA Estimation\nIn this section, we show our final results for the task of VA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='4.6. Results for VA Estimation\nIn this section, we show our final results for the task of VA\nestimation. The model is evaluated by CCC for valence and\narousal. Table 4 presents the F1 results on the official vali-\ndation set and five-fold cross-validation set.\n6\nTable 2. The expression F1 scores (in %) of models that are trained and tested on different folds (including the original training/validation\nset of Aff-Wild2 dataset).\nVal Set\nNeutral\nAnger\nDisgust\nFear\nHappiness\nSadness\nSurprise\nOther\nAvg.\nOfficial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Happiness\nSadness\nSurprise\nOther\nAvg.\nOfficial\n70.21\n73.93\n50.34\n21.83\n59.05\n66.41\n36.51\n66.11\n55.55\nfold-1\n70.06\n37.21\n32.12\n22.71\n61.77\n77.61\n45.62', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='22.71\n61.77\n77.61\n45.62\n51.58\n49.83\nfold-2\n67.36\n44.45\n21.21\n42.50\n62.22\n78.24\n36.67\n70.00\n52.83\nfold-3\n73.64\n71.60\n45.01\n23.25', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='73.64\n71.60\n45.01\n23.25\n47.67\n77.05\n46.81\n65.56\n56.32\nfold-4\n65.41\n71.00\n53.70\n23.27\n61.62\n61.79\n27.76\n72.68\n54.65\nfold-5\n64.03', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='72.68\n54.65\nfold-5\n64.03\n31.23\n35.66\n67.64\n67.97\n69.75\n52.12\n55.64\n55.51\nTable 3. The Pearson’s correlations of models that are trained and tested on different folds (including the original training/validation set of\nHume-Vidmimic2. ).\nAdmiration\nAmusement\nDetermination\nEmpathic Pain\nExcitement\nJoy', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Admiration\nAmusement\nDetermination\nEmpathic Pain\nExcitement\nJoy\nAvg.\nOfficial\n0.5942\n0.4982\n0.5090\n0.2275\n0.4961\n0.4580\n0.4638\nfold-1\n0.5880\n0.4842\n0.4914\n0.2089\n0.4852\n0.4338', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='0.4914\n0.2089\n0.4852\n0.4338\n0.4486\nfold-2\n0.5193\n0.4385\n0.4031\n0.3715\n0.3734\n0.3717\n0.4129\nfold-3\n0.5195\n0.4496\n0.3947\n0.4924\n0.4129\n0.3843', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='0.3947\n0.4924\n0.4129\n0.3843\n0.4422\nfold-4\n0.5955\n0.4950\n0.5134\n0.2492\n0.5068\n0.4576\n0.4696\nfold-5\n0.5199\n0.4377\n0.4040\n0.3739\n0.3717\n0.3719', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='0.4040\n0.3739\n0.3717\n0.3719\n0.4132\nTable 4. The VA CCC scores of models that are trained and tested\non different folds (including the original training/validation set of\nAff-Wild2 dataset).\nVal Set\nValence\nArousal\nAvg.\nOfficial\n0.5523\n0.6531\n0.6027\nfold-1\n0.6408\n0.6195\n0.6302', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fold-1\n0.6408\n0.6195\n0.6302\nfold-2\n0.6033\n0.6758\n0.6395\nfold-3\n0.6773\n0.6961\n0.6867\nfold-4\n0.6752\n0.6486\n0.6619\nfold-5\n0.6591\n0.7019\n0.6801', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fold-5\n0.6591\n0.7019\n0.6801\n4.7. Results for EMI Estimation\nIn this section, we show our final results for the task of EMI\nestimation. The model is evaluated by Pearson’s correla-\ntions. Table 3 presents Pearson’s correlation scores on the\nofficial validation set and five-fold cross-validation set.\n5. Conclusion\nIn summary, our study contributes to advancing Affec-\ntive Behavior Analysis, aiming to make technology emo-\ntionally intelligent. Through a comprehensive evaluation\nof the ABAW competition, we address five competitive\ntracks. Our method designs integrate emotional cues from', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='tionally intelligent. Through a comprehensive evaluation\nof the ABAW competition, we address five competitive\ntracks. Our method designs integrate emotional cues from\nmulti-modal data, ensuring robust expression features. We\nachieve significant performance across all tracks, indicating\nthe effectiveness of our approach. These results highlight\nthe potential of our method in enhancing human-machine\ninteractions and technological advancements toward de-\nvices understanding and responding to human emotions.\nReferences\n[1] Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras\nKoerich, Simon Bacon, and Eric Granger.\nGuided inter-\npretable facial expression recognition via spatial action unit', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Koerich, Simon Bacon, and Eric Granger.\nGuided inter-\npretable facial expression recognition via spatial action unit\ncues. arXiv preprint arXiv:2402.00281, 2024. 1, 2\n[2] Patricia J Bota, Chen Wang, Ana LN Fred, and Hugo Pl´acido\nDa Silva.\nA review, current challenges, and future pos-\nsibilities on emotion recognition using machine learning\nand physiological signals. IEEE access, 7:140990–141020,\n2019. 3\n[3] Paul Buitelaar, Ian D Wood, Sapna Negi, Mihael Arcan,\nJohn P McCrae, Andrejs Abele, Cecile Robin, Vladimir', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='John P McCrae, Andrejs Abele, Cecile Robin, Vladimir\nAndryushechkin,\nHousam Ziad,\nHesam Sagha,\net al.\nMixedemotions: An open-source toolbox for multimodal\nemotion analysis. IEEE Transactions on Multimedia, 20(9):\n2454–2465, 2018. 3\n[4] Felipe Zago Canal, Tobias Rossi M¨uller, Jhennifer Cristine\nMatias, Gustavo Gino Scotton, Antonio Reis de Sa Junior,\nEliane Pozzebon, and Antonio Carlos Sobieranski. A survey\non facial emotion recognition techniques: A state-of-the-art', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Eliane Pozzebon, and Antonio Carlos Sobieranski. A survey\non facial emotion recognition techniques: A state-of-the-art\nliterature review. Information Sciences, 582:593–617, 2022.\n2\n[5] Haifeng Chen, Dongmei Jiang, and Hichem Sahli. Trans-\nformer encoder with multi-modal multi-head attention for\ncontinuous affect recognition. IEEE Transactions on Mul-\ntimedia, 23:4171–4183, 2020. 3\n7\n[6] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew\nZisserman. Vggsound: A large-scale audio-visual dataset.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[6] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew\nZisserman. Vggsound: A large-scale audio-visual dataset.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages\n721–725. IEEE, 2020. 4\n[7] Zijun Cui, Tengfei Song, Yuru Wang, and Qiang Ji. Knowl-\nedge augmented deep neural networks for joint facial expres-\nsion and action unit recognition. In Proceedings of the 34th\nInternational Conference on Neural Information Processing\nSystems, Red Hook, NY, USA, 2020. Curran Associates Inc.\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='International Conference on Neural Information Processing\nSystems, Red Hook, NY, USA, 2020. Curran Associates Inc.\n2\n[8] Zijun Cui, Chenyi Kuang, Tian Gao, Kartik Talamadupula,\nand Qiang Ji. Biomechanics-guided facial action unit de-\ntection through force modeling.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8694–8703, 2023. 2\n[9] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kot-\nsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-\nlevel face localisation in the wild.\nIn Proceedings of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='sia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-\nlevel face localisation in the wild.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5203–5212, 2020. 5, 6\n[10] Shilpa Devalal and A Karthikeyan.\nLora technology-an\noverview. In 2018 second international conference on elec-\ntronics, communication and aerospace technology (ICECA),\npages 284–290. IEEE, 2018. 4\n[11] Muhterem Dindar, Sanna J¨arvel¨a, Sara Ahola, Xiaohua\nHuang, and Guoying Zhao.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[11] Muhterem Dindar, Sanna J¨arvel¨a, Sara Ahola, Xiaohua\nHuang, and Guoying Zhao.\nLeaders and followers iden-\ntified by emotional mimicry during collaborative learning:\nA facial expression recognition study on emotional valence.\nIEEE Transactions on Affective Computing, 13(3):1390–\n1400, 2020. 2\n[12] Rongkang Dong and Kin-Man Lam. Bi-center loss for com-\npound facial expression recognition. IEEE Signal Processing\nLetters, 2024. 1, 2\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Letters, 2024. 1, 2\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3\n[14] Andrius Dzedzickis, Art¯uras Kaklauskas, and Vytautas\nBucinskas. Human emotion recognition: Review of sensors', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[14] Andrius Dzedzickis, Art¯uras Kaklauskas, and Vytautas\nBucinskas. Human emotion recognition: Review of sensors\nand methods. Sensors, 20(3):592, 2020. 3\n[15] Paul Ekman and Wallace V Friesen. Facial action coding\nsystem. Environmental Psychology & Nonverbal Behavior.\n2\n[16] Amir Hossein Farzaneh and Xiaojun Qi. Facial expression\nrecognition in the wild via deep attentive center loss. In Pro-\nceedings of the IEEE/CVF winter conference on applications\nof computer vision, pages 2402–2411, 2021. 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ceedings of the IEEE/CVF winter conference on applications\nof computer vision, pages 2402–2411, 2021. 1\n[17] Chiara Filippini, David Perpetuini, Daniela Cardone, Anto-\nnio Maria Chiarelli, and Arcangelo Merla. Thermal infrared\nimaging-based affective computing and its application to fa-\ncilitate human robot interaction: A review. Applied Sciences,\n10(8):2924, 2020. 1\n[18] Matthias Franz, Marc A Nordmann, Claudius Rehagel, Ralf\nSch¨afer, Tobias M¨uller, and Daniel Lundqvist. It is in your', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Sch¨afer, Tobias M¨uller, and Daniel Lundqvist. It is in your\nface—alexithymia impairs facial mimicry. Emotion, 21(7):\n1537, 2021. 1\n[19] Riccardo Gervasi, Federico Barravecchia, Luca Mastrogia-\ncomo, and Fiorenzo Franceschini. Applications of affective\ncomputing in human-robot interaction: State-of-art and chal-\nlenges for manufacturing. Proceedings of the Institution of\nMechanical Engineers, Part B: Journal of Engineering Man-\nufacture, 237(6-7):815–832, 2023. 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Mechanical Engineers, Part B: Journal of Engineering Man-\nufacture, 237(6-7):815–832, 2023. 1\n[20] Jianzhu Guo, Zhen Lei, Jun Wan, Egils Avots, Noushin Ha-\njarolasvadi, Boris Knyazev, Artem Kuharenko, Julio C Sil-\nveira Jacques Junior, Xavier Bar´o, Hasan Demirel, et al.\nDominant and complementary emotion recognition from still\nimages of faces. IEEE Access, 6:26391–26403, 2018. 2\n[21] Luma Akram Harbawee. Artificial Intelligence Tools for Fa-\ncial Expression Analysis. PhD thesis, University of Exeter', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[21] Luma Akram Harbawee. Artificial Intelligence Tools for Fa-\ncial Expression Analysis. PhD thesis, University of Exeter\n(United Kingdom), 2019. 2\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 16000–\n16009, 2022. 2\n[23] Shuangjiang He, Huijuan Zhao, Li Yu, Jinqiao Xiang, Con-\ngju Du, and Juan Jing. Compound facial expression recogni-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='gju Du, and Juan Jing. Compound facial expression recogni-\ntion with multi-domain fusion expression based on adversar-\nial learning. In 2022 IEEE International Conference on Sys-\ntems, Man, and Cybernetics (SMC), pages 688–693. IEEE,\n2022. 1, 2\n[24] Alison C Holland, Garret O’Connell, and Isabel Dziobek.\nFacial mimicry, empathy, and emotion recognition: a meta-\nanalysis of correlations. Cognition and Emotion, 35(1):150–\n168, 2021. 1\n[25] Essam H Houssein, Asmaa Hammad, and Abdelmgeid A\nAli.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[25] Essam H Houssein, Asmaa Hammad, and Abdelmgeid A\nAli.\nHuman emotion recognition from eeg-based brain–\ncomputer interface using machine learning: a comprehensive\nreview. Neural Computing and Applications, 34(15):12527–\n12557, 2022. 2\n[26] Xincheng Ju, Dong Zhang, Junhui Li, and Guodong Zhou.\nTransformer-based label set generation for multi-modal\nmulti-label emotion detection. In Proceedings of the 28th\nACM international conference on multimedia, pages 512–\n520, 2020. 3\n[27] Dimitrios Kollias. Abaw: Valence-arousal estimation, ex-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='520, 2020. 3\n[27] Dimitrios Kollias. Abaw: Valence-arousal estimation, ex-\npression recognition, action unit detection & multi-task\nlearning challenges. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n2328–2336, 2022. 1, 3\n[28] Dimitrios Kollias. Multi-label compound expression recog-\nnition:\nC-expr database & network.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5589–5598, 2023. 1, 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5589–5598, 2023. 1, 2\n[29] Dimitrios Kollias and Stefanos Zafeiriou. Aff-wild2: Ex-\ntending the aff-wild database for affect recognition. arXiv\npreprint arXiv:1811.07770, 2018. 1\n[30] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,\naction unit recognition: Aff-wild2, multi-task learning and\narcface. arXiv preprint arXiv:1910.04855, 2019. 1\n[31] Dimitrios Kollias and Stefanos Zafeiriou.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[31] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis\nin-the-wild: Valence-arousal, expressions, action units and a\n8\nunified framework. arXiv preprint arXiv:2103.15792, 2021.\n1, 3\n[32] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-\ntive behavior in the second abaw2 competition. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 3652–3660, 2021. 1\n[33] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[33] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos\nZafeiriou.\nFace behavior a la carte:\nExpressions, af-\nfect and action units in a single network.\narXiv preprint\narXiv:1910.11111, 2019.\n[34] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,\nAthanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,\nIrene Kotsia, and Stefanos Zafeiriou. Deep affect prediction\nin-the-wild: Aff-wild database and challenge, deep architec-\ntures, and beyond. International Journal of Computer Vision,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='in-the-wild: Aff-wild database and challenge, deep architec-\ntures, and beyond. International Journal of Computer Vision,\npages 1–23, 2019.\n[35] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos\nZafeiriou.\nAnalysing affective behavior in the first abaw\n2020 competition. In 2020 15th IEEE International Confer-\nence on Automatic Face and Gesture Recognition (FG 2020),\npages 637–643. IEEE, 2020.\n[36] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos\nZafeiriou. Distribution matching for heterogeneous multi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[36] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos\nZafeiriou. Distribution matching for heterogeneous multi-\ntask learning:\na large-scale face study.\narXiv preprint\narXiv:2105.03790, 2021.\n[37] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan\nCowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-\nmation, expression recognition, action unit detection & emo-\ntional reaction intensity estimation challenges. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='tional reaction intensity estimation challenges. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5888–5897, 2023. 1, 2, 3\n[38] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan\nCowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-\nmation, expression recognition, action unit detection & emo-\ntional reaction intensity estimation challenges. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5888–5897, 2023.\n[39] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[39] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-\nfanos Zafeiriou, Chunchang Shao, and Guanyu Hu. The 6th\naffective behavior analysis in-the-wild (abaw) competition.\narXiv preprint arXiv:2402.19344, 2024. 1\n[40] Jure Kranjec, S Beguˇs, G Gerˇsak, and J Drnovˇsek. Non-\ncontact heart rate and heart rate variability measurements: A\nreview. Biomedical signal processing and control, 13:102–\n112, 2014. 3\n[41] Beibei Kuang, Xueting Li, Xintong Li, Mingxiao Lin, Shan-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='112, 2014. 3\n[41] Beibei Kuang, Xueting Li, Xintong Li, Mingxiao Lin, Shan-\nrou Liu, and Ping Hu.\nThe effect of eye gaze direc-\ntion on emotional mimicry: A multimodal study with elec-\ntromyography and electroencephalography.\nNeuroImage,\n226:117604, 2021. 1, 2\n[42] Bharat Lal, Raffaele Gravina, Fanny Spagnolo, and Pasquale\nCorsonello. Compressed sensing approach for physiological\nsignals: A review. IEEE Sensors Journal, 2023. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Corsonello. Compressed sensing approach for physiological\nsignals: A review. IEEE Sensors Journal, 2023. 3\n[43] Hanting Li, Mingzhe Sui, Zhaoqing Zhu, et al. Nr-dfernet:\nNoise-robust network for dynamic facial expression recogni-\ntion. arXiv preprint arXiv:2206.04975, 2022. 3\n[44] Shan Li and Weihong Deng. Deep facial expression recog-\nnition: A survey. IEEE transactions on affective computing,\n13(3):1195–1215, 2020. 1, 2, 3\n[45] Shan Li, Weihong Deng, and JunPing Du. Reliable crowd-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[45] Shan Li, Weihong Deng, and JunPing Du. Reliable crowd-\nsourcing and deep locality-preserving learning for expres-\nsion recognition in the wild. In 2017 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n2584–2593. IEEE, 2017. 6\n[46] Yante Li, Xiaohua Huang, and Guoying Zhao.\nMicro-\nexpression action unit detection with spatial and channel at-\ntention. Neurocomputing, 436:221–231, 2021. 1, 2\n[47] Xiaolong Liu, Lei Sun, Wenqiang Jiang, Fengyuan Zhang,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[47] Xiaolong Liu, Lei Sun, Wenqiang Jiang, Fengyuan Zhang,\nYuanyuan Deng, Zhaopei Huang, Liyu Meng, Yuchen Liu,\nand Chuanhe Liu. Evaef: Ensemble valence-arousal estima-\ntion framework in the wild. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5862–5870, 2023. 1\n[48] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In Proceedings of\nthe IEEE international conference on computer vision, pages\n3730–3738, 2015. 4, 6', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the IEEE international conference on computer vision, pages\n3730–3738, 2015. 4, 6\n[49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[50] Danielle Lottridge, Mark Chignell, and Aleksandra Jovicic.\nAffective interaction: Understanding, evaluating, and de-\nsigning for human emotion. Reviews of Human Factors and\nErgonomics, 7(1):197–217, 2011. 3\n[51] Fuyan Ma, Bin Sun, and Shutao Li. Spatio-temporal trans-\nformer for dynamic facial expression recognition in the wild.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[51] Fuyan Ma, Bin Sun, and Shutao Li. Spatio-temporal trans-\nformer for dynamic facial expression recognition in the wild.\narXiv preprint arXiv:2205.04749, 2022. 3\n[52] Javier Mar´ın-Morales, Carmen Llinares, Jaime Guixeres,\nand Mariano Alca˜niz. Emotion recognition in immersive vir-\ntual reality: From statistics to affective computing. Sensors,\n20(18):5163, 2020. 3\n[53] Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma-\nhoor. Affectnet: A database for facial expression, valence,\nand arousal computing in the wild. IEEE Transactions on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='hoor. Affectnet: A database for facial expression, valence,\nand arousal computing in the wild. IEEE Transactions on\nAffective Computing, 10(1):18–31, 2017. 4, 6\n[54] Dang-Khanh Nguyen, Ngoc-Huynh Ho, Sudarshan Pant, and\nHyung-Jeong Yang. A transformer-based approach to video\nframe-level prediction in affective behaviour analysis in-the-\nwild. arXiv preprint arXiv:2303.09293, 2023. 1\n[55] R Gnana Praveen, Patrick Cardinal, and Eric Granger.\nAudio-visual fusion for emotion recognition in the valence-\narousal space using joint cross-attention. IEEE Transactions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Audio-visual fusion for emotion recognition in the valence-\narousal space using joint cross-attention. IEEE Transactions\non Biometrics, Behavior, and Identity Science, 2023. 1\n[56] Minglun Ren, Nengying Chen, and Hui Qiu.\nHuman-\nmachine collaborative decision-making:\nAn evolutionary\nroadmap based on cognitive intelligence. International Jour-\nnal of Social Robotics, 15(7):1101–1114, 2023. 1\n[57] I Michael Revina and WR Sam Emmanuel.\nA survey on\nhuman face expression recognition techniques. Journal of\nKing Saud University-Computer and Information Sciences,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='A survey on\nhuman face expression recognition techniques. Journal of\nKing Saud University-Computer and Information Sciences,\n33(6):619–628, 2021. 1, 3\n[58] Albert D Ritzhaupt, Rui Huang, Max Sommer, Jiawen Zhu,\nAnita Stephen, Natercia Valle, John Hampton, and Jingwei\nLi.\nA meta-analysis on the influence of gamification in\n9\nformal educational settings on affective and behavioral out-\ncomes. Educational Technology Research and Development,\n69(5):2493–2522, 2021. 1\n[59] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep ex-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[59] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep ex-\npectation of real and apparent age from a single image with-\nout facial landmarks. International Journal of Computer Vi-\nsion, 126(2):144–157, 2018. 4, 6\n[60] Jiahui She, Yibo Hu, Hailin Shi, Jun Wang, Qiu Shen, and\nTao Mei.\nDive into ambiguity: Latent distribution min-\ning and pairwise uncertainty estimation for facial expression\nrecognition. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 6248–6257,\n2021. 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='computer vision and pattern recognition, pages 6248–6257,\n2021. 1\n[61] Gopendra Vikram Singh, Mauajama Firdaus, Asif Ekbal, and\nPushpak Bhattacharyya. Emoint-trans: A multimodal trans-\nformer for identifying emotions and intents in social con-\nversations. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 31:290–300, 2022. 3\n[62] Rukshani Somarathna, Tomasz Bednarz, and Gelareh Mo-\nhammadi. Virtual reality for emotion elicitation–a review.\nIEEE Transactions on Affective Computing, 2022. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='hammadi. Virtual reality for emotion elicitation–a review.\nIEEE Transactions on Affective Computing, 2022. 3\n[63] Boˇstjan ˇSumak, Saˇsa Brdnik, and Maja Puˇsnik.\nSen-\nsors and artificial intelligence methods and algorithms for\nhuman–computer intelligent interaction: A systematic map-\nping study. Sensors, 22(1):20, 2021. 1\n[64] Martina\nSzab´oov´a,\nMartin\nSarnovsk`y,\nViera\nMaslej Kreˇsˇn´akov´a, and Krist´ına Machov´a.\nEmotion\nanalysis in human–robot interaction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Maslej Kreˇsˇn´akov´a, and Krist´ına Machov´a.\nEmotion\nanalysis in human–robot interaction.\nElectronics, 9(11):\n1761, 2020. 1\n[65] Gauthier Tallec, Edouard Yvinec, Arnaud Dapogny, and\nKevin Bailly. Multi-label transformer for action unit detec-\ntion. arXiv preprint arXiv:2203.12531, 2022. 1, 2\n[66] Giovanna Varni, Isabelle Hupont, Chloe Clavel, and Mo-\nhamed Chetouani. Computational study of primitive emo-\ntional contagion in dyadic interactions. IEEE Transactions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='hamed Chetouani. Computational study of primitive emo-\ntional contagion in dyadic interactions. IEEE Transactions\non Affective Computing, 11(2):258–271, 2017. 2\n[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[68] Juntao Wang and Tsunenori Mine. Multi-task learning for\nemotion recognition in conversation with emotion shift. In\nProceedings of the 37th Pacific Asia Conference on Lan-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='emotion recognition in conversation with emotion shift. In\nProceedings of the 37th Pacific Asia Conference on Lan-\nguage, Information and Computation, pages 257–266, 2023.\n2\n[69] Kai Wang, Xiaojiang Peng, Jianfei Yang, Shijian Lu, and\nYu Qiao. Suppressing uncertainties for large-scale facial ex-\npression recognition. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6897–6906, 2020. 1, 3\n[70] Tanja SH Wingenbach, Mark Brosnan, Monique C Pfaltz,\nPeter Peyk, and Chris Ashwin. Perception of discrete emo-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[70] Tanja SH Wingenbach, Mark Brosnan, Monique C Pfaltz,\nPeter Peyk, and Chris Ashwin. Perception of discrete emo-\ntions in others: Evidence for distinct facial mimicry patterns.\nScientific reports, 10(1):4692, 2020. 1, 2\n[71] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal\nlearning with transformers: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2023. 2\n[72] Xinyu Yang, Yizhuo Dong, and Juan Li.\nReview of data\nfeatures-based music emotion recognition methods. Multi-\nmedia systems, 24:365–389, 2018. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='features-based music emotion recognition methods. Multi-\nmedia systems, 24:365–389, 2018. 3\n[73] Dongjie Ye, Zhangkai Ni, Hanli Wang, Jian Zhang, Shiqi\nWang, and Sam Kwong. Csformer: Bridging convolution\nand transformer for compressive sensing. IEEE Transactions\non Image Processing, 2023. 3\n[74] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-\ning face representation from scratch.\narXiv preprint\narXiv:1411.7923, 2014. 4, 6\n[75] Yufeng Yin, Minh Tran, Di Chang, Xinrui Wang, and Mo-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[75] Yufeng Yin, Minh Tran, Di Chang, Xinrui Wang, and Mo-\nhammad Soleymani. Multi-modal facial action unit detec-\ntion with large pre-trained models for the 5th competition\non affective behavior analysis in-the-wild.\narXiv preprint\narXiv:2303.10590, 2023. 1\n[76] Lin Yue, Weitong Chen, Xue Li, Wanli Zuo, and Minghao\nYin. A survey of sentiment analysis in social media. Knowl-\nedge and Information Systems, 60:617–663, 2019. 2\n[77] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[77] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,\nAthanasios Papaioannou, Guoying Zhao, and Irene Kot-\nsia. Aff-wild: Valence and arousal ‘in-the-wild’challenge.\nIn Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2017 IEEE Conference on, pages 1980–1987.\nIEEE, 2017. 1\n[78] Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng\nZhang, Yu Ding, Runze Wu, Tangjie Lv, and Changjie Fan.\nPrior aided streaming network for multi-task affective anal-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Zhang, Yu Ding, Runze Wu, Tangjie Lv, and Changjie Fan.\nPrior aided streaming network for multi-task affective anal-\nysis. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV) Workshops, pages 3539–\n3549, 2021. 2\n[79] Wei Zhang, Feng Qiu, Suzhen Wang, Hao Zeng, Zhimeng\nZhang, Rudong An, Bowen Ma, and Yu Ding. Transformer-\nbased multimodal information fusion for facial expression\nanalysis.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2428–\n2437, 2022. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='on Computer Vision and Pattern Recognition, pages 2428–\n2437, 2022. 2\n[80] Wei Zhang, Bowen Ma, Feng Qiu, and Yu Ding.\nMulti-\nmodal facial affective analysis based on masked autoencoder.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5792–5801, 2023. 1\n[81] Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan,\nMichael Reale, Andy Horowitz, Peng Liu, and Jeffrey M Gi-\nrard. Bp4d-spontaneous: a high-resolution spontaneous 3d\ndynamic facial expression database. Image and Vision Com-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='rard. Bp4d-spontaneous: a high-resolution spontaneous 3d\ndynamic facial expression database. Image and Vision Com-\nputing, 32(10):692–706, 2014. 6\n[82] Yong Zhang, Weiming Dong, Bao-Gang Hu, and Qiang Ji.\nClassifier learning with prior probabilities for facial action\nunit recognition. In 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 5108–5116,\n2018. 2\n[83] Zengqun Zhao and Qingshan Liu. Former-dfer: Dynamic\nfacial expression recognition transformer.\nIn Proceedings', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[83] Zengqun Zhao and Qingshan Liu. Former-dfer: Dynamic\nfacial expression recognition transformer.\nIn Proceedings\nof the 29th ACM International Conference on Multimedia,\npages 1553–1561, 2021. 3\n[84] Zengqun Zhao, Qingshan Liu, and Shanmin Wang. Learning\ndeep global multi-scale and local attention features for facial\nexpression recognition in the wild. IEEE Transactions on\nImage Processing, 30:6544–6556, 2021. 1\n10\n[85] Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='10\n[85] Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie\nHuang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Da-\nlong Du, et al. Webface260m: A benchmark unveiling the\npower of million-scale deep face recognition. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10492–10502, 2021. 4, 6\n11', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc61fe50> 111
cuda:2
[Document(page_content='Affective Behaviour Analysis via Integrating Multi-Modal Knowledge\nWei Zhang1,∗, Feng Qiu1,∗, Chen Liu1,2, Lincheng Li1, †, Heming Du2, Tiancheng Guo2, Xin Yu2\n1 Netease Fuxi AI Lab\n2 The University of Queensland\n{zhangwei05, qiufeng, lilincheng}@corp.netease.com, chen.liu7@uqconnect.edu.au,\n{Heming.du, xin.yu}@uq.edu.au, alan5gtc@gmail.com\nAbstract\nAffective Behavior Analysis aims to facilitate technol-\nogy emotionally smart, creating a world where devices can\nunderstand and react to our emotions as humans do. To', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Affective Behavior Analysis aims to facilitate technol-\nogy emotionally smart, creating a world where devices can\nunderstand and react to our emotions as humans do. To\ncomprehensively evaluate the authenticity and applicabil-\nity of emotional behavior analysis techniques in natural\nenvironments, the 6th competition on Affective Behavior\nAnalysis in-the-wild (ABAW) utilizes the Aff-Wild2, Hume-\nVidmimic2, and C-EXPR-DB datasets to set up five com-\npetitive tracks, i.e., Valence-Arousal (VA) Estimation, Ex-\npression (EXPR) Recognition, Action Unit (AU) Detection,\nCompound Expression (CE) Recognition, and Emotional\nMimicry Intensity (EMI) Estimation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Compound Expression (CE) Recognition, and Emotional\nMimicry Intensity (EMI) Estimation.\nIn this paper, we\npresent our method designs for the five tasks. Specifically,\nour design mainly includes three aspects: 1) Utilizing a\ntransformer-based feature fusion module to fully integrate\nemotional information provided by audio signals, visual im-\nages, and transcripts, offering high-quality expression fea-\ntures for the downstream tasks. 2) To achieve high-quality\nfacial feature representations, we employ Masked-Auto En-\ncoder as the visual features extraction model and fine-tune it\nwith our facial dataset. 3) Considering the complexity of the\nvideo collection scenes, we conduct a more detailed dataset\ndivision based on scene characteristics and train the classi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='with our facial dataset. 3) Considering the complexity of the\nvideo collection scenes, we conduct a more detailed dataset\ndivision based on scene characteristics and train the classi-\nfier for each scene. Extensive experiments demonstrate the\nsuperiority of our designs.\n1. Introduction\nAffective Behavior Analysis is dedicated to enhancing the\nemotional intelligence of artificial intelligence systems by\nanalyzing and understanding human emotional behavior\n[30, 32–39, 54, 58, 75, 77, 80].\nIt involves identifying\nand interpreting the emotions and feelings people express\nthrough facial expressions, voice, body language, etc. The\nEqual contribution\n†Corresponding author\ngoal is to enable computers and robots to better under-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Equal contribution\n†Corresponding author\ngoal is to enable computers and robots to better under-\nstand human emotional states for more natural and effec-\ntive human-machine interactions, support mental monitor-\ning, and improve applications in education, entertainment,\nand social interactions [17, 19, 56, 63, 64].\nThe\n6th\nAffective\nBehavior\nAnalysis\ncompetition\n(ABAW6) has set up the following five tasks to analyze\nvarious aspects of human emotions and expressions. Ac-\ntion Unit (AU) Detection aims to identify facial action\ntypes from the Facia Action Coding System based on fa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='tion Unit (AU) Detection aims to identify facial action\ntypes from the Facia Action Coding System based on fa-\ncial muscle movements [1, 37, 46, 65].\nCompound Ex-\npression (CE) Recognition requires recognizing complex\nexpressions that combine two or more basic expressions\n[12, 23, 60, 69]. Emotional Mimicry Intensity (EMI) Es-\ntimation evaluates the intensity of an individual’s emotional\nmimicry [18, 24, 41, 70]. Expression Recognition (EXPR)\nidentifies basic emotional expressions like happiness, sad-\nness, and anger [16, 44, 57, 69, 84]. Valence-arousal (VA)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ness, and anger [16, 44, 57, 69, 84]. Valence-arousal (VA)\nestimation determines people’s emotional states on contin-\nuous emotional dimensions, where “valence” refers to the\npositivity or negativity of the emotion, and “arousal” refers\nto the level of emotional activation [27, 31, 37, 47, 55].\nTo enhance the applicability of affective behavior anal-\nysis techniques in the real world, ABAW6 assesses the\nmethod performance on Aff-Wild2 [29], C-EXPR-DB [28],\nand Hume-Vidmimic2 [39], in which videos are captured in\nuncontrolled natural environments. Specifically, Aff-Wild2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='and Hume-Vidmimic2 [39], in which videos are captured in\nuncontrolled natural environments. Specifically, Aff-Wild2\nshowcases individuals of different skin tones, ages, and gen-\nders, under varied lighting, with assorted backgrounds and\nhead poses, thereby enriching its diversity and applicabil-\nity. C-EXPR-DB is designed to analyze multiple emotions\nthat occur simultaneously on the face. It consists of videos\nsourced from YouTube, which feature naturally occurring\nemotions and expressions. Hume-Vidmimic2 emphasizes\ncapturing and analyzing the complexity of human emotions\nin a manner that closely mirrors natural human interactions.\nIt bridges the gap between the controlled environment of\nmost emotion recognition datasets and the unpredictability\nand richness of the natural world.\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='It bridges the gap between the controlled environment of\nmost emotion recognition datasets and the unpredictability\nand richness of the natural world.\n1\narXiv:2403.10825v1  [cs.CV]  16 Mar 2024\nBased on the characteristics of the above datasets, we es-\ntablish our objectives to fully utilize the emotional informa-\ntion provided in multimodal data and to enhance the appli-\ncability of our method in real-world scenarios. In this paper,\nwe detail our method designs in three aspects. Firstly, to ob-\ntain high-quality image features. we integrate a large-scale\nfacial image dataset and utilize the self-supervised model\nMasked Auto Encoder (MAE) [22, 79] to learn deep feature', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='facial image dataset and utilize the self-supervised model\nMasked Auto Encoder (MAE) [22, 79] to learn deep feature\nrepresentations from these emotional data, enhancing the\nperformance of downstream tasks. Moreover, we leverage\na transformer-based model to fuse the multi-modal infor-\nmation. This architecture facilitates the interactions across\nmodalities (i.e., audio, visual, text) and provides scalable,\nefficient, and effective solutions for integrating multimodal\ninformation [71]. Lastly, we adopt an ensemble learning\nstrategy to improve the applicability of our method in var-\nious scenes. In this strategy, we divide the whole dataset\ninto multiple sub-datasets according to their distinct back-\nground characteristics and assign these sub-datasets to dif-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='into multiple sub-datasets according to their distinct back-\nground characteristics and assign these sub-datasets to dif-\nferent classifiers. After that, we integrate the outputs of\nthese classifiers to obtain the final prediction results.\nExperiments conducted on the three datasets demon-\nstrate the effectiveness of our design choices. Overall, our\ncontributions are three-fold:\nWe integrate a large-scale facial expression dataset and\nfine-tune MAE on it to obtain an effective facial ex-\npression feature extractor, enhancing the performance for\ndownstream tasks.\nWe employ a transformer-based multi-modal integration\nmodel to facilitate the interactions of multi-modalities,\nenriching the expression features extracted from multi-\nmodal data.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='model to facilitate the interactions of multi-modalities,\nenriching the expression features extracted from multi-\nmodal data.\nWe adopt an ensemble learning strategy, which trains\nmultiple classifiers on sub-datasets with different back-\nground characteristics and ensemble the results of these\nclassifiers to attain the final results. This strategy enables\nour method to generalize better in various environments.\n2. Related Work\n2.1. Action Unit Detection\nDetecting Action Units (AU) in the wild is a challenging\nyet crucial advancement task in facial expression analysis,\npushing the boundaries of applicability from controlled lab-\noratory settings to real-world environments [1, 46, 65]. This\nendeavor addresses the inherent variability in lighting, pose,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='oratory settings to real-world environments [1, 46, 65]. This\nendeavor addresses the inherent variability in lighting, pose,\nocclusion, and emotional context encountered in natural en-\nvironments [37]. Recent works highlight the effectiveness\nof multi-task frameworks in leveraging extra regularization,\nsuch as the extra label constraint, to enhance detection per-\nformance. Zhang et al. [78] introduce a streaming model\nto concurrently execute AU detection, expression, recogni-\ntion, and Valence-Arousal (VA) regression. Cui et al. [8]\npresent a biomechanics-guided AU detection approach to\nexplicitly incorporate facial biomechanics for AU detection.\nMoreover, to achieve robust and generalized AU detection,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='present a biomechanics-guided AU detection approach to\nexplicitly incorporate facial biomechanics for AU detection.\nMoreover, to achieve robust and generalized AU detection,\nsome works take generic knowledge (i.e. static spatial mus-\ncle relationships) into account [7], while others consider in-\ntegrating multi-modal knowledge to obtain rich expression\nfeatures [82].\n2.2. Compound Expression Recognition\nCompound Expression Recognition (CER) gains attention\nfor identifying complex facial expressions that convey a\ncombination of basic emotions, reflecting more nuanced hu-\nman emotional states [12, 23]. Typical methods focus on\nrecognizing basic emotional expressions with deep learning\nmethods, paving the way for more advanced methods ca-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='recognizing basic emotional expressions with deep learning\nmethods, paving the way for more advanced methods ca-\npable of deciphering compound expressions [4, 20, 25, 76].\nNotable efforts in this area include leveraging convolutional\nneural networks for feature extraction and employing re-\ncurrent neural networks or attention mechanisms to capture\nthe subtleties and dynamics of facial expressions over time.\nResearchers have also explored multi-task learning frame-\nworks to simultaneously recognize basic expressions more\naccurately and robustly [21, 44, 68]. Due to the complex-\nity of human emotions in the real world, detecting a single\nexpression is not suitable for real-life scenarios. Therefore,\nDimitrios [28] curates a Multi-Label Compound Expression', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='expression is not suitable for real-life scenarios. Therefore,\nDimitrios [28] curates a Multi-Label Compound Expression\ndataset, C-EXPR. Besides, he also proposes C-EXPR-NET,\nwhich addresses both CER and AU detection tasks simulta-\nneously, achieving improved results in recognizing multiple\nexpressions [28].\n2.3. Emotional Mimicry Intensity Estimation\nEmotional Mimicry Intensity (EMI) Estimation delves into\nthe nuanced realm of how individuals replicate and respond\nto the emotional expressions of others [41, 70]. It aims to\nquantify the degree of mimicry and its emotional impact.\nTraditionally, facial mimicry has been quantified through\nthe activation of facial muscles, either measured by elec-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Traditionally, facial mimicry has been quantified through\nthe activation of facial muscles, either measured by elec-\ntromyography (EMG) or analyzed through the frequency\nand intensity of facial muscle movements via the Facial\nAction Coding System (FACS) [15]. However, these tech-\nniques are either invasive or require significant time and ef-\nfort. Recent advancements [11, 11, 66] leverage computer\nvision and statistical methods to estimate facial expressions,\npostures, and emotions from video recordings, enabling the\nidentification of facial and behavioral mimicry. Despite be-\ning currently less precise than physiological signal-based\nmeasurements, this video-based approach is non-invasive,\nautomatable, and applicable to multimodal contexts, mak-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='measurements, this video-based approach is non-invasive,\nautomatable, and applicable to multimodal contexts, mak-\ning it scalable for real-time, real-world uses, such as in\nhuman-agent social interactions.\n2\nI am very happy to attend…\nInput Frames\nK\nAudio Signal\nText Description\nOnly for EMI\nImage\nEncoder\nAudio\nEncoder\nText\nEncoder\nConcat\nTransformer\nEncoder𝑻𝟏\nDownstream Tasks\nAU\nCE\nEMI', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Encoder𝑻𝟏\nDownstream Tasks\nAU\nCE\nEMI\nEXPR\nVA\nTransformer\nEncoder 𝑻𝟐\nTransformer\nEncoder 𝑻𝟑\n…\nResults\nEnsemble\nEnsemble Learning\n𝐹𝐼\n𝐹𝐴\n𝐹𝑇\nFigure 1. The overview of our proposed method. We first utilize the images in the facial image datasets to train the Image Encoder in a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='𝐹𝑇\nFigure 1. The overview of our proposed method. We first utilize the images in the facial image datasets to train the Image Encoder in a\nself-supervised manner, thus obtaining the visual feature FI. Then we leverage the pre-trained audio encoder and text encoder to attain the\naudio feature FA and text feature FT . Note that we only devise the text encoder for the EMI task. Subsequently, we concat these features\nand feed them into the Transformer Encoders. Here, we train these encoders on subsets divided based on background characteristics.\nFinally, we employ a voting strategy to attain the final results.\n2.4. Expression Recognition\nExpression Recognition has witnessed substantial growth,\ndriven by the integration of psychological insights and ad-\nvanced deep learning techniques [44, 57, 69]. Recently, the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='driven by the integration of psychological insights and ad-\nvanced deep learning techniques [44, 57, 69]. Recently, the\nadaptation of transformer-based models from natural lan-\nguage processing (NLP) [67] to computer vision tasks [13]\nhas led to their application in extracting spatial and tem-\nporal features from video sequences for emotion recogni-\ntion.\nNotably, Zhao et al.\n[83] introduce a transformer\nmodel specifically for dynamic facial expression recogni-\ntion, the Former-DFER, which includes CSFormer [73] and\nT-Former [73] modules to learn spatial and temporal fea-\ntures, respectively.\nMa et al.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='T-Former [73] modules to learn spatial and temporal fea-\ntures, respectively.\nMa et al.\n[51] developed a Spatio-\nTemporal Transformer (STT) that captures both spatial and\ntemporal information through a transformer-based encoder.\nAdditionally, Li et al. [43] proposed the NR-DFERNet,\ndesigned to minimize the influence of noisy frames within\nvideo sequences. While these advancements represent sig-\nnificant progress in addressing the challenges of dynamic\nfacial expression recognition (DFER) with discrete labels,\nthey overlook the interference from the background in im-\nages. To address this, we incorporate ensemble learning\ninto our method.\n2.5. Valence-arousal Estimation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ages. To address this, we incorporate ensemble learning\ninto our method.\n2.5. Valence-arousal Estimation\nValence-arousal estimation focuses on mapping emotional\nstates onto a two-dimensional space, where valence repre-\nsents the positivity or negativity of emotion, and arousal in-\ndicates its intensity or activation level [27, 31, 37]. Conven-\ntional approaches mainly relied on physiological signals,\nsuch as heart rate or skin conductance, to estimate these di-\nmensions [2, 40, 42]. However, with advancements in deep\nlearning, researchers shift towards leveraging visual and au-\nditory cues from facial expressions, voice tones, and body\nlanguage. Notably, convolutional neural networks and re-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ditory cues from facial expressions, voice tones, and body\nlanguage. Notably, convolutional neural networks and re-\ncurrent neural networks have been extensively applied to\ncapture the nuanced and dynamic aspects of emotions from\nimages, videos, and audio data [3, 52, 72].\nRecent studies introduce transformer models to better\nhandle the sequential and contextual nature of emotional\nexpressions in multi-modal data [5, 26, 61]. These improve-\nments have not only improved the accuracy and efficiency\nof valence-arousal estimation but also broadened its appli-\ncability in real-world scenarios, such as human-computer\ninteraction and mental health assessment [14, 50, 62]. De-\nspite progress, challenges remain in capturing the complex', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='interaction and mental health assessment [14, 50, 62]. De-\nspite progress, challenges remain in capturing the complex\nand subjective nature of emotions, necessitating further re-\nsearch into model interpretability and the integration of di-\nverse data sources.\n3. Method\nIn this section, we describe our method for analyzing hu-\nman affective behavior. The architecture flow is illustrated\nin Fig. 1. The proposed approach addresses two critical\nproblems: 1) the emotional information in the multimodal\ndata is not fully explored and 2) the model has poor gener-\nalization ability for videos with complex backgrounds. For\na clear exposition, we first introduce how we utilize the en-\ncoders to extract features from multi-modal data in Sec. 3.1.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='a clear exposition, we first introduce how we utilize the en-\ncoders to extract features from multi-modal data in Sec. 3.1.\nThen we detail the transformer-based multi-modal feature\nfusion method in Sec. 3.2. Finally, in Sec. 3.3, we present\nthe ensemble learning strategy that is leveraged to enhance\nthe model generalization ability.\n3\n3.1. Feature Extraction Encoder\nImage Encoder. In this work, we employ MAE as the im-\nage encoder since its self-supervised training manner en-\nables the extracted features more generalizable.\nTo fur-\nther attain powerful and expressive features, we construct\na large-scale facial image dataset which consists of Affect-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='To fur-\nther attain powerful and expressive features, we construct\na large-scale facial image dataset which consists of Affect-\nNet [53], CASIA-WebFace [74], CelebA [48], IMDB-WIKI\n[59], and WebFace260M [85].\nThe total number of our\nintegrated dataset is 262M. Based on the integrated facial\ndataset, we finetune MAE through facial image reconstruc-\ntion.\nSpecifically, in the pre-training phase, our method\nadopts the “mask-then-reconstruct” strategy. Here images\nare dissected into multiple patches (measuring 16×16 pix-\nels), with a random selection of 75% being obscured. These', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='are dissected into multiple patches (measuring 16×16 pix-\nels), with a random selection of 75% being obscured. These\nmasked images are then input into the encoder, while the de-\ncoder restores them to the corresponding original. We adopt\nthe pixel-wise L2 loss to optimize the model, ensuring the\nreconstructed facial images closely mirror the originals.\nAfter the pre-training, we modify the model for specific\ndownstream tasks by detaching the MAE decoder and incor-\nporating a fully connected layer to the end of the encoder.\nThis alternation facilitates the model to better adapt to the\ndownstream tasks.\nAudio Encoder. Considering that the tone and intonation of\nthe speech can also reflect certain emotional information,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='downstream tasks.\nAudio Encoder. Considering that the tone and intonation of\nthe speech can also reflect certain emotional information,\nwe leverage VGGish [6] as our audio encoder to generate\nthe audio representation. Given that VGGish is trained on\nthe large-scale dataset VGGSound and can capture a wide\nrange of audio features, we directly utilize it as the feature\nextractor without training on our dataset.\nText Encoder. Compared to other tracks, EMI not only pro-\nvides audio and visual frames but also includes a transcript\nfor each video. Here, we employ the large off-the-shelf\nmodel LoRA [10] to extract features from the transcript.\n3.2. Transformer-based Multi-modal Fusion\nWe fuse features across different modalities to obtain more', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='3.2. Transformer-based Multi-modal Fusion\nWe fuse features across different modalities to obtain more\nreliable emotional features and utilize the fused feature for\ndownstream tasks. By combining information from vari-\nous modalities such as visual, audio, and text, we achieve a\nmore comprehensive and accurate emotion representation.\nTo align the three modalities at the temporal dimension,\nwe trim each video into multiple clips with k frames. For\neach frame, we employ our image encoder to extract the\nvisual feature fI. In this fashion, we attain the visual fea-\nture F K×d\nI\nfor the whole clip. Here, d represents the fea-\nture dimension. Meanwhile, we employ the audio and text\nencoders to generate the features for the whole clip, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ture dimension. Meanwhile, we employ the audio and text\nencoders to generate the features for the whole clip, and\nthe features are expressed by F 1×d\nA\nand F 1×d\nT\n, respectively.\nSubsequently, we concat these features and input them into\nthe Transformer Encoder. Specifically, our transformer en-\ncoder consists of four encoder layers with a dropout rate of\n0.3. The output is then fed into a fully connected layer to\nadjust the final output dimension according to the task re-\nquirements. Note that, at the feature fused stage, the image\nencoder, audio encoder, and text encoder are all fixed, while\nonly the transformer encoder as well as the fully connected', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='encoder, audio encoder, and text encoder are all fixed, while\nonly the transformer encoder as well as the fully connected\nlayer are trainable.\n3.3. Ensemble Learning\nTo improve the applicability of affective behavior analysis\nmethods, the 6th ABAW leverages the datasets collected\nfrom the real world as the test data. Given the complex\nbackgrounds in the videos, we adopt the ensemble learn-\ning strategy to enable our method robust against complex\nenvironments.\nSpecifically, we first partition the dataset\ninto multiple subsets according to the background charac-\nteristics, ensuring each subset contains images with similar\nbackground properties. Next, we separately train the classi-\nfiers for each subset to effectively capture emotional infor-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='teristics, ensuring each subset contains images with similar\nbackground properties. Next, we separately train the classi-\nfiers for each subset to effectively capture emotional infor-\nmation within the images.\nDuring the inference stage, we integrate predictions from\nclassifiers on each subset via a voting method. Specifically,\nfor each sample, we allow classifiers from each subset to\nclassify it and record the predictions from each classifier.\nFinally, we employ a voting mechanism based on these pre-\ndictions to determine the ultimate label. Here, we select the\nlabel with the highest number of votes as the final classi-\nfication result. Our voting method effectively reduces er-\nrors caused by biases in classifiers from individual subsets,\nthereby enhancing overall classification performance.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fication result. Our voting method effectively reduces er-\nrors caused by biases in classifiers from individual subsets,\nthereby enhancing overall classification performance.\n3.4. Training Objectives\nObjectives for Image Encoder. To enhance the adaptabil-\nity of the Image Encoder across various tasks, we fine-tune\nit for each downstream task. Specifically, when dealing with\nAU and EXPR, we optimize the model via cross-entropy\nloss LAU CE and LEXP R−CE, respectively. They are de-\nfined as follows:\nLAU−CE = − 1\n12\n12\nX\nj=1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='LAU−CE = − 1\n12\n12\nX\nj=1\nWauj [yj log ˆyj + (1 − yj) log (1 − ˆyj)] ,\n(1)\nLEXP R−CE = −1\n8\n8\nX\nj=1\nWexp−jzj log ˆzj,\n(2)\nwhere ˆy and ˆz represent the predicted results for the action\nunit and expression category respectively, whereas y and z\ndenote the ground truth values for the action unit and ex-\npression category.\nIn the VA task, to better capture the correlation between', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='denote the ground truth values for the action unit and ex-\npression category.\nIn the VA task, to better capture the correlation between\nvalence and arousal and thus improve the accuracy of emo-\ntion recognition, we leverage the consistency correlation co-\n4\nefficient as the model optimization function, defined as:\nCCC(X, ˆ\nX) =\n2ρX ˆ\nX δX δ ˆ\nX\nδ2\nX + δ2\nˆ\nX +\nTable 1. The AU F1 scores (in %) of models that are trained and tested on different folds (including the original training/validation set of\nAff-Wild2 dataset).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Aff-Wild2 dataset).\nVal Set\nAU1\nAU2\nAU4\nAU6\nAU7\nAU10\nAU12\nAU15\nAU23\nAU24\nAU25\nAU26\nAvg.\nOfficial\n55.29\n51.40\n65.81\n68.61\n76.08\n75.00\n75.24\n37.65\n18.89', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='75.00\n75.24\n37.65\n18.89\n30.89\n83.41\n44.98\n56.94\nfold-1\n62.61\n46.20\n71.22\n77.71\n67.44\n69.69\n74.62\n36.32\n29.43\n21.75\n81.56\n40.73', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='29.43\n21.75\n81.56\n40.73\n56.61\nfold-2\n64.23\n54.35\n73.85\n77.33\n77.49\n76.70\n80.74\n29.05\n28.96\n18.47\n87.71\n43.63\n59.37\nfold-3\n58.55', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='43.63\n59.37\nfold-3\n58.55\n48.37\n60.05\n71.22\n72.43\n74.29\n75.43\n29.81\n19.52\n32.86\n83.37\n47.63\n56.13\nfold-4\n53.34\n39.34\n66.26\n70.67', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='53.34\n39.34\n66.26\n70.67\n66.51\n69.39\n71.76\n39.49\n25.17\n32.40\n82.27\n40.05\n54.72\nfold-5\n53.50\n44.68\n63.45\n72.02\n69.72\n74.00\n78.24', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='72.02\n69.72\n74.00\n78.24\n38.81\n23.67\n7.56\n81.24\n43.67\n54.22\nvideos of around 2.7M frames that are annotated in terms of\nthe 6 basic expressions (i.e., anger, disgust, fear, happiness,\nsadness, surprise), plus the neutral state, plus a category\n‘other’ that denotes expressions/affective states other than\nthe 6 basic ones. The performance measure is the average\nF1 Score across all 8 categories. The VA estimation track', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the 6 basic ones. The performance measure is the average\nF1 Score across all 8 categories. The VA estimation track\nutilizes 594 videos of around 3M frames of 584 subjects an-\nnotated in terms of valence and arousal. The performance\nmeasure is the mean Concordance Correlation Coefficient\n(CCC) of valence and arousal.\nThe fourth track of ABAW6 utilizes 56 videos which\nare from the C-EXPR-DB database.\nThe complete C-\nEXPR-DB dataset contains 400 videos totaling approxi-\nmately 200,000 frames, with each frame annotated for 12\ncompound expressions. For this track, the task is to pre-\ndict 7 compound expressions for each frame in a subset of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='compound expressions. For this track, the task is to pre-\ndict 7 compound expressions for each frame in a subset of\nthe C-EXPR-DB videos. Specifically, the 7 compound ex-\npressions are Fearfully Surprised, Happily Surprised, Sadly\nSurprised, Disgustedly Surprised, Angrily Surprised, Sadly\nFearful, and Sadly Angry. The evaluation metric for this\ntrack is the average F1 Score across all 7 categories.\nThe fifth track of ABAW6 is based on the multimodal\nHume-Vidmimic2 dataset which consists of more than\n15,000 videos totaling over 25 hours. Each subject of this\ndataset needs to imitate a ‘seed’ video, replicating the spe-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='15,000 videos totaling over 25 hours. Each subject of this\ndataset needs to imitate a ‘seed’ video, replicating the spe-\ncific emotion displayed by the individual in the video. Then\nthe imitators are asked to annotate the emotional intensity of\nthe seed video using a range of predefined emotional cate-\ngories (Admiration, Amusement, Determination, Empathic\nPain, Excitement, and Joy). A normalized score from 0 to 1\nis provided as a ground truth value for each seed video and\neach performance video of the imitator. The evaluation met-\nric for this track is the average Pearson’s correlation across\nthe 6 emotion dimensions.\nIn addition to the official datasets mentioned above, we\nalso used some additional data from the open-source and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the 6 emotion dimensions.\nIn addition to the official datasets mentioned above, we\nalso used some additional data from the open-source and\nprivate datasets. For the AU detection track, we use the ex-\ntra dataset BP4D [81] to supplement some of the limited\nAU categories in Aff-wild2. For the expression recognition\ntrack, we use the extra dataset RAF-DB [45] and Affect-\nNet [53] to supplement the Anger, Disgust, and Fear data.\nFor the fourth track, we utilize our private video dataset and\nannotated these videos based on the rules of 7 compound\nexpressions for training and testing.\n4.3. Implementatal Setting\nWe utilize retinaface [9] to detect faces for each frame and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='expressions for training and testing.\n4.3. Implementatal Setting\nWe utilize retinaface [9] to detect faces for each frame and\nnormalize them to a size of 224x224.\nWe pre-train an\nMAE on a large facial images dataset that consists of sev-\neral open-source face images datasets (i.e., AffectNet [53],\nCASIA-WebFace [74], CelebA [48] and IMDB-WIKI [59],\nWebface260M [85]). We use this MAE as the basic feature\nextractor to capture the visual information for facial images\nin each track. The pre-training process is trained for 800\nepochs with a batch size of 4096 on 8 NVIDIA A30 GPUs,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='in each track. The pre-training process is trained for 800\nepochs with a batch size of 4096 on 8 NVIDIA A30 GPUs,\nusing the AdamW optimizer [49]. For the tasks of AU de-\ntection, expression recognition and VA estimation, we in-\ncorporate the temporal, audio, and other information to fur-\nther improve the performance. At this stage, the training\ndata consists of continuous video clips of 100 frames. The\nlearning rate is set as 0.0001 using the AdamW optimizer.\nTo reduce the gap caused by data division, we conduct five-\nfold cross-validation for all the tracks.\n4.4. Results for AU Detection\nIn this section, we show our final results for the task of AU', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fold cross-validation for all the tracks.\n4.4. Results for AU Detection\nIn this section, we show our final results for the task of AU\ndetection. The model is evaluated by the average F1 score\nfor 12 AUs. Table 1 presents the F1 results on the official\nvalidation set and five-fold cross-validation set.\n4.5. Results for Expression Recognition\nIn this section, we show our final results for the task of ex-\npression recognition. The model is evaluated by the average\nF1 score for 8 categories. Table 2 presents the F1 results on\nthe official validation set and five-fold cross-validation set.\n4.6. Results for VA Estimation\nIn this section, we show our final results for the task of VA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='4.6. Results for VA Estimation\nIn this section, we show our final results for the task of VA\nestimation. The model is evaluated by CCC for valence and\narousal. Table 4 presents the F1 results on the official vali-\ndation set and five-fold cross-validation set.\n6\nTable 2. The expression F1 scores (in %) of models that are trained and tested on different folds (including the original training/validation\nset of Aff-Wild2 dataset).\nVal Set\nNeutral\nAnger\nDisgust\nFear\nHappiness\nSadness\nSurprise\nOther\nAvg.\nOfficial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Happiness\nSadness\nSurprise\nOther\nAvg.\nOfficial\n70.21\n73.93\n50.34\n21.83\n59.05\n66.41\n36.51\n66.11\n55.55\nfold-1\n70.06\n37.21\n32.12\n22.71\n61.77\n77.61\n45.62', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='22.71\n61.77\n77.61\n45.62\n51.58\n49.83\nfold-2\n67.36\n44.45\n21.21\n42.50\n62.22\n78.24\n36.67\n70.00\n52.83\nfold-3\n73.64\n71.60\n45.01\n23.25', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='73.64\n71.60\n45.01\n23.25\n47.67\n77.05\n46.81\n65.56\n56.32\nfold-4\n65.41\n71.00\n53.70\n23.27\n61.62\n61.79\n27.76\n72.68\n54.65\nfold-5\n64.03', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='72.68\n54.65\nfold-5\n64.03\n31.23\n35.66\n67.64\n67.97\n69.75\n52.12\n55.64\n55.51\nTable 3. The Pearson’s correlations of models that are trained and tested on different folds (including the original training/validation set of\nHume-Vidmimic2. ).\nAdmiration\nAmusement\nDetermination\nEmpathic Pain\nExcitement\nJoy', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Admiration\nAmusement\nDetermination\nEmpathic Pain\nExcitement\nJoy\nAvg.\nOfficial\n0.5942\n0.4982\n0.5090\n0.2275\n0.4961\n0.4580\n0.4638\nfold-1\n0.5880\n0.4842\n0.4914\n0.2089\n0.4852\n0.4338', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='0.4914\n0.2089\n0.4852\n0.4338\n0.4486\nfold-2\n0.5193\n0.4385\n0.4031\n0.3715\n0.3734\n0.3717\n0.4129\nfold-3\n0.5195\n0.4496\n0.3947\n0.4924\n0.4129\n0.3843', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='0.3947\n0.4924\n0.4129\n0.3843\n0.4422\nfold-4\n0.5955\n0.4950\n0.5134\n0.2492\n0.5068\n0.4576\n0.4696\nfold-5\n0.5199\n0.4377\n0.4040\n0.3739\n0.3717\n0.3719', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='0.4040\n0.3739\n0.3717\n0.3719\n0.4132\nTable 4. The VA CCC scores of models that are trained and tested\non different folds (including the original training/validation set of\nAff-Wild2 dataset).\nVal Set\nValence\nArousal\nAvg.\nOfficial\n0.5523\n0.6531\n0.6027\nfold-1\n0.6408\n0.6195\n0.6302', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fold-1\n0.6408\n0.6195\n0.6302\nfold-2\n0.6033\n0.6758\n0.6395\nfold-3\n0.6773\n0.6961\n0.6867\nfold-4\n0.6752\n0.6486\n0.6619\nfold-5\n0.6591\n0.7019\n0.6801', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='fold-5\n0.6591\n0.7019\n0.6801\n4.7. Results for EMI Estimation\nIn this section, we show our final results for the task of EMI\nestimation. The model is evaluated by Pearson’s correla-\ntions. Table 3 presents Pearson’s correlation scores on the\nofficial validation set and five-fold cross-validation set.\n5. Conclusion\nIn summary, our study contributes to advancing Affec-\ntive Behavior Analysis, aiming to make technology emo-\ntionally intelligent. Through a comprehensive evaluation\nof the ABAW competition, we address five competitive\ntracks. Our method designs integrate emotional cues from', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='tionally intelligent. Through a comprehensive evaluation\nof the ABAW competition, we address five competitive\ntracks. Our method designs integrate emotional cues from\nmulti-modal data, ensuring robust expression features. We\nachieve significant performance across all tracks, indicating\nthe effectiveness of our approach. These results highlight\nthe potential of our method in enhancing human-machine\ninteractions and technological advancements toward de-\nvices understanding and responding to human emotions.\nReferences\n[1] Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras\nKoerich, Simon Bacon, and Eric Granger.\nGuided inter-\npretable facial expression recognition via spatial action unit', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Koerich, Simon Bacon, and Eric Granger.\nGuided inter-\npretable facial expression recognition via spatial action unit\ncues. arXiv preprint arXiv:2402.00281, 2024. 1, 2\n[2] Patricia J Bota, Chen Wang, Ana LN Fred, and Hugo Pl´acido\nDa Silva.\nA review, current challenges, and future pos-\nsibilities on emotion recognition using machine learning\nand physiological signals. IEEE access, 7:140990–141020,\n2019. 3\n[3] Paul Buitelaar, Ian D Wood, Sapna Negi, Mihael Arcan,\nJohn P McCrae, Andrejs Abele, Cecile Robin, Vladimir', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='John P McCrae, Andrejs Abele, Cecile Robin, Vladimir\nAndryushechkin,\nHousam Ziad,\nHesam Sagha,\net al.\nMixedemotions: An open-source toolbox for multimodal\nemotion analysis. IEEE Transactions on Multimedia, 20(9):\n2454–2465, 2018. 3\n[4] Felipe Zago Canal, Tobias Rossi M¨uller, Jhennifer Cristine\nMatias, Gustavo Gino Scotton, Antonio Reis de Sa Junior,\nEliane Pozzebon, and Antonio Carlos Sobieranski. A survey\non facial emotion recognition techniques: A state-of-the-art', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Eliane Pozzebon, and Antonio Carlos Sobieranski. A survey\non facial emotion recognition techniques: A state-of-the-art\nliterature review. Information Sciences, 582:593–617, 2022.\n2\n[5] Haifeng Chen, Dongmei Jiang, and Hichem Sahli. Trans-\nformer encoder with multi-modal multi-head attention for\ncontinuous affect recognition. IEEE Transactions on Mul-\ntimedia, 23:4171–4183, 2020. 3\n7\n[6] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew\nZisserman. Vggsound: A large-scale audio-visual dataset.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[6] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew\nZisserman. Vggsound: A large-scale audio-visual dataset.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages\n721–725. IEEE, 2020. 4\n[7] Zijun Cui, Tengfei Song, Yuru Wang, and Qiang Ji. Knowl-\nedge augmented deep neural networks for joint facial expres-\nsion and action unit recognition. In Proceedings of the 34th\nInternational Conference on Neural Information Processing\nSystems, Red Hook, NY, USA, 2020. Curran Associates Inc.\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='International Conference on Neural Information Processing\nSystems, Red Hook, NY, USA, 2020. Curran Associates Inc.\n2\n[8] Zijun Cui, Chenyi Kuang, Tian Gao, Kartik Talamadupula,\nand Qiang Ji. Biomechanics-guided facial action unit de-\ntection through force modeling.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8694–8703, 2023. 2\n[9] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kot-\nsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-\nlevel face localisation in the wild.\nIn Proceedings of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='sia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-\nlevel face localisation in the wild.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5203–5212, 2020. 5, 6\n[10] Shilpa Devalal and A Karthikeyan.\nLora technology-an\noverview. In 2018 second international conference on elec-\ntronics, communication and aerospace technology (ICECA),\npages 284–290. IEEE, 2018. 4\n[11] Muhterem Dindar, Sanna J¨arvel¨a, Sara Ahola, Xiaohua\nHuang, and Guoying Zhao.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[11] Muhterem Dindar, Sanna J¨arvel¨a, Sara Ahola, Xiaohua\nHuang, and Guoying Zhao.\nLeaders and followers iden-\ntified by emotional mimicry during collaborative learning:\nA facial expression recognition study on emotional valence.\nIEEE Transactions on Affective Computing, 13(3):1390–\n1400, 2020. 2\n[12] Rongkang Dong and Kin-Man Lam. Bi-center loss for com-\npound facial expression recognition. IEEE Signal Processing\nLetters, 2024. 1, 2\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Letters, 2024. 1, 2\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3\n[14] Andrius Dzedzickis, Art¯uras Kaklauskas, and Vytautas\nBucinskas. Human emotion recognition: Review of sensors', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[14] Andrius Dzedzickis, Art¯uras Kaklauskas, and Vytautas\nBucinskas. Human emotion recognition: Review of sensors\nand methods. Sensors, 20(3):592, 2020. 3\n[15] Paul Ekman and Wallace V Friesen. Facial action coding\nsystem. Environmental Psychology & Nonverbal Behavior.\n2\n[16] Amir Hossein Farzaneh and Xiaojun Qi. Facial expression\nrecognition in the wild via deep attentive center loss. In Pro-\nceedings of the IEEE/CVF winter conference on applications\nof computer vision, pages 2402–2411, 2021. 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='ceedings of the IEEE/CVF winter conference on applications\nof computer vision, pages 2402–2411, 2021. 1\n[17] Chiara Filippini, David Perpetuini, Daniela Cardone, Anto-\nnio Maria Chiarelli, and Arcangelo Merla. Thermal infrared\nimaging-based affective computing and its application to fa-\ncilitate human robot interaction: A review. Applied Sciences,\n10(8):2924, 2020. 1\n[18] Matthias Franz, Marc A Nordmann, Claudius Rehagel, Ralf\nSch¨afer, Tobias M¨uller, and Daniel Lundqvist. It is in your', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Sch¨afer, Tobias M¨uller, and Daniel Lundqvist. It is in your\nface—alexithymia impairs facial mimicry. Emotion, 21(7):\n1537, 2021. 1\n[19] Riccardo Gervasi, Federico Barravecchia, Luca Mastrogia-\ncomo, and Fiorenzo Franceschini. Applications of affective\ncomputing in human-robot interaction: State-of-art and chal-\nlenges for manufacturing. Proceedings of the Institution of\nMechanical Engineers, Part B: Journal of Engineering Man-\nufacture, 237(6-7):815–832, 2023. 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Mechanical Engineers, Part B: Journal of Engineering Man-\nufacture, 237(6-7):815–832, 2023. 1\n[20] Jianzhu Guo, Zhen Lei, Jun Wan, Egils Avots, Noushin Ha-\njarolasvadi, Boris Knyazev, Artem Kuharenko, Julio C Sil-\nveira Jacques Junior, Xavier Bar´o, Hasan Demirel, et al.\nDominant and complementary emotion recognition from still\nimages of faces. IEEE Access, 6:26391–26403, 2018. 2\n[21] Luma Akram Harbawee. Artificial Intelligence Tools for Fa-\ncial Expression Analysis. PhD thesis, University of Exeter', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[21] Luma Akram Harbawee. Artificial Intelligence Tools for Fa-\ncial Expression Analysis. PhD thesis, University of Exeter\n(United Kingdom), 2019. 2\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 16000–\n16009, 2022. 2\n[23] Shuangjiang He, Huijuan Zhao, Li Yu, Jinqiao Xiang, Con-\ngju Du, and Juan Jing. Compound facial expression recogni-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='gju Du, and Juan Jing. Compound facial expression recogni-\ntion with multi-domain fusion expression based on adversar-\nial learning. In 2022 IEEE International Conference on Sys-\ntems, Man, and Cybernetics (SMC), pages 688–693. IEEE,\n2022. 1, 2\n[24] Alison C Holland, Garret O’Connell, and Isabel Dziobek.\nFacial mimicry, empathy, and emotion recognition: a meta-\nanalysis of correlations. Cognition and Emotion, 35(1):150–\n168, 2021. 1\n[25] Essam H Houssein, Asmaa Hammad, and Abdelmgeid A\nAli.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[25] Essam H Houssein, Asmaa Hammad, and Abdelmgeid A\nAli.\nHuman emotion recognition from eeg-based brain–\ncomputer interface using machine learning: a comprehensive\nreview. Neural Computing and Applications, 34(15):12527–\n12557, 2022. 2\n[26] Xincheng Ju, Dong Zhang, Junhui Li, and Guodong Zhou.\nTransformer-based label set generation for multi-modal\nmulti-label emotion detection. In Proceedings of the 28th\nACM international conference on multimedia, pages 512–\n520, 2020. 3\n[27] Dimitrios Kollias. Abaw: Valence-arousal estimation, ex-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='520, 2020. 3\n[27] Dimitrios Kollias. Abaw: Valence-arousal estimation, ex-\npression recognition, action unit detection & multi-task\nlearning challenges. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n2328–2336, 2022. 1, 3\n[28] Dimitrios Kollias. Multi-label compound expression recog-\nnition:\nC-expr database & network.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5589–5598, 2023. 1, 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5589–5598, 2023. 1, 2\n[29] Dimitrios Kollias and Stefanos Zafeiriou. Aff-wild2: Ex-\ntending the aff-wild database for affect recognition. arXiv\npreprint arXiv:1811.07770, 2018. 1\n[30] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,\naction unit recognition: Aff-wild2, multi-task learning and\narcface. arXiv preprint arXiv:1910.04855, 2019. 1\n[31] Dimitrios Kollias and Stefanos Zafeiriou.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[31] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis\nin-the-wild: Valence-arousal, expressions, action units and a\n8\nunified framework. arXiv preprint arXiv:2103.15792, 2021.\n1, 3\n[32] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-\ntive behavior in the second abaw2 competition. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 3652–3660, 2021. 1\n[33] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[33] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos\nZafeiriou.\nFace behavior a la carte:\nExpressions, af-\nfect and action units in a single network.\narXiv preprint\narXiv:1910.11111, 2019.\n[34] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,\nAthanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,\nIrene Kotsia, and Stefanos Zafeiriou. Deep affect prediction\nin-the-wild: Aff-wild database and challenge, deep architec-\ntures, and beyond. International Journal of Computer Vision,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='in-the-wild: Aff-wild database and challenge, deep architec-\ntures, and beyond. International Journal of Computer Vision,\npages 1–23, 2019.\n[35] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos\nZafeiriou.\nAnalysing affective behavior in the first abaw\n2020 competition. In 2020 15th IEEE International Confer-\nence on Automatic Face and Gesture Recognition (FG 2020),\npages 637–643. IEEE, 2020.\n[36] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos\nZafeiriou. Distribution matching for heterogeneous multi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[36] Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos\nZafeiriou. Distribution matching for heterogeneous multi-\ntask learning:\na large-scale face study.\narXiv preprint\narXiv:2105.03790, 2021.\n[37] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan\nCowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-\nmation, expression recognition, action unit detection & emo-\ntional reaction intensity estimation challenges. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='tional reaction intensity estimation challenges. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5888–5897, 2023. 1, 2, 3\n[38] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan\nCowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-\nmation, expression recognition, action unit detection & emo-\ntional reaction intensity estimation challenges. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5888–5897, 2023.\n[39] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[39] Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Ste-\nfanos Zafeiriou, Chunchang Shao, and Guanyu Hu. The 6th\naffective behavior analysis in-the-wild (abaw) competition.\narXiv preprint arXiv:2402.19344, 2024. 1\n[40] Jure Kranjec, S Beguˇs, G Gerˇsak, and J Drnovˇsek. Non-\ncontact heart rate and heart rate variability measurements: A\nreview. Biomedical signal processing and control, 13:102–\n112, 2014. 3\n[41] Beibei Kuang, Xueting Li, Xintong Li, Mingxiao Lin, Shan-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='112, 2014. 3\n[41] Beibei Kuang, Xueting Li, Xintong Li, Mingxiao Lin, Shan-\nrou Liu, and Ping Hu.\nThe effect of eye gaze direc-\ntion on emotional mimicry: A multimodal study with elec-\ntromyography and electroencephalography.\nNeuroImage,\n226:117604, 2021. 1, 2\n[42] Bharat Lal, Raffaele Gravina, Fanny Spagnolo, and Pasquale\nCorsonello. Compressed sensing approach for physiological\nsignals: A review. IEEE Sensors Journal, 2023. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Corsonello. Compressed sensing approach for physiological\nsignals: A review. IEEE Sensors Journal, 2023. 3\n[43] Hanting Li, Mingzhe Sui, Zhaoqing Zhu, et al. Nr-dfernet:\nNoise-robust network for dynamic facial expression recogni-\ntion. arXiv preprint arXiv:2206.04975, 2022. 3\n[44] Shan Li and Weihong Deng. Deep facial expression recog-\nnition: A survey. IEEE transactions on affective computing,\n13(3):1195–1215, 2020. 1, 2, 3\n[45] Shan Li, Weihong Deng, and JunPing Du. Reliable crowd-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[45] Shan Li, Weihong Deng, and JunPing Du. Reliable crowd-\nsourcing and deep locality-preserving learning for expres-\nsion recognition in the wild. In 2017 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n2584–2593. IEEE, 2017. 6\n[46] Yante Li, Xiaohua Huang, and Guoying Zhao.\nMicro-\nexpression action unit detection with spatial and channel at-\ntention. Neurocomputing, 436:221–231, 2021. 1, 2\n[47] Xiaolong Liu, Lei Sun, Wenqiang Jiang, Fengyuan Zhang,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[47] Xiaolong Liu, Lei Sun, Wenqiang Jiang, Fengyuan Zhang,\nYuanyuan Deng, Zhaopei Huang, Liyu Meng, Yuchen Liu,\nand Chuanhe Liu. Evaef: Ensemble valence-arousal estima-\ntion framework in the wild. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5862–5870, 2023. 1\n[48] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In Proceedings of\nthe IEEE international conference on computer vision, pages\n3730–3738, 2015. 4, 6', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='the IEEE international conference on computer vision, pages\n3730–3738, 2015. 4, 6\n[49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[50] Danielle Lottridge, Mark Chignell, and Aleksandra Jovicic.\nAffective interaction: Understanding, evaluating, and de-\nsigning for human emotion. Reviews of Human Factors and\nErgonomics, 7(1):197–217, 2011. 3\n[51] Fuyan Ma, Bin Sun, and Shutao Li. Spatio-temporal trans-\nformer for dynamic facial expression recognition in the wild.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[51] Fuyan Ma, Bin Sun, and Shutao Li. Spatio-temporal trans-\nformer for dynamic facial expression recognition in the wild.\narXiv preprint arXiv:2205.04749, 2022. 3\n[52] Javier Mar´ın-Morales, Carmen Llinares, Jaime Guixeres,\nand Mariano Alca˜niz. Emotion recognition in immersive vir-\ntual reality: From statistics to affective computing. Sensors,\n20(18):5163, 2020. 3\n[53] Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma-\nhoor. Affectnet: A database for facial expression, valence,\nand arousal computing in the wild. IEEE Transactions on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='hoor. Affectnet: A database for facial expression, valence,\nand arousal computing in the wild. IEEE Transactions on\nAffective Computing, 10(1):18–31, 2017. 4, 6\n[54] Dang-Khanh Nguyen, Ngoc-Huynh Ho, Sudarshan Pant, and\nHyung-Jeong Yang. A transformer-based approach to video\nframe-level prediction in affective behaviour analysis in-the-\nwild. arXiv preprint arXiv:2303.09293, 2023. 1\n[55] R Gnana Praveen, Patrick Cardinal, and Eric Granger.\nAudio-visual fusion for emotion recognition in the valence-\narousal space using joint cross-attention. IEEE Transactions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Audio-visual fusion for emotion recognition in the valence-\narousal space using joint cross-attention. IEEE Transactions\non Biometrics, Behavior, and Identity Science, 2023. 1\n[56] Minglun Ren, Nengying Chen, and Hui Qiu.\nHuman-\nmachine collaborative decision-making:\nAn evolutionary\nroadmap based on cognitive intelligence. International Jour-\nnal of Social Robotics, 15(7):1101–1114, 2023. 1\n[57] I Michael Revina and WR Sam Emmanuel.\nA survey on\nhuman face expression recognition techniques. Journal of\nKing Saud University-Computer and Information Sciences,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='A survey on\nhuman face expression recognition techniques. Journal of\nKing Saud University-Computer and Information Sciences,\n33(6):619–628, 2021. 1, 3\n[58] Albert D Ritzhaupt, Rui Huang, Max Sommer, Jiawen Zhu,\nAnita Stephen, Natercia Valle, John Hampton, and Jingwei\nLi.\nA meta-analysis on the influence of gamification in\n9\nformal educational settings on affective and behavioral out-\ncomes. Educational Technology Research and Development,\n69(5):2493–2522, 2021. 1\n[59] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep ex-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[59] Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep ex-\npectation of real and apparent age from a single image with-\nout facial landmarks. International Journal of Computer Vi-\nsion, 126(2):144–157, 2018. 4, 6\n[60] Jiahui She, Yibo Hu, Hailin Shi, Jun Wang, Qiu Shen, and\nTao Mei.\nDive into ambiguity: Latent distribution min-\ning and pairwise uncertainty estimation for facial expression\nrecognition. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 6248–6257,\n2021. 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='computer vision and pattern recognition, pages 6248–6257,\n2021. 1\n[61] Gopendra Vikram Singh, Mauajama Firdaus, Asif Ekbal, and\nPushpak Bhattacharyya. Emoint-trans: A multimodal trans-\nformer for identifying emotions and intents in social con-\nversations. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 31:290–300, 2022. 3\n[62] Rukshani Somarathna, Tomasz Bednarz, and Gelareh Mo-\nhammadi. Virtual reality for emotion elicitation–a review.\nIEEE Transactions on Affective Computing, 2022. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='hammadi. Virtual reality for emotion elicitation–a review.\nIEEE Transactions on Affective Computing, 2022. 3\n[63] Boˇstjan ˇSumak, Saˇsa Brdnik, and Maja Puˇsnik.\nSen-\nsors and artificial intelligence methods and algorithms for\nhuman–computer intelligent interaction: A systematic map-\nping study. Sensors, 22(1):20, 2021. 1\n[64] Martina\nSzab´oov´a,\nMartin\nSarnovsk`y,\nViera\nMaslej Kreˇsˇn´akov´a, and Krist´ına Machov´a.\nEmotion\nanalysis in human–robot interaction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Maslej Kreˇsˇn´akov´a, and Krist´ına Machov´a.\nEmotion\nanalysis in human–robot interaction.\nElectronics, 9(11):\n1761, 2020. 1\n[65] Gauthier Tallec, Edouard Yvinec, Arnaud Dapogny, and\nKevin Bailly. Multi-label transformer for action unit detec-\ntion. arXiv preprint arXiv:2203.12531, 2022. 1, 2\n[66] Giovanna Varni, Isabelle Hupont, Chloe Clavel, and Mo-\nhamed Chetouani. Computational study of primitive emo-\ntional contagion in dyadic interactions. IEEE Transactions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='hamed Chetouani. Computational study of primitive emo-\ntional contagion in dyadic interactions. IEEE Transactions\non Affective Computing, 11(2):258–271, 2017. 2\n[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[68] Juntao Wang and Tsunenori Mine. Multi-task learning for\nemotion recognition in conversation with emotion shift. In\nProceedings of the 37th Pacific Asia Conference on Lan-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='emotion recognition in conversation with emotion shift. In\nProceedings of the 37th Pacific Asia Conference on Lan-\nguage, Information and Computation, pages 257–266, 2023.\n2\n[69] Kai Wang, Xiaojiang Peng, Jianfei Yang, Shijian Lu, and\nYu Qiao. Suppressing uncertainties for large-scale facial ex-\npression recognition. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6897–6906, 2020. 1, 3\n[70] Tanja SH Wingenbach, Mark Brosnan, Monique C Pfaltz,\nPeter Peyk, and Chris Ashwin. Perception of discrete emo-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[70] Tanja SH Wingenbach, Mark Brosnan, Monique C Pfaltz,\nPeter Peyk, and Chris Ashwin. Perception of discrete emo-\ntions in others: Evidence for distinct facial mimicry patterns.\nScientific reports, 10(1):4692, 2020. 1, 2\n[71] Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal\nlearning with transformers: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2023. 2\n[72] Xinyu Yang, Yizhuo Dong, and Juan Li.\nReview of data\nfeatures-based music emotion recognition methods. Multi-\nmedia systems, 24:365–389, 2018. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='features-based music emotion recognition methods. Multi-\nmedia systems, 24:365–389, 2018. 3\n[73] Dongjie Ye, Zhangkai Ni, Hanli Wang, Jian Zhang, Shiqi\nWang, and Sam Kwong. Csformer: Bridging convolution\nand transformer for compressive sensing. IEEE Transactions\non Image Processing, 2023. 3\n[74] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-\ning face representation from scratch.\narXiv preprint\narXiv:1411.7923, 2014. 4, 6\n[75] Yufeng Yin, Minh Tran, Di Chang, Xinrui Wang, and Mo-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[75] Yufeng Yin, Minh Tran, Di Chang, Xinrui Wang, and Mo-\nhammad Soleymani. Multi-modal facial action unit detec-\ntion with large pre-trained models for the 5th competition\non affective behavior analysis in-the-wild.\narXiv preprint\narXiv:2303.10590, 2023. 1\n[76] Lin Yue, Weitong Chen, Xue Li, Wanli Zuo, and Minghao\nYin. A survey of sentiment analysis in social media. Knowl-\nedge and Information Systems, 60:617–663, 2019. 2\n[77] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[77] Stefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,\nAthanasios Papaioannou, Guoying Zhao, and Irene Kot-\nsia. Aff-wild: Valence and arousal ‘in-the-wild’challenge.\nIn Computer Vision and Pattern Recognition Workshops\n(CVPRW), 2017 IEEE Conference on, pages 1980–1987.\nIEEE, 2017. 1\n[78] Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng\nZhang, Yu Ding, Runze Wu, Tangjie Lv, and Changjie Fan.\nPrior aided streaming network for multi-task affective anal-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='Zhang, Yu Ding, Runze Wu, Tangjie Lv, and Changjie Fan.\nPrior aided streaming network for multi-task affective anal-\nysis. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV) Workshops, pages 3539–\n3549, 2021. 2\n[79] Wei Zhang, Feng Qiu, Suzhen Wang, Hao Zeng, Zhimeng\nZhang, Rudong An, Bowen Ma, and Yu Ding. Transformer-\nbased multimodal information fusion for facial expression\nanalysis.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2428–\n2437, 2022. 2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='on Computer Vision and Pattern Recognition, pages 2428–\n2437, 2022. 2\n[80] Wei Zhang, Bowen Ma, Feng Qiu, and Yu Ding.\nMulti-\nmodal facial affective analysis based on masked autoencoder.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5792–5801, 2023. 1\n[81] Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan,\nMichael Reale, Andy Horowitz, Peng Liu, and Jeffrey M Gi-\nrard. Bp4d-spontaneous: a high-resolution spontaneous 3d\ndynamic facial expression database. Image and Vision Com-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='rard. Bp4d-spontaneous: a high-resolution spontaneous 3d\ndynamic facial expression database. Image and Vision Com-\nputing, 32(10):692–706, 2014. 6\n[82] Yong Zhang, Weiming Dong, Bao-Gang Hu, and Qiang Ji.\nClassifier learning with prior probabilities for facial action\nunit recognition. In 2018 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 5108–5116,\n2018. 2\n[83] Zengqun Zhao and Qingshan Liu. Former-dfer: Dynamic\nfacial expression recognition transformer.\nIn Proceedings', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='[83] Zengqun Zhao and Qingshan Liu. Former-dfer: Dynamic\nfacial expression recognition transformer.\nIn Proceedings\nof the 29th ACM International Conference on Multimedia,\npages 1553–1561, 2021. 3\n[84] Zengqun Zhao, Qingshan Liu, and Shanmin Wang. Learning\ndeep global multi-scale and local attention features for facial\nexpression recognition in the wild. IEEE Transactions on\nImage Processing, 30:6544–6556, 2021. 1\n10\n[85] Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'}), Document(page_content='10\n[85] Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie\nHuang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Da-\nlong Du, et al. Webface260m: A benchmark unveiling the\npower of million-scale deep face recognition. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10492–10502, 2021. 4, 6\n11', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpuheaomt1/2403.10825v1.pdf'})]
cuda:2
讲讲这篇文章的摘要 tmpuheaomt1
[]
cuda:2
[1.092532, 1.129525, 1.1345373, 1.1356086, 1.1367137, 1.1403013]
讲讲这篇文章的摘要 tmpuheaomt1
[]
cuda:2
[1.092532, 1.129525, 1.1345373, 1.1356086, 1.1367137, 1.1403013]
能讲讲他使用的方法吗 tmpuheaomt1
[History(role='user', content='讲讲这篇文章的摘要'), History(role='assistant', content='这篇文章主要涉及情感识别领域的研究，包括 Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention. IEEE Transactions on Biometrics, Behavior, and Identity Science, 2023. 1 [56]，Human-machine collaborative decision-making: An evolutionary roadmap based on cognitive intelligence. International Journal of Social Robotics, 15(7):1101–1114, 2023. 1 [57]，A survey on human face expression recognition techniques. Journal of King Saud University-Computer and Information Sciences, 33(6):619–628, 2021. 1, 3 [58]，A meta-analysis on the influence of gamification in formal educational settings on affective and behavioral outcomes. Educational Technology Research and Development, 69(5):2493–2522, 2021. 1，Deep Koerich, Simon Bacon, and Eric Granger. Guided inter-interpretable facial expression recognition via spatial action unit cues. arXiv preprint arXiv:2402.00281, 2024. 1, 2，Patricia J Bota, Chen Wang, Ana LN Fred, and Hugo Placido Da Silva. A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals. IEEE access, 7:140990–141020, 2019. 3，Paul Buitelaar, Ian D Wood, Sapna Negi, Mihael Arcan, John P McCrae, Andrejs Abele, Cecile Robin, Vladimir Tanja SH Wingenbach, Mark Brosnan, Monique C Pfaltz, Peter Peyk, and Chris Ashwin. Perception of discrete emotions in others: Evidence for distinct facial mimicry patterns. Scientific reports, 10(1):4692, 2020. 1, 2，Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal learning with transformers: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2，Xinyu Yang, Yizhuo Dong, and Juan Li. Review of data features-based music emotion recognition methods. Multi-media systems, 24:365–389, 2018. 3，hammadi. Virtual reality for emotion elicitation–a review. IEEE Transactions on Affective Computing, 2022. 3，Boˇstjan 夏令时，Sa智慧城市和智能系统：一种系统映射研究。传感器，22(1):20, 2021. 1，Martina SzabovOVa, Martin SarnovskY, Viera Maslej Kre登入，Kristina MachovA. Emotion analysis in human-robot interaction. hammadi. 计算原始情感情感在双人互动中的情感传染现象的计算研究。IEEE Transactions on Affective Computing, 11(2):258–271, 2017. 2，Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3，Juntao Wang and Tsunenori Mine. 多任务学习在情感变化对话中的情感识别。 In Proceedings of the 37th Pacific Asia Conference on Language and Information Technologies, 2022.')]
cuda:2
[1.1985486, 1.2001252, 1.2037909, 1.2096717, 1.2269979, 1.2373164]
能讲讲他使用的方法吗 tmpuheaomt1
[History(role='user', content='讲讲这篇文章的摘要'), History(role='assistant', content='这篇文章主要涉及情感识别领域的研究，包括 Audio-visual fusion for emotion recognition in the valence-arousal space using joint cross-attention. IEEE Transactions on Biometrics, Behavior, and Identity Science, 2023. 1 [56]，Human-machine collaborative decision-making: An evolutionary roadmap based on cognitive intelligence. International Journal of Social Robotics, 15(7):1101–1114, 2023. 1 [57]，A survey on human face expression recognition techniques. Journal of King Saud University-Computer and Information Sciences, 33(6):619–628, 2021. 1, 3 [58]，A meta-analysis on the influence of gamification in formal educational settings on affective and behavioral outcomes. Educational Technology Research and Development, 69(5):2493–2522, 2021. 1，Deep Koerich, Simon Bacon, and Eric Granger. Guided inter-interpretable facial expression recognition via spatial action unit cues. arXiv preprint arXiv:2402.00281, 2024. 1, 2，Patricia J Bota, Chen Wang, Ana LN Fred, and Hugo Placido Da Silva. A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals. IEEE access, 7:140990–141020, 2019. 3，Paul Buitelaar, Ian D Wood, Sapna Negi, Mihael Arcan, John P McCrae, Andrejs Abele, Cecile Robin, Vladimir Tanja SH Wingenbach, Mark Brosnan, Monique C Pfaltz, Peter Peyk, and Chris Ashwin. Perception of discrete emotions in others: Evidence for distinct facial mimicry patterns. Scientific reports, 10(1):4692, 2020. 1, 2，Peng Xu, Xiatian Zhu, and David A Clifton. Multimodal learning with transformers: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 2，Xinyu Yang, Yizhuo Dong, and Juan Li. Review of data features-based music emotion recognition methods. Multi-media systems, 24:365–389, 2018. 3，hammadi. Virtual reality for emotion elicitation–a review. IEEE Transactions on Affective Computing, 2022. 3，Boˇstjan 夏令时，Sa智慧城市和智能系统：一种系统映射研究。传感器，22(1):20, 2021. 1，Martina SzabovOVa, Martin SarnovskY, Viera Maslej Kre登入，Kristina MachovA. Emotion analysis in human-robot interaction. hammadi. 计算原始情感情感在双人互动中的情感传染现象的计算研究。IEEE Transactions on Affective Computing, 11(2):258–271, 2017. 2，Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3，Juntao Wang and Tsunenori Mine. 多任务学习在情感变化对话中的情感识别。 In Proceedings of the 37th Pacific Asia Conference on Language and Information Technologies, 2022.')]
cuda:2
[1.1985486, 1.2001252, 1.2037909, 1.2096717, 1.2269979, 1.2373164]
[UploadFile(filename='05473f62-adf9-4adf-9196-f589d9c96780.txt', size=5283, headers=Headers({'content-disposition': 'form-data; name="files"; filename="05473f62-adf9-4adf-9196-f589d9c96780.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpw3w8908t, tmpw3w8908t
File: 05473f62-adf9-4adf-9196-f589d9c96780.txt, msg: 成功上传文件 05473f62-adf9-4adf-9196-f589d9c96780.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc2e98d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpw3w8908t/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
cuda:2
[UploadFile(filename='c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt', size=5576, headers=Headers({'content-disposition': 'form-data; name="files"; filename="c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp1oh4u5fv, tmp1oh4u5fv
File: c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, msg: 成功上传文件 c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, docs: [Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5b4c50> 111
cuda:2
[Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1oh4u5fv/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
cuda:2
[UploadFile(filename='c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt', size=5576, headers=Headers({'content-disposition': 'form-data; name="files"; filename="c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpy5dd8tdi, tmpy5dd8tdi
File: c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, msg: 成功上传文件 c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, docs: [Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5a2b10> 111
cuda:2
[Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpy5dd8tdi/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
cuda:2
[UploadFile(filename='c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt', size=5576, headers=Headers({'content-disposition': 'form-data; name="files"; filename="c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp2eum_szk, tmp2eum_szk
File: c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, msg: 成功上传文件 c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, docs: [Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc25fc50> 111
cuda:2
[Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp2eum_szk/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
cuda:2
[UploadFile(filename='c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt', size=5576, headers=Headers({'content-disposition': 'form-data; name="files"; filename="c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp9lyz2xo2, tmp9lyz2xo2
File: c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, msg: 成功上传文件 c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, docs: [Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc7062d0> 111
cuda:2
[Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9lyz2xo2/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
cuda:2
[UploadFile(filename='c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt', size=5576, headers=Headers({'content-disposition': 'form-data; name="files"; filename="c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpapog95xs, tmpapog95xs
File: c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, msg: 成功上传文件 c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt, docs: [Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc3d96d0> 111
cuda:2
[Document(page_content='Decompose, Adjust, Compose: Effective Normalization by Playing with  Frequency for Domain Generalization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Domain generalization (DG) is a principal task to evaluate the robustness ofcomputer vision models. Many previous studies have used normalization for DG.In normalization, statistics and normalized features are regarded as style andcontent, respectively. However, it has a content variation problem whenremoving style because the boundary between content and style is unclear. Thisstudy addresses this problem from the frequency domain perspective, whereamplitude and phase are considered as style and content, respectively. First,we verify the quantitative phase variation of normalization through themathematical derivation of the Fourier transform formula. Then, based on this,we propose a novel normalization method, PCNorm, which eliminates style only asthe preserving content through spectral decomposition. Furthermore, we proposeadvanced PCNorm variants, CCNorm and SCNorm, which adjust the degrees ofvariations in content and style, respectively. Thus, they can learndomain-agnostic representations for DG.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Thus, they can learndomain-agnostic representations for DG. With the normalization methods, wepropose ResNet-variant models, DAC-P and DAC-SC, which are robust to the domaingap. The proposed models outperform other recent DG methods. The DAC-SCachieves an average state-of-the-art performance of 65.6% on five datasets:PACS, VLCS, Office-Home, DomainNet, and TerraIncognita.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver  Distraction Detection\nEnsuring traffic safety and mitigating accidents in modern driving is ofparamount importance, and computer vision technologies have the potential tosignificantly contribute to this goal. This paper presents a multi-modal VisionTransformer for Driver Distraction Detection (termed ViT-DD), whichincorporates inductive information from training signals related to bothdistraction detection and driver emotion recognition. Additionally, aself-learning algorithm is developed, allowing for the seamless integration ofdriver data without emotion labels into the multi-task training process ofViT-DD. Experimental results reveal that the proposed ViT-DD surpasses existingstate-of-the-art methods for driver distraction detection by 6.5% and 0.9% onthe SFDDD and AUCDD datasets, respectively.\nSemantic Segmentation Using Super Resolution Technique as Pre-Processing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Semantic Segmentation Using Super Resolution Technique as Pre-Processing\nCombining high-level and low-level visual tasks is a common technique in thefield of computer vision. This work integrates the technique of image superresolution to semantic segmentation for document image binarization. Itdemonstrates that using image super-resolution as a preprocessing step caneffectively enhance the results and performance of semantic segmentation.\nGIST: Generating Image-Specific Text for Fine-grained Object  Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Recent vision-language models outperform vision-only models on many imageclassification tasks. However, because of the absence of paired text/imagedescriptions, it remains difficult to fine-tune these models for fine-grainedimage classification. In this work, we propose a method, GIST, for generatingimage-specific fine-grained text descriptions from image-only datasets, andshow that these text descriptions can be used to improve classification. Keyparts of our method include 1. prompting a pretrained large language model withdomain-specific prompts to generate diverse fine-grained text descriptions foreach class and 2. using a pretrained vision-language model to match each imageto label-preserving text descriptions that capture relevant visual features inthe image. We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='We demonstrate the utility of GIST by fine-tuning vision-languagemodels on the image-and-generated-text pairs to learn an alignedvision-language representation space for improved classification. We evaluateour learned representation space in full-shot and few-shot scenarios acrossfour diverse fine-grained classification datasets, each from a differentdomain. Our method achieves an average improvement of $4.1\\%$ in accuracy overCLIP linear probes and an average of $1.1\\%$ improvement in accuracy over theprevious state-of-the-art image-text classification method on the full-shotdatasets. Our method achieves similar improvements across few-shot regimes.Code is available at https://github.com/emu1729/GIST.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Adversarially-Aware Robust Object Detector', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='Object detection, as a fundamental computer vision task, has achieved aremarkable progress with the emergence of deep neural networks. Nevertheless,few works explore the adversarial robustness of object detectors to resistadversarial attacks for practical applications in various real-world scenarios.Detectors have been greatly challenged by unnoticeable perturbation, with sharpperformance drop on clean images and extremely poor performance on adversarialimages. In this work, we empirically explore the model training for adversarialrobustness in object detection, which greatly attributes to the conflictbetween learning clean images and adversarial images. To mitigate this issue,we propose a Robust Detector (RobustDet) based on adversarially-awareconvolution to disentangle gradients for model learning on clean andadversarial images. RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'}), Document(page_content='RobustDet also employs the Adversarial Image Discriminator(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliablerobustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate thatour model effectively disentangles gradients and significantly enhances thedetection robustness with maintaining the detection ability on clean images.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpapog95xs/c3b6aa39-4835-43f9-ab21-7ef9ccbf8eae.txt'})]
cuda:2
[UploadFile(filename='464b629c-2e55-413e-a911-79a3abfb3906.txt', size=6286, headers=Headers({'content-disposition': 'form-data; name="files"; filename="464b629c-2e55-413e-a911-79a3abfb3906.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpu003_j_i, tmpu003_j_i
File: 464b629c-2e55-413e-a911-79a3abfb3906.txt, msg: 成功上传文件 464b629c-2e55-413e-a911-79a3abfb3906.txt, docs: [Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc25c7d0> 111
cuda:2
[Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpu003_j_i/464b629c-2e55-413e-a911-79a3abfb3906.txt'})]
cuda:2
[UploadFile(filename='c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt', size=6286, headers=Headers({'content-disposition': 'form-data; name="files"; filename="c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp1040rysb, tmp1040rysb
[UploadFile(filename='c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt', size=6286, headers=Headers({'content-disposition': 'form-data; name="files"; filename="c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp7g19cw2f, tmp7g19cw2f
File: c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt, msg: 成功上传文件 c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt, docs: [Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc25fe50> 111
cuda:2
File: c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt, msg: 成功上传文件 c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt, docs: [Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbd5d16d0> 111
cuda:2
[Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp1040rysb/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'})]
cuda:2
[Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp7g19cw2f/c3c26a68-0c1a-4023-82db-d7b0003c36fe.txt'})]
cuda:2
cuda:2
cuda:2
[UploadFile(filename='None.txt', size=5515, headers=Headers({'content-disposition': 'form-data; name="files"; filename="None.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp9t_xfvvo, tmp9t_xfvvo
[UploadFile(filename='None.txt', size=5515, headers=Headers({'content-disposition': 'form-data; name="files"; filename="None.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpydvo4j3f, tmpydvo4j3f
File: None.txt, msg: 成功上传文件 None.txt, docs: [Document(page_content='A Efficient Multimodal Framework for Large Scale Emotion Recognition by  Fusing Music and Electrodermal Activity Signals', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='Considerable attention has been paid for physiological signal-based emotionrecognition in field of affective computing. For the reliability and userfriendly acquisition, Electrodermal Activity (EDA) has great advantage inpractical applications. However, the EDA-based emotion recognition withhundreds of subjects still lacks effective solution. In this paper, our workmakes an attempt to fuse the subject individual EDA features and the externalevoked music features. And we propose an end-to-end multimodal framework, the1-dimensional residual temporal and channel attention network (RTCAN-1D). ForEDA features, the novel convex optimization-based EDA (CvxEDA) method isapplied to decompose EDA signals into pahsic and tonic signals for mining thedynamic and steady features. The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation. For music features, we process the music signalwith the open source toolkit openSMILE to obtain external feature vectors. Theindividual emotion features from EDA signals and external emotion benchmarksfrom music are fused in the classifing layers. We have conducted systematiccomparisons on three multimodal datasets (PMEmo, DEAP, AMIGOS) for 2-classesvalance/arousal emotion recognition.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='Our proposed RTCAN-1D outperforms theexisting state-of-the-art models, which also validate that our work provides anreliable and efficient solution for large scale emotion recognition. Our codehas been released at https://github.com/guanghaoyin/RTCAN-1D.\nAudRandAug: Random Image Augmentations for Audio Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='AudRandAug: Random Image Augmentations for Audio Classification\nData augmentation has proven to be effective in training neural networks.Recently, a method called RandAug was proposed, randomly selecting dataaugmentation techniques from a predefined search space. RandAug hasdemonstrated significant performance improvements for image-related tasks whileimposing minimal computational overhead. However, no prior research hasexplored the application of RandAug specifically for audio data augmentation,which converts audio into an image-like pattern. To address this gap, weintroduce AudRandAug, an adaptation of RandAug for audio data. AudRandAugselects data augmentation policies from a dedicated audio search space. Toevaluate the effectiveness of AudRandAug, we conducted experiments usingvarious models and datasets. Our findings indicate that AudRandAug outperformsother existing data augmentation methods regarding accuracy performance.\nSparsePipe: Parallel Deep Learning for 3D Point Clouds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='We propose SparsePipe, an efficient and asynchronous parallelism approach forhandling 3D point clouds with multi-GPU training. SparsePipe is built tosupport 3D sparse data such as point clouds. It achieves this by adoptinggeneralized convolutions with sparse tensor representation to build expressivehigh-dimensional convolutional neural networks. Compared to dense solutions,the new models can efficiently process irregular point clouds without denselysliding over the entire space, significantly reducing the memory requirementsand allowing higher resolutions of the underlying 3D volumes for betterperformance. SparsePipe exploits intra-batch parallelism that partitions input data intomultiple processors and further improves the training throughput withinter-batch pipelining to overlap communication and computing. Besides, itsuitably partitions the model when the GPUs are heterogeneous such that thecomputing is load-balanced with reduced communication overhead. Using experimental results on an eight-GPU platform, we show that SparsePipecan parallelize effectively and obtain better performance on current pointcloud benchmarks for both training and inference, compared to its densesolutions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='LocoGAN -- Locally Convolutional GAN\nIn the paper we construct a fully convolutional GAN model: LocoGAN, whichlatent space is given by noise-like images of possibly different resolutions.The learning is local, i.e. we process not the whole noise-like image, but thesub-images of a fixed size. As a consequence LocoGAN can produce images ofarbitrary dimensions e.g. LSUN bedroom data set. Another advantage of ourapproach comes from the fact that we use the position channels, which allowsthe generation of fully periodic (e.g. cylindrical panoramic images) or almostperiodic ,,infinitely long" images (e.g. wall-papers).\nRoom-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding\nWe introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger(more paths and instructions) than other VLN datasets. It emphasizes the roleof language in VLN by addressing known biases in paths and eliciting morereferences to visible entities. Furthermore, each word in an instruction istime-aligned to the virtual poses of instruction creators and validators. Weestablish baseline scores for monolingual and multilingual settings andmultitask learning when including Room-to-Room annotations. We also provideresults for a model that learns from synchronized pose traces by focusing onlyon portions of the panorama attended to in human demonstrations. The size,scope and detail of RxR dramatically expands the frontier for research onembodied language agents in simulated, photo-realistic environments.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa550c550> 111
cuda:2
File: None.txt, msg: 成功上传文件 None.txt, docs: [Document(page_content='A Efficient Multimodal Framework for Large Scale Emotion Recognition by  Fusing Music and Electrodermal Activity Signals', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='Considerable attention has been paid for physiological signal-based emotionrecognition in field of affective computing. For the reliability and userfriendly acquisition, Electrodermal Activity (EDA) has great advantage inpractical applications. However, the EDA-based emotion recognition withhundreds of subjects still lacks effective solution. In this paper, our workmakes an attempt to fuse the subject individual EDA features and the externalevoked music features. And we propose an end-to-end multimodal framework, the1-dimensional residual temporal and channel attention network (RTCAN-1D). ForEDA features, the novel convex optimization-based EDA (CvxEDA) method isapplied to decompose EDA signals into pahsic and tonic signals for mining thedynamic and steady features. The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation. For music features, we process the music signalwith the open source toolkit openSMILE to obtain external feature vectors. Theindividual emotion features from EDA signals and external emotion benchmarksfrom music are fused in the classifing layers. We have conducted systematiccomparisons on three multimodal datasets (PMEmo, DEAP, AMIGOS) for 2-classesvalance/arousal emotion recognition.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='Our proposed RTCAN-1D outperforms theexisting state-of-the-art models, which also validate that our work provides anreliable and efficient solution for large scale emotion recognition. Our codehas been released at https://github.com/guanghaoyin/RTCAN-1D.\nAudRandAug: Random Image Augmentations for Audio Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='AudRandAug: Random Image Augmentations for Audio Classification\nData augmentation has proven to be effective in training neural networks.Recently, a method called RandAug was proposed, randomly selecting dataaugmentation techniques from a predefined search space. RandAug hasdemonstrated significant performance improvements for image-related tasks whileimposing minimal computational overhead. However, no prior research hasexplored the application of RandAug specifically for audio data augmentation,which converts audio into an image-like pattern. To address this gap, weintroduce AudRandAug, an adaptation of RandAug for audio data. AudRandAugselects data augmentation policies from a dedicated audio search space. Toevaluate the effectiveness of AudRandAug, we conducted experiments usingvarious models and datasets. Our findings indicate that AudRandAug outperformsother existing data augmentation methods regarding accuracy performance.\nSparsePipe: Parallel Deep Learning for 3D Point Clouds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='We propose SparsePipe, an efficient and asynchronous parallelism approach forhandling 3D point clouds with multi-GPU training. SparsePipe is built tosupport 3D sparse data such as point clouds. It achieves this by adoptinggeneralized convolutions with sparse tensor representation to build expressivehigh-dimensional convolutional neural networks. Compared to dense solutions,the new models can efficiently process irregular point clouds without denselysliding over the entire space, significantly reducing the memory requirementsand allowing higher resolutions of the underlying 3D volumes for betterperformance. SparsePipe exploits intra-batch parallelism that partitions input data intomultiple processors and further improves the training throughput withinter-batch pipelining to overlap communication and computing. Besides, itsuitably partitions the model when the GPUs are heterogeneous such that thecomputing is load-balanced with reduced communication overhead. Using experimental results on an eight-GPU platform, we show that SparsePipecan parallelize effectively and obtain better performance on current pointcloud benchmarks for both training and inference, compared to its densesolutions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='LocoGAN -- Locally Convolutional GAN\nIn the paper we construct a fully convolutional GAN model: LocoGAN, whichlatent space is given by noise-like images of possibly different resolutions.The learning is local, i.e. we process not the whole noise-like image, but thesub-images of a fixed size. As a consequence LocoGAN can produce images ofarbitrary dimensions e.g. LSUN bedroom data set. Another advantage of ourapproach comes from the fact that we use the position channels, which allowsthe generation of fully periodic (e.g. cylindrical panoramic images) or almostperiodic ,,infinitely long" images (e.g. wall-papers).\nRoom-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding\nWe introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger(more paths and instructions) than other VLN datasets. It emphasizes the roleof language in VLN by addressing known biases in paths and eliciting morereferences to visible entities. Furthermore, each word in an instruction istime-aligned to the virtual poses of instruction creators and validators. Weestablish baseline scores for monolingual and multilingual settings andmultitask learning when including Room-to-Room annotations. We also provideresults for a model that learns from synchronized pose traces by focusing onlyon portions of the panorama attended to in human demonstrations. The size,scope and detail of RxR dramatically expands the frontier for research onembodied language agents in simulated, photo-realistic environments.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa578e5d0> 111
cuda:2
[Document(page_content='A Efficient Multimodal Framework for Large Scale Emotion Recognition by  Fusing Music and Electrodermal Activity Signals', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='Considerable attention has been paid for physiological signal-based emotionrecognition in field of affective computing. For the reliability and userfriendly acquisition, Electrodermal Activity (EDA) has great advantage inpractical applications. However, the EDA-based emotion recognition withhundreds of subjects still lacks effective solution. In this paper, our workmakes an attempt to fuse the subject individual EDA features and the externalevoked music features. And we propose an end-to-end multimodal framework, the1-dimensional residual temporal and channel attention network (RTCAN-1D). ForEDA features, the novel convex optimization-based EDA (CvxEDA) method isapplied to decompose EDA signals into pahsic and tonic signals for mining thedynamic and steady features. The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation. For music features, we process the music signalwith the open source toolkit openSMILE to obtain external feature vectors. Theindividual emotion features from EDA signals and external emotion benchmarksfrom music are fused in the classifing layers. We have conducted systematiccomparisons on three multimodal datasets (PMEmo, DEAP, AMIGOS) for 2-classesvalance/arousal emotion recognition.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='Our proposed RTCAN-1D outperforms theexisting state-of-the-art models, which also validate that our work provides anreliable and efficient solution for large scale emotion recognition. Our codehas been released at https://github.com/guanghaoyin/RTCAN-1D.\nAudRandAug: Random Image Augmentations for Audio Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='AudRandAug: Random Image Augmentations for Audio Classification\nData augmentation has proven to be effective in training neural networks.Recently, a method called RandAug was proposed, randomly selecting dataaugmentation techniques from a predefined search space. RandAug hasdemonstrated significant performance improvements for image-related tasks whileimposing minimal computational overhead. However, no prior research hasexplored the application of RandAug specifically for audio data augmentation,which converts audio into an image-like pattern. To address this gap, weintroduce AudRandAug, an adaptation of RandAug for audio data. AudRandAugselects data augmentation policies from a dedicated audio search space. Toevaluate the effectiveness of AudRandAug, we conducted experiments usingvarious models and datasets. Our findings indicate that AudRandAug outperformsother existing data augmentation methods regarding accuracy performance.\nSparsePipe: Parallel Deep Learning for 3D Point Clouds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='We propose SparsePipe, an efficient and asynchronous parallelism approach forhandling 3D point clouds with multi-GPU training. SparsePipe is built tosupport 3D sparse data such as point clouds. It achieves this by adoptinggeneralized convolutions with sparse tensor representation to build expressivehigh-dimensional convolutional neural networks. Compared to dense solutions,the new models can efficiently process irregular point clouds without denselysliding over the entire space, significantly reducing the memory requirementsand allowing higher resolutions of the underlying 3D volumes for betterperformance. SparsePipe exploits intra-batch parallelism that partitions input data intomultiple processors and further improves the training throughput withinter-batch pipelining to overlap communication and computing. Besides, itsuitably partitions the model when the GPUs are heterogeneous such that thecomputing is load-balanced with reduced communication overhead. Using experimental results on an eight-GPU platform, we show that SparsePipecan parallelize effectively and obtain better performance on current pointcloud benchmarks for both training and inference, compared to its densesolutions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='LocoGAN -- Locally Convolutional GAN\nIn the paper we construct a fully convolutional GAN model: LocoGAN, whichlatent space is given by noise-like images of possibly different resolutions.The learning is local, i.e. we process not the whole noise-like image, but thesub-images of a fixed size. As a consequence LocoGAN can produce images ofarbitrary dimensions e.g. LSUN bedroom data set. Another advantage of ourapproach comes from the fact that we use the position channels, which allowsthe generation of fully periodic (e.g. cylindrical panoramic images) or almostperiodic ,,infinitely long" images (e.g. wall-papers).\nRoom-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'}), Document(page_content='Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding\nWe introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger(more paths and instructions) than other VLN datasets. It emphasizes the roleof language in VLN by addressing known biases in paths and eliciting morereferences to visible entities. Furthermore, each word in an instruction istime-aligned to the virtual poses of instruction creators and validators. Weestablish baseline scores for monolingual and multilingual settings andmultitask learning when including Room-to-Room annotations. We also provideresults for a model that learns from synchronized pose traces by focusing onlyon portions of the panorama attended to in human demonstrations. The size,scope and detail of RxR dramatically expands the frontier for research onembodied language agents in simulated, photo-realistic environments.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp9t_xfvvo/None.txt'})]
cuda:2
[Document(page_content='A Efficient Multimodal Framework for Large Scale Emotion Recognition by  Fusing Music and Electrodermal Activity Signals', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='Considerable attention has been paid for physiological signal-based emotionrecognition in field of affective computing. For the reliability and userfriendly acquisition, Electrodermal Activity (EDA) has great advantage inpractical applications. However, the EDA-based emotion recognition withhundreds of subjects still lacks effective solution. In this paper, our workmakes an attempt to fuse the subject individual EDA features and the externalevoked music features. And we propose an end-to-end multimodal framework, the1-dimensional residual temporal and channel attention network (RTCAN-1D). ForEDA features, the novel convex optimization-based EDA (CvxEDA) method isapplied to decompose EDA signals into pahsic and tonic signals for mining thedynamic and steady features. The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='The channel-temporal attention mechanism forEDA-based emotion recognition is firstly involved to improve the temporal- andchannel-wise representation. For music features, we process the music signalwith the open source toolkit openSMILE to obtain external feature vectors. Theindividual emotion features from EDA signals and external emotion benchmarksfrom music are fused in the classifing layers. We have conducted systematiccomparisons on three multimodal datasets (PMEmo, DEAP, AMIGOS) for 2-classesvalance/arousal emotion recognition.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='Our proposed RTCAN-1D outperforms theexisting state-of-the-art models, which also validate that our work provides anreliable and efficient solution for large scale emotion recognition. Our codehas been released at https://github.com/guanghaoyin/RTCAN-1D.\nAudRandAug: Random Image Augmentations for Audio Classification', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='AudRandAug: Random Image Augmentations for Audio Classification\nData augmentation has proven to be effective in training neural networks.Recently, a method called RandAug was proposed, randomly selecting dataaugmentation techniques from a predefined search space. RandAug hasdemonstrated significant performance improvements for image-related tasks whileimposing minimal computational overhead. However, no prior research hasexplored the application of RandAug specifically for audio data augmentation,which converts audio into an image-like pattern. To address this gap, weintroduce AudRandAug, an adaptation of RandAug for audio data. AudRandAugselects data augmentation policies from a dedicated audio search space. Toevaluate the effectiveness of AudRandAug, we conducted experiments usingvarious models and datasets. Our findings indicate that AudRandAug outperformsother existing data augmentation methods regarding accuracy performance.\nSparsePipe: Parallel Deep Learning for 3D Point Clouds', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='We propose SparsePipe, an efficient and asynchronous parallelism approach forhandling 3D point clouds with multi-GPU training. SparsePipe is built tosupport 3D sparse data such as point clouds. It achieves this by adoptinggeneralized convolutions with sparse tensor representation to build expressivehigh-dimensional convolutional neural networks. Compared to dense solutions,the new models can efficiently process irregular point clouds without denselysliding over the entire space, significantly reducing the memory requirementsand allowing higher resolutions of the underlying 3D volumes for betterperformance. SparsePipe exploits intra-batch parallelism that partitions input data intomultiple processors and further improves the training throughput withinter-batch pipelining to overlap communication and computing. Besides, itsuitably partitions the model when the GPUs are heterogeneous such that thecomputing is load-balanced with reduced communication overhead. Using experimental results on an eight-GPU platform, we show that SparsePipecan parallelize effectively and obtain better performance on current pointcloud benchmarks for both training and inference, compared to its densesolutions.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='LocoGAN -- Locally Convolutional GAN\nIn the paper we construct a fully convolutional GAN model: LocoGAN, whichlatent space is given by noise-like images of possibly different resolutions.The learning is local, i.e. we process not the whole noise-like image, but thesub-images of a fixed size. As a consequence LocoGAN can produce images ofarbitrary dimensions e.g. LSUN bedroom data set. Another advantage of ourapproach comes from the fact that we use the position channels, which allowsthe generation of fully periodic (e.g. cylindrical panoramic images) or almostperiodic ,,infinitely long" images (e.g. wall-papers).\nRoom-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'}), Document(page_content='Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense  Spatiotemporal Grounding\nWe introduce Room-Across-Room (RxR), a new Vision-and-Language Navigation(VLN) dataset. RxR is multilingual (English, Hindi, and Telugu) and larger(more paths and instructions) than other VLN datasets. It emphasizes the roleof language in VLN by addressing known biases in paths and eliciting morereferences to visible entities. Furthermore, each word in an instruction istime-aligned to the virtual poses of instruction creators and validators. Weestablish baseline scores for monolingual and multilingual settings andmultitask learning when including Room-to-Room annotations. We also provideresults for a model that learns from synchronized pose traces by focusing onlyon portions of the panorama attended to in human demonstrations. The size,scope and detail of RxR dramatically expands the frontier for research onembodied language agents in simulated, photo-realistic environments.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpydvo4j3f/None.txt'})]
cuda:2
cuda:2
[UploadFile(filename='a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt', size=4356, headers=Headers({'content-disposition': 'form-data; name="files"; filename="a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpakdfdxdg, tmpakdfdxdg
File: a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt, msg: 成功上传文件 a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt, docs: [Document(page_content='Deep Transfer Convolutional Neural Network and Extreme Learning Machine  for Lung Nodule Diagnosis on CT images\nSome content of the article needs to be kept secret\nMultimodal Research in Vision and Language: A Review of Current and  Emerging Trends', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Some content of the article needs to be kept secret\nMultimodal Research in Vision and Language: A Review of Current and  Emerging Trends\nDeep Learning and its applications have cascaded impactful research anddevelopment with a diverse range of modalities present in the real-world data.More recently, this has enhanced research interests in the intersection of theVision and Language arena with its numerous applications and fast-paced growth.In this paper, we present a detailed overview of the latest trends in researchpertaining to visual and language modalities. We look at its applications intheir task formulations and how to solve various problems related to semanticperception and content generation. We also address task-specific trends, alongwith their evaluation strategies and upcoming challenges. Moreover, we shedsome light on multi-disciplinary patterns and insights that have emerged in therecent past, directing this field towards more modular and transparentintelligent systems. This survey identifies key trends gravitating recentliterature in VisLang research and attempts to unearth directions that thefield is heading towards.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='WiCV 2021: The Eighth Women In Computer Vision Workshop\nIn this paper, we present the details of Women in Computer Vision Workshop -WiCV 2021, organized alongside the virtual CVPR 2021. It provides a voice to aminority (female) group in the computer vision community and focuses onincreasing the visibility of these researchers, both in academia and industry.WiCV believes that such an event can play an important role in lowering thegender imbalance in the field of computer vision. WiCV is organized each yearwhere it provides a)~opportunity for collaboration between researchers fromminority groups, b)~mentorship to female junior researchers, c)~financialsupport to presenters to overcome monetary burden and d)~large and diversechoice of role models, who can serve as examples to younger researchers at thebeginning of their careers. In this paper, we present a report on the workshopprogram, trends over the past years, a summary of statistics regardingpresenters, attendees, and sponsorship for the WiCV 2021 workshop.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive  Review', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Tea leaf diseases are a major challenge to agricultural productivity, withfar-reaching implications for yield and quality in the tea industry. The riseof machine learning has enabled the development of innovative approaches tocombat these diseases. Early detection and diagnosis are crucial for effectivecrop management. For predicting tea leaf disease, several automated systemshave already been developed using different image processing techniques. Thispaper delivers a systematic review of the literature on machine learningmethodologies applied to diagnose tea leaf disease via image classification. Itthoroughly evaluates the strengths and constraints of various VisionTransformer models, including Inception Convolutional Vision Transformer(ICVT), GreenViT, PlantXViT, PlantViT, MSCVT, Transfer Learning Model & VisionTransformer (TLMViT), IterationViT, IEM-ViT.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Moreover, this paper also reviewsmodels like Dense Convolutional Network (DenseNet), Residual Neural Network(ResNet)-50V2, YOLOv5, YOLOv7, Convolutional Neural Network (CNN), Deep CNN,Non-dominated Sorting Genetic Algorithm (NSGA-II), MobileNetv2, andLesion-Aware Visual Transformer. These machine-learning models have been testedon various datasets, demonstrating their real-world applicability. This reviewstudy not only highlights current progress in the field but also providesvaluable insights for future research directions in the machine learning-baseddetection and classification of tea leaf diseases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='HUMS2023 Data Challenge Result Submission\nWe implemented a simple method for early detection in this research. Theimplemented methods are plotting the given mat files and analyzing scalogramimages generated by performing Continuous Wavelet Transform (CWT) on thesamples. Also, finding the mean, standard deviation (STD), and peak-to-peak(P2P) values from each signal also helped detect faulty signs. We haveimplemented the autoregressive integrated moving average (ARIMA) method totrack the progression.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc49e450> 111
cuda:2
[Document(page_content='Deep Transfer Convolutional Neural Network and Extreme Learning Machine  for Lung Nodule Diagnosis on CT images\nSome content of the article needs to be kept secret\nMultimodal Research in Vision and Language: A Review of Current and  Emerging Trends', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Some content of the article needs to be kept secret\nMultimodal Research in Vision and Language: A Review of Current and  Emerging Trends\nDeep Learning and its applications have cascaded impactful research anddevelopment with a diverse range of modalities present in the real-world data.More recently, this has enhanced research interests in the intersection of theVision and Language arena with its numerous applications and fast-paced growth.In this paper, we present a detailed overview of the latest trends in researchpertaining to visual and language modalities. We look at its applications intheir task formulations and how to solve various problems related to semanticperception and content generation. We also address task-specific trends, alongwith their evaluation strategies and upcoming challenges. Moreover, we shedsome light on multi-disciplinary patterns and insights that have emerged in therecent past, directing this field towards more modular and transparentintelligent systems. This survey identifies key trends gravitating recentliterature in VisLang research and attempts to unearth directions that thefield is heading towards.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='WiCV 2021: The Eighth Women In Computer Vision Workshop\nIn this paper, we present the details of Women in Computer Vision Workshop -WiCV 2021, organized alongside the virtual CVPR 2021. It provides a voice to aminority (female) group in the computer vision community and focuses onincreasing the visibility of these researchers, both in academia and industry.WiCV believes that such an event can play an important role in lowering thegender imbalance in the field of computer vision. WiCV is organized each yearwhere it provides a)~opportunity for collaboration between researchers fromminority groups, b)~mentorship to female junior researchers, c)~financialsupport to presenters to overcome monetary burden and d)~large and diversechoice of role models, who can serve as examples to younger researchers at thebeginning of their careers. In this paper, we present a report on the workshopprogram, trends over the past years, a summary of statistics regardingpresenters, attendees, and sponsorship for the WiCV 2021 workshop.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive  Review', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Tea leaf diseases are a major challenge to agricultural productivity, withfar-reaching implications for yield and quality in the tea industry. The riseof machine learning has enabled the development of innovative approaches tocombat these diseases. Early detection and diagnosis are crucial for effectivecrop management. For predicting tea leaf disease, several automated systemshave already been developed using different image processing techniques. Thispaper delivers a systematic review of the literature on machine learningmethodologies applied to diagnose tea leaf disease via image classification. Itthoroughly evaluates the strengths and constraints of various VisionTransformer models, including Inception Convolutional Vision Transformer(ICVT), GreenViT, PlantXViT, PlantViT, MSCVT, Transfer Learning Model & VisionTransformer (TLMViT), IterationViT, IEM-ViT.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='Moreover, this paper also reviewsmodels like Dense Convolutional Network (DenseNet), Residual Neural Network(ResNet)-50V2, YOLOv5, YOLOv7, Convolutional Neural Network (CNN), Deep CNN,Non-dominated Sorting Genetic Algorithm (NSGA-II), MobileNetv2, andLesion-Aware Visual Transformer. These machine-learning models have been testedon various datasets, demonstrating their real-world applicability. This reviewstudy not only highlights current progress in the field but also providesvaluable insights for future research directions in the machine learning-baseddetection and classification of tea leaf diseases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'}), Document(page_content='HUMS2023 Data Challenge Result Submission\nWe implemented a simple method for early detection in this research. Theimplemented methods are plotting the given mat files and analyzing scalogramimages generated by performing Continuous Wavelet Transform (CWT) on thesamples. Also, finding the mean, standard deviation (STD), and peak-to-peak(P2P) values from each signal also helped detect faulty signs. We haveimplemented the autoregressive integrated moving average (ARIMA) method totrack the progression.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpakdfdxdg/a66cc190-5fe5-4127-9745-ebb5d1afcbb8.txt'})]
cuda:2
[UploadFile(filename='README.md', size=775, headers=Headers({'content-disposition': 'form-data; name="files"; filename="README.md"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmptvixnq_c, tmptvixnq_c
File: README.md, msg: 成功上传文件 README.md, docs: [Document(page_content='Issues & Updates\n进行更新之后可以将更新内容、时间和需解决的问题写到本README文档中\n3/18\n将文档信息放入Document文件夹并添加需求分析文档模板，以后将所有文档中的相关图片都放到Document中的Image文件夹中。\n后续需求文档编写可以根据文档模板的条目要求先写到markdown文档中，最后再一起整理成word文档模板形式。\n4/1\n增加前后端文件夹，为开发作准备\n总readme增加仓库结构\n仓库结构\nBUAA-software-engineering-2024\n    ├─Document      过程性文档\n    │  ├─Image      图片\n    │  └─Weekly     周报文件夹\n    ├─Frontend      前端\n    └─Backend       后端', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptvixnq_c/README.md'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc2a13d0> 111
cuda:2
[Document(page_content='Issues & Updates\n进行更新之后可以将更新内容、时间和需解决的问题写到本README文档中\n3/18\n将文档信息放入Document文件夹并添加需求分析文档模板，以后将所有文档中的相关图片都放到Document中的Image文件夹中。\n后续需求文档编写可以根据文档模板的条目要求先写到markdown文档中，最后再一起整理成word文档模板形式。\n4/1\n增加前后端文件夹，为开发作准备\n总readme增加仓库结构\n仓库结构\nBUAA-software-engineering-2024\n    ├─Document      过程性文档\n    │  ├─Image      图片\n    │  └─Weekly     周报文件夹\n    ├─Frontend      前端\n    └─Backend       后端', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmptvixnq_c/README.md'})]
cuda:2
[UploadFile(filename='README.md', size=775, headers=Headers({'content-disposition': 'form-data; name="files"; filename="README.md"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp_ipve2r_, tmp_ipve2r_
File: README.md, msg: 成功上传文件 README.md, docs: [Document(page_content='Issues & Updates\n进行更新之后可以将更新内容、时间和需解决的问题写到本README文档中\n3/18\n将文档信息放入Document文件夹并添加需求分析文档模板，以后将所有文档中的相关图片都放到Document中的Image文件夹中。\n后续需求文档编写可以根据文档模板的条目要求先写到markdown文档中，最后再一起整理成word文档模板形式。\n4/1\n增加前后端文件夹，为开发作准备\n总readme增加仓库结构\n仓库结构\nBUAA-software-engineering-2024\n    ├─Document      过程性文档\n    │  ├─Image      图片\n    │  └─Weekly     周报文件夹\n    ├─Frontend      前端\n    └─Backend       后端', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_ipve2r_/README.md'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc108790> 111
cuda:2
[Document(page_content='Issues & Updates\n进行更新之后可以将更新内容、时间和需解决的问题写到本README文档中\n3/18\n将文档信息放入Document文件夹并添加需求分析文档模板，以后将所有文档中的相关图片都放到Document中的Image文件夹中。\n后续需求文档编写可以根据文档模板的条目要求先写到markdown文档中，最后再一起整理成word文档模板形式。\n4/1\n增加前后端文件夹，为开发作准备\n总readme增加仓库结构\n仓库结构\nBUAA-software-engineering-2024\n    ├─Document      过程性文档\n    │  ├─Image      图片\n    │  └─Weekly     周报文件夹\n    ├─Frontend      前端\n    └─Backend       后端', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp_ipve2r_/README.md'})]
cuda:2
[UploadFile(filename='6c287c4a-dfb7-403e-ac29-bc81833c7041.txt', size=6286, headers=Headers({'content-disposition': 'form-data; name="files"; filename="6c287c4a-dfb7-403e-ac29-bc81833c7041.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp8f4yurr0, tmp8f4yurr0
File: 6c287c4a-dfb7-403e-ac29-bc81833c7041.txt, msg: 成功上传文件 6c287c4a-dfb7-403e-ac29-bc81833c7041.txt, docs: [Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc1ac550> 111
cuda:2
[Document(page_content='Fast Differentiable Matrix Square Root and Inverse Square Root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content="Computing the matrix square root and its inverse in a differentiable manneris important in a variety of computer vision tasks. Previous methods eitheradopt the Singular Value Decomposition (SVD) to explicitly factorize the matrixor use the Newton-Schulz iteration (NS iteration) to derive the approximatesolution. However, both methods are not computationally efficient enough ineither the forward pass or the backward pass. In this paper, we propose twomore efficient variants to compute the differentiable matrix square root andthe inverse square root. For the forward propagation, one method is to useMatrix Taylor Polynomial (MTP), and the other method is to use Matrix Pad\\'eApproximants (MPA). The backward gradient is computed by iteratively solvingthe continuous-time Lyapunov equation using the matrix sign function. A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='A seriesof numerical tests show that both methods yield considerable speed-up comparedwith the SVD or the NS iteration. Moreover, we validate the effectiveness ofour methods in several real-world applications, including de-correlated batchnormalization, second-order vision transformer, global covariance pooling forlarge-scale and fine-grained recognition, attentive covariance pooling forvideo recognition, and neural style transfer. The experimental resultsdemonstrate that our methods can also achieve competitive and even slightlybetter performances. The Pytorch implementation is available athttps://github.com/KingJamesSong/FastDifferentiableMatSqrt', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='StRDAN: Synthetic-to-Real Domain Adaptation Network for Vehicle  Re-Identification\nVehicle re-identification aims to obtain the same vehicles from vehicleimages. This is challenging but essential for analyzing and predicting trafficflow in the city. Although deep learning methods have achieved enormousprogress for this task, their large data requirement is a critical shortcoming.Therefore, we propose a synthetic-to-real domain adaptation network (StRDAN)framework, which can be trained with inexpensive large-scale synthetic and realdata to improve performance. The StRDAN training method combines domainadaptation and semi-supervised learning methods and their associated losses.StRDAN offers significant improvement over the baseline model, which can onlybe trained using real data, for VeRi and CityFlow-ReID datasets, achieving 3.1%and 12.9% improved mean average precision, respectively.\nSatellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Satellite Imagery and AI: A New Era in Ocean Conservation, from Research  to Deployment and Impact\nIllegal, unreported, and unregulated (IUU) fishing poses a global threat toocean habitats. Publicly available satellite data offered by NASA and theEuropean Space Agency (ESA) provide an opportunity to actively monitor thisactivity. Effectively leveraging satellite data for maritime conservationrequires highly reliable machine learning models operating globally withminimal latency. This paper introduces three specialized computer vision modelsdesigned for synthetic aperture radar (Sentinel-1), optical imagery(Sentinel-2), and nighttime lights (Suomi-NPP/NOAA-20). It also presents bestpractices for developing and delivering real-time computer vision services forconservation. These models have been deployed in Skylight, a real time maritimemonitoring platform, which is provided at no cost to users worldwide.\nMVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale  Robotic Navigation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Autonomous navigation emerges from both motion and local visual perception inreal-world environments. However, most successful robotic motion estimationmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual placerecognition-VPR) are often separately used for mapping and localization tasks.Conversely, recent reinforcement learning (RL) based methods for visualnavigation rely on the quality of GPS data reception, which may not be reliablewhen directly using it as ground truth across multiple, month-spaced traversalsin large environments. In this paper, we propose a novel motion and visualperception approach, dubbed MVP, that unifies these two sensor modalities forlarge-scale, target-driven navigation tasks. Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Our MVP-based method can learnfaster, and is more accurate and robust to both extreme environmental changesand poor GPS data than corresponding vision-only navigation methods. MVPtemporally incorporates compact image representations, obtained using VPR, withoptimized motion estimation data, including but not limited to those from VO oroptimized radar odometry (RO), to efficiently learn self-supervised navigationpolicies via RL. We evaluate our method on two large real-world datasets,Oxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,night, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,summer) conditions using the new CityLearn framework; an interactiveenvironment for efficiently training navigation agents.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content='Our experimentalresults, on traversals of the Oxford RobotCar dataset with no GPS data, showthat MVP can achieve 53% and 93% navigation success rate using VO and RO,respectively, compared to 7% for a vision-only method. We additionally report atrade-off between the RL success rate and the motion estimation precision.\nAffectGAN: Affect-Based Generative Art Driven by Semantics', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'}), Document(page_content="AffectGAN: Affect-Based Generative Art Driven by Semantics\nThis paper introduces a novel method for generating artistic images thatexpress particular affective states. Leveraging state-of-the-art deep learningmethods for visual generation (through generative adversarial networks),semantic models from OpenAI, and the annotated dataset of the visual artencyclopedia WikiArt, our AffectGAN model is able to generate images based onspecific or broad semantic prompts and intended affective outcomes. A smalldataset of 32 images generated by AffectGAN is annotated by 50 participants interms of the particular emotion they elicit, as well as their quality andnovelty. Results show that for most instances the intended emotion used as aprompt for image generation matches the participants' responses. Thissmall-scale study brings forth a new vision towards blending affectivecomputing with computational creativity, enabling generative systems withintentionality in terms of the emotions they wish their output to elicit.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8f4yurr0/6c287c4a-dfb7-403e-ac29-bc81833c7041.txt'})]
cuda:2
[UploadFile(filename='Fast Differentiable Matrix Square Root and Inverse Square Root.pdf', size=20129369, headers=Headers({'content-disposition': 'form-data; name="files"; filename="Fast Differentiable Matrix Square Root and Inverse Square Root.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp8q9w298o, tmp8q9w298o
File: Fast Differentiable Matrix Square Root and Inverse Square Root.pdf, msg: 成功上传文件 Fast Differentiable Matrix Square Root and Inverse Square Root.pdf, docs: [Document(page_content='IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1\nFast Differentiable Matrix Square Root and\nInverse Square Root\nYue Song, Member, IEEE, Nicu Sebe, Senior Member, IEEE, Wei Wang, Member, IEEE\nAbstract—Computing the matrix square root and its inverse in a differentiable manner is important in a variety of computer vision tasks.\nPrevious methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz\niteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efﬁcient enough in either the\nforward pass or the backward pass. In this paper, we propose two more efﬁcient variants to compute the differentiable matrix square root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='forward pass or the backward pass. In this paper, we propose two more efﬁcient variants to compute the differentiable matrix square root\nand the inverse square root. For the forward propagation, one method is to use Matrix Taylor Polynomial (MTP), and the other method is\nto use Matrix Pad´e Approximants (MPA). The backward gradient is computed by iteratively solving the continuous-time Lyapunov equation\nusing the matrix sign function. A series of numerical tests show that both methods yield considerable speed-up compared with the SVD or\nthe NS iteration. Moreover, we validate the effectiveness of our methods in several real-world applications, including de-correlated batch\nnormalization, second-order vision transformer, global covariance pooling for large-scale and ﬁne-grained recognition, attentive\ncovariance pooling for video recognition, and neural style transfer. The experiments demonstrate that our methods can also achieve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='covariance pooling for video recognition, and neural style transfer. The experiments demonstrate that our methods can also achieve\ncompetitive and even slightly better performances. Code is available at https://github.com/KingJamesSong/FastDifferentiableMatSqrt.\nIndex Terms—Differentiable Matrix Decomposition, Decorrelated Batch Normalization, Global Covariance Pooling, Neural Style Transfer.\n!\n1\nINTRODUCTION\nConsider a positive semi-deﬁnite matrix A. The principle\nsquare root A\n1\n2 and the inverse square root A− 1\n2 are mathe-\nmatically of practical interests, mainly because some desired\nspectral properties can be obtained by such transformations.\nAn exemplary illustration is given in Fig. 1. As can be', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='matically of practical interests, mainly because some desired\nspectral properties can be obtained by such transformations.\nAn exemplary illustration is given in Fig. 1. As can be\nseen, the matrix square root can shrink/stretch the feature\nvariances along with the direction of principle components,\nwhich is known as an effective spectral normalization for\ncovariance matrices. The inverse square root, on the other\nhand, can be used to whiten the data, i.e., make the data\nhas a unit variance in each dimension. These appealing\nspectral properties are very useful in many computer vision\napplications. In Global Covariance Pooling (GCP) [1], [2], [3],\n[4] and other related high-order representation methods [5],\n[6], the matrix square root is often used to normalize the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[4] and other related high-order representation methods [5],\n[6], the matrix square root is often used to normalize the\nhigh-order feature, which can beneﬁt some classiﬁcation\ntasks like general visual recognition [2], [3], [5], ﬁne-grained\nvisual categorization [7], and video action recognition [6].\nThe inverse square root is used as the whitening transform to\neliminate the feature correlation, which is widely applied in\ndecorrelated Batch Normalization (BN) [8], [9], [10] and other\nrelated models that involve the whitening transform [11],\n[12]. In the ﬁeld of neural style transfer, both the matrix\nsquare root and its inverse are adopted to perform successive\nWhitening and Coloring Transform (WCT) to transfer the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='square root and its inverse are adopted to perform successive\nWhitening and Coloring Transform (WCT) to transfer the\nstyle information for better generation ﬁdelity [13], [14], [15].\nTo compute the matrix square root, the standard method\nis via Singular Value Decomposition (SVD). Given the real\nYue Song, Nicu Sebe, and Wei Wang are with the Department of\nInformation Engineering and Computer Science, University of Trento,\nTrento 38123, Italy.\nE-mail: {yue.song, nicu.sebe, wei.wang}@unitn.it\nManuscript received April 19, 2005; revised August 26, 2015.\nFig. 1: Exemplary visualization of the matrix square root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Manuscript received April 19, 2005; revised August 26, 2015.\nFig. 1: Exemplary visualization of the matrix square root\nand its inverse. Given the original data X∈R2×n, the matrix\nsquare root performs an effective spectral normalization by\nstretching the data along the axis of small variances and\nsqueezing the data in the direction with large variances,\nwhile the inverse square root transforms the data into the\nuncorrelated structure that has unit variance in all directions.\nsymmetric matrix A, its matrix square root is computed as:\nA\n1\n2 = (UΛUT )\n1\n2 = UΛ\n1\n2 UT\n(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 = UΛ\n1\n2 UT\n(1)\nwhere U is the eigenvector matrix, and Λ is the diagonal\neigenvalue matrix. As derived by Ionescu et al. [16], the\npartial derivative of the eigendecomposition is calculated as:\n∂l\n∂A = U\n\x10\nKT ⊙ (UT ∂l\n∂U) + ( ∂l\n∂Λ)diag\n\x11\nUT\n(2)\nwhere l is the loss function, ⊙ denotes the element-wise\nproduct, and ()diag represents the operation of setting the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='(2)\nwhere l is the loss function, ⊙ denotes the element-wise\nproduct, and ()diag represents the operation of setting the\noff-diagonal entries to zero. Despite the long-studied theories\nand well-developed algorithms of SVD, there exist two\nobstacles when integrating it into deep learning frameworks.\narXiv:2201.12543v2  [cs.CV]  19 Oct 2022\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nOne issue is the back-propagation instability. For the matrix\nK deﬁned in eq. (2), its off-diagonal entry is Kij=1/(λi−λj),\nwhere λi and λj are involved eigenvalues. When the two', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='where λi and λj are involved eigenvalues. When the two\neigenvalues are close and small, the gradient is very likely\nto explode, i.e., Kij→∞. This issue has been solved by some\nmethods that use approximation techniques to estimate the\ngradients [4], [17], [18]. The other problem is the expen-\nsive time cost of the forward eigendecomposition. As the\nSVD is not supported well by GPUs [19], performing the\neigendecomposition on the deep learning platforms is rather\ntime-consuming. Incorporating the SVD with deep models\ncould add extra burdens to the training process. Particularly\nfor batched matrices, modern deep learning frameworks,\nsuch as Tensorﬂow and Pytorch, give limited optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='for batched matrices, modern deep learning frameworks,\nsuch as Tensorﬂow and Pytorch, give limited optimization\nfor the matrix decomposition within the mini-batch. They\ninevitably use a for-loop to conduct the SVD one matrix by\nanother. However, how to efﬁciently perform the SVD in\nthe context of deep learning has not been touched by the\nresearch community.\nTo avoid explicit eigendecomposition, one commonly\nused alternative is the Newton-Schulz iteration (NS itera-\ntion) [20], [21] which modiﬁes the ordinary Newton iteration\nby replacing the matrix inverse but preserving the quadratic\nconvergence. Compared with SVD, the NS iteration is rich\nin matrix multiplication and more GPU-friendly. Thus, this', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='convergence. Compared with SVD, the NS iteration is rich\nin matrix multiplication and more GPU-friendly. Thus, this\ntechnique has been widely used to approximate the matrix\nsquare root in different applications [1], [3], [9]. The forward\ncomputation relies on the following coupled iterations:\nYk+1 = 1\n2Yk(3I − ZkYk), Zk+1 = 1\n2(3I − ZkYk)Zk\n(3)\nwhere Yk and Zk converge to A\n1\n2 and A− 1\n2 , respectively.\nSince the NS iteration only converges locally (i.e., ||A||2<1),\nwe need to pre-normalize the initial matrix and post-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Since the NS iteration only converges locally (i.e., ||A||2<1),\nwe need to pre-normalize the initial matrix and post-\ncompensate the resultant approximation as Y0=\n1\n||A||F A\nand A\n1\n2 =\np\n||A||FYk. Each forward iteration involves 3\nmatrix multiplications, which is more efﬁcient than the\nforward pass of SVD. However, the backward pass of the\nNS iteration takes 14 matrix multiplications per iteration.\nConsider that the NS iteration often takes 5 iterations to\nachieve reasonable performances [3], [9]. The backward pass\nis much more time-costing than the backward algorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='achieve reasonable performances [3], [9]. The backward pass\nis much more time-costing than the backward algorithm\nof SVD. The speed improvement could be larger if a more\nefﬁcient backward algorithm is developed.\nTo address the drawbacks of SVD and NS iteration, i.e.\nthe low efﬁciency in either the forward or backward pass,\nwe derive two methods that are efﬁcient in both forward\nand backward propagation to compute the differentiable\nmatrix square root and its inverse. In the forward pass\n(FP), we propose using Matrix Taylor Polynomial (MTP)\nand Matrix Pad´e Approximants (MPA) for approximating\nthe matrix square root. The former approach is slightly faster\nbut the latter is more numerically accurate. Both methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the matrix square root. The former approach is slightly faster\nbut the latter is more numerically accurate. Both methods\nyield considerable speed-up compared with the SVD or the\nNS iteration in the forward computation. The proposed MTP\nand MPA can be also used to approximate the inverse square\nroot without any additional computational cost. For the\nbackward pass (BP), we consider the gradient function as a\nLyapunov equation and propose an iterative solution using\nthe matrix sign function. The backward pass costs fewer\nmatrix multiplications and is more computationally efﬁcient\nthan the NS iteration. Our proposed iterative Lyapunov\nsolver applies to both the matrix square root and the inverse\nsquare root. The only difference is that deriving the gradient\nof inverse square root requires 3 more matrix multiplications', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='square root. The only difference is that deriving the gradient\nof inverse square root requires 3 more matrix multiplications\nthan computing that of matrix square root.\nThrough a series of numerical tests, we show that the\nproposed MTP-Lya and MPA-Lya deliver consistent speed\nimprovement for different batch sizes, matrix dimensions,\nand some hyper-parameters (e.g., degrees of power series to\nmatch and iteration times). Moreover, our proposed MPA-\nLya consistently gives a better approximation of the matrix\nsquare root and its inverse than the NS iteration. Besides the\nnumerical tests, we conduct extensive experiments in a num-\nber of computer vision applications, including decorrelated\nbatch normalization, second-order vision transformer, global\ncovariance pooling for large-scale and ﬁne-grained image', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='batch normalization, second-order vision transformer, global\ncovariance pooling for large-scale and ﬁne-grained image\nrecognition, attentive global covariance pooling for video\naction recognition, and neural style transfer. Our methods\ncan achieve competitive performances against the SVD and\nthe NS iteration with the least amount of time overhead.\nOur MPA is suitable in use cases where the high precision\nis needed, while our MTP works in applications where\nthe accuracy is less demanded but the efﬁciency is more\nimportant. The contributions of the paper are twofold:\nWe propose two fast methods that compute the differ-\nentiable matrix square root and the inverse square root.\nThe forward propagation relies on the matrix Taylor\npolynomial or matrix Pad´e approximant, while an iterative', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='entiable matrix square root and the inverse square root.\nThe forward propagation relies on the matrix Taylor\npolynomial or matrix Pad´e approximant, while an iterative\nbackward gradient solver is derived from the Lyapunov\nequation using the matrix sign function.\nOur proposed algorithms are validated by a series of\nnumerical tests and several real-world computer vision\napplications. The experimental results demonstrate that\nour methods have a faster calculation speed and also have\nvery competitive performances.\nThis paper is an expanded version of [22]. In the confer-\nence paper [22], the proposed fast algorithms only apply to\nthe matrix square root A\n1\n2 . For the application of inverse\nsquare root A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the matrix square root A\n1\n2 . For the application of inverse\nsquare root A− 1\n2 , we have to solve the linear system or\ncompute the matrix inverse. However, both techniques are\nnot GPU-efﬁcient enough and could add extra computational\nburdens to the training. In this extended manuscript, we\ntarget the drawback and extend our algorithm to the case\nof inverse square root, which avoids the expensive compu-\ntation and allows for faster calculation in more application\nscenarios. Compared with computing the matrix square root,\ncomputing the inverse square root consumes the same time\ncomplexity in the FP and requires 3 more matrix multiplica-\ntions in the BP. The paper thus presents a complete solution to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='complexity in the FP and requires 3 more matrix multiplica-\ntions in the BP. The paper thus presents a complete solution to\nthe efﬁciency issue of the differentiable spectral layer. Besides\nthe algorithm extension, our method is validated in more\ncomputer vision applications: global covariance pooling for\nimage/video recognition and neural style transfer. We also\nshed light on the peculiar incompatibility of NS iteration and\nLyapunov solver discussed in Sec. 5.7.3.\nThe rest of the paper is organized as follows: Sec. 2\ndescribes the computational methods and applications of\ndifferentiable matrix square root and its inverse. Sec. 3\nintroduces our method that computes the end-to-end matrix\nsquare root, and Sec. 4 presents the extension of our method', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='introduces our method that computes the end-to-end matrix\nsquare root, and Sec. 4 presents the extension of our method\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\nto the inverse square root. Sec. 5 provides the experimental\nresults, the ablation studies, and some in-depth analysis.\nFinally, Sec. 6 summarizes the conclusions.\n2\nRELATED WORK\nIn this section, we recap the previous approaches that\ncompute the differentiable matrix square root and the inverse\nsquare root, followed by a discussion on the usage in some\napplications of deep learning and computer vision.\n2.1\nComputational Methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='square root, followed by a discussion on the usage in some\napplications of deep learning and computer vision.\n2.1\nComputational Methods\nIonescu et al. [16], [23] ﬁrst formulate the theory of matrix\nback-propagation, making it possible to integrate a spectral\nmeta-layer into neural networks. Existing approaches that\ncompute the differentiable matrix square root and its inverse\nare mainly based on the SVD or NS iteration. The SVD\ncalculates the accurate solution but suffers from backward\ninstability and expensive time cost, whereas the NS iteration\ncomputes the approximate solution but is more GPU-friendly.\nFor the backward algorithm of SVD, several methods have\nbeen proposed to resolve this gradient explosion issue [4],', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computes the approximate solution but is more GPU-friendly.\nFor the backward algorithm of SVD, several methods have\nbeen proposed to resolve this gradient explosion issue [4],\n[17], [18], [24], [25]. Wang et al. [17] propose to apply Power\nIteration (PI) to approximate the SVD gradient. Recently,\nSong et al. [4] propose to rely on Pad´e approximants to\nclosely estimate the backward gradient of SVD.\nTo avoid explicit eigendecomposition, Lin et al. [1] pro-\npose to substitute SVD with the NS iteration. Following this\nwork, Li et al. [2] and Huang et al. [8] adopt the NS iteration\nin the task of global covariance pooling and decorrelated\nbatch normalization, respectively. For the backward pass', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='in the task of global covariance pooling and decorrelated\nbatch normalization, respectively. For the backward pass\nof the differentiable matrix square root, Lin et al. [1] also\nsuggest viewing the gradient function as a Lyapunov equa-\ntion. However, their proposed exact solution is infeasible\nto compute practically, and the suggested Bartels-Steward\nalgorithm [26] requires explicit eigendecomposition or Schur\ndecomposition, which is again not GPU-friendly. By contrast,\nour proposed iterative solution using the matrix sign function\nis more computationally efﬁcient and achieves comparable\nperformances against the Bartels-Steward algorithm (see the\nablation study in Sec. 5.7.3).\n2.2\nApplications', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ablation study in Sec. 5.7.3).\n2.2\nApplications\n2.2.1\nGlobal Covariance Pooling\nOne successful application of the differentiable matrix square\nroot is the Global Covariance Pooling (GCP), which is a\nmeta-layer inserted before the FC layer of deep models to\ncompute the matrix square root of the feature covariance.\nEquipped with the GCP meta-layers, existing deep models\nhave achieved state-of-the-art performances on both generic\nand ﬁne-grained visual recognition [1], [2], [3], [4], [7], [27],\n[28], [29]. Inspired by recent advances of transformers [30],', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[28], [29]. Inspired by recent advances of transformers [30],\nXie et al. [5] integrate the GCP meta-layer into the vision\ntransformer [31] to exploit the second-order statistics of\nthe high-level visual tokens, which solves the issue that\nvision transformers need pre-training on ultra-large-scale\ndatasets. More recently, Gao et al. [6] propose an attentive\nand temporal-based GCP model for video action recognition.\n2.2.2\nDecorrelated Batch Normalization\nAnother line of research proposes to use ZCA whitening,\nwhich applies the inverse square root of the covariance to\nwhiten the feature, as an alternative scheme for the standard\nbatch normalization [32]. The whitening procedure, a.k.a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='whiten the feature, as an alternative scheme for the standard\nbatch normalization [32]. The whitening procedure, a.k.a\ndecorrelated batch normalization, does not only standardize\nthe feature but also eliminates the data correlation. The\ndecorrelated batch normalization can improve both the\noptimization efﬁciency and generalization ability of deep\nneural networks [8], [9], [10], [11], [12], [33], [34], [35], [36].\n2.2.3\nWhitening and Coloring Transform\nThe WCT [13] is also an active research ﬁeld where the differ-\nentiable matrix square root and its inverse are widely used.\nIn general, the WCT performs successively the whitening', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='entiable matrix square root and its inverse are widely used.\nIn general, the WCT performs successively the whitening\ntransform (using inverse square root) and the coloring trans-\nform (using matrix square root) on the multi-scale features\nto preserve the content of current image but carrying the\nstyle of another image. During the past few years, the WCT\nmethods have achieved remarkable progress in universal\nstyle transfer [13], [37], [38], domain adaptation [15], [39],\nand image translation [14], [40].\nBesides the three main applications discussed above,\nthere are still some minor applications, such as semantic\nsegmentation [41] and super resolution [42].\nTABLE 1: Summary of mathematical notation and symbol.\nAp', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='segmentation [41] and super resolution [42].\nTABLE 1: Summary of mathematical notation and symbol.\nAp\nMatrix p-th power.\nI\nIdentity matrix.\n|| · ||F\nMatrix Frobenius norm.\n\x10n\nk\n\x11\nBinomial coefﬁcients calculated as n!/k! (n−k)!.\nvec(·)\nUnrolling matrix into vector.\n⊗\nMatrix Kronecker product.\nsign(A)\nMatrix sign function calculated as A(A2)− 1\n2\n∂l\n∂A', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Matrix sign function calculated as A(A2)− 1\n2\n∂l\n∂A\nPartial derivative of loss l w.r.t. matrix A\n3\nFAST DIFFERENTIABLE MATRIX SQUARE ROOT\nTable 1 summarizes the notation we will use from now on.\nThis section presents the forward pass and the backward\npropagation of our fast differentiable matrix square root. For\nthe inverse square root, we introduce the derivation in Sec. 4.\n3.1\nForward Pass\n3.1.1\nMatrix Taylor Polynomial\nWe begin with motivating the Taylor series for the scalar case.\nConsider the following power series:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Matrix Taylor Polynomial\nWe begin with motivating the Taylor series for the scalar case.\nConsider the following power series:\n(1 − z)\n1\n2 = 1 −\n∞\nX\nk=1\n1\n2\nk\n! zk\n(4)\nwhere\n1\n2\nk\n!\ndenotes the binomial coefﬁcients that involve\nfractions, and the series converges when z<1 according to\nthe Cauchy root test. For the matrix case, the power series\ncan be similarly deﬁned by:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the Cauchy root test. For the matrix case, the power series\ncan be similarly deﬁned by:\n(I − Z)\n1\n2 = I −\n∞\nX\nk=1\n1\n2\nk\n! Zk\n(5)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\nwhere I is the identity matrix. Let us substitute Z with (I−A),\nwe can obtain:\nA\n1\n2 = I −\n∞\nX\nk=1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n1\n2 = I −\n∞\nX\nk=1\n1\n2\nk\n! (I − A)k\n(6)\nSimilar with the scalar case, the power series converge\nonly if ||(I − A)||p<1, where || · ||p denotes any vector-\ninduced matrix norms. To circumvent this issue, we can\nﬁrst pre-normalize the matrix A by dividing ||A||F. This\ncan guarantee the convergence as ||I−\nA\n||A||F ||p<1 is always\nsatisﬁed. Afterwards, the matrix square root A\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n||A||F ||p<1 is always\nsatisﬁed. Afterwards, the matrix square root A\n1\n2 is post-\ncompensated by multiplying\np\n||A||F. Integrated with these\ntwo operations, eq. (6) can be re-formulated as:\nA\n1\n2 =\nq\n||A||F ·\n\x10\nI −\n∞\nX\nk=1\n1\n2\nk\n! (I −\nA\n||A||F', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2\nk\n! (I −\nA\n||A||F\n)k\x11\n(7)\nTruncating the series to a certain degree K yields the MTP\napproximation for the matrix square root. For the MTP of\ndegree K, K−1 matrix multiplications are needed.\n3.1.2\nMatrix Pad´e Approximant\nFig. 2: The function (1 − z)\n1\n2 in the range of |z| < 1 and its\napproximation including Taylor polynomial, Newton-Schulz\niteration, and Pad´e approximants. The Pad´e approximants\nconsistently achieves a better estimation for other approxi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='iteration, and Pad´e approximants. The Pad´e approximants\nconsistently achieves a better estimation for other approxi-\nmation schemes for any possible input values.\nThe MTP enjoys the fast calculation, but it converges\nuniformly and sometimes suffers from the so-called ”hump\nphenomenon”, i.e., the intermediate terms of the series grow\nquickly but cancel each other in the summation, which\nresults in a large approximation error. Expanding the series\nto a higher degree does not solve this issue either. The\nMPA, which adopts two polynomials of smaller degrees\nto construct a rational approximation, is able to avoid this\ncaveat. To visually illustrate this impact, we depict the\napproximation of the scalar square root in Fig. 2. The Pad´e\napproximants consistently deliver a better approximation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='approximation of the scalar square root in Fig. 2. The Pad´e\napproximants consistently deliver a better approximation\nthan NS iteration and Taylor polynomial. In particular, when\nthe input is close to the convergence boundary (z=1) where\nNS iteration and Taylor polynomials suffer from a larger\napproximation error, our Pad´e approximants still present a\nreasonable estimation. The superior property also generalizes\nto the matrix case.\nThe MPA is computed as the fraction of two sets of\npolynomials: denominator polynomial PN\nn=1 qnzn and nu-\nmerator polynomial PM\nm=1 pmzm. The coefﬁcients qn and\npm are pre-computed by matching to the corresponding\nTaylor series. Given the power series of scalar in eq. (4),', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='pm are pre-computed by matching to the corresponding\nTaylor series. Given the power series of scalar in eq. (4),\nFig. 3: Python-like pseudo-codes for Pad´e coefﬁcients.\nthe coefﬁcients of a [M, N] scalar Pad´e approximant are\ncomputed by matching to the series of degree M+N+1:\n1 − PM\nm=1 pmzm\n1 − PN\nn=1 qnzn = 1 −\nM+N\nX\nk=1\n1\n2\nk\n! zk\n(8)\nwhere pm and qn also apply to the matrix case. This matching', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k\n! zk\n(8)\nwhere pm and qn also apply to the matrix case. This matching\ngives rise to a system of linear equations:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n−\n1\n2\n1\n! − q1 = −p1,\n−\n1\n2\n2\n! +\n1\n2\n1\n! q1 − q2 = −p2,\n−\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n1\n! q1 − q2 = −p2,\n−\n1\n2\nM\n! +\n1\n2\nM − 1\n! q1 + · · · − qM = pM,\n· · · · ·\n(9)\nSolving these equations directly determines the coefﬁcients.\nWe give the Python-like pseudo-codes in Fig. 3. The numer-\nator polynomial and denominator polynomials of MPA are\ngiven by:\nPM = I −\nM\nX\nm=1\npm(I −', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='given by:\nPM = I −\nM\nX\nm=1\npm(I −\nA\n||A||F\n)m,\nQN = I −\nN\nX\nn=1\nqn(I −\nA\n||A||F\n)n.\n(10)\nThen the MPA for approximating the matrix square root is\ncomputed as:\nA\n1\n2 =\nq\n||A||FQ−1\nN PM.\n(11)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 =\nq\n||A||FQ−1\nN PM.\n(11)\nCompared with the MTP, the MPA trades off half of the ma-\ntrix multiplications with one matrix inverse, which slightly\nincreases the computational cost but converges more quickly\nand delivers better approximation abilities. Moreover, we\nnote that the matrix inverse can be avoided, as eq. (11) can be\nmore efﬁciently and numerically stably computed by solving\nthe linear system QNA\n1\n2 =\np\n||A||FPM. According to Van et\nal. [43], diagonal Pad´e approximants (i.e., PM and QN have', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='p\n||A||FPM. According to Van et\nal. [43], diagonal Pad´e approximants (i.e., PM and QN have\nthe same degree) usually yield better approximation than the\nnon-diagonal ones. Therefore, to match the MPA and MTP\nof the same degree, we set M=N= K−1\n2\n.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\nTABLE 2: Comparison of forward operations. For the matrix\nsquare root and its inverse, our MPA/MTP consumes the\nsame complexity. The cost of 1 NS iteration is about that of\nMTP of 4 degrees and about that of MPA of 2 degrees.\nOp.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='same complexity. The cost of 1 NS iteration is about that of\nMTP of 4 degrees and about that of MPA of 2 degrees.\nOp.\nMTP\nMPA\nNS iteration\nMat. Mul.\nK−1\n(K−1)/2\n3 × #iters\nMat. Inv.\n0\n1\n0\nTable 2 summarizes the forward computational com-\nplexity. As suggested in Li et al. [3] and Huang et al. [9],\nthe iteration times for NS iteration are often set as 5 such\nthat reasonable performances can be achieved. That is, to\nconsume the same complexity as the NS iteration does, our', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='that reasonable performances can be achieved. That is, to\nconsume the same complexity as the NS iteration does, our\nMTP and MPA can match to the power series up to degree\n16. However, as illustrated in Fig. 4, our MPA achieves\nbetter accuracy than the NS iteration even at degree 8. This\nobservation implies that our MPA is a better option in terms\nof both accuracy and speed.\n3.2\nBackward Pass\nThough one can manually derive the gradient of the MPA\nand MTP, their backward algorithms are computationally\nexpensive as they involve the matrix power up to degree K,\nwhere K can be arbitrarily large. Relying on the AutoGrad\npackage of deep learning frameworks can be both time-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='where K can be arbitrarily large. Relying on the AutoGrad\npackage of deep learning frameworks can be both time-\nand memory-consuming since the gradients of intermediate\nvariables would be computed and the matrix inverse of MPA\nis involved. To attain a more efﬁcient backward algorithm,\nwe propose to iteratively solve the gradient equation using\nthe matrix sign function. Given the matrix A and its square\nroot A\n1\n2 , since we have A\n1\n2 A\n1\n2 =A, a perturbation on A\nleads to:\nA\n1\n2 dA\n1\n2 + dA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n1\n2 dA\n1\n2 + dA\n1\n2 A\n1\n2 = dA\n(12)\nUsing the chain rule, the gradient function of the matrix\nsquare root satisﬁes:\nA\n1\n2 ∂l\n∂A + ∂l\n∂AA\n1\n2 =\n∂l\n∂A\n1\n2\n(13)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂l\n∂A\n1\n2\n(13)\nAs pointed out by Li et al. [1], eq. (13) actually deﬁnes the\ncontinuous-time Lyapunov equation (BX+XB=C) or a\nspecial case of Sylvester equation (BX+XD=C). The closed-\nform solution is given by:\nvec( ∂l\n∂A) =\n\x10\nA\n1\n2 ⊗ I + I ⊗ A\n1\n2\n\x11−1\nvec( ∂l\n∂A\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n\x11−1\nvec( ∂l\n∂A\n1\n2 )\n(14)\nwhere vec(·) denotes unrolling a matrix to vectors, and ⊗ is\nthe Kronecker product. Although the closed-form solution\nexists theoretically, it cannot be computed in practice due to\nthe huge memory consumption of the Kronecker product.\nSupposing that both A\n1\n2 and I are of size 256×256, the\nKronecker product A\n1\n2 ⊗I would take the dimension of\n2562×2562, which is infeasible to compute or store. Another', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 ⊗I would take the dimension of\n2562×2562, which is infeasible to compute or store. Another\napproach to solve eq. (13) is via the Bartels-Stewart algo-\nrithm [26]. However, it requires explicit eigendecomposition\nor Schulz decomposition, which is not GPU-friendly and\ncomputationally expensive.\nTo attain a GPU-friendly gradient solver, we propose\nto use the matrix sign function and iteratively solve the\nLyapunov equation. Solving the Sylvester equation via\nmatrix sign function has been long studied in the literature\nof numerical analysis [44], [45], [46]. One notable line of\nresearch is using the family of Newton iterations. Consider', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='of numerical analysis [44], [45], [46]. One notable line of\nresearch is using the family of Newton iterations. Consider\nthe following continuous Lyapunov function:\nBX + XB = C\n(15)\nwhere B refers to A\n1\n2 in eq. (13), C represents\n∂l\n∂A\n1\n2 , and X\ndenotes the seeking solution\n∂l\n∂A. Eq. (15) can be represented\nby the following block using a Jordan decomposition:\nH =\n\x14B\nC\n0\n−B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='H =\n\x14B\nC\n0\n−B\n\x15\n=\n\x14I\nX\n0\nI\n\x15 \x14B\n0\n0\n−B\n\x15 \x14I\nX\n0\nI\n\x15−1\n(16)\nThe matrix sign function is invariant to the Jordan canonical\nform or spectral decomposition. This property allows the use\nof Newton’s iterations for iteratively solving the Lyapunov\nfunction. Speciﬁcally, we have:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='of Newton’s iterations for iteratively solving the Lyapunov\nfunction. Speciﬁcally, we have:\nLemma 1 (Matrix Sign Function [21]). For a given matrix\nH with no eigenvalues on the imaginary axis, its sign function\nhas the following properties: 1) sign(H)2 = I; 2) if H has the\nJordan decomposition H=TMT−1, then its sign function satisﬁes\nsign(H)=Tsign(M)T−1.\nWe give the complete proof in the Supplementary Ma-\nterial. Lemma 1.1 shows that sign(H) is the matrix square\nroot of the identity matrix, which indicates the possibility\nof using Newton’s root-ﬁnding method to derive the solu-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='root of the identity matrix, which indicates the possibility\nof using Newton’s root-ﬁnding method to derive the solu-\ntion [21]. Here we also adopt the Newton-Schulz iteration,\nthe modiﬁed inverse-free and multiplication-rich Newton\niteration, to iteratively compute sign(H). This leads to the\ncoupled iteration as:\nBk+1 = 1\n2Bk(3I − B2\nk),\nCk+1 = 1\n2\n\x10\n− B2\nkCk + BkCkBk + Ck(3I − B2\nk)\n\x11\n.\n(17)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k)\n\x11\n.\n(17)\nThe equation above deﬁnes two coupled iterations for solving\nthe Lyapunov equation. Since the NS iteration converges only\nlocally, i.e., converges when ||H2\nk−I||<1, here we divide H0\nby ||B||F to meet the convergence condition. This normal-\nization deﬁnes the initialization B0=\nB\n||B||F and C0=\nC\n||B||F .\nRelying on Lemma 1.2, the sign function of eq. (16) can be\nalso calculated as:\nsign(H) = sign\n\x10 \x14B\nC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='also calculated as:\nsign(H) = sign\n\x10 \x14B\nC\n0\n−B\n\x15 \x11\n=\n\x14I\n2X\n0\n−I\n\x15\n(18)\nAs indicated above, the iterations in eq. (17) have the\nconvergence:\nlim\nk→∞ Bk = I, lim\nk→∞ Ck = 2X\n(19)\nAfter iterating k times, we can get the approximate solution\nX= 1\n2Ck. Instead of choosing setting iteration times, one can', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='After iterating k times, we can get the approximate solution\nX= 1\n2Ck. Instead of choosing setting iteration times, one can\nalso set the termination criterion by checking the convergence\n||Bk − I||F<τ, where τ is the pre-deﬁned tolerance.\nTable 3 compares the backward computation complexity\nof the iterative Lyapunov solver and the NS iteration. Our\nproposed Lyapunov solver spends fewer matrix multiplica-\ntions and is thus more efﬁcient than the NS iteration. Even\nif we iterate the Lyapunov solver more times (e.g., 7 or 8),\nit still costs less time than the backward calculation of NS\niteration that iterates 5 times.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='it still costs less time than the backward calculation of NS\niteration that iterates 5 times.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\nTABLE 3: Comparison of backward operations. For the\ninverse square root, our Lyapunov solver uses marginally\n3 more matrix multiplications. The cost of 1 NS iteration is\nabout that of 2 iterations of Lyapunov solver.\nOp.\nLya (Mat. Sqrt.)\nLya (Inv. Sqrt.)\nNS iteration\nMat. Mul.\n6 × #iters\n3 + 6 × #iters\n4 + 10 × #iters', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='6 × #iters\n3 + 6 × #iters\n4 + 10 × #iters\nMat. Inv.\n0\n0\n0\n4\nFAST DIFFERENTIABLE INVERSE SQUARE ROOT\nIn this section, we introduce the extension of our algorithm\nto the inverse square root.\n4.1\nForward Pass\n4.1.1\nMatrix Taylor Polynomial\nTo derive the MTP of inverse square root, we need to match\nto the following power series:\n(1 − z)− 1\n2 = 1 +\n∞\nX', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='(1 − z)− 1\n2 = 1 +\n∞\nX\nk=1\n− 1\n2\nk\n! zk\n(20)\nSimilar with the procedure of the matrix square root in eqs. (5)\nand (6), the MTP approximation can be computed as:\nA− 1\n2 = I +\n∞\nX\nk=1\n− 1\n2\nk\n! (I −\nA\n||A||F\n)k\n(21)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k\n! (I −\nA\n||A||F\n)k\n(21)\nInstead of the post-normalization of matrix square root by\nmultiplying\np\n||A||F as done in eq. (7), we need to divide\np\n||A||F for computing the inverse square root:\nA− 1\n2 =\n1\np\n||A||F\n\x10\nI +\n∞\nX\nk=1\n− 1\n2\nk\n! (I −', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='X\nk=1\n− 1\n2\nk\n! (I −\nA\n||A||F\n)k\x11\n(22)\nCompared with the MTP of matrix square root in the\nsame degree, the inverse square root consumes the same\ncomputational complexity.\n4.1.2\nMatrix Pad´e Approximant\nThe matrix square root A\n1\n2 of our MPA is calculated as\np\n||A||FQ−1\nN PM. For the inverse square root, we can directly\ncompute the inverse as:\nA− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='||A||FQ−1\nN PM. For the inverse square root, we can directly\ncompute the inverse as:\nA− 1\n2 = (\nq\n||A||FQ−1\nN PM)−1 =\n1\np\n||A||F\nP−1\nM QN\n(23)\nThe extension to inverse square root comes for free as it\ndoes not require additional computation. For both the matrix\nsquare root and inverse square root, the matrix polynomials\nQN and PM need to be ﬁrst computed, and then one matrix\ninverse or solving the linear system is required.\nAnother approach to derive the MPA for inverse square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='inverse or solving the linear system is required.\nAnother approach to derive the MPA for inverse square\nroot is to match the power series in eq. (20) and construct the\nMPA again. The matching is calculated as:\n1 + PM\nm=1 rmzm\n1 + PN\nn=1 snzn = 1 +\nM+N\nX\nk=1\n− 1\n2\nk\n! zk\n(24)\nwhere rm and sn denote the new Pad´e coefﬁcients. Then the\nmatrix polynomials are computed as:\nRM = I +\nM\nX', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='matrix polynomials are computed as:\nRM = I +\nM\nX\nm=1\nrm(I −\nA\n||A||F\n)m,\nSN = I +\nN\nX\nn=1\nsn(I −\nA\n||A||F\n)n.\n(25)\nThe MPA for approximating the inverse square root is\ncalculated as:\nA− 1\n2 =\n1\np\n||A||F\nS−1\nN RM.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 =\n1\np\n||A||F\nS−1\nN RM.\n(26)\nThis method for deriving MPA also leads to the same\ncomplexity. Notice that these two different computation\nmethods are equivalent to each other. Speciﬁcally, we have:\nProposition 1. The diagonal MPA\n1\n√\n||A||F S−1\nN RM is equivalent\nto the diagonal MPA\n1\n√\n||A||F P−1\nM QN, and the relation pm=−sn\nand qn= − rm hold for any m=n.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='||A||F P−1\nM QN, and the relation pm=−sn\nand qn= − rm hold for any m=n.\nWe give the detailed proof in Supplementary Material.\nSince two sets of MPA are equivalent, we adopt the imple-\nmentation of inverse square root in eq. (23) throughout our\nexperiments, as it shares the same PM and QN with the\nmatrix square root.\n4.2\nBackward Pass\nFor the inverse square root, we can also rely on the iterative\nLyapunov solver for the gradient computation. Consider the\nfollowing relation:\nA\n1\n2 A− 1\n2 = I.\n(27)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n1\n2 A− 1\n2 = I.\n(27)\nA perturbation on both sides leads to:\ndA\n1\n2 A− 1\n2 + A\n1\n2 dA− 1\n2 = dI.\n(28)\nUsing the chain rule, we can obtain the gradient equation\nafter some arrangements:\n∂l\n∂A\n1\n2 = −A− 1\n2\n∂l\n∂A− 1\n2 A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n∂l\n∂A− 1\n2 A− 1\n2 .\n(29)\nInjecting this equation into eq. (13) leads to the re-\nformulation:\nA\n1\n2 ∂l\n∂A + ∂l\n∂AA\n1\n2 = −A− 1\n2\n∂l\n∂A− 1\n2 A− 1\n2\nA− 1\n2 ∂l\n∂A + ∂l', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\nA− 1\n2 ∂l\n∂A + ∂l\n∂AA− 1\n2 = −A−1\n∂l\n∂A− 1\n2 A−1.\n(30)\nAs can be seen, now the gradient function resembles the\ncontinuous Lyapunov equation again. The only difference\nwith eq. (13) is the r.h.s. term, which can be easily computed\nas −(A− 1\n2 )2\n∂l\n∂A− 1\n2 (A− 1\n2 )2 with 3 matrix multiplications.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂l\n∂A− 1\n2 (A− 1\n2 )2 with 3 matrix multiplications.\nFor the new iterative solver of the Lyapunov equation\nBX+XB=C, we have the following initialization:\nB0 =\nA− 1\n2\n||A− 1\n2 ||F\n= ||A\n1\n2 ||FA− 1\n2\nC0 =\n−A−1\n∂l\n∂A− 1\n2 A−1\n||A− 1\n2 ||F', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂A− 1\n2 A−1\n||A− 1\n2 ||F\n= −||A\n1\n2 ||FA−1\n∂l\n∂A− 1\n2 A−1.\n(31)\nThen we use the coupled NS iteration to compute the\ngradient\n∂l\n∂A= 1\n2Ck. Table 3 presents the complexity of\nthe backward algorithms. Compared with the gradient of\nmatrix square root, this extension marginally increases the\ncomputational complexity by 3 more matrix multiplications,\nwhich is more efﬁcient than a matrix inverse or solving a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computational complexity by 3 more matrix multiplications,\nwhich is more efﬁcient than a matrix inverse or solving a\nlinear system.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\n5\nEXPERIMENTS\nIn the experimental section, we ﬁrst perform a series of\nnumerical tests to compare our proposed method with SVD\nand NS iteration. Subsequently, we evaluate our methods\nin several real-world applications, including decorrelated\nbatch normalization, second-order vision transformer, global\ncovariance pooling for image/video recognition, and neural\nstyle transfer. The implementation details are kindly referred\nto the Supplementary Material.\n5.1\nBaselines', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='style transfer. The implementation details are kindly referred\nto the Supplementary Material.\n5.1\nBaselines\nIn the numerical tests, we compare our two methods\nagainst SVD and NS iteration. For the various computer\nvision experiments, our methods are compared with more\ndifferentiable SVD baselines where each one has its speciﬁc\ngradient computation. These methods include (1) Power\nIteration (PI), (2) SVD-PI [17], (3) SVD-Taylor [4], [18], and\n(4) SVD-Pad´e [4]. We put the detailed illustration of baseline\nmethods in the Supplementary Material.\n5.2\nNumerical Tests\nTo comprehensively evaluate the numerical performance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='methods in the Supplementary Material.\n5.2\nNumerical Tests\nTo comprehensively evaluate the numerical performance\nand stability, we compare the speed and error for the input\nof different batch sizes, matrices in various dimensions,\ndifferent iteration times of the backward pass, and different\npolynomial degrees of the forward pass. In each of the\nfollowing tests, the comparison is based on 10, 000 random\ncovariance matrices and the matrix size is consistently\n64×64 unless explicitly speciﬁed. The error is measured by\ncalculating the Mean Absolute Error (MAE) and Normalized\nRoot Mean Square Error (NRMSE) of the matrix square root\ncomputed by the approximate methods (NS iteration, MTP,\nand MPA) and the accurate method (SVD).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computed by the approximate methods (NS iteration, MTP,\nand MPA) and the accurate method (SVD).\nFor our algorithm of fast inverse square root, since the\ntheory behind the algorithm is in essence the same with\nthe matrix square root, they are expected to have similar\nnumerical properties. The difference mainly lie in the forward\nerror and backward speed. Thereby, we conduct the FP error\nanalysis and the BP speed analysis for the inverse square\nroot in Sec. 5.2.1 and Sec. 5.2.2, respectively. For the error\nanalysis, we compute the error of whitening transform by\n||σ(A− 1\n2 X)−I||F where σ(·) denotes the extracted eigen-\nvalues. In the other numerical tests, we only evaluate the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 X)−I||F where σ(·) denotes the extracted eigen-\nvalues. In the other numerical tests, we only evaluate the\nproperties of the algorithm for the matrix square root.\n5.2.1\nForward Error versus Speed\nBoth the NS iteration and our methods have a hyper-\nparameter to tune in the forward pass, i.e., iteration times\nfor NS iteration and polynomial degrees for our MPA and\nMTP. To validate the impact, we measure the speed and\nerror of both matrix square root and its inverse for different\nhyper-parameters. The degrees of our MPA and MTP vary\nfrom 6 to 18, and the iteration times of NS iteration range\nfrom 3 to 7. As can be observed from Fig. 4, our MTP has the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='from 6 to 18, and the iteration times of NS iteration range\nfrom 3 to 7. As can be observed from Fig. 4, our MTP has the\nleast computational time, and our MPA consumes slightly\nmore time than MTP but provides a closer approximation.\nMoreover, the curve of our MPA consistently lies below that\nof the NS iteration, demonstrating our MPA is a better choice\nin terms of both speed and accuracy.\nFig. 4: The comparison of speed and error in the FP for\nthe matrix square root (left) and the inverse square root\n(right). Our MPA computes the more accurate and faster\nsolution than the NS iteration, and our MTP enjoys the\nfastest calculation speed.\nFig. 5: The speed comparison in the backward pass. Our', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='solution than the NS iteration, and our MTP enjoys the\nfastest calculation speed.\nFig. 5: The speed comparison in the backward pass. Our\nLyapunov solver is more efﬁcient than NS iteration as fewer\nmatrix multiplications are involved. Our solver for inverse\nsquare root only slightly increases the computational cost.\n5.2.2\nBackward Speed versus Iteration\nFig. 5 compares the speed of our backward Lyapunov solver\nand the NS iteration versus different iteration times. The\nresult is coherent with the complexity analysis in Table 3: our\nLyapunov solver is much more efﬁcient than NS iteration.\nFor the NS iteration of 5 times, our Lyapunov solver still', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Lyapunov solver is much more efﬁcient than NS iteration.\nFor the NS iteration of 5 times, our Lyapunov solver still\nhas an advantage even when we iterate 8 times. Moreover,\nthe extension of our Lyapunov solver for inverse square root\nonly marginally increases the computational cost and is sill\nmuch faster than the NS iteration.\nFig. 6: Speed comparison for each method versus different\nbatch sizes. Our methods are more batch-efﬁcient than the\nSVD or NS iteration.\n5.2.3\nSpeed versus Batch Size\nIn certain applications such as covariance pooling and in-\nstance whitening, the input could be batched matrices instead\nof a single matrix. To compare the speed for batched input,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='stance whitening, the input could be batched matrices instead\nof a single matrix. To compare the speed for batched input,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\nwe conduct another numerical test. The hyper-parameter\nchoices follow our experimental settings in decorrelated\nbatch normalization. As seen in Fig. 6, our MPA-Lya and\nMTP-Lya are consistently more efﬁcient than the NS iteration\nand SVD. To give a concrete example, when the batch size is\n64, our MPA-Lya is 2.58X faster than NS iteration and 27.25X\nfaster than SVD, while our MTP-Lya is 5.82X faster than the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='faster than SVD, while our MTP-Lya is 5.82X faster than the\nNS iteration and 61.32X faster than SVD.\nAs discussed before, the current SVD implementation\nadopts a for-loop to compute each matrix one by one within\nthe mini-batch. This accounts for why the time consumption\nof SVD grows almost linearly with the batch size. For the\nNS iteration, the backward pass is not as batch-friendly\nas our Lyapunov solver. The gradient calculation requires\nmeasuring the trace and handling the multiplication for each\nmatrix in the batch, which has to be accomplished ineluctably\nby a for-loop. Our backward pass can be more efﬁciently\nimplemented by batched matrix multiplication.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='by a for-loop. Our backward pass can be more efﬁciently\nimplemented by batched matrix multiplication.\nFig. 7: The speed comparison (left) and the error comparison\n(middle and right) for matrices in different dimensions. Our\nMPA-Lya is consistently faster and more accurate than NS\niteration for different matrix dimensions. Since the SVD\nis accurate by default, other approximate methods are\ncompared with SVD to measure the error.\n5.2.4\nSpeed and Error versus Matrix Dimension\nIn the last numerical test, we compare the speed and error\nfor matrices in different dimensions. The hyper-parameter\nsettings also follow our experiments of ZCA whitening. As\nseen from Fig. 7 left, our proposed MPA-Lya and MTP-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='settings also follow our experiments of ZCA whitening. As\nseen from Fig. 7 left, our proposed MPA-Lya and MTP-\nLya consistently outperform others in terms of speed. In\nparticular, when the matrix size is very small (<32), the NS\niteration does not hold a speed advantage over the SVD. By\ncontrast, our proposed methods still have competitive speed\nagainst the SVD. Fig. 7 right presents the approximation error\nusing metrics MAE and NRMSE. Both metrics agree well\nwith each other and demonstrate that our MPA-Lya always\nhas a better approximation than the NS iteration, whereas\nour MTP-Lya gives a worse estimation but takes the least\ntime consumption, which can be considered as a trade-off\nbetween speed and accuracy.\n5.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='time consumption, which can be considered as a trade-off\nbetween speed and accuracy.\n5.3\nDecorrelated Batch Normalization\nAs a substitute of ordinary BN, the decorrelated BN [8]\napplies the ZCA whitening transform to eliminate the\ncorrelation of the data. Consider the reshaped feature map\nX∈RC×BHW . The whitening procedure ﬁrst computes its\nsample covariance as:\nA=(X − µ(X))(X − µ(X))T +ϵI\n(32)\nwhere A∈RC×C, µ(X) is the mean of X, and ϵ is a small\nconstant to make the covariance strictly positive deﬁnite.\nAfterwards, the inverse square root is calculated to whiten', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='constant to make the covariance strictly positive deﬁnite.\nAfterwards, the inverse square root is calculated to whiten\nthe feature map:\nXwhitend = A− 1\n2 X\n(33)\nBy doing so, the eigenvalues of X are all ones, i.e., the feature\nis uncorrelated. During the training process, the training\nstatistics are stored for the inference phase. We insert the\ndecorrelated BN layer after the ﬁrst convolutional layer of\nResNet [47], and the proposed methods and other baselines\nare used to compute A− 1\n2 .\nTable 4 displays the speed and validation error on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='are used to compute A− 1\n2 .\nTable 4 displays the speed and validation error on\nCIFAR10 and CIFAR100 [48]. The ordinary SVD with clipping\ngradient (SVD-Clip) is inferior to other SVD baselines, and\nthe SVD computation on GPU is slower than that on CPU.\nOur MTP-Lya is 1.16X faster than NS iteration and 1.32X\nfaster than SVD-Pad´e, and our MPA-Lya is 1.14X and 1.30X\nfaster. Furthermore, our MPA-Lya achieves state-of-the-art\nperformances across datasets and models. Our MTP-Lya has\ncomparable performances on ResNet-18 but slightly falls', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='performances across datasets and models. Our MTP-Lya has\ncomparable performances on ResNet-18 but slightly falls\nbehind on ResNet-50. We guess this is mainly because the\nrelatively large approximation error of MTP might affect\nlittle on the small model but can hurt the large model. On\nCIFAR100 with ResNet-50, our MPA-Lya slightly falls behind\nNS iteration in the average validation error. As a larger and\ndeeper model, ResNet-50 is likely to have worse-conditioned\nmatrices than ResNet-18. Since our MPA involves solving a\nlinear system, processing a very ill-conditioned matrix could\nlead to some round-off errors. In this case, NS iteration might\nhave a chance to slightly outperform our MPA-Lya. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='lead to some round-off errors. In this case, NS iteration might\nhave a chance to slightly outperform our MPA-Lya. However,\nthis is a rare situation; our MPA-Lya beats NS iteration in\nmost following experiments.\n5.4\nGlobal Covariance Pooling\nFor the application of global covariance pooling, we evaluate\nour method in three different tasks, including large-scale\nvisual recognition, ﬁne-grained visual categorization, and\nvideo action recognition. Since the GCP method requires the\nvery accurate matrix square root [4], our MTP-Lya cannot\nachieve reasonable performances due to the relatively large\napproximation error. Therefore, we do not take it into account\nfor comparison throughout the GCP experiments.\n5.4.1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='approximation error. Therefore, we do not take it into account\nfor comparison throughout the GCP experiments.\n5.4.1\nLarge-scale Visual Recognition\nFig. 8: Overview of the GCP network [2], [3], [4] for large-\nscale and ﬁne-grained visual recognition.\nFig. 8 displays the architecture of a typical GCP network.\nDifferent from the standard CNNs, the covariance square\nroot of the last convolutional feature is used as the global\nrepresentation. Considering the ﬁnal convolutional feature\nX∈RB×C×HW , a GCP meta-layer ﬁrst computes the sample\ncovariance as:\nP = X¯IXT , ¯I = 1\nN (I − 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='covariance as:\nP = X¯IXT , ¯I = 1\nN (I − 1\nN 11T )\n(34)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\nTABLE 4: Validation error of ZCA whitening methods. The covariance matrix is of size 1×64×64. The time consumption is\nmeasured for computing the inverse square root (BP+FP). For each method, we report the results based on ﬁve runs.\nMethods\nTime (ms)\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std\nmin\nmean±std\nmin\nmean±std\nmin\nSVD-Clip\n3.37\n4.88±0.25\n4.65\n21.60±0.39\n21.19\n20.50±0.33\n20.17\nSVD-PI (GPU)\n5.27\n4.57±0.10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='SVD-PI (GPU)\n5.27\n4.57±0.10\n4.45\n21.35±0.25\n21.05\n19.97±0.41\n19.27\nSVD-PI\n3.49\n4.59±0.09\n4.44\n21.39±0.23\n21.04\n19.94±0.44\n19.28\nSVD-Taylor\n3.41', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='19.94±0.44\n19.28\nSVD-Taylor\n3.41\n4.50±0.08\n4.40\n21.14±0.20\n20.91\n19.81±0.24\n19.26\nSVD-Pad´e\n3.39\n4.65±0.11\n4.50\n21.41±0.15\n21.26\n20.25±0.23\n19.98', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='21.26\n20.25±0.23\n19.98\nNS Iteration\n2.96\n4.57±0.15\n4.37\n21.24±0.20\n21.01\n19.39±0.30\n19.01\nOur MPA-Lya\n2.61\n4.39±0.09\n4.25\n21.11±0.12\n20.95\n19.55±0.20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='21.11±0.12\n20.95\n19.55±0.20\n19.24\nOur MTP-Lya\n2.56\n4.49±0.13\n4.31\n21.42±0.21\n21.24\n20.55±0.37\n20.12\nwhere ¯I represents the centering matrix, I denotes the identity\nmatrix, and 1 is a column vector whose values are all ones,\nrespectively. Afterwards, the matrix square root is conducted\nfor normalization:\nQ ≜ P', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='respectively. Afterwards, the matrix square root is conducted\nfor normalization:\nQ ≜ P\n1\n2 = (UΛUT )\n1\n2 = UΛ\n1\n2 UT\n(35)\nwhere the normalized covariance matrix Q is fed to the FC\nlayer. Our method is applied to calculate Q.\nTABLE 5: Comparison of validation accuracy (%) on Im-\nageNet [49] and ResNet-50 [47]. The covariance is of size\n256×256×256, and the time consumption is measured for\ncomputing the matrix square root (FP+BP).\nMethods\nTime (ms)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computing the matrix square root (FP+BP).\nMethods\nTime (ms)\nTop-1 Acc.\nTop-5 Acc.\nSVD-Taylor\n2349.12\n77.09\n93.33\nSVD-Pad´e\n2335.56\n77.33\n93.49\nNS iteration\n164.43\n77.19\n93.40\nOur MPA-Lya\n110.61\n77.13\n93.45', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Our MPA-Lya\n110.61\n77.13\n93.45\nTable 5 presents the speed comparison and the validation\nerror of GCP ResNet-50 [47] models on ImageNet [49]. Our\nMPA-Lya not only achieves very competitive performance\nbut also has the least time consumption. The speed of our\nmethod is about 21X faster than the SVD and 1.5X faster\nthan the NS iteration.\n5.4.2\nFine-grained Visual Recognition\nTABLE 6: Comparison of validation accuracy on ﬁne-grained\nbenchmarks and ResNet-50 [47]. The covariance is of size', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='TABLE 6: Comparison of validation accuracy on ﬁne-grained\nbenchmarks and ResNet-50 [47]. The covariance is of size\n10×64×64, and the time consumption is measured for\ncomputing the matrix square root (FP+BP).\nMethods\nTime (ms)\nBirds\nAircrafts\nCars\nSVD-Taylor\n32.13\n86.9\n89.9\n92.3\nSVD-Pad´e\n31.54\n87.2\n90.5\n92.8\nNS iteration', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='87.2\n90.5\n92.8\nNS iteration\n5.79\n87.3\n89.5\n91.7\nOur MPA-Lya\n3.89\n87.8\n91.0\n92.5\nIn line with other GCP works [2], [3], [4], after training on\nImageNet, the model is subsequently ﬁne-tuned on each ﬁne-\ngrained dataset. Table 6 compares the time consumption and\nvalidation accuracy on three commonly used ﬁne-grained\nbenchmarks, namely Caltech University Birds (Birds) [50],', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='validation accuracy on three commonly used ﬁne-grained\nbenchmarks, namely Caltech University Birds (Birds) [50],\nFGVC Aircrafts (Aircrafts) [51], and Stanford Cars (Cars) [52].\nAs can be observed, our MPA-Lya consumes 50% less time\nthan the NS iteration and is about 8X faster than the SVD.\nMoreover, the performance of our method is slightly better\nthan other baselines on Birds [50] and Aircrafts [51]. The\nevaluation result on Cars [52] is also comparable.\n5.4.3\nVideo Action Recognition\nFig. 9: Architecture of the temporal-attentive GCP network\nfor video action recognition [6]. The channel and spatial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Video Action Recognition\nFig. 9: Architecture of the temporal-attentive GCP network\nfor video action recognition [6]. The channel and spatial\nattention is used to make the covariance more attentive.\nBesides the application of image recognition, the GCP\nmethods can be also used for the task of video recognition [6].\nFig. 9 displays the overview of the temporal-attentive GCP\nmodel for video action recognition. The temporal covariance\nis computed in a sliding window manner by involving both\nintra- and inter-frame correlations. Supposing the kernel\nsize of the sliding window is 3, then temporal covariance is\ncomputed as:\nTemp.Cov. (Xl) = Xl−1XT\nl−1 + XlXT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computed as:\nTemp.Cov. (Xl) = Xl−1XT\nl−1 + XlXT\nl + Xl+1XT\nl+1\n|\n{z\n}\nintra−frame covariance\n+ Xl−1XT\nl + XlXT\nl−1 + · · · + Xl+1XT\nl\n|\n{z\n}\ninter−frame covariance\n(36)\nFinally, the matrix square root of the attentive temporal-\nbased covariance is computed and passed to the FC layer.\nThe spectral methods are used to compute the matrix square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='based covariance is computed and passed to the FC layer.\nThe spectral methods are used to compute the matrix square\nroot of the attentive covariance Temp.Cov. (Xl).\nWe present the validation accuracy and time cost for the\nvideo action recognition in Table 7. For the computation\nspeed, our MPA-Lya is about 1.74X faster than the NS itera-\ntion and is about 10.82X faster than the SVD. Furthermore,\nour MPA-Lya achieves the best performance on HMDB51,\nwhile the result on UCF101 is also very competitive.\nTo sum up, our MPA-Lya has demonstrated its general ap-\nplicability in the GCP models for different tasks. In particular,\nwithout the sacriﬁce of performance, our method can bring', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='plicability in the GCP models for different tasks. In particular,\nwithout the sacriﬁce of performance, our method can bring\nconsiderable speed improvements. This could be beneﬁcial\nfor faster training and inference. In certain experiments\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n10\nTABLE\n7:\nValidation\ntop-1/top-5\naccuracy\n(%)\non\nHMBD51 [53] and UCF101 [54] with backbone TEA R50 [55].\nThe covariance matrix is of size 16×128×128, and the time\nconsumption is measured for computing the matrix square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='The covariance matrix is of size 16×128×128, and the time\nconsumption is measured for computing the matrix square\nroot (BP+FP).\nMethods\nTime (ms)\nHMBD51\nUCF101\nSVD-Taylor\n76.17\n73.79/93.84\n95.00/99.60\nSVD-Pad´e\n75.25\n73.89/93.79\n94.13/99.47\nNS Iteration\n12.11', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='94.13/99.47\nNS Iteration\n12.11\n72.75/93.86\n94.16/99.50\nOur MPA-Lya\n6.95\n74.05/93.99\n94.24/99.58\nsuch as ﬁne-grained classiﬁcation, the approximate methods\n(MPA-Lya and NS iteration) can marginally outperform\naccurate SVD. This phenomenon has been similarly observed\nin related studies [3], [4], [9], and one likely reason is that the\nSVD does not have as healthy gradients as the approximate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='in related studies [3], [4], [9], and one likely reason is that the\nSVD does not have as healthy gradients as the approximate\nmethods. This might negatively inﬂuence the optimization\nprocess and consequently the performance would degrade.\n5.5\nNeural Style Transfer\nFig. 10: The architecture overview of our model for neural\nstyle transfer. Two encoders take input of the style and\ncontent image respectively, and generate the multi-scale\ncontent/style features. A decoder is applied to absorb the\nfeature and perform the WCT process at 5 different scales,\nwhich outputs a pair of images that exchange the styles.\nFinally, a discriminator is further adopted to tell apart the\nauthenticity of the images.\nWe adopt the WCT process in the network architecture', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Finally, a discriminator is further adopted to tell apart the\nauthenticity of the images.\nWe adopt the WCT process in the network architecture\nproposed in Cho et al. [14] for neural style transfer. Fig. 10\ndisplays the overview of the model. The WCT performs\nsuccessive whitening and coloring transform on the content\nand style feature. Consider the reshaped content feature\nXc∈RB×C×HW and the style feature Xs∈RB×C×HW . The\nstyle information is ﬁrst removed from the content as:\nXwhitened\nc\n=\n\x10\n(Xc − µ(Xc))(Xc − µ(Xc))T \x11− 1\n2 Xc', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='\x10\n(Xc − µ(Xc))(Xc − µ(Xc))T \x11− 1\n2 Xc\n(37)\nThen we extract the desired style information from the style\nfeature Xs and transfer it to the whitened content feature:\nXcolored\nc\n=\n\x10\n(Xs−µ(Xs))(Xs−µ(Xs))T \x11 1\n2 Xwhitened\nc\n(38)\nThe resultant feature Xcolored\nc\nis compensated with the mean\nof style feature and combined with the original content\nfeature:\nX = α(Xcolored', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='is compensated with the mean\nof style feature and combined with the original content\nfeature:\nX = α(Xcolored\nc\n+ µ(Xs)) + (1 − α)Xc\n(39)\nwhere α is a weight bounded in [0, 1] to control the strength\nof style transfer. In this experiment, both the matrix square\nroot and inverse square root are computed.\nTABLE 8: The LPIPS [56] score and user preference (%) on\nArtworks [57] dataset. The covariance is of size 4×256×256.\nWe measure the time consumption of whitening and coloring\ntransform that is conducted 10 times to exchange the style\nand content feature at different network depths.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='We measure the time consumption of whitening and coloring\ntransform that is conducted 10 times to exchange the style\nand content feature at different network depths.\nMethods Time (ms) LPIPS [56] (↑) Preference (↑)\nSVD-Taylor\n447.12\n0.5276\n16.25\nSVD-Pad´e\n445.23\n0.5422\n19.25\nNS iteration\n94.37\n0.5578\n17.00\nOur MPA-Lya\n69.23', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0.5578\n17.00\nOur MPA-Lya\n69.23\n0.5615\n24.75\nOur MTP-Lya\n40.97\n0.5489\n18.50\nTable 8 presents the quantitative evaluation using the\nLPIPS [56] score and user preference. The speed of our MPA-\nLya and MTP-Lya is signiﬁcantly faster than other methods.\nSpeciﬁcally, our MTP-Lya is 2.3X faster than the NS iteration\nand 10.9X faster than the SVD, while our MPA-Lya consumes', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='and 10.9X faster than the SVD, while our MPA-Lya consumes\n1.4X less time than the NS iteration and 6.4X less time than\nthe SVD. Moreover, our MPA-Lya achieves the best LPIPS\nscore and user preference. The performance of our MTP-\nLya is also very competitive. Fig. 11 displays the exemplary\nvisual comparison. Our methods can effectively transfer the\nstyle information and preserve the original content, leading\nto transferred images with a more coherent style and better\nvisual appeal. We give detailed evaluation results on each\nsubset and more visual examples in Supplementary Material.\nFig. 11: Visual examples of the neural style transfer on\nArtworks [57] dataset. Our methods generate sharper images\nwith more coherent style and better visual appeal. The red', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Artworks [57] dataset. Our methods generate sharper images\nwith more coherent style and better visual appeal. The red\nrectangular indicates regions with subtle details.\n5.6\nSecond-order Vision Transformer\nThe ordinary vision transformer [31] attaches an empty\nclass token to the sequence of visual tokens and only uses\nthe class token for prediction, which may not exploit the\nrich semantics embedded in the visual tokens. Instead, The\nSecond-order Vision Transformer (So-ViT) [5] proposes to\nleverage the high-level visual tokens to assist the task of\nclassiﬁcation:\ny = FC(c) + FC\n\x10\n(XXT )\n1\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='y = FC(c) + FC\n\x10\n(XXT )\n1\n2\n\x11\n(40)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n11\nTABLE 9: Validation top-1/top-5 accuracy of the second-order vision transformer on ImageNet [49]. The covariance is of size\n64×48×48, where 64 is the mini-batch size. The time cost is measured for computing the matrix square root (BP+FP).\nMethods\nTime (ms)\nArchitecture\nSo-ViT-7\nSo-ViT-10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Methods\nTime (ms)\nArchitecture\nSo-ViT-7\nSo-ViT-10\nSo-ViT-14\nPI\n1.84\n75.93/93.04\n77.96/94.18\n82.16/96.02 (303 epoch)\nSVD-PI\n83.43\n76.55/93.42\n78.53/94.40\n82.16/96.01 (278 epoch)\nSVD-Taylor\n83.29', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='82.16/96.01 (278 epoch)\nSVD-Taylor\n83.29\n76.66/93.52\n78.64/94.49\n82.15/96.02 (271 epoch)\nSVD-Pad´e\n83.25\n76.71/93.49\n78.77/94.51\n82.17/96.02 (265 epoch)\nNS Iteration\n10.38\n76.50/93.44\n78.50/94.44', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='10.38\n76.50/93.44\n78.50/94.44\n82.16/96.01 (280 epoch)\nOur MPA-Lya\n3.25\n76.84/93.46\n78.83/94.58\n82.17/96.03 (254 epoch)\nOur MTP-Lya\n2.39\n76.46/93.26\n78.44/94.33\n82.16/96.02 (279 epoch)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='78.44/94.33\n82.16/96.02 (279 epoch)\nFig. 12: The scheme of So-ViT [5]. The covariance square root\nof the visual tokens are computed to assist the classiﬁcation.\nIn the original vision transformer [31], only the class token is\nutilized for class predictions.\nwhere c is the output class token, X denotes the visual token,\nand y is the combined class predictions. We show the model\noverview in Fig. 12. Equipped with the covariance pooling\nlayer, So-ViT removes the need for pre-training on the ultra-\nlarge-scale datasets and achieves competitive performance\neven when trained from scratch. To reduce the computational', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='large-scale datasets and achieves competitive performance\neven when trained from scratch. To reduce the computational\nbudget, So-ViT further proposes to use Power Iteration (PI) to\napproximate the dominant eigenvector. We use our methods\nto compute the matrix square root of the covariance XXT .\nTable 9 compares the speed and performances on three\nSo-ViT architectures with different depths. Our proposed\nmethods signiﬁcantly outperform the SVD and NS iteration\nin terms of speed. To be more speciﬁc, our MPA-Lya is 3.19X\nfaster than the NS iteration and 25.63X faster than SVD-Pad´e,\nand our MTP-Lya is 4.34X faster than the NS iteration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='and our MTP-Lya is 4.34X faster than the NS iteration and\n34.85X faster than SVD-Pad´e. For the So-ViT-7 and So-ViT-10,\nour MPA-Lya achieves the best evaluation results and even\nslightly outperforms the SVD-based methods. Moreover, on\nthe So-ViT-14 model where the performances are saturated,\nour method converges faster and spends fewer training\nepochs. The performance of our MTP-Lya is also on par\nwith the other methods. The PI suggested in the So-ViT only\ncomputes the dominant eigenpair but neglects the rest. In\nspite of the fast speed, the performance is not comparable\nwith other methods.\n5.7', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='spite of the fast speed, the performance is not comparable\nwith other methods.\n5.7\nAblation Studies\nWe conduct three ablation studies to illustrate the impact\nof the degree of power series in the forward pass, the\ntermination criterion during the back-propagation, and the\npossibility of combining our Lyapunov solver with the SVD\nand the NS iteration.\n5.7.1\nDegree of Power series to Match for Forward Pass\nTable 10 displays the performance of our MPA-Lya for\ndifferent degrees of power series. As we use more terms\nof the power series, the approximation error gets smaller and\nthe performance gets steady improvements from the degree', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='different degrees of power series. As we use more terms\nof the power series, the approximation error gets smaller and\nthe performance gets steady improvements from the degree\n[3, 3] to [5, 5]. When the degree of our MPA is increased from\n[5, 5] to [6, 6], there are only marginal improvements. We\nhence set the forward degrees as [5, 5] for our MPA and as\n11 for our MTP as a trade-off between speed and accuracy.\nTABLE 10: Performance of our MPA-Lya versus different\ndegrees of power series to match.\nDegrees Time (ms)\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std min mean±std min mean±std min\n[3, 3]\n0.80\n4.64±0.11 4.54 21.35±0.18 21.20 20.14±0.43 19.56\n[4, 4]\n0.86\n4.55±0.08 4.51 21.26±0.22 21.03 19.87±0.29 19.64\n[6, 6]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[6, 6]\n0.98\n4.45±0.07 4.33 21.09±0.14 21.04 19.51±0.24 19.26\n[5, 5]\n0.93\n4.39±0.09 4.25 21.11±0.12 20.95 19.55±0.20 19.24\n5.7.2\nTermination Criterion for Backward Pass\nTable 11 compares the performance of backward algo-\nrithms with different termination criteria as well as the\nexact solution computed by the Bartels-Steward algorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='rithms with different termination criteria as well as the\nexact solution computed by the Bartels-Steward algorithm\n(BS algorithm) [26]. Since the NS iteration has the prop-\nerty of quadratic convergence, the errors ||Bk−I||F and\n||0.5Ck − X||F decrease at a larger rate for more iteration\ntimes. When we iterate more than 7 times, the error becomes\nsufﬁciently neglectable, i.e., the NS iteration almost converges.\nMoreover, from 8 iterations to 9 iterations, there are no\nobvious performance improvements. We thus terminate the\niterations after iterating 8 times.\nThe exact gradient calculated by the BS algorithm does\nnot yield the best results. Instead, it only achieves the least', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='iterations after iterating 8 times.\nThe exact gradient calculated by the BS algorithm does\nnot yield the best results. Instead, it only achieves the least\nﬂuctuation on ResNet-50 and other results are inferior to\nour iterative solver. This is because the formulation of our\nLyapunov equation is based on the assumption that the\naccurate matrix square root is computed, but in practice we\nonly compute the approximate one in the forward pass. In\nthis case, calculating the accurate gradient of the approximate\nmatrix square root might not necessarily work better than the\napproximate gradient of the approximate matrix square root.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12\nTABLE 11: Performance of our MPA-Lya versus different iteration times. The residual errors ||Bk−I|| and ||0.5Ck − X||F\nare measured based on 10, 000 randomly sampled matrices.\nMethods Time (ms) ||Bk−I||F ||0.5Ck−X||F\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std min mean±std min mean±std min\nBS algorithm\n2.34\n–\n–', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='mean±std min mean±std min mean±std min\nBS algorithm\n2.34\n–\n–\n4.57±0.10 4.45 21.20±0.23 21.01 19.60±0.16 19.55\n#iter 5\n1.14\n≈0.3541\n≈0.2049\n4.48±0.13 4.31 21.15±0.24 20.84 20.03±0.19 19.78\n#iter 6\n1.33\n≈0.0410', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='#iter 6\n1.33\n≈0.0410\n≈0.0231\n4.43±0.10 4.28 21.16±0.19 20.93 19.83±0.24 19.57\n#iter 7\n1.52\n≈7e−4\n≈3.5e−4\n4.45±0.11 4.29 21.18±0.20 20.95 19.69±0.20 19.38\n#iter 9\n1.83\n≈2e−7\n≈7e−6', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='#iter 9\n1.83\n≈2e−7\n≈7e−6\n4.40±0.07 4.28 21.08±0.15 20.89 19.52±0.22 19.25\n#iter 8\n1.62\n≈3e−7\n≈7e−6\n4.39±0.09 4.25 21.11±0.12 20.95 19.55±0.20 19.24\n5.7.3\nLyapunov Solver as A General Backward Algorithm\nWe note that our proposed iterative Lyapunov solver is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='5.7.3\nLyapunov Solver as A General Backward Algorithm\nWe note that our proposed iterative Lyapunov solver is a\ngeneral backward algorithm for computing the matrix square\nroot. That is to say, it should be also compatible with the\nSVD and NS iteration as the forward pass.\nFor the NS-Lya, our previous conference paper [22] shows\nthat the NS iteration used in [2], [21] cannot converge on any\ndatasets. In this extended manuscript, we found out that the\nunderlying reason is the inconsistency between the FP and\nBP. The NS iteration of [2], [21] is a coupled iteration that\nuse two variables Yk and Zk to compute the matrix square\nroot. For the BP algorithm, the NS iteration is deﬁned to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='use two variables Yk and Zk to compute the matrix square\nroot. For the BP algorithm, the NS iteration is deﬁned to\ncompute the matrix sign and only uses one variable Yk. The\nterm Zk is not involved in the BP and we have no control\nover the gradient back-propagating through it, which results\nin the non-convergence of the model. To resolve this issue,\nwe propose to change the forward coupled NS iteration to a\nvariant that uses one variable as:\nZk+1 = 1\n2(3Zk − Z3\nk\nA\n||A||F\n)\n(41)\nwhere Zk+1 converges to the inverse square root A− 1\n2 . This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content=')\n(41)\nwhere Zk+1 converges to the inverse square root A− 1\n2 . This\nvariant of NS iteration is often used to directly compute the\ninverse square root [9], [58]. The Z0 is initialization with\nI, and post-compensation is calculated as Zk =\n1\n√\n||A||F Zk.\nAlthough the modiﬁed NS iteration uses only one variable,\nwe note that it is an equivalent representation with the\nprevious NS iteration. More formally, we have:\nProposition 2. The one-variable NS iteration of [9], [58] is\nequivalent to the two-variable NS iteration of [1], [2], [21].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Proposition 2. The one-variable NS iteration of [9], [58] is\nequivalent to the two-variable NS iteration of [1], [2], [21].\nWe give the proof in the Supplementary Material. The\nmodiﬁed forward NS iteration is compatible with our iter-\native Lyapunov solver. Table 12 compares the performance\nof different methods that use the Lyapunov solver as the\nbackward algorithm. Both the SVD-Lya and NS-Lya achieve\ncompetitive performances.\nTABLE 12: Performance comparison of SVD-Lya and NS-Lya.\nMethods Time (ms)\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std min mean±std min mean±std min\nSVD-Lya\n4.47\n4.45±0.16 4.20 21.24±0.24 21.02 19.41±0.11 19.26\nNS-Lya\n2.88\n4.51±0.14 4.34 21.16±0.17 20.94 19.65±0.35 19.39\nMPA-Lya\n2.61', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='MPA-Lya\n2.61\n4.39±0.09 4.25 21.11±0.12 20.95 19.55±0.20 19.24\nMTP-Lya\n2.46\n4.49±0.13 4.31 21.42±0.21 21.24 20.55±0.37 20.12\n6\nCONCLUSION\nIn this paper, we propose two fast methods to compute\nthe differentiable matrix square root and the inverse square\nroot. In the forward pass, the MTP and MPA are applied\nto approximate the matrix square root, while an iterative', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='root. In the forward pass, the MTP and MPA are applied\nto approximate the matrix square root, while an iterative\nLyapunov solver is proposed to solve the gradient function\nfor back-propagation. A number of numerical tests and com-\nputer vision applications demonstrate that our methods can\nachieve both the fast speed and competitive performances.\nREFERENCES\n[1]\nT.-Y. Lin and S. Maji, “Improved bilinear pooling with cnns,” BMVC,\n2017.\n[2]\nP. Li, J. Xie, Q. Wang, and W. Zuo, “Is second-order information\nhelpful for large-scale visual recognition?” in ICCV, 2017.\n[3]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='helpful for large-scale visual recognition?” in ICCV, 2017.\n[3]\nP. Li, J. Xie, Q. Wang, and Z. Gao, “Towards faster training of\nglobal covariance pooling networks by iterative matrix square root\nnormalization,” in CVPR, 2018.\n[4]\nY. Song, N. Sebe, and W. Wang, “Why approximate matrix square\nroot outperforms accurate svd in global covariance pooling?” in\nICCV, 2021.\n[5]\nJ. Xie, R. Zeng, Q. Wang, Z. Zhou, and P. Li, “So-vit: Mind visual', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[5]\nJ. Xie, R. Zeng, Q. Wang, Z. Zhou, and P. Li, “So-vit: Mind visual\ntokens for vision transformer,” arXiv preprint arXiv:2104.10935, 2021.\n[6]\nZ. Gao, Q. Wang, B. Zhang, Q. Hu, and P. Li, “Temporal-attentive\ncovariance pooling networks for video recognition,” in NeurIPS,\n2021.\n[7]\nY. Song, N. Sebe, and W. Wang, “On the eigenvalues of global\ncovariance pooling for ﬁne-grained visual recognition,” IEEE\nTPAMI, 2022.\n[8]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='covariance pooling for ﬁne-grained visual recognition,” IEEE\nTPAMI, 2022.\n[8]\nL. Huang, D. Yang, B. Lang, and J. Deng, “Decorrelated batch\nnormalization,” in CVPR, 2018.\n[9]\nL. Huang, Y. Zhou, F. Zhu, L. Liu, and L. Shao, “Iterative normal-\nization: Beyond standardization towards efﬁcient whitening,” in\nCVPR, 2019.\n[10] L. Huang, L. Zhao, Y. Zhou, F. Zhu, L. Liu, and L. Shao, “An\ninvestigation into the stochasticity of batch whitening,” in CVPR,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='investigation into the stochasticity of batch whitening,” in CVPR,\n2020.\n[11] A. Siarohin, E. Sangineto, and N. Sebe, “Whitening and coloring\nbatch transform for gans,” in ICLR, 2018.\n[12] A. Ermolov, A. Siarohin, E. Sangineto, and N. Sebe, “Whitening for\nself-supervised representation learning,” in ICML, 2021.\n[13] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Universal\nstyle transfer via feature transforms,” in NeurIPS, 2017.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='style transfer via feature transforms,” in NeurIPS, 2017.\n[14] W. Cho, S. Choi, D. K. Park, I. Shin, and J. Choo, “Image-to-\nimage translation via group-wise deep whitening-and-coloring\ntransformation,” in CVPR, 2019.\n[15] S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, and J. Choo, “Robustnet:\nImproving domain generalization in urban-scene segmentation via\ninstance selective whitening,” in CVPR, 2021.\n[16] C. Ionescu, O. Vantzos, and C. Sminchisescu, “Training deep\nnetworks with structured layers by matrix backpropagation,” arXiv', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='networks with structured layers by matrix backpropagation,” arXiv\npreprint arXiv:1509.07838, 2015.\n[17] W.\nWang,\nZ.\nDang,\nY.\nHu,\nP.\nFua,\nand\nM.\nSalzmann,\n“Backpropagation-friendly eigendecomposition,” in NeurIPS, 2019.\n[18] ——, “Robust differentiable svd,” TPAMI, 2021.\n[19] S. Lahabar and P. Narayanan, “Singular value decomposition on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[19] S. Lahabar and P. Narayanan, “Singular value decomposition on\ngpu using cuda,” in 2009 IEEE International Symposium on Parallel\n& Distributed Processing.\nIEEE, 2009, pp. 1–10.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n13\n[20] G. Schulz, “Iterative berechung der reziproken matrix,” ZAMM-\nJournal of Applied Mathematics and Mechanics/Zeitschrift f¨ur Ange-\nwandte Mathematik und Mechanik, vol. 13, no. 1, pp. 57–59, 1933.\n[21] N. J. Higham, Functions of matrices: theory and computation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[21] N. J. Higham, Functions of matrices: theory and computation.\nSIAM,\n2008.\n[22] Y. Song, N. Sebe, and W. Wang, “Fast differentiable matrix square\nroot,” in ICLR, 2022.\n[23] C. Ionescu, O. Vantzos, and C. Sminchisescu, “Matrix backpropa-\ngation for deep networks with structured layers,” in ICCV, 2015.\n[24] Z. Dang, K. M. Yi, Y. Hu, F. Wang, P. Fua, and M. Salzmann,\n“Eigendecomposition-Free Training of Deep Networks with Zero\nEigenvalue-Based Losses,” in ECCV, 2018.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='“Eigendecomposition-Free Training of Deep Networks with Zero\nEigenvalue-Based Losses,” in ECCV, 2018.\n[25] Z. Dang, K. Yi, F. Wang, Y. Hu, P. Fua, and M. Salzmann,\n“Eigendecomposition-Free Training of Deep Networks for Linear\nLeast-Square Problems,” TPAMI, 2020.\n[26] R. H. Bartels and G. W. Stewart, “Solution of the matrix equation\nax+ xb= c [f4],” Communications of the ACM, vol. 15, no. 9, pp.\n820–826, 1972.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='820–826, 1972.\n[27] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for\nﬁne-grained visual recognition,” in ICCV, 2015.\n[28] Q. Wang, P. Li, Q. Hu, P. Zhu, and W. Zuo, “Deep global generalized\ngaussian networks,” in CVPR, 2019.\n[29] Q. Wang, J. Xie, W. Zuo, L. Zhang, and P. Li, “Deep cnns meet\nglobal covariance pooling: Better representation and generalization,”\nTPAMI, 2020.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='global covariance pooling: Better representation and generalization,”\nTPAMI, 2020.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017.\n[31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al., “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” in ICLR, 2020.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='et al., “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” in ICLR, 2020.\n[32] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in ICML,\n2015.\n[33] X. Pan, X. Zhan, J. Shi, X. Tang, and P. Luo, “Switchable whitening\nfor deep representation learning,” in ICCV, 2019.\n[34] L. Huang, Y. Zhou, L. Liu, F. Zhu, and L. Shao, “Group whitening:\nBalancing learning efﬁciency and representational capacity,” in\nCVPR, 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Balancing learning efﬁciency and representational capacity,” in\nCVPR, 2021.\n[35] S. Zhang, E. Nezhadarya, H. Fashandi, J. Liu, D. Graham, and\nM. Shah, “Stochastic whitening batch normalization,” in CVPR,\n2021.\n[36] Y. Cho, H. Cho, Y. Kim, and J. Kim, “Improving generalization\nof batch whitening by convolutional unit optimization,” in ICCV,\n2021.\n[37] Y. Li, M.-Y. Liu, X. Li, M.-H. Yang, and J. Kautz, “A closed-form\nsolution to photorealistic image stylization,” in ECCV, 2018.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='solution to photorealistic image stylization,” in ECCV, 2018.\n[38] Z. Wang, L. Zhao, H. Chen, L. Qiu, Q. Mo, S. Lin, W. Xing,\nand D. Lu, “Diversiﬁed arbitrary style transfer via deep feature\nperturbation,” in CVPR, 2020.\n[39] A. Abramov, C. Bayer, and C. Heller, “Keep it simple: Im-\nage statistics matching for domain adaptation,” arXiv preprint\narXiv:2005.12551, 2020.\n[40] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Improved texture\nnetworks: Maximizing quality and diversity in feed-forward', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='networks: Maximizing quality and diversity in feed-forward\nstylization and texture synthesis,” in CVPR, 2017.\n[41] Q. Sun, Z. Zhang, and P. Li, “Second-order encoding networks for\nsemantic segmentation,” Neurocomputing, 2021.\n[42] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, “Second-order\nattention network for single image super-resolution,” in CVPR,\n2019.\n[43] W. Van Assche, “Pad´e and hermite-pad´e approximation and\northogonality,” arXiv preprint math/0609094, 2006.\n[44] J. D. Roberts, “Linear model reduction and solution of the algebraic', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[44] J. D. Roberts, “Linear model reduction and solution of the algebraic\nriccati equation by use of the sign function,” International Journal of\nControl, vol. 32, no. 4, pp. 677–687, 1980.\n[45] C. S. Kenney and A. J. Laub, “The matrix sign function,” IEEE\ntransactions on automatic control, vol. 40, no. 8, pp. 1330–1348, 1995.\n[46] P. Benner, E. S. Quintana-Ort´ı, and G. Quintana-Ort´ı, “Solving\nstable sylvester equations via rational iterative schemes,” Journal of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='stable sylvester equations via rational iterative schemes,” Journal of\nScientiﬁc Computing, vol. 28, no. 1, pp. 51–83, 2006.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016.\n[48] A. Krizhevsky, “Learning multiple layers of features from tiny\nimages,” Master’s thesis, University of Tront, 2009.\n[49] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in CVPR, 2009.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A large-scale hierarchical image database,” in CVPR, 2009.\n[50] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie,\nand P. Perona, “Caltech-UCSD Birds 200,” California Institute of\nTechnology, Tech. Rep. CNS-TR-2010-001, 2010.\n[51] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi,\n“Fine-grained visual classiﬁcation of aircraft,” arXiv preprint\narXiv:1306.5151, 2013.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='“Fine-grained visual classiﬁcation of aircraft,” arXiv preprint\narXiv:1306.5151, 2013.\n[52] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object represen-\ntations for ﬁne-grained categorization,” in 4th International IEEE\nWorkshop on 3D Representation and Recognition (3dRR-13), Sydney,\nAustralia, 2013.\n[53] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “HMDB:\na large video database for human motion recognition,” in ICCV,\n2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='a large video database for human motion recognition,” in ICCV,\n2011.\n[54] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101\nhuman actions classes from videos in the wild,” arXiv preprint\narXiv:1212.0402, 2012.\n[55] Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, and L. Wang, “Tea: Temporal\nexcitation and aggregation for action recognition,” in CVPR, 2020.\n[56] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The\nunreasonable effectiveness of deep features as a perceptual metric,”', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='unreasonable effectiveness of deep features as a perceptual metric,”\nin CVPR, 2018.\n[57] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image\ntranslation with conditional adversarial networks,” in CVPR, 2017.\n[58] D. A. Bini, N. J. Higham, and B. Meini, “Algorithms for the matrix\np th root,” Numerical Algorithms, vol. 39, no. 4, pp. 349–378, 2005.\n[59] G. A. Baker and J. L. Gammel, The Pad´e approximant in theoretical\nphysics.\nAcademic Press, 1970.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='physics.\nAcademic Press, 1970.\n[60] H. Stahl, “Spurious poles in pad´e approximation,” Journal of\ncomputational and applied mathematics, vol. 99, no. 1-2, pp. 511–527,\n1998.\n[61] G. A. Baker, “Defects and the convergence of pad´e approximants,”\nActa Applicandae Mathematica, vol. 61, no. 1, pp. 37–52, 2000.\nYue Song received the B.Sc. cum laude from KU\nLeuven, Belgium and the joint M.Sc. summa cum\nlaude from the University of Trento, Italy and KTH\nRoyal Institute of Technology, Sweden. Currently,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='laude from the University of Trento, Italy and KTH\nRoyal Institute of Technology, Sweden. Currently,\nhe is a Ph.D. student with the Multimedia and\nHuman Understanding Group (MHUG) at the Uni-\nversity of Trento, Italy. His research interests are\ncomputer vision, deep learning, and numerical\nanalysis and optimization.\nNicu Sebe is Professor with the University of\nTrento, Italy, leading the research in the areas\nof multimedia information retrieval and human\nbehavior understanding. He was the General\nCo- Chair of ACM Multimedia 2013, and the\nProgram Chair of ACM Multimedia 2007 and\n2011, ECCV 2016, ICCV 2017 and ICPR 2020.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2011, ECCV 2016, ICCV 2017 and ICPR 2020.\nHe is a fellow of the International Association for\nPattern Recognition.\nWei Wang is an Assistant Professor of Computer\nScience at University of Trento, Italy. Previously,\nafter obtaining his PhD from University of Trento\nin 2018, he became a Postdoc at EPFL, Switzer-\nland. His research interests include machine\nlearning and its application to computer vision\nand multimedia analysis.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n14\nAPPENDIX A\nSUMMARY OF ALGORITHM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='14\nAPPENDIX A\nSUMMARY OF ALGORITHM\nAlgorithm. 1 and Algorithm. 2 summarize the forward pass\n(FP) and the backward pass (BP) of our proposed methods,\nrespectively. The hyper-parameter K in Algorithm. 1 means\nthe degrees of power series, and T in Algorithm. 2 denotes\nthe iteration times.\nAlgorithm 1: FP of our MTP and MPA for the matrix\nsquare root and the inverse square root.\nInput: A and K\nOutput: A\n1\n2 or A− 1\n2\nif MTP then\n// FP method is MTP\nif Matrix Square Root then', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\nif MTP then\n// FP method is MTP\nif Matrix Square Root then\nA\n1\n2 ←I− PK\nk=1\n1\n2\nk\n! (I −\nA\n||A||F )k;\nelse\nA− 1\n2 ←I+ P∞\nk=1\n− 1\n2\nk\n! (I −\nA\n||A||F )k\nend\nelse\n// FP method is MPA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n||A||F )k\nend\nelse\n// FP method is MPA\nM← K−1\n2\n, N← K−1\n2\n;\nPM←I− PM\nm=1 pm(I −\nA\n||A||F )m;\nQN←I− PN\nn=1 qn(I −\nA\n||A||F )n;\nif Matrix Square Root then\nA\n1\n2 ←Q−1\nN PM;\nelse\nA− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 ←Q−1\nN PM;\nelse\nA− 1\n2 ←P−1\nM QN;\nend\nend\nif Matrix Square Root then\nPost-compensate A\n1\n2 ←\np\n||A||F · A\n1\n2\nelse\nPost-compensate A− 1\n2 ←\n1\n√\n||A||F · A− 1\n2\nend\nAPPENDIX B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='√\n||A||F · A− 1\n2\nend\nAPPENDIX B\nTHEORETICAL DERIVATION AND PROOF\nB.1\nIterative Lyapunov Function Solver\nLemma 1 (Matrix Sign Function [21]). For a given matrix\nH with no eigenvalues on the imaginary axis, its sign function\nhas the following properties: 1) sign(H)2 = I; 2) if H has the\nJordan decomposition H=TMT−1, then its sign function satisﬁes\nsign(H)=Tsign(M)T−1.\nProof. The ﬁrst property is easy to prove. Consider the SVD\nof USVT = H. As the sign depends on the positiveness of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Proof. The ﬁrst property is easy to prove. Consider the SVD\nof USVT = H. As the sign depends on the positiveness of\nthe eigenvale, the square of sign function is computed as:\nsign(H)2 = sign(S)2\n(42)\nSince all eigenvalues are real, we have sign(S)2=I, and the\nﬁrst property is proved. The alternative deﬁnition of matrix\nsign function is given by:\nsign(H) = H(H2)− 1\n2\n(43)\nAlgorithm 2: BP of our Lyapunov solver for the\nmatrix square root and the inverse square root.\nInput:\n∂l\n∂A', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='matrix square root and the inverse square root.\nInput:\n∂l\n∂A\n1\n2 or\n∂l\n∂A− 1\n2 , A\n1\n2 or A− 1\n2 , and T\nOutput:\n∂l\n∂A\nif Matrix Square Root then\nB0←A\n1\n2 , C0←\n∂l\n∂A\n1\n2 , i←0 ;\nelse\nB0←A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂A\n1\n2 , i←0 ;\nelse\nB0←A− 1\n2 , C0← − A−1\n∂l\n∂A− 1\n2 A−1, i←0;\nend\nNormalize B0←\nB0\n||B0||F , C0←\nC0\n||B0||F ;\nwhile i < T do\n// Coupled iteration\nBk+1← 1\n2Bk(3I − B2\nk) ;\nCk+1← 1\n2\n\x10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k) ;\nCk+1← 1\n2\n\x10\n− B2\nkCk + BkCkBk + Ck(3I − B2\nk)\n\x11\n;\ni←i + 1;\nend\n∂l\n∂A← 1\n2Ck ;\nInjecting sign(H)=Tsign(M)T−1 into the above equation\nleads to\nsign(H) = TMT−1(TM2T)− 1\n2\n= TMT−1Tsign(M)M−1T−1\n= Tsign(M)T−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n= TMT−1Tsign(M)M−1T−1\n= Tsign(M)T−1\n(44)\nThe second property gets proved.\nNow we switch how to derive the iterative solver for\nmatrix sign function in detail. Lemma 1.1 shows that sign(H)\nis the matrix square root of the identity matrix. We use the\nNewton-Schulz iteration to compute sign(H) as:\nHk+1= = 1\n2Hk(3I − H2\nk)\n=1\n2\n\x14Bk(3I−B2\nk)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k)\n=1\n2\n\x14Bk(3I−B2\nk)\n3Ck − Bk(BkCk−CkBk)−CkB2\nk\n0\n−Bk(3I−B2\nk)\n\x15\n(45)\nLemma 1.2 indicates an alternative approach to compute the\nsign function as:\nsign(H) = sign\n\x10 \x14B\nC\n0\n−B\n\x15 \x11\n=\n\x14I\nX\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='−B\n\x15 \x11\n=\n\x14I\nX\n0\nI\n\x15\nsign\n\x10 \x14B\n0\n0\n−B\n\x15 \x11 \x14I\nX\n0\nI\n\x15−1\n=\n\x14I\nX\n0\nI\n\x15 \x14I\n0\n0\n−I\n\x15 \x14I\n−X\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0\n0\n−I\n\x15 \x14I\n−X\n0\nI\n\x15\n=\n\x14I\n2X\n0\n−I\n\x15\n(46)\nThe above two equations deﬁne the coupled iterations and\nthe convergence.\nB.2\nEquivalence of two sets of MPA\nProposition 1. The diagonal MPA\n1\n√\n||A||F S−1\nN RM is equivalent\nto the diagonal MPA\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='√\n||A||F S−1\nN RM is equivalent\nto the diagonal MPA\n1\n√\n||A||F P−1\nM QN, and the relation pm=−sn\nand qn= − rm hold for any m=n.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n15\nProof. Though Pad´e approximants are derived out of a ﬁnite\nTaylor series, they are asymptotic to their inﬁnite Taylor\nseries [43]. Let f(z)=(1 − z)\n1\n2 and f(z)−1=(1 − z)− 1\n2 . We', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 and f(z)−1=(1 − z)− 1\n2 . We\nhave the relation:\n1 + PM\nm=1 rmzm\n1 + PN\nn=1 snzn = f(z)−1 + R(zM+N+1)\n1 − PM\nm=1 pmzm\n1 − PN\nn=1 qnzn = f(z) + R(zM+N+1)\n(47)\nwhere R(zM+N+1) is the discarded higher-order term. Since\nf(z) =\n1\nf(z)−1 , we have:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='f(z) =\n1\nf(z)−1 , we have:\n1 + PM\nm=1 rmzm\n1 + PN\nn=1 snzn = 1 − PN\nn=1 qnzn\n1 − PM\nm=1 pmzm .\n(48)\nNow we have two sets of Pad´e approximants at both sides.\nSince the numerator and denominator of Pad´e approximants\nare relatively prime to each other by deﬁnition [59], the two\nsets of Pad´e approximants are equivalent and we have:\npm = −sn, qn = −rm\n(49)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='sets of Pad´e approximants are equivalent and we have:\npm = −sn, qn = −rm\n(49)\nGeneralized to the matrix case, this leads to:\nPM = SN, QN = RM.\n(50)\nTherefore, we also have S−1\nN RM=P−1\nM QN. The two sets of\nMPA are actually the same representation when m=n.\nB.3\nEquivalence of Newton-Schulz Iteration\nProposition 2. The one-variable NS iteration of [9], [58] is\nequivalent to the two-variable NS iteration of [1], [2], [21].\nProof. For the two-variable NS iteration, the coupled iteration\nis computed as:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Proof. For the two-variable NS iteration, the coupled iteration\nis computed as:\nYk+1 = 1\n2Yk(3I − ZkYk), Zk+1 = 1\n2(3I − ZkYk)Zk (51)\nwhere Yk and Zk converge to A\n1\n2 and A− 1\n2 , respectively.\nThe two variables are initialized as Y0=\nA\n||A||F and Z0=I.\nSince the two variables have the relation Z−1\nk Yk=\nA\n||A||F ,\nwe can replace Yk in eq. (51) with Zk\nA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n||A||F ,\nwe can replace Yk in eq. (51) with Zk\nA\n||A||F :\nZk+1 = 1\n2(3I − Z2\nk\nA\n||A||F\n)Zk\n(52)\nNotice that A and Zk have the same eigenspace and their\nmatrix product commutes, i.e., AZk=ZkA. Therefore, the\nabove equation can be further simpliﬁed as:\nZk+1 = 1\n2(3Zk − Z3\nk\nA\n||A||F\n)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2(3Zk − Z3\nk\nA\n||A||F\n)\n(53)\nAs indicated above, the two seemingly different NS iterations\nare in essence equivalent.\nAPPENDIX C\nBASELINES\nIn the experiment section, we compare our proposed two\nmethods with the following baselines:\nPower Iteration (PI). It is suggested in the original So-ViT\nto compute only the dominant eigenpair.\nSVD-PI [17] that uses PI to compute the gradients of SVD.\nSVD-Taylor [4], [18] that applies the Taylor polynomial to\napproximate the gradients.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='SVD-Taylor [4], [18] that applies the Taylor polynomial to\napproximate the gradients.\nSVD-Pad´e [4] that proposes to closely approximate the\nSVD gradients using Pad´e approximants. Notice that our\nMTP/MPA used in the FP is fundamentally different from\nthe Taylor polynomial or Pad´e approximants used in the\nBP of SVD-Pad´e. For our method, we use Matrix Taylor\nPolynomial (MTP) and Matrix Pad´e Approximants (MPA)\nto derive the matrix square root in the FP. For the SVD-\nPad´e, they use scalar Taylor polynomial and scalar Pad´e\napproximants to approximate the gradient\n1\nλi−λj in the BP.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='approximants to approximate the gradient\n1\nλi−λj in the BP.\nThat is to say, their aim is to use the technique to compute\nthe gradient and this will not involve the back-propagation\nof Taylor polynomial or Pad´e approximants.\nNS iteration [20], [21] that uses the Newton-Schulz iteration\nto compute the matrix square root. It has been widely\napplied in different tasks, including covariance pooling [3]\nand ZCA whitening [8]. We note that although [9] and [21]\nuse different forms of NS iteration, the two representations\nare equivalent to each other (see the proof in the paper).\nThe modiﬁed NS iteration in [9] just replaces Yk with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='are equivalent to each other (see the proof in the paper).\nThe modiﬁed NS iteration in [9] just replaces Yk with\nZkA and re-formulates the iteration using one variable.\nThe computation complexity is still the same.\nAs the ordinary differentiable SVD suffers from the\ngradient explosion issue and easily causes the program to\nfail, we do not take it into account for comparison.\nUnlike previous methods such as SVD and NS iteration,\nour MPA-Lya/MTP-Lya does not have a consistent FP and\nBP algorithm. However, we do not think it will bring any\ncaveat to the stability or performance. Our MTP and MPA\ndo not need coupled iteration in the FP and always have\ngradient back-propagating through A\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='do not need coupled iteration in the FP and always have\ngradient back-propagating through A\n1\n2 or A− 1\n2 in the BP,\nwhich could guarantee the training stability. Moreover, our\nablation study implies that our BP Lyapunov solver ap-\nproximates the real gradient very well (i.e., ||Bk−I||F<3e−7\nand ||0.5Ck−X||F<7e−6). Also, our extensive experiments\ndemonstrate the superior performances. In light of these\nexperimental results, we argue that as long as the BP\nalgorithm is accurate enough, the inconsistency between\nthe BP and FP is not an issue.\nAPPENDIX D\nEXPERIMENTAL SETTINGS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the BP and FP is not an issue.\nAPPENDIX D\nEXPERIMENTAL SETTINGS\nAll the source codes are implemented in Pytorch. For the\nSVD methods, the forward eigendecomposition is performed\non the CPU using the ofﬁcial Pytorch function TORCH.SVD,\nwhich calls the LAPACK’s routine gesdd that uses the\nDivide-and-Conquer algorithm for the fast calculation. All\nthe numerical tests are conducted on a single workstation\nequipped with a Tesla K40 GPU and a 6-core Intel(R) Xeon(R)\nGPU @ 2.20GHz.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n16\nFor our method throughout all the experiments, in the\nforward pass, we match the MTP to the power series of\ndegree 11 and set the degree for both numerator and\ndenominator of our MPA as 5. We keep iterating 8 times\nfor our backward Lyapunov solver.\nNow we turn to the implementation details for each\nexperiment in the paper.\nD.1\nDecorrelated Batch Normalization\nFig. 13: The architecture changes of ResNet models in\nthe experiment of ZCA whitening. The decorrelated batch\nnormalization layer is inserted after the ﬁrst convolutional', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the experiment of ZCA whitening. The decorrelated batch\nnormalization layer is inserted after the ﬁrst convolutional\nlayer. The kernel sizes, the stride of the ﬁrst convolution\nlayer, and the stride of the ﬁrst ResNet block are changed\ncorrespondingly.\nFig. 13 displays the detailed architecture changes of\nResNet. Suggested by [29], we truncate the Taylor polynomial\nto degree 20 for SVD-Taylor. To make Pad´e approximant\nmatch the same degree with Taylor polynomial, we set the\ndegree of both numerator and denominator to 10 for SVD-\nPad´e. For SVD-PI, the iteration times are also set as 20. For\nthe NS iteration, according to the setting in [3], [8], we set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Pad´e. For SVD-PI, the iteration times are also set as 20. For\nthe NS iteration, according to the setting in [3], [8], we set\nthe iteration times to 5. The other experimental settings\nfollow the implementation in [18]. We use the workstation\nequipped with a Tesla K40 GPU and a 6-core Intel(R) Xeon(R)\nGPU @ 2.20GHz for training. Notice that in our previous\nconference paper, we ﬁrst calculate the matrix square root\nA\n1\n2 and then compute Xwhitend by solving the linear system\nA\n1\n2 Xwhitend=X. Thanks to the algorithm extension to the\ninverse square root, we can directly computes A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 Xwhitend=X. Thanks to the algorithm extension to the\ninverse square root, we can directly computes A− 1\n2 in this\npaper.\nD.2\nSecond-order Vision Transformer\nWe use 8 Tesla G40 GPUs for distributed training and the\nNVIDIA Apex mixed-precision trainer is used. Except that\nthe spectral layer uses the single-precision (i.e., ﬂoat32), other\nlayers use the half-precision (i.e., ﬂoat16) to accelerate the\ntraining. Other implementation details follow the experimen-\ntal setting of the original So-ViT [5]. Following the experiment\nof covariance pooling for CNNs [4], the degrees of Taylor', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='tal setting of the original So-ViT [5]. Following the experiment\nof covariance pooling for CNNs [4], the degrees of Taylor\npolynomial are truncated to 100 for SVD-Taylor, and the\ndegree of both the numerator and denominator of Pad´e\napproximants are set to 50 for SVD-Pad´e. The iteration times\nof SVD-PI are set to 100. In the experiment of covariance\npooling, more terms of the Taylor series are used because\nthe covariance pooling meta-layer requires more accurate\ngradient estimation [4].\nFor the SVD-based methods, usually the double-precision\nis required to ensure an effective numerical representation\nof the eigenvalues. Using a lower precision would make the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='For the SVD-based methods, usually the double-precision\nis required to ensure an effective numerical representation\nof the eigenvalues. Using a lower precision would make the\nmodel fail to converge at the beginning of the training [4].\nThis is particularly severe for vision transformers which are\nknown slow and hard to converge in the early training stage.\nOne may consider to cast the tensor into double-precision (64\nbits) to alleviate this issue. However, this will trigger much\nlarger gradient and introduce round-off errors when the\ngradient is passed to previous layer in half-precision (16 bits).\nTo avoid this caveat, we ﬁrst apply the NS iteration to train\nthe network for 50 epochs, then switch to the corresponding\nSVD method and continue the training till the end. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the network for 50 epochs, then switch to the corresponding\nSVD method and continue the training till the end. This\nhybrid approach can avoid the non-convergence of the SVD\nmethods at the beginning of the training phase.\nD.3\nGlobal Covariance Pooling\nFor the experiment on large-scale and ﬁne-grained image\nrecognition, we refer to [4] for all the experimental settings.\nIn the video action recognition experiment [6], the iteration\ntime for NS iteration is set as 5. Othe implementation details\nare unchanged.\nD.4\nNeural Style Transfer\nFor the loss functions, we follow the settings in [14] and\nuse the cycle-consistent reconstruction loss in both the latent', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Neural Style Transfer\nFor the loss functions, we follow the settings in [14] and\nuse the cycle-consistent reconstruction loss in both the latent\nand the pixel space. The image is resized to the resolution\nof 216×216 before passing to the network, and the model is\ntrained for 100, 000 iterations. The batch size is set to 4.\nTable 13 and Fig. 14 present the detailed quantitative\nevaluation and more visual comparison, respectively. As sug-\ngested in [13], [38], we use the LPIPS [56] score and the user\npreference as the evaluation metrics. For the LPIPS metric,\nwe compute the score between each pair of transferred image\nand the content image. A higher LPIPS score implies that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='preference as the evaluation metrics. For the LPIPS metric,\nwe compute the score between each pair of transferred image\nand the content image. A higher LPIPS score implies that\nthe image carries less content information but more style\ninformation. For the user study, we randomly select 100\nimages from each dataset and ask 20 volunteers to vote for\nthe image that characterizes more the style information. In\nsome cases where the volunteer thinks none of the images\ncorrectly carries the style, he/she can abstain and does not\nvote for any one.\nAPPENDIX E\nCOMPARISON OF LYAPUNOV SOLVER AGAINST IM-\nPLICIT FUNCTION AND AUTOMATIC DIFFERENTIA-\nTION', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='PLICIT FUNCTION AND AUTOMATIC DIFFERENTIA-\nTION\nBesides our proposed custom Lyapunov gradient solver,\none may consider alternative gradient computation schemes,\nsuch as reverse-mode automatic differentiation (RMAD)\nand implicit function (IF). For the RMAD, the backward\npass indeed takes roughly the same operation costs as the\nforward pass. Considering that our MPA uses two sets of\nmatrix power polynomials and one matrix inverse, using\nRMAD for the gradient computation would be less efﬁcient\nthan the Lyapunov solver which only involves matrix\nmultiplications. Moreover, the gradient of some intermediate\nvariables of MPA would be calculated in the RMAD, which\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='variables of MPA would be calculated in the RMAD, which\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n17\nTABLE 13: The detailed LPIPS [56] score and user preference (%) on each subset of Artworks dataset.\nMethods\nLPIPS [56] Score (↑)\nUser Preference (↑)\nCezanne\nMonet\nVangogh\nUkiyoe\nAverage\nCezanne\nMonet\nVangogh\nUkiyoe\nAverage\nSVD-Taylor\n0.4937\n0.4820', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Average\nSVD-Taylor\n0.4937\n0.4820\n0.6074\n0.5274\n0.5276\n15\n16\n25\n9\n16.25\nSVD-Pad´e\n0.6179\n0.4783\n0.5307\n0.5419\n0.5422\n28\n13\n15\n21', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0.5422\n28\n13\n15\n21\n19.25\nNS iteration\n0.5328\n0.5329\n0.5386\n0.6270\n0.5578\n11\n18\n21\n18\n17.00\nOur MPA-Lya\n0.6332\n0.5291\n0.4511\n0.6325', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0.6332\n0.5291\n0.4511\n0.6325\n0.5615\n25\n29\n18\n27\n24.75\nOur MTP-Lya\n0.6080\n0.4826\n0.4796\n0.6253\n0.5489\n17\n21\n17\n19\n18.50', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='17\n21\n17\n19\n18.50\nFig. 14: More exemplary visualizations on Artworks [57] dataset. Our methods generate sharper images with more coherent\nstyle and better visual appeal. The red rectangular indicates regions with subtle details.\nwould further increase unnecessary memory costs. For the\nIF, the function for matrix square root can be deﬁned as\nf(A, A\n1\n2 ) = (A\n1\n2 )2 − A where A\n1\n2 can be regarded as\na function of A. Performing implicit differentiation and\nmultiplying both sides with\n∂l', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 can be regarded as\na function of A. Performing implicit differentiation and\nmultiplying both sides with\n∂l\n∂A\n1\n2 would lead to the gradient\nequation\n∂l\n∂A = −( ∂f\n∂A\n1\n2 )−1 ∂f\n∂A\n∂l\n∂A\n1\n2 . The memory usage of IF\nshould be small since only the gradient of f is introduced in\nthe computation. However, the time cost can be high due to\nthe function gradient evaluation ∂f\n∂A and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the computation. However, the time cost can be high due to\nthe function gradient evaluation ∂f\n∂A and\n∂f\n∂A\n1\n2 as well as the\nmatrix inverse computation.\nTABLE 14: Backward time and speed comparison for batched\nmatrices of size 64×64×64. We use MPA for forward pass,\nand the evaluation is averaged on 1, 000 randomly generated\nmatrices.\nMethod\nSpeed (ms)\nMemory (MB)\nLyapunov\n2.19\n1.99\nRMAD\n5.69', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Lyapunov\n2.19\n1.99\nRMAD\n5.69\n3.08\nIF\n4.71\n2.03\nTable 14 compares the speed and memory consumption.\nOur Lyapunov solver outperforms both schemes in terms of\nspeed and memory. The memory usage of IF is competitive,\nwhich also meets our expectation. In general, our Lyapunov-\nbased solver can be viewed as a well-optimized RMAD\ncompiler with the least memory and time consumption.\nAPPENDIX F\nSTABILITY OF PAD´E APPROXIMANTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='compiler with the least memory and time consumption.\nAPPENDIX F\nSTABILITY OF PAD´E APPROXIMANTS\nWhen there is the presence of spurious poles [60], [61], the\nPad´e approximants are very likely to suffer from the well-\nknown defects of instability. The spurious poles mean that\nwhen the approximated function has very close poles and\nzeros, the corresponding Pad´e approximants will also have\nclose poles and zeros. Consequently, the Pad´e approximants\nwill become very unstable in the region of defects (i.e.,\nwhen the input is in the neighborhood of poles and zeros).\nGeneralized to the matrix case, the spurious poles can happen\nwhen the determinant of the matrix denominator is zero (i.e.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Generalized to the matrix case, the spurious poles can happen\nwhen the determinant of the matrix denominator is zero (i.e.\ndet (QN) = 0).\nHowever, in our case, the approximated function for\nmatrix square root is (1−z)\n1\n2 for |z| < 1, which only has one\nzero at z = 1 and does not have any poles. For the inverse\nsquare root, the approximated function (1 − z)− 1\n2 has one\npole but does not have an zeros. Therefore, the spurious pole\ndoes not exist in our approximation and there are no defects\nof our Pad´e approximants.\nNow we brieﬂy prove this claim for the matrix square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='does not exist in our approximation and there are no defects\nof our Pad´e approximants.\nNow we brieﬂy prove this claim for the matrix square\nroot. The proof for the inverse square root can be given\nsimilarly, and we omit it here for conciseness. Consider the\ndenominator of our Pad´e approximants:\nQN = I −\nN\nX\nn=1\nqn(I −\nA\n||A||F\n)n\n(54)\nIts determinant is calculated as:\ndet (QN) =\nY\ni=1\n(1 −\nN\nX', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='det (QN) =\nY\ni=1\n(1 −\nN\nX\nn=1\nqn(1 −\nλi\nqP\ni λ2\ni\n)n)\n(55)\nThe\ncoefﬁcients\nqn\nof\nour\n[5, 5]\nPad´e\napproximant\nare\npre-computed\nas', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Pad´e\napproximant\nare\npre-computed\nas\n[2.25, −1.75, 0.54675, −0.05859375, 0.0009765625].\nLet\nxi denotes (1 −\nλi\n√P\ni λ2\ni ). Then xi is in the range of [0, 1],\nand we have:\nf(xi) = 1 − 2.25xi + 1.75x2\ni − 0.54675x3\ni +\n+0.05859375x4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='i − 0.54675x3\ni +\n+0.05859375x4\ni − 0.0009765625x5\ni ;\ndet (QN) =\nY\ni=1\n(f(xi)).\n(56)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n18\nThe polynomial f(xi) does not have any zero in the range of\nx∈[0, 1]. The minimal is 0.0108672 when x = 1. This implies\nthat det (QN) ̸= 0 always holds for any QN and our Pad´e', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='that det (QN) ̸= 0 always holds for any QN and our Pad´e\napproximants do not have any pole. Accordingly, there will\nbe no spurious poles and defects. Hence, our MPA is deemed\nstable. Throughout our experiments, we do not encounter\nany instability issue of our MPA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc62cb90> 111
cuda:2
[Document(page_content='IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1\nFast Differentiable Matrix Square Root and\nInverse Square Root\nYue Song, Member, IEEE, Nicu Sebe, Senior Member, IEEE, Wei Wang, Member, IEEE\nAbstract—Computing the matrix square root and its inverse in a differentiable manner is important in a variety of computer vision tasks.\nPrevious methods either adopt the Singular Value Decomposition (SVD) to explicitly factorize the matrix or use the Newton-Schulz\niteration (NS iteration) to derive the approximate solution. However, both methods are not computationally efﬁcient enough in either the\nforward pass or the backward pass. In this paper, we propose two more efﬁcient variants to compute the differentiable matrix square root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='forward pass or the backward pass. In this paper, we propose two more efﬁcient variants to compute the differentiable matrix square root\nand the inverse square root. For the forward propagation, one method is to use Matrix Taylor Polynomial (MTP), and the other method is\nto use Matrix Pad´e Approximants (MPA). The backward gradient is computed by iteratively solving the continuous-time Lyapunov equation\nusing the matrix sign function. A series of numerical tests show that both methods yield considerable speed-up compared with the SVD or\nthe NS iteration. Moreover, we validate the effectiveness of our methods in several real-world applications, including de-correlated batch\nnormalization, second-order vision transformer, global covariance pooling for large-scale and ﬁne-grained recognition, attentive\ncovariance pooling for video recognition, and neural style transfer. The experiments demonstrate that our methods can also achieve', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='covariance pooling for video recognition, and neural style transfer. The experiments demonstrate that our methods can also achieve\ncompetitive and even slightly better performances. Code is available at https://github.com/KingJamesSong/FastDifferentiableMatSqrt.\nIndex Terms—Differentiable Matrix Decomposition, Decorrelated Batch Normalization, Global Covariance Pooling, Neural Style Transfer.\n!\n1\nINTRODUCTION\nConsider a positive semi-deﬁnite matrix A. The principle\nsquare root A\n1\n2 and the inverse square root A− 1\n2 are mathe-\nmatically of practical interests, mainly because some desired\nspectral properties can be obtained by such transformations.\nAn exemplary illustration is given in Fig. 1. As can be', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='matically of practical interests, mainly because some desired\nspectral properties can be obtained by such transformations.\nAn exemplary illustration is given in Fig. 1. As can be\nseen, the matrix square root can shrink/stretch the feature\nvariances along with the direction of principle components,\nwhich is known as an effective spectral normalization for\ncovariance matrices. The inverse square root, on the other\nhand, can be used to whiten the data, i.e., make the data\nhas a unit variance in each dimension. These appealing\nspectral properties are very useful in many computer vision\napplications. In Global Covariance Pooling (GCP) [1], [2], [3],\n[4] and other related high-order representation methods [5],\n[6], the matrix square root is often used to normalize the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[4] and other related high-order representation methods [5],\n[6], the matrix square root is often used to normalize the\nhigh-order feature, which can beneﬁt some classiﬁcation\ntasks like general visual recognition [2], [3], [5], ﬁne-grained\nvisual categorization [7], and video action recognition [6].\nThe inverse square root is used as the whitening transform to\neliminate the feature correlation, which is widely applied in\ndecorrelated Batch Normalization (BN) [8], [9], [10] and other\nrelated models that involve the whitening transform [11],\n[12]. In the ﬁeld of neural style transfer, both the matrix\nsquare root and its inverse are adopted to perform successive\nWhitening and Coloring Transform (WCT) to transfer the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='square root and its inverse are adopted to perform successive\nWhitening and Coloring Transform (WCT) to transfer the\nstyle information for better generation ﬁdelity [13], [14], [15].\nTo compute the matrix square root, the standard method\nis via Singular Value Decomposition (SVD). Given the real\nYue Song, Nicu Sebe, and Wei Wang are with the Department of\nInformation Engineering and Computer Science, University of Trento,\nTrento 38123, Italy.\nE-mail: {yue.song, nicu.sebe, wei.wang}@unitn.it\nManuscript received April 19, 2005; revised August 26, 2015.\nFig. 1: Exemplary visualization of the matrix square root', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Manuscript received April 19, 2005; revised August 26, 2015.\nFig. 1: Exemplary visualization of the matrix square root\nand its inverse. Given the original data X∈R2×n, the matrix\nsquare root performs an effective spectral normalization by\nstretching the data along the axis of small variances and\nsqueezing the data in the direction with large variances,\nwhile the inverse square root transforms the data into the\nuncorrelated structure that has unit variance in all directions.\nsymmetric matrix A, its matrix square root is computed as:\nA\n1\n2 = (UΛUT )\n1\n2 = UΛ\n1\n2 UT\n(1)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 = UΛ\n1\n2 UT\n(1)\nwhere U is the eigenvector matrix, and Λ is the diagonal\neigenvalue matrix. As derived by Ionescu et al. [16], the\npartial derivative of the eigendecomposition is calculated as:\n∂l\n∂A = U\n\x10\nKT ⊙ (UT ∂l\n∂U) + ( ∂l\n∂Λ)diag\n\x11\nUT\n(2)\nwhere l is the loss function, ⊙ denotes the element-wise\nproduct, and ()diag represents the operation of setting the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='(2)\nwhere l is the loss function, ⊙ denotes the element-wise\nproduct, and ()diag represents the operation of setting the\noff-diagonal entries to zero. Despite the long-studied theories\nand well-developed algorithms of SVD, there exist two\nobstacles when integrating it into deep learning frameworks.\narXiv:2201.12543v2  [cs.CV]  19 Oct 2022\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n2\nOne issue is the back-propagation instability. For the matrix\nK deﬁned in eq. (2), its off-diagonal entry is Kij=1/(λi−λj),\nwhere λi and λj are involved eigenvalues. When the two', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='where λi and λj are involved eigenvalues. When the two\neigenvalues are close and small, the gradient is very likely\nto explode, i.e., Kij→∞. This issue has been solved by some\nmethods that use approximation techniques to estimate the\ngradients [4], [17], [18]. The other problem is the expen-\nsive time cost of the forward eigendecomposition. As the\nSVD is not supported well by GPUs [19], performing the\neigendecomposition on the deep learning platforms is rather\ntime-consuming. Incorporating the SVD with deep models\ncould add extra burdens to the training process. Particularly\nfor batched matrices, modern deep learning frameworks,\nsuch as Tensorﬂow and Pytorch, give limited optimization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='for batched matrices, modern deep learning frameworks,\nsuch as Tensorﬂow and Pytorch, give limited optimization\nfor the matrix decomposition within the mini-batch. They\ninevitably use a for-loop to conduct the SVD one matrix by\nanother. However, how to efﬁciently perform the SVD in\nthe context of deep learning has not been touched by the\nresearch community.\nTo avoid explicit eigendecomposition, one commonly\nused alternative is the Newton-Schulz iteration (NS itera-\ntion) [20], [21] which modiﬁes the ordinary Newton iteration\nby replacing the matrix inverse but preserving the quadratic\nconvergence. Compared with SVD, the NS iteration is rich\nin matrix multiplication and more GPU-friendly. Thus, this', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='convergence. Compared with SVD, the NS iteration is rich\nin matrix multiplication and more GPU-friendly. Thus, this\ntechnique has been widely used to approximate the matrix\nsquare root in different applications [1], [3], [9]. The forward\ncomputation relies on the following coupled iterations:\nYk+1 = 1\n2Yk(3I − ZkYk), Zk+1 = 1\n2(3I − ZkYk)Zk\n(3)\nwhere Yk and Zk converge to A\n1\n2 and A− 1\n2 , respectively.\nSince the NS iteration only converges locally (i.e., ||A||2<1),\nwe need to pre-normalize the initial matrix and post-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Since the NS iteration only converges locally (i.e., ||A||2<1),\nwe need to pre-normalize the initial matrix and post-\ncompensate the resultant approximation as Y0=\n1\n||A||F A\nand A\n1\n2 =\np\n||A||FYk. Each forward iteration involves 3\nmatrix multiplications, which is more efﬁcient than the\nforward pass of SVD. However, the backward pass of the\nNS iteration takes 14 matrix multiplications per iteration.\nConsider that the NS iteration often takes 5 iterations to\nachieve reasonable performances [3], [9]. The backward pass\nis much more time-costing than the backward algorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='achieve reasonable performances [3], [9]. The backward pass\nis much more time-costing than the backward algorithm\nof SVD. The speed improvement could be larger if a more\nefﬁcient backward algorithm is developed.\nTo address the drawbacks of SVD and NS iteration, i.e.\nthe low efﬁciency in either the forward or backward pass,\nwe derive two methods that are efﬁcient in both forward\nand backward propagation to compute the differentiable\nmatrix square root and its inverse. In the forward pass\n(FP), we propose using Matrix Taylor Polynomial (MTP)\nand Matrix Pad´e Approximants (MPA) for approximating\nthe matrix square root. The former approach is slightly faster\nbut the latter is more numerically accurate. Both methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the matrix square root. The former approach is slightly faster\nbut the latter is more numerically accurate. Both methods\nyield considerable speed-up compared with the SVD or the\nNS iteration in the forward computation. The proposed MTP\nand MPA can be also used to approximate the inverse square\nroot without any additional computational cost. For the\nbackward pass (BP), we consider the gradient function as a\nLyapunov equation and propose an iterative solution using\nthe matrix sign function. The backward pass costs fewer\nmatrix multiplications and is more computationally efﬁcient\nthan the NS iteration. Our proposed iterative Lyapunov\nsolver applies to both the matrix square root and the inverse\nsquare root. The only difference is that deriving the gradient\nof inverse square root requires 3 more matrix multiplications', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='square root. The only difference is that deriving the gradient\nof inverse square root requires 3 more matrix multiplications\nthan computing that of matrix square root.\nThrough a series of numerical tests, we show that the\nproposed MTP-Lya and MPA-Lya deliver consistent speed\nimprovement for different batch sizes, matrix dimensions,\nand some hyper-parameters (e.g., degrees of power series to\nmatch and iteration times). Moreover, our proposed MPA-\nLya consistently gives a better approximation of the matrix\nsquare root and its inverse than the NS iteration. Besides the\nnumerical tests, we conduct extensive experiments in a num-\nber of computer vision applications, including decorrelated\nbatch normalization, second-order vision transformer, global\ncovariance pooling for large-scale and ﬁne-grained image', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='batch normalization, second-order vision transformer, global\ncovariance pooling for large-scale and ﬁne-grained image\nrecognition, attentive global covariance pooling for video\naction recognition, and neural style transfer. Our methods\ncan achieve competitive performances against the SVD and\nthe NS iteration with the least amount of time overhead.\nOur MPA is suitable in use cases where the high precision\nis needed, while our MTP works in applications where\nthe accuracy is less demanded but the efﬁciency is more\nimportant. The contributions of the paper are twofold:\nWe propose two fast methods that compute the differ-\nentiable matrix square root and the inverse square root.\nThe forward propagation relies on the matrix Taylor\npolynomial or matrix Pad´e approximant, while an iterative', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='entiable matrix square root and the inverse square root.\nThe forward propagation relies on the matrix Taylor\npolynomial or matrix Pad´e approximant, while an iterative\nbackward gradient solver is derived from the Lyapunov\nequation using the matrix sign function.\nOur proposed algorithms are validated by a series of\nnumerical tests and several real-world computer vision\napplications. The experimental results demonstrate that\nour methods have a faster calculation speed and also have\nvery competitive performances.\nThis paper is an expanded version of [22]. In the confer-\nence paper [22], the proposed fast algorithms only apply to\nthe matrix square root A\n1\n2 . For the application of inverse\nsquare root A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the matrix square root A\n1\n2 . For the application of inverse\nsquare root A− 1\n2 , we have to solve the linear system or\ncompute the matrix inverse. However, both techniques are\nnot GPU-efﬁcient enough and could add extra computational\nburdens to the training. In this extended manuscript, we\ntarget the drawback and extend our algorithm to the case\nof inverse square root, which avoids the expensive compu-\ntation and allows for faster calculation in more application\nscenarios. Compared with computing the matrix square root,\ncomputing the inverse square root consumes the same time\ncomplexity in the FP and requires 3 more matrix multiplica-\ntions in the BP. The paper thus presents a complete solution to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='complexity in the FP and requires 3 more matrix multiplica-\ntions in the BP. The paper thus presents a complete solution to\nthe efﬁciency issue of the differentiable spectral layer. Besides\nthe algorithm extension, our method is validated in more\ncomputer vision applications: global covariance pooling for\nimage/video recognition and neural style transfer. We also\nshed light on the peculiar incompatibility of NS iteration and\nLyapunov solver discussed in Sec. 5.7.3.\nThe rest of the paper is organized as follows: Sec. 2\ndescribes the computational methods and applications of\ndifferentiable matrix square root and its inverse. Sec. 3\nintroduces our method that computes the end-to-end matrix\nsquare root, and Sec. 4 presents the extension of our method', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='introduces our method that computes the end-to-end matrix\nsquare root, and Sec. 4 presents the extension of our method\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n3\nto the inverse square root. Sec. 5 provides the experimental\nresults, the ablation studies, and some in-depth analysis.\nFinally, Sec. 6 summarizes the conclusions.\n2\nRELATED WORK\nIn this section, we recap the previous approaches that\ncompute the differentiable matrix square root and the inverse\nsquare root, followed by a discussion on the usage in some\napplications of deep learning and computer vision.\n2.1\nComputational Methods', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='square root, followed by a discussion on the usage in some\napplications of deep learning and computer vision.\n2.1\nComputational Methods\nIonescu et al. [16], [23] ﬁrst formulate the theory of matrix\nback-propagation, making it possible to integrate a spectral\nmeta-layer into neural networks. Existing approaches that\ncompute the differentiable matrix square root and its inverse\nare mainly based on the SVD or NS iteration. The SVD\ncalculates the accurate solution but suffers from backward\ninstability and expensive time cost, whereas the NS iteration\ncomputes the approximate solution but is more GPU-friendly.\nFor the backward algorithm of SVD, several methods have\nbeen proposed to resolve this gradient explosion issue [4],', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computes the approximate solution but is more GPU-friendly.\nFor the backward algorithm of SVD, several methods have\nbeen proposed to resolve this gradient explosion issue [4],\n[17], [18], [24], [25]. Wang et al. [17] propose to apply Power\nIteration (PI) to approximate the SVD gradient. Recently,\nSong et al. [4] propose to rely on Pad´e approximants to\nclosely estimate the backward gradient of SVD.\nTo avoid explicit eigendecomposition, Lin et al. [1] pro-\npose to substitute SVD with the NS iteration. Following this\nwork, Li et al. [2] and Huang et al. [8] adopt the NS iteration\nin the task of global covariance pooling and decorrelated\nbatch normalization, respectively. For the backward pass', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='in the task of global covariance pooling and decorrelated\nbatch normalization, respectively. For the backward pass\nof the differentiable matrix square root, Lin et al. [1] also\nsuggest viewing the gradient function as a Lyapunov equa-\ntion. However, their proposed exact solution is infeasible\nto compute practically, and the suggested Bartels-Steward\nalgorithm [26] requires explicit eigendecomposition or Schur\ndecomposition, which is again not GPU-friendly. By contrast,\nour proposed iterative solution using the matrix sign function\nis more computationally efﬁcient and achieves comparable\nperformances against the Bartels-Steward algorithm (see the\nablation study in Sec. 5.7.3).\n2.2\nApplications', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ablation study in Sec. 5.7.3).\n2.2\nApplications\n2.2.1\nGlobal Covariance Pooling\nOne successful application of the differentiable matrix square\nroot is the Global Covariance Pooling (GCP), which is a\nmeta-layer inserted before the FC layer of deep models to\ncompute the matrix square root of the feature covariance.\nEquipped with the GCP meta-layers, existing deep models\nhave achieved state-of-the-art performances on both generic\nand ﬁne-grained visual recognition [1], [2], [3], [4], [7], [27],\n[28], [29]. Inspired by recent advances of transformers [30],', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[28], [29]. Inspired by recent advances of transformers [30],\nXie et al. [5] integrate the GCP meta-layer into the vision\ntransformer [31] to exploit the second-order statistics of\nthe high-level visual tokens, which solves the issue that\nvision transformers need pre-training on ultra-large-scale\ndatasets. More recently, Gao et al. [6] propose an attentive\nand temporal-based GCP model for video action recognition.\n2.2.2\nDecorrelated Batch Normalization\nAnother line of research proposes to use ZCA whitening,\nwhich applies the inverse square root of the covariance to\nwhiten the feature, as an alternative scheme for the standard\nbatch normalization [32]. The whitening procedure, a.k.a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='whiten the feature, as an alternative scheme for the standard\nbatch normalization [32]. The whitening procedure, a.k.a\ndecorrelated batch normalization, does not only standardize\nthe feature but also eliminates the data correlation. The\ndecorrelated batch normalization can improve both the\noptimization efﬁciency and generalization ability of deep\nneural networks [8], [9], [10], [11], [12], [33], [34], [35], [36].\n2.2.3\nWhitening and Coloring Transform\nThe WCT [13] is also an active research ﬁeld where the differ-\nentiable matrix square root and its inverse are widely used.\nIn general, the WCT performs successively the whitening', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='entiable matrix square root and its inverse are widely used.\nIn general, the WCT performs successively the whitening\ntransform (using inverse square root) and the coloring trans-\nform (using matrix square root) on the multi-scale features\nto preserve the content of current image but carrying the\nstyle of another image. During the past few years, the WCT\nmethods have achieved remarkable progress in universal\nstyle transfer [13], [37], [38], domain adaptation [15], [39],\nand image translation [14], [40].\nBesides the three main applications discussed above,\nthere are still some minor applications, such as semantic\nsegmentation [41] and super resolution [42].\nTABLE 1: Summary of mathematical notation and symbol.\nAp', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='segmentation [41] and super resolution [42].\nTABLE 1: Summary of mathematical notation and symbol.\nAp\nMatrix p-th power.\nI\nIdentity matrix.\n|| · ||F\nMatrix Frobenius norm.\n\x10n\nk\n\x11\nBinomial coefﬁcients calculated as n!/k! (n−k)!.\nvec(·)\nUnrolling matrix into vector.\n⊗\nMatrix Kronecker product.\nsign(A)\nMatrix sign function calculated as A(A2)− 1\n2\n∂l\n∂A', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Matrix sign function calculated as A(A2)− 1\n2\n∂l\n∂A\nPartial derivative of loss l w.r.t. matrix A\n3\nFAST DIFFERENTIABLE MATRIX SQUARE ROOT\nTable 1 summarizes the notation we will use from now on.\nThis section presents the forward pass and the backward\npropagation of our fast differentiable matrix square root. For\nthe inverse square root, we introduce the derivation in Sec. 4.\n3.1\nForward Pass\n3.1.1\nMatrix Taylor Polynomial\nWe begin with motivating the Taylor series for the scalar case.\nConsider the following power series:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Matrix Taylor Polynomial\nWe begin with motivating the Taylor series for the scalar case.\nConsider the following power series:\n(1 − z)\n1\n2 = 1 −\n∞\nX\nk=1\n1\n2\nk\n! zk\n(4)\nwhere\n1\n2\nk\n!\ndenotes the binomial coefﬁcients that involve\nfractions, and the series converges when z<1 according to\nthe Cauchy root test. For the matrix case, the power series\ncan be similarly deﬁned by:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the Cauchy root test. For the matrix case, the power series\ncan be similarly deﬁned by:\n(I − Z)\n1\n2 = I −\n∞\nX\nk=1\n1\n2\nk\n! Zk\n(5)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n4\nwhere I is the identity matrix. Let us substitute Z with (I−A),\nwe can obtain:\nA\n1\n2 = I −\n∞\nX\nk=1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n1\n2 = I −\n∞\nX\nk=1\n1\n2\nk\n! (I − A)k\n(6)\nSimilar with the scalar case, the power series converge\nonly if ||(I − A)||p<1, where || · ||p denotes any vector-\ninduced matrix norms. To circumvent this issue, we can\nﬁrst pre-normalize the matrix A by dividing ||A||F. This\ncan guarantee the convergence as ||I−\nA\n||A||F ||p<1 is always\nsatisﬁed. Afterwards, the matrix square root A\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n||A||F ||p<1 is always\nsatisﬁed. Afterwards, the matrix square root A\n1\n2 is post-\ncompensated by multiplying\np\n||A||F. Integrated with these\ntwo operations, eq. (6) can be re-formulated as:\nA\n1\n2 =\nq\n||A||F ·\n\x10\nI −\n∞\nX\nk=1\n1\n2\nk\n! (I −\nA\n||A||F', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2\nk\n! (I −\nA\n||A||F\n)k\x11\n(7)\nTruncating the series to a certain degree K yields the MTP\napproximation for the matrix square root. For the MTP of\ndegree K, K−1 matrix multiplications are needed.\n3.1.2\nMatrix Pad´e Approximant\nFig. 2: The function (1 − z)\n1\n2 in the range of |z| < 1 and its\napproximation including Taylor polynomial, Newton-Schulz\niteration, and Pad´e approximants. The Pad´e approximants\nconsistently achieves a better estimation for other approxi-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='iteration, and Pad´e approximants. The Pad´e approximants\nconsistently achieves a better estimation for other approxi-\nmation schemes for any possible input values.\nThe MTP enjoys the fast calculation, but it converges\nuniformly and sometimes suffers from the so-called ”hump\nphenomenon”, i.e., the intermediate terms of the series grow\nquickly but cancel each other in the summation, which\nresults in a large approximation error. Expanding the series\nto a higher degree does not solve this issue either. The\nMPA, which adopts two polynomials of smaller degrees\nto construct a rational approximation, is able to avoid this\ncaveat. To visually illustrate this impact, we depict the\napproximation of the scalar square root in Fig. 2. The Pad´e\napproximants consistently deliver a better approximation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='approximation of the scalar square root in Fig. 2. The Pad´e\napproximants consistently deliver a better approximation\nthan NS iteration and Taylor polynomial. In particular, when\nthe input is close to the convergence boundary (z=1) where\nNS iteration and Taylor polynomials suffer from a larger\napproximation error, our Pad´e approximants still present a\nreasonable estimation. The superior property also generalizes\nto the matrix case.\nThe MPA is computed as the fraction of two sets of\npolynomials: denominator polynomial PN\nn=1 qnzn and nu-\nmerator polynomial PM\nm=1 pmzm. The coefﬁcients qn and\npm are pre-computed by matching to the corresponding\nTaylor series. Given the power series of scalar in eq. (4),', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='pm are pre-computed by matching to the corresponding\nTaylor series. Given the power series of scalar in eq. (4),\nFig. 3: Python-like pseudo-codes for Pad´e coefﬁcients.\nthe coefﬁcients of a [M, N] scalar Pad´e approximant are\ncomputed by matching to the series of degree M+N+1:\n1 − PM\nm=1 pmzm\n1 − PN\nn=1 qnzn = 1 −\nM+N\nX\nk=1\n1\n2\nk\n! zk\n(8)\nwhere pm and qn also apply to the matrix case. This matching', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k\n! zk\n(8)\nwhere pm and qn also apply to the matrix case. This matching\ngives rise to a system of linear equations:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n−\n1\n2\n1\n! − q1 = −p1,\n−\n1\n2\n2\n! +\n1\n2\n1\n! q1 − q2 = −p2,\n−\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n1\n! q1 − q2 = −p2,\n−\n1\n2\nM\n! +\n1\n2\nM − 1\n! q1 + · · · − qM = pM,\n· · · · ·\n(9)\nSolving these equations directly determines the coefﬁcients.\nWe give the Python-like pseudo-codes in Fig. 3. The numer-\nator polynomial and denominator polynomials of MPA are\ngiven by:\nPM = I −\nM\nX\nm=1\npm(I −', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='given by:\nPM = I −\nM\nX\nm=1\npm(I −\nA\n||A||F\n)m,\nQN = I −\nN\nX\nn=1\nqn(I −\nA\n||A||F\n)n.\n(10)\nThen the MPA for approximating the matrix square root is\ncomputed as:\nA\n1\n2 =\nq\n||A||FQ−1\nN PM.\n(11)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 =\nq\n||A||FQ−1\nN PM.\n(11)\nCompared with the MTP, the MPA trades off half of the ma-\ntrix multiplications with one matrix inverse, which slightly\nincreases the computational cost but converges more quickly\nand delivers better approximation abilities. Moreover, we\nnote that the matrix inverse can be avoided, as eq. (11) can be\nmore efﬁciently and numerically stably computed by solving\nthe linear system QNA\n1\n2 =\np\n||A||FPM. According to Van et\nal. [43], diagonal Pad´e approximants (i.e., PM and QN have', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='p\n||A||FPM. According to Van et\nal. [43], diagonal Pad´e approximants (i.e., PM and QN have\nthe same degree) usually yield better approximation than the\nnon-diagonal ones. Therefore, to match the MPA and MTP\nof the same degree, we set M=N= K−1\n2\n.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n5\nTABLE 2: Comparison of forward operations. For the matrix\nsquare root and its inverse, our MPA/MTP consumes the\nsame complexity. The cost of 1 NS iteration is about that of\nMTP of 4 degrees and about that of MPA of 2 degrees.\nOp.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='same complexity. The cost of 1 NS iteration is about that of\nMTP of 4 degrees and about that of MPA of 2 degrees.\nOp.\nMTP\nMPA\nNS iteration\nMat. Mul.\nK−1\n(K−1)/2\n3 × #iters\nMat. Inv.\n0\n1\n0\nTable 2 summarizes the forward computational com-\nplexity. As suggested in Li et al. [3] and Huang et al. [9],\nthe iteration times for NS iteration are often set as 5 such\nthat reasonable performances can be achieved. That is, to\nconsume the same complexity as the NS iteration does, our', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='that reasonable performances can be achieved. That is, to\nconsume the same complexity as the NS iteration does, our\nMTP and MPA can match to the power series up to degree\n16. However, as illustrated in Fig. 4, our MPA achieves\nbetter accuracy than the NS iteration even at degree 8. This\nobservation implies that our MPA is a better option in terms\nof both accuracy and speed.\n3.2\nBackward Pass\nThough one can manually derive the gradient of the MPA\nand MTP, their backward algorithms are computationally\nexpensive as they involve the matrix power up to degree K,\nwhere K can be arbitrarily large. Relying on the AutoGrad\npackage of deep learning frameworks can be both time-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='where K can be arbitrarily large. Relying on the AutoGrad\npackage of deep learning frameworks can be both time-\nand memory-consuming since the gradients of intermediate\nvariables would be computed and the matrix inverse of MPA\nis involved. To attain a more efﬁcient backward algorithm,\nwe propose to iteratively solve the gradient equation using\nthe matrix sign function. Given the matrix A and its square\nroot A\n1\n2 , since we have A\n1\n2 A\n1\n2 =A, a perturbation on A\nleads to:\nA\n1\n2 dA\n1\n2 + dA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n1\n2 dA\n1\n2 + dA\n1\n2 A\n1\n2 = dA\n(12)\nUsing the chain rule, the gradient function of the matrix\nsquare root satisﬁes:\nA\n1\n2 ∂l\n∂A + ∂l\n∂AA\n1\n2 =\n∂l\n∂A\n1\n2\n(13)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂l\n∂A\n1\n2\n(13)\nAs pointed out by Li et al. [1], eq. (13) actually deﬁnes the\ncontinuous-time Lyapunov equation (BX+XB=C) or a\nspecial case of Sylvester equation (BX+XD=C). The closed-\nform solution is given by:\nvec( ∂l\n∂A) =\n\x10\nA\n1\n2 ⊗ I + I ⊗ A\n1\n2\n\x11−1\nvec( ∂l\n∂A\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n\x11−1\nvec( ∂l\n∂A\n1\n2 )\n(14)\nwhere vec(·) denotes unrolling a matrix to vectors, and ⊗ is\nthe Kronecker product. Although the closed-form solution\nexists theoretically, it cannot be computed in practice due to\nthe huge memory consumption of the Kronecker product.\nSupposing that both A\n1\n2 and I are of size 256×256, the\nKronecker product A\n1\n2 ⊗I would take the dimension of\n2562×2562, which is infeasible to compute or store. Another', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 ⊗I would take the dimension of\n2562×2562, which is infeasible to compute or store. Another\napproach to solve eq. (13) is via the Bartels-Stewart algo-\nrithm [26]. However, it requires explicit eigendecomposition\nor Schulz decomposition, which is not GPU-friendly and\ncomputationally expensive.\nTo attain a GPU-friendly gradient solver, we propose\nto use the matrix sign function and iteratively solve the\nLyapunov equation. Solving the Sylvester equation via\nmatrix sign function has been long studied in the literature\nof numerical analysis [44], [45], [46]. One notable line of\nresearch is using the family of Newton iterations. Consider', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='of numerical analysis [44], [45], [46]. One notable line of\nresearch is using the family of Newton iterations. Consider\nthe following continuous Lyapunov function:\nBX + XB = C\n(15)\nwhere B refers to A\n1\n2 in eq. (13), C represents\n∂l\n∂A\n1\n2 , and X\ndenotes the seeking solution\n∂l\n∂A. Eq. (15) can be represented\nby the following block using a Jordan decomposition:\nH =\n\x14B\nC\n0\n−B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='H =\n\x14B\nC\n0\n−B\n\x15\n=\n\x14I\nX\n0\nI\n\x15 \x14B\n0\n0\n−B\n\x15 \x14I\nX\n0\nI\n\x15−1\n(16)\nThe matrix sign function is invariant to the Jordan canonical\nform or spectral decomposition. This property allows the use\nof Newton’s iterations for iteratively solving the Lyapunov\nfunction. Speciﬁcally, we have:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='of Newton’s iterations for iteratively solving the Lyapunov\nfunction. Speciﬁcally, we have:\nLemma 1 (Matrix Sign Function [21]). For a given matrix\nH with no eigenvalues on the imaginary axis, its sign function\nhas the following properties: 1) sign(H)2 = I; 2) if H has the\nJordan decomposition H=TMT−1, then its sign function satisﬁes\nsign(H)=Tsign(M)T−1.\nWe give the complete proof in the Supplementary Ma-\nterial. Lemma 1.1 shows that sign(H) is the matrix square\nroot of the identity matrix, which indicates the possibility\nof using Newton’s root-ﬁnding method to derive the solu-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='root of the identity matrix, which indicates the possibility\nof using Newton’s root-ﬁnding method to derive the solu-\ntion [21]. Here we also adopt the Newton-Schulz iteration,\nthe modiﬁed inverse-free and multiplication-rich Newton\niteration, to iteratively compute sign(H). This leads to the\ncoupled iteration as:\nBk+1 = 1\n2Bk(3I − B2\nk),\nCk+1 = 1\n2\n\x10\n− B2\nkCk + BkCkBk + Ck(3I − B2\nk)\n\x11\n.\n(17)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k)\n\x11\n.\n(17)\nThe equation above deﬁnes two coupled iterations for solving\nthe Lyapunov equation. Since the NS iteration converges only\nlocally, i.e., converges when ||H2\nk−I||<1, here we divide H0\nby ||B||F to meet the convergence condition. This normal-\nization deﬁnes the initialization B0=\nB\n||B||F and C0=\nC\n||B||F .\nRelying on Lemma 1.2, the sign function of eq. (16) can be\nalso calculated as:\nsign(H) = sign\n\x10 \x14B\nC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='also calculated as:\nsign(H) = sign\n\x10 \x14B\nC\n0\n−B\n\x15 \x11\n=\n\x14I\n2X\n0\n−I\n\x15\n(18)\nAs indicated above, the iterations in eq. (17) have the\nconvergence:\nlim\nk→∞ Bk = I, lim\nk→∞ Ck = 2X\n(19)\nAfter iterating k times, we can get the approximate solution\nX= 1\n2Ck. Instead of choosing setting iteration times, one can', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='After iterating k times, we can get the approximate solution\nX= 1\n2Ck. Instead of choosing setting iteration times, one can\nalso set the termination criterion by checking the convergence\n||Bk − I||F<τ, where τ is the pre-deﬁned tolerance.\nTable 3 compares the backward computation complexity\nof the iterative Lyapunov solver and the NS iteration. Our\nproposed Lyapunov solver spends fewer matrix multiplica-\ntions and is thus more efﬁcient than the NS iteration. Even\nif we iterate the Lyapunov solver more times (e.g., 7 or 8),\nit still costs less time than the backward calculation of NS\niteration that iterates 5 times.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='it still costs less time than the backward calculation of NS\niteration that iterates 5 times.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n6\nTABLE 3: Comparison of backward operations. For the\ninverse square root, our Lyapunov solver uses marginally\n3 more matrix multiplications. The cost of 1 NS iteration is\nabout that of 2 iterations of Lyapunov solver.\nOp.\nLya (Mat. Sqrt.)\nLya (Inv. Sqrt.)\nNS iteration\nMat. Mul.\n6 × #iters\n3 + 6 × #iters\n4 + 10 × #iters', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='6 × #iters\n3 + 6 × #iters\n4 + 10 × #iters\nMat. Inv.\n0\n0\n0\n4\nFAST DIFFERENTIABLE INVERSE SQUARE ROOT\nIn this section, we introduce the extension of our algorithm\nto the inverse square root.\n4.1\nForward Pass\n4.1.1\nMatrix Taylor Polynomial\nTo derive the MTP of inverse square root, we need to match\nto the following power series:\n(1 − z)− 1\n2 = 1 +\n∞\nX', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='(1 − z)− 1\n2 = 1 +\n∞\nX\nk=1\n− 1\n2\nk\n! zk\n(20)\nSimilar with the procedure of the matrix square root in eqs. (5)\nand (6), the MTP approximation can be computed as:\nA− 1\n2 = I +\n∞\nX\nk=1\n− 1\n2\nk\n! (I −\nA\n||A||F\n)k\n(21)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k\n! (I −\nA\n||A||F\n)k\n(21)\nInstead of the post-normalization of matrix square root by\nmultiplying\np\n||A||F as done in eq. (7), we need to divide\np\n||A||F for computing the inverse square root:\nA− 1\n2 =\n1\np\n||A||F\n\x10\nI +\n∞\nX\nk=1\n− 1\n2\nk\n! (I −', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='X\nk=1\n− 1\n2\nk\n! (I −\nA\n||A||F\n)k\x11\n(22)\nCompared with the MTP of matrix square root in the\nsame degree, the inverse square root consumes the same\ncomputational complexity.\n4.1.2\nMatrix Pad´e Approximant\nThe matrix square root A\n1\n2 of our MPA is calculated as\np\n||A||FQ−1\nN PM. For the inverse square root, we can directly\ncompute the inverse as:\nA− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='||A||FQ−1\nN PM. For the inverse square root, we can directly\ncompute the inverse as:\nA− 1\n2 = (\nq\n||A||FQ−1\nN PM)−1 =\n1\np\n||A||F\nP−1\nM QN\n(23)\nThe extension to inverse square root comes for free as it\ndoes not require additional computation. For both the matrix\nsquare root and inverse square root, the matrix polynomials\nQN and PM need to be ﬁrst computed, and then one matrix\ninverse or solving the linear system is required.\nAnother approach to derive the MPA for inverse square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='inverse or solving the linear system is required.\nAnother approach to derive the MPA for inverse square\nroot is to match the power series in eq. (20) and construct the\nMPA again. The matching is calculated as:\n1 + PM\nm=1 rmzm\n1 + PN\nn=1 snzn = 1 +\nM+N\nX\nk=1\n− 1\n2\nk\n! zk\n(24)\nwhere rm and sn denote the new Pad´e coefﬁcients. Then the\nmatrix polynomials are computed as:\nRM = I +\nM\nX', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='matrix polynomials are computed as:\nRM = I +\nM\nX\nm=1\nrm(I −\nA\n||A||F\n)m,\nSN = I +\nN\nX\nn=1\nsn(I −\nA\n||A||F\n)n.\n(25)\nThe MPA for approximating the inverse square root is\ncalculated as:\nA− 1\n2 =\n1\np\n||A||F\nS−1\nN RM.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 =\n1\np\n||A||F\nS−1\nN RM.\n(26)\nThis method for deriving MPA also leads to the same\ncomplexity. Notice that these two different computation\nmethods are equivalent to each other. Speciﬁcally, we have:\nProposition 1. The diagonal MPA\n1\n√\n||A||F S−1\nN RM is equivalent\nto the diagonal MPA\n1\n√\n||A||F P−1\nM QN, and the relation pm=−sn\nand qn= − rm hold for any m=n.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='||A||F P−1\nM QN, and the relation pm=−sn\nand qn= − rm hold for any m=n.\nWe give the detailed proof in Supplementary Material.\nSince two sets of MPA are equivalent, we adopt the imple-\nmentation of inverse square root in eq. (23) throughout our\nexperiments, as it shares the same PM and QN with the\nmatrix square root.\n4.2\nBackward Pass\nFor the inverse square root, we can also rely on the iterative\nLyapunov solver for the gradient computation. Consider the\nfollowing relation:\nA\n1\n2 A− 1\n2 = I.\n(27)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n1\n2 A− 1\n2 = I.\n(27)\nA perturbation on both sides leads to:\ndA\n1\n2 A− 1\n2 + A\n1\n2 dA− 1\n2 = dI.\n(28)\nUsing the chain rule, we can obtain the gradient equation\nafter some arrangements:\n∂l\n∂A\n1\n2 = −A− 1\n2\n∂l\n∂A− 1\n2 A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n∂l\n∂A− 1\n2 A− 1\n2 .\n(29)\nInjecting this equation into eq. (13) leads to the re-\nformulation:\nA\n1\n2 ∂l\n∂A + ∂l\n∂AA\n1\n2 = −A− 1\n2\n∂l\n∂A− 1\n2 A− 1\n2\nA− 1\n2 ∂l\n∂A + ∂l', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\nA− 1\n2 ∂l\n∂A + ∂l\n∂AA− 1\n2 = −A−1\n∂l\n∂A− 1\n2 A−1.\n(30)\nAs can be seen, now the gradient function resembles the\ncontinuous Lyapunov equation again. The only difference\nwith eq. (13) is the r.h.s. term, which can be easily computed\nas −(A− 1\n2 )2\n∂l\n∂A− 1\n2 (A− 1\n2 )2 with 3 matrix multiplications.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂l\n∂A− 1\n2 (A− 1\n2 )2 with 3 matrix multiplications.\nFor the new iterative solver of the Lyapunov equation\nBX+XB=C, we have the following initialization:\nB0 =\nA− 1\n2\n||A− 1\n2 ||F\n= ||A\n1\n2 ||FA− 1\n2\nC0 =\n−A−1\n∂l\n∂A− 1\n2 A−1\n||A− 1\n2 ||F', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂A− 1\n2 A−1\n||A− 1\n2 ||F\n= −||A\n1\n2 ||FA−1\n∂l\n∂A− 1\n2 A−1.\n(31)\nThen we use the coupled NS iteration to compute the\ngradient\n∂l\n∂A= 1\n2Ck. Table 3 presents the complexity of\nthe backward algorithms. Compared with the gradient of\nmatrix square root, this extension marginally increases the\ncomputational complexity by 3 more matrix multiplications,\nwhich is more efﬁcient than a matrix inverse or solving a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computational complexity by 3 more matrix multiplications,\nwhich is more efﬁcient than a matrix inverse or solving a\nlinear system.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n7\n5\nEXPERIMENTS\nIn the experimental section, we ﬁrst perform a series of\nnumerical tests to compare our proposed method with SVD\nand NS iteration. Subsequently, we evaluate our methods\nin several real-world applications, including decorrelated\nbatch normalization, second-order vision transformer, global\ncovariance pooling for image/video recognition, and neural\nstyle transfer. The implementation details are kindly referred\nto the Supplementary Material.\n5.1\nBaselines', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='style transfer. The implementation details are kindly referred\nto the Supplementary Material.\n5.1\nBaselines\nIn the numerical tests, we compare our two methods\nagainst SVD and NS iteration. For the various computer\nvision experiments, our methods are compared with more\ndifferentiable SVD baselines where each one has its speciﬁc\ngradient computation. These methods include (1) Power\nIteration (PI), (2) SVD-PI [17], (3) SVD-Taylor [4], [18], and\n(4) SVD-Pad´e [4]. We put the detailed illustration of baseline\nmethods in the Supplementary Material.\n5.2\nNumerical Tests\nTo comprehensively evaluate the numerical performance', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='methods in the Supplementary Material.\n5.2\nNumerical Tests\nTo comprehensively evaluate the numerical performance\nand stability, we compare the speed and error for the input\nof different batch sizes, matrices in various dimensions,\ndifferent iteration times of the backward pass, and different\npolynomial degrees of the forward pass. In each of the\nfollowing tests, the comparison is based on 10, 000 random\ncovariance matrices and the matrix size is consistently\n64×64 unless explicitly speciﬁed. The error is measured by\ncalculating the Mean Absolute Error (MAE) and Normalized\nRoot Mean Square Error (NRMSE) of the matrix square root\ncomputed by the approximate methods (NS iteration, MTP,\nand MPA) and the accurate method (SVD).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computed by the approximate methods (NS iteration, MTP,\nand MPA) and the accurate method (SVD).\nFor our algorithm of fast inverse square root, since the\ntheory behind the algorithm is in essence the same with\nthe matrix square root, they are expected to have similar\nnumerical properties. The difference mainly lie in the forward\nerror and backward speed. Thereby, we conduct the FP error\nanalysis and the BP speed analysis for the inverse square\nroot in Sec. 5.2.1 and Sec. 5.2.2, respectively. For the error\nanalysis, we compute the error of whitening transform by\n||σ(A− 1\n2 X)−I||F where σ(·) denotes the extracted eigen-\nvalues. In the other numerical tests, we only evaluate the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 X)−I||F where σ(·) denotes the extracted eigen-\nvalues. In the other numerical tests, we only evaluate the\nproperties of the algorithm for the matrix square root.\n5.2.1\nForward Error versus Speed\nBoth the NS iteration and our methods have a hyper-\nparameter to tune in the forward pass, i.e., iteration times\nfor NS iteration and polynomial degrees for our MPA and\nMTP. To validate the impact, we measure the speed and\nerror of both matrix square root and its inverse for different\nhyper-parameters. The degrees of our MPA and MTP vary\nfrom 6 to 18, and the iteration times of NS iteration range\nfrom 3 to 7. As can be observed from Fig. 4, our MTP has the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='from 6 to 18, and the iteration times of NS iteration range\nfrom 3 to 7. As can be observed from Fig. 4, our MTP has the\nleast computational time, and our MPA consumes slightly\nmore time than MTP but provides a closer approximation.\nMoreover, the curve of our MPA consistently lies below that\nof the NS iteration, demonstrating our MPA is a better choice\nin terms of both speed and accuracy.\nFig. 4: The comparison of speed and error in the FP for\nthe matrix square root (left) and the inverse square root\n(right). Our MPA computes the more accurate and faster\nsolution than the NS iteration, and our MTP enjoys the\nfastest calculation speed.\nFig. 5: The speed comparison in the backward pass. Our', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='solution than the NS iteration, and our MTP enjoys the\nfastest calculation speed.\nFig. 5: The speed comparison in the backward pass. Our\nLyapunov solver is more efﬁcient than NS iteration as fewer\nmatrix multiplications are involved. Our solver for inverse\nsquare root only slightly increases the computational cost.\n5.2.2\nBackward Speed versus Iteration\nFig. 5 compares the speed of our backward Lyapunov solver\nand the NS iteration versus different iteration times. The\nresult is coherent with the complexity analysis in Table 3: our\nLyapunov solver is much more efﬁcient than NS iteration.\nFor the NS iteration of 5 times, our Lyapunov solver still', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Lyapunov solver is much more efﬁcient than NS iteration.\nFor the NS iteration of 5 times, our Lyapunov solver still\nhas an advantage even when we iterate 8 times. Moreover,\nthe extension of our Lyapunov solver for inverse square root\nonly marginally increases the computational cost and is sill\nmuch faster than the NS iteration.\nFig. 6: Speed comparison for each method versus different\nbatch sizes. Our methods are more batch-efﬁcient than the\nSVD or NS iteration.\n5.2.3\nSpeed versus Batch Size\nIn certain applications such as covariance pooling and in-\nstance whitening, the input could be batched matrices instead\nof a single matrix. To compare the speed for batched input,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='stance whitening, the input could be batched matrices instead\nof a single matrix. To compare the speed for batched input,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n8\nwe conduct another numerical test. The hyper-parameter\nchoices follow our experimental settings in decorrelated\nbatch normalization. As seen in Fig. 6, our MPA-Lya and\nMTP-Lya are consistently more efﬁcient than the NS iteration\nand SVD. To give a concrete example, when the batch size is\n64, our MPA-Lya is 2.58X faster than NS iteration and 27.25X\nfaster than SVD, while our MTP-Lya is 5.82X faster than the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='faster than SVD, while our MTP-Lya is 5.82X faster than the\nNS iteration and 61.32X faster than SVD.\nAs discussed before, the current SVD implementation\nadopts a for-loop to compute each matrix one by one within\nthe mini-batch. This accounts for why the time consumption\nof SVD grows almost linearly with the batch size. For the\nNS iteration, the backward pass is not as batch-friendly\nas our Lyapunov solver. The gradient calculation requires\nmeasuring the trace and handling the multiplication for each\nmatrix in the batch, which has to be accomplished ineluctably\nby a for-loop. Our backward pass can be more efﬁciently\nimplemented by batched matrix multiplication.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='by a for-loop. Our backward pass can be more efﬁciently\nimplemented by batched matrix multiplication.\nFig. 7: The speed comparison (left) and the error comparison\n(middle and right) for matrices in different dimensions. Our\nMPA-Lya is consistently faster and more accurate than NS\niteration for different matrix dimensions. Since the SVD\nis accurate by default, other approximate methods are\ncompared with SVD to measure the error.\n5.2.4\nSpeed and Error versus Matrix Dimension\nIn the last numerical test, we compare the speed and error\nfor matrices in different dimensions. The hyper-parameter\nsettings also follow our experiments of ZCA whitening. As\nseen from Fig. 7 left, our proposed MPA-Lya and MTP-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='settings also follow our experiments of ZCA whitening. As\nseen from Fig. 7 left, our proposed MPA-Lya and MTP-\nLya consistently outperform others in terms of speed. In\nparticular, when the matrix size is very small (<32), the NS\niteration does not hold a speed advantage over the SVD. By\ncontrast, our proposed methods still have competitive speed\nagainst the SVD. Fig. 7 right presents the approximation error\nusing metrics MAE and NRMSE. Both metrics agree well\nwith each other and demonstrate that our MPA-Lya always\nhas a better approximation than the NS iteration, whereas\nour MTP-Lya gives a worse estimation but takes the least\ntime consumption, which can be considered as a trade-off\nbetween speed and accuracy.\n5.3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='time consumption, which can be considered as a trade-off\nbetween speed and accuracy.\n5.3\nDecorrelated Batch Normalization\nAs a substitute of ordinary BN, the decorrelated BN [8]\napplies the ZCA whitening transform to eliminate the\ncorrelation of the data. Consider the reshaped feature map\nX∈RC×BHW . The whitening procedure ﬁrst computes its\nsample covariance as:\nA=(X − µ(X))(X − µ(X))T +ϵI\n(32)\nwhere A∈RC×C, µ(X) is the mean of X, and ϵ is a small\nconstant to make the covariance strictly positive deﬁnite.\nAfterwards, the inverse square root is calculated to whiten', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='constant to make the covariance strictly positive deﬁnite.\nAfterwards, the inverse square root is calculated to whiten\nthe feature map:\nXwhitend = A− 1\n2 X\n(33)\nBy doing so, the eigenvalues of X are all ones, i.e., the feature\nis uncorrelated. During the training process, the training\nstatistics are stored for the inference phase. We insert the\ndecorrelated BN layer after the ﬁrst convolutional layer of\nResNet [47], and the proposed methods and other baselines\nare used to compute A− 1\n2 .\nTable 4 displays the speed and validation error on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='are used to compute A− 1\n2 .\nTable 4 displays the speed and validation error on\nCIFAR10 and CIFAR100 [48]. The ordinary SVD with clipping\ngradient (SVD-Clip) is inferior to other SVD baselines, and\nthe SVD computation on GPU is slower than that on CPU.\nOur MTP-Lya is 1.16X faster than NS iteration and 1.32X\nfaster than SVD-Pad´e, and our MPA-Lya is 1.14X and 1.30X\nfaster. Furthermore, our MPA-Lya achieves state-of-the-art\nperformances across datasets and models. Our MTP-Lya has\ncomparable performances on ResNet-18 but slightly falls', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='performances across datasets and models. Our MTP-Lya has\ncomparable performances on ResNet-18 but slightly falls\nbehind on ResNet-50. We guess this is mainly because the\nrelatively large approximation error of MTP might affect\nlittle on the small model but can hurt the large model. On\nCIFAR100 with ResNet-50, our MPA-Lya slightly falls behind\nNS iteration in the average validation error. As a larger and\ndeeper model, ResNet-50 is likely to have worse-conditioned\nmatrices than ResNet-18. Since our MPA involves solving a\nlinear system, processing a very ill-conditioned matrix could\nlead to some round-off errors. In this case, NS iteration might\nhave a chance to slightly outperform our MPA-Lya. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='lead to some round-off errors. In this case, NS iteration might\nhave a chance to slightly outperform our MPA-Lya. However,\nthis is a rare situation; our MPA-Lya beats NS iteration in\nmost following experiments.\n5.4\nGlobal Covariance Pooling\nFor the application of global covariance pooling, we evaluate\nour method in three different tasks, including large-scale\nvisual recognition, ﬁne-grained visual categorization, and\nvideo action recognition. Since the GCP method requires the\nvery accurate matrix square root [4], our MTP-Lya cannot\nachieve reasonable performances due to the relatively large\napproximation error. Therefore, we do not take it into account\nfor comparison throughout the GCP experiments.\n5.4.1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='approximation error. Therefore, we do not take it into account\nfor comparison throughout the GCP experiments.\n5.4.1\nLarge-scale Visual Recognition\nFig. 8: Overview of the GCP network [2], [3], [4] for large-\nscale and ﬁne-grained visual recognition.\nFig. 8 displays the architecture of a typical GCP network.\nDifferent from the standard CNNs, the covariance square\nroot of the last convolutional feature is used as the global\nrepresentation. Considering the ﬁnal convolutional feature\nX∈RB×C×HW , a GCP meta-layer ﬁrst computes the sample\ncovariance as:\nP = X¯IXT , ¯I = 1\nN (I − 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='covariance as:\nP = X¯IXT , ¯I = 1\nN (I − 1\nN 11T )\n(34)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n9\nTABLE 4: Validation error of ZCA whitening methods. The covariance matrix is of size 1×64×64. The time consumption is\nmeasured for computing the inverse square root (BP+FP). For each method, we report the results based on ﬁve runs.\nMethods\nTime (ms)\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std\nmin\nmean±std\nmin\nmean±std\nmin\nSVD-Clip\n3.37\n4.88±0.25\n4.65\n21.60±0.39\n21.19\n20.50±0.33\n20.17\nSVD-PI (GPU)\n5.27\n4.57±0.10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='SVD-PI (GPU)\n5.27\n4.57±0.10\n4.45\n21.35±0.25\n21.05\n19.97±0.41\n19.27\nSVD-PI\n3.49\n4.59±0.09\n4.44\n21.39±0.23\n21.04\n19.94±0.44\n19.28\nSVD-Taylor\n3.41', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='19.94±0.44\n19.28\nSVD-Taylor\n3.41\n4.50±0.08\n4.40\n21.14±0.20\n20.91\n19.81±0.24\n19.26\nSVD-Pad´e\n3.39\n4.65±0.11\n4.50\n21.41±0.15\n21.26\n20.25±0.23\n19.98', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='21.26\n20.25±0.23\n19.98\nNS Iteration\n2.96\n4.57±0.15\n4.37\n21.24±0.20\n21.01\n19.39±0.30\n19.01\nOur MPA-Lya\n2.61\n4.39±0.09\n4.25\n21.11±0.12\n20.95\n19.55±0.20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='21.11±0.12\n20.95\n19.55±0.20\n19.24\nOur MTP-Lya\n2.56\n4.49±0.13\n4.31\n21.42±0.21\n21.24\n20.55±0.37\n20.12\nwhere ¯I represents the centering matrix, I denotes the identity\nmatrix, and 1 is a column vector whose values are all ones,\nrespectively. Afterwards, the matrix square root is conducted\nfor normalization:\nQ ≜ P', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='respectively. Afterwards, the matrix square root is conducted\nfor normalization:\nQ ≜ P\n1\n2 = (UΛUT )\n1\n2 = UΛ\n1\n2 UT\n(35)\nwhere the normalized covariance matrix Q is fed to the FC\nlayer. Our method is applied to calculate Q.\nTABLE 5: Comparison of validation accuracy (%) on Im-\nageNet [49] and ResNet-50 [47]. The covariance is of size\n256×256×256, and the time consumption is measured for\ncomputing the matrix square root (FP+BP).\nMethods\nTime (ms)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computing the matrix square root (FP+BP).\nMethods\nTime (ms)\nTop-1 Acc.\nTop-5 Acc.\nSVD-Taylor\n2349.12\n77.09\n93.33\nSVD-Pad´e\n2335.56\n77.33\n93.49\nNS iteration\n164.43\n77.19\n93.40\nOur MPA-Lya\n110.61\n77.13\n93.45', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Our MPA-Lya\n110.61\n77.13\n93.45\nTable 5 presents the speed comparison and the validation\nerror of GCP ResNet-50 [47] models on ImageNet [49]. Our\nMPA-Lya not only achieves very competitive performance\nbut also has the least time consumption. The speed of our\nmethod is about 21X faster than the SVD and 1.5X faster\nthan the NS iteration.\n5.4.2\nFine-grained Visual Recognition\nTABLE 6: Comparison of validation accuracy on ﬁne-grained\nbenchmarks and ResNet-50 [47]. The covariance is of size', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='TABLE 6: Comparison of validation accuracy on ﬁne-grained\nbenchmarks and ResNet-50 [47]. The covariance is of size\n10×64×64, and the time consumption is measured for\ncomputing the matrix square root (FP+BP).\nMethods\nTime (ms)\nBirds\nAircrafts\nCars\nSVD-Taylor\n32.13\n86.9\n89.9\n92.3\nSVD-Pad´e\n31.54\n87.2\n90.5\n92.8\nNS iteration', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='87.2\n90.5\n92.8\nNS iteration\n5.79\n87.3\n89.5\n91.7\nOur MPA-Lya\n3.89\n87.8\n91.0\n92.5\nIn line with other GCP works [2], [3], [4], after training on\nImageNet, the model is subsequently ﬁne-tuned on each ﬁne-\ngrained dataset. Table 6 compares the time consumption and\nvalidation accuracy on three commonly used ﬁne-grained\nbenchmarks, namely Caltech University Birds (Birds) [50],', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='validation accuracy on three commonly used ﬁne-grained\nbenchmarks, namely Caltech University Birds (Birds) [50],\nFGVC Aircrafts (Aircrafts) [51], and Stanford Cars (Cars) [52].\nAs can be observed, our MPA-Lya consumes 50% less time\nthan the NS iteration and is about 8X faster than the SVD.\nMoreover, the performance of our method is slightly better\nthan other baselines on Birds [50] and Aircrafts [51]. The\nevaluation result on Cars [52] is also comparable.\n5.4.3\nVideo Action Recognition\nFig. 9: Architecture of the temporal-attentive GCP network\nfor video action recognition [6]. The channel and spatial', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Video Action Recognition\nFig. 9: Architecture of the temporal-attentive GCP network\nfor video action recognition [6]. The channel and spatial\nattention is used to make the covariance more attentive.\nBesides the application of image recognition, the GCP\nmethods can be also used for the task of video recognition [6].\nFig. 9 displays the overview of the temporal-attentive GCP\nmodel for video action recognition. The temporal covariance\nis computed in a sliding window manner by involving both\nintra- and inter-frame correlations. Supposing the kernel\nsize of the sliding window is 3, then temporal covariance is\ncomputed as:\nTemp.Cov. (Xl) = Xl−1XT\nl−1 + XlXT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='computed as:\nTemp.Cov. (Xl) = Xl−1XT\nl−1 + XlXT\nl + Xl+1XT\nl+1\n|\n{z\n}\nintra−frame covariance\n+ Xl−1XT\nl + XlXT\nl−1 + · · · + Xl+1XT\nl\n|\n{z\n}\ninter−frame covariance\n(36)\nFinally, the matrix square root of the attentive temporal-\nbased covariance is computed and passed to the FC layer.\nThe spectral methods are used to compute the matrix square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='based covariance is computed and passed to the FC layer.\nThe spectral methods are used to compute the matrix square\nroot of the attentive covariance Temp.Cov. (Xl).\nWe present the validation accuracy and time cost for the\nvideo action recognition in Table 7. For the computation\nspeed, our MPA-Lya is about 1.74X faster than the NS itera-\ntion and is about 10.82X faster than the SVD. Furthermore,\nour MPA-Lya achieves the best performance on HMDB51,\nwhile the result on UCF101 is also very competitive.\nTo sum up, our MPA-Lya has demonstrated its general ap-\nplicability in the GCP models for different tasks. In particular,\nwithout the sacriﬁce of performance, our method can bring', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='plicability in the GCP models for different tasks. In particular,\nwithout the sacriﬁce of performance, our method can bring\nconsiderable speed improvements. This could be beneﬁcial\nfor faster training and inference. In certain experiments\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n10\nTABLE\n7:\nValidation\ntop-1/top-5\naccuracy\n(%)\non\nHMBD51 [53] and UCF101 [54] with backbone TEA R50 [55].\nThe covariance matrix is of size 16×128×128, and the time\nconsumption is measured for computing the matrix square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='The covariance matrix is of size 16×128×128, and the time\nconsumption is measured for computing the matrix square\nroot (BP+FP).\nMethods\nTime (ms)\nHMBD51\nUCF101\nSVD-Taylor\n76.17\n73.79/93.84\n95.00/99.60\nSVD-Pad´e\n75.25\n73.89/93.79\n94.13/99.47\nNS Iteration\n12.11', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='94.13/99.47\nNS Iteration\n12.11\n72.75/93.86\n94.16/99.50\nOur MPA-Lya\n6.95\n74.05/93.99\n94.24/99.58\nsuch as ﬁne-grained classiﬁcation, the approximate methods\n(MPA-Lya and NS iteration) can marginally outperform\naccurate SVD. This phenomenon has been similarly observed\nin related studies [3], [4], [9], and one likely reason is that the\nSVD does not have as healthy gradients as the approximate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='in related studies [3], [4], [9], and one likely reason is that the\nSVD does not have as healthy gradients as the approximate\nmethods. This might negatively inﬂuence the optimization\nprocess and consequently the performance would degrade.\n5.5\nNeural Style Transfer\nFig. 10: The architecture overview of our model for neural\nstyle transfer. Two encoders take input of the style and\ncontent image respectively, and generate the multi-scale\ncontent/style features. A decoder is applied to absorb the\nfeature and perform the WCT process at 5 different scales,\nwhich outputs a pair of images that exchange the styles.\nFinally, a discriminator is further adopted to tell apart the\nauthenticity of the images.\nWe adopt the WCT process in the network architecture', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Finally, a discriminator is further adopted to tell apart the\nauthenticity of the images.\nWe adopt the WCT process in the network architecture\nproposed in Cho et al. [14] for neural style transfer. Fig. 10\ndisplays the overview of the model. The WCT performs\nsuccessive whitening and coloring transform on the content\nand style feature. Consider the reshaped content feature\nXc∈RB×C×HW and the style feature Xs∈RB×C×HW . The\nstyle information is ﬁrst removed from the content as:\nXwhitened\nc\n=\n\x10\n(Xc − µ(Xc))(Xc − µ(Xc))T \x11− 1\n2 Xc', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='\x10\n(Xc − µ(Xc))(Xc − µ(Xc))T \x11− 1\n2 Xc\n(37)\nThen we extract the desired style information from the style\nfeature Xs and transfer it to the whitened content feature:\nXcolored\nc\n=\n\x10\n(Xs−µ(Xs))(Xs−µ(Xs))T \x11 1\n2 Xwhitened\nc\n(38)\nThe resultant feature Xcolored\nc\nis compensated with the mean\nof style feature and combined with the original content\nfeature:\nX = α(Xcolored', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='is compensated with the mean\nof style feature and combined with the original content\nfeature:\nX = α(Xcolored\nc\n+ µ(Xs)) + (1 − α)Xc\n(39)\nwhere α is a weight bounded in [0, 1] to control the strength\nof style transfer. In this experiment, both the matrix square\nroot and inverse square root are computed.\nTABLE 8: The LPIPS [56] score and user preference (%) on\nArtworks [57] dataset. The covariance is of size 4×256×256.\nWe measure the time consumption of whitening and coloring\ntransform that is conducted 10 times to exchange the style\nand content feature at different network depths.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='We measure the time consumption of whitening and coloring\ntransform that is conducted 10 times to exchange the style\nand content feature at different network depths.\nMethods Time (ms) LPIPS [56] (↑) Preference (↑)\nSVD-Taylor\n447.12\n0.5276\n16.25\nSVD-Pad´e\n445.23\n0.5422\n19.25\nNS iteration\n94.37\n0.5578\n17.00\nOur MPA-Lya\n69.23', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0.5578\n17.00\nOur MPA-Lya\n69.23\n0.5615\n24.75\nOur MTP-Lya\n40.97\n0.5489\n18.50\nTable 8 presents the quantitative evaluation using the\nLPIPS [56] score and user preference. The speed of our MPA-\nLya and MTP-Lya is signiﬁcantly faster than other methods.\nSpeciﬁcally, our MTP-Lya is 2.3X faster than the NS iteration\nand 10.9X faster than the SVD, while our MPA-Lya consumes', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='and 10.9X faster than the SVD, while our MPA-Lya consumes\n1.4X less time than the NS iteration and 6.4X less time than\nthe SVD. Moreover, our MPA-Lya achieves the best LPIPS\nscore and user preference. The performance of our MTP-\nLya is also very competitive. Fig. 11 displays the exemplary\nvisual comparison. Our methods can effectively transfer the\nstyle information and preserve the original content, leading\nto transferred images with a more coherent style and better\nvisual appeal. We give detailed evaluation results on each\nsubset and more visual examples in Supplementary Material.\nFig. 11: Visual examples of the neural style transfer on\nArtworks [57] dataset. Our methods generate sharper images\nwith more coherent style and better visual appeal. The red', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Artworks [57] dataset. Our methods generate sharper images\nwith more coherent style and better visual appeal. The red\nrectangular indicates regions with subtle details.\n5.6\nSecond-order Vision Transformer\nThe ordinary vision transformer [31] attaches an empty\nclass token to the sequence of visual tokens and only uses\nthe class token for prediction, which may not exploit the\nrich semantics embedded in the visual tokens. Instead, The\nSecond-order Vision Transformer (So-ViT) [5] proposes to\nleverage the high-level visual tokens to assist the task of\nclassiﬁcation:\ny = FC(c) + FC\n\x10\n(XXT )\n1\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='y = FC(c) + FC\n\x10\n(XXT )\n1\n2\n\x11\n(40)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n11\nTABLE 9: Validation top-1/top-5 accuracy of the second-order vision transformer on ImageNet [49]. The covariance is of size\n64×48×48, where 64 is the mini-batch size. The time cost is measured for computing the matrix square root (BP+FP).\nMethods\nTime (ms)\nArchitecture\nSo-ViT-7\nSo-ViT-10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Methods\nTime (ms)\nArchitecture\nSo-ViT-7\nSo-ViT-10\nSo-ViT-14\nPI\n1.84\n75.93/93.04\n77.96/94.18\n82.16/96.02 (303 epoch)\nSVD-PI\n83.43\n76.55/93.42\n78.53/94.40\n82.16/96.01 (278 epoch)\nSVD-Taylor\n83.29', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='82.16/96.01 (278 epoch)\nSVD-Taylor\n83.29\n76.66/93.52\n78.64/94.49\n82.15/96.02 (271 epoch)\nSVD-Pad´e\n83.25\n76.71/93.49\n78.77/94.51\n82.17/96.02 (265 epoch)\nNS Iteration\n10.38\n76.50/93.44\n78.50/94.44', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='10.38\n76.50/93.44\n78.50/94.44\n82.16/96.01 (280 epoch)\nOur MPA-Lya\n3.25\n76.84/93.46\n78.83/94.58\n82.17/96.03 (254 epoch)\nOur MTP-Lya\n2.39\n76.46/93.26\n78.44/94.33\n82.16/96.02 (279 epoch)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='78.44/94.33\n82.16/96.02 (279 epoch)\nFig. 12: The scheme of So-ViT [5]. The covariance square root\nof the visual tokens are computed to assist the classiﬁcation.\nIn the original vision transformer [31], only the class token is\nutilized for class predictions.\nwhere c is the output class token, X denotes the visual token,\nand y is the combined class predictions. We show the model\noverview in Fig. 12. Equipped with the covariance pooling\nlayer, So-ViT removes the need for pre-training on the ultra-\nlarge-scale datasets and achieves competitive performance\neven when trained from scratch. To reduce the computational', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='large-scale datasets and achieves competitive performance\neven when trained from scratch. To reduce the computational\nbudget, So-ViT further proposes to use Power Iteration (PI) to\napproximate the dominant eigenvector. We use our methods\nto compute the matrix square root of the covariance XXT .\nTable 9 compares the speed and performances on three\nSo-ViT architectures with different depths. Our proposed\nmethods signiﬁcantly outperform the SVD and NS iteration\nin terms of speed. To be more speciﬁc, our MPA-Lya is 3.19X\nfaster than the NS iteration and 25.63X faster than SVD-Pad´e,\nand our MTP-Lya is 4.34X faster than the NS iteration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='and our MTP-Lya is 4.34X faster than the NS iteration and\n34.85X faster than SVD-Pad´e. For the So-ViT-7 and So-ViT-10,\nour MPA-Lya achieves the best evaluation results and even\nslightly outperforms the SVD-based methods. Moreover, on\nthe So-ViT-14 model where the performances are saturated,\nour method converges faster and spends fewer training\nepochs. The performance of our MTP-Lya is also on par\nwith the other methods. The PI suggested in the So-ViT only\ncomputes the dominant eigenpair but neglects the rest. In\nspite of the fast speed, the performance is not comparable\nwith other methods.\n5.7', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='spite of the fast speed, the performance is not comparable\nwith other methods.\n5.7\nAblation Studies\nWe conduct three ablation studies to illustrate the impact\nof the degree of power series in the forward pass, the\ntermination criterion during the back-propagation, and the\npossibility of combining our Lyapunov solver with the SVD\nand the NS iteration.\n5.7.1\nDegree of Power series to Match for Forward Pass\nTable 10 displays the performance of our MPA-Lya for\ndifferent degrees of power series. As we use more terms\nof the power series, the approximation error gets smaller and\nthe performance gets steady improvements from the degree', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='different degrees of power series. As we use more terms\nof the power series, the approximation error gets smaller and\nthe performance gets steady improvements from the degree\n[3, 3] to [5, 5]. When the degree of our MPA is increased from\n[5, 5] to [6, 6], there are only marginal improvements. We\nhence set the forward degrees as [5, 5] for our MPA and as\n11 for our MTP as a trade-off between speed and accuracy.\nTABLE 10: Performance of our MPA-Lya versus different\ndegrees of power series to match.\nDegrees Time (ms)\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std min mean±std min mean±std min\n[3, 3]\n0.80\n4.64±0.11 4.54 21.35±0.18 21.20 20.14±0.43 19.56\n[4, 4]\n0.86\n4.55±0.08 4.51 21.26±0.22 21.03 19.87±0.29 19.64\n[6, 6]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[6, 6]\n0.98\n4.45±0.07 4.33 21.09±0.14 21.04 19.51±0.24 19.26\n[5, 5]\n0.93\n4.39±0.09 4.25 21.11±0.12 20.95 19.55±0.20 19.24\n5.7.2\nTermination Criterion for Backward Pass\nTable 11 compares the performance of backward algo-\nrithms with different termination criteria as well as the\nexact solution computed by the Bartels-Steward algorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='rithms with different termination criteria as well as the\nexact solution computed by the Bartels-Steward algorithm\n(BS algorithm) [26]. Since the NS iteration has the prop-\nerty of quadratic convergence, the errors ||Bk−I||F and\n||0.5Ck − X||F decrease at a larger rate for more iteration\ntimes. When we iterate more than 7 times, the error becomes\nsufﬁciently neglectable, i.e., the NS iteration almost converges.\nMoreover, from 8 iterations to 9 iterations, there are no\nobvious performance improvements. We thus terminate the\niterations after iterating 8 times.\nThe exact gradient calculated by the BS algorithm does\nnot yield the best results. Instead, it only achieves the least', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='iterations after iterating 8 times.\nThe exact gradient calculated by the BS algorithm does\nnot yield the best results. Instead, it only achieves the least\nﬂuctuation on ResNet-50 and other results are inferior to\nour iterative solver. This is because the formulation of our\nLyapunov equation is based on the assumption that the\naccurate matrix square root is computed, but in practice we\nonly compute the approximate one in the forward pass. In\nthis case, calculating the accurate gradient of the approximate\nmatrix square root might not necessarily work better than the\napproximate gradient of the approximate matrix square root.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n12\nTABLE 11: Performance of our MPA-Lya versus different iteration times. The residual errors ||Bk−I|| and ||0.5Ck − X||F\nare measured based on 10, 000 randomly sampled matrices.\nMethods Time (ms) ||Bk−I||F ||0.5Ck−X||F\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std min mean±std min mean±std min\nBS algorithm\n2.34\n–\n–', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='mean±std min mean±std min mean±std min\nBS algorithm\n2.34\n–\n–\n4.57±0.10 4.45 21.20±0.23 21.01 19.60±0.16 19.55\n#iter 5\n1.14\n≈0.3541\n≈0.2049\n4.48±0.13 4.31 21.15±0.24 20.84 20.03±0.19 19.78\n#iter 6\n1.33\n≈0.0410', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='#iter 6\n1.33\n≈0.0410\n≈0.0231\n4.43±0.10 4.28 21.16±0.19 20.93 19.83±0.24 19.57\n#iter 7\n1.52\n≈7e−4\n≈3.5e−4\n4.45±0.11 4.29 21.18±0.20 20.95 19.69±0.20 19.38\n#iter 9\n1.83\n≈2e−7\n≈7e−6', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='#iter 9\n1.83\n≈2e−7\n≈7e−6\n4.40±0.07 4.28 21.08±0.15 20.89 19.52±0.22 19.25\n#iter 8\n1.62\n≈3e−7\n≈7e−6\n4.39±0.09 4.25 21.11±0.12 20.95 19.55±0.20 19.24\n5.7.3\nLyapunov Solver as A General Backward Algorithm\nWe note that our proposed iterative Lyapunov solver is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='5.7.3\nLyapunov Solver as A General Backward Algorithm\nWe note that our proposed iterative Lyapunov solver is a\ngeneral backward algorithm for computing the matrix square\nroot. That is to say, it should be also compatible with the\nSVD and NS iteration as the forward pass.\nFor the NS-Lya, our previous conference paper [22] shows\nthat the NS iteration used in [2], [21] cannot converge on any\ndatasets. In this extended manuscript, we found out that the\nunderlying reason is the inconsistency between the FP and\nBP. The NS iteration of [2], [21] is a coupled iteration that\nuse two variables Yk and Zk to compute the matrix square\nroot. For the BP algorithm, the NS iteration is deﬁned to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='use two variables Yk and Zk to compute the matrix square\nroot. For the BP algorithm, the NS iteration is deﬁned to\ncompute the matrix sign and only uses one variable Yk. The\nterm Zk is not involved in the BP and we have no control\nover the gradient back-propagating through it, which results\nin the non-convergence of the model. To resolve this issue,\nwe propose to change the forward coupled NS iteration to a\nvariant that uses one variable as:\nZk+1 = 1\n2(3Zk − Z3\nk\nA\n||A||F\n)\n(41)\nwhere Zk+1 converges to the inverse square root A− 1\n2 . This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content=')\n(41)\nwhere Zk+1 converges to the inverse square root A− 1\n2 . This\nvariant of NS iteration is often used to directly compute the\ninverse square root [9], [58]. The Z0 is initialization with\nI, and post-compensation is calculated as Zk =\n1\n√\n||A||F Zk.\nAlthough the modiﬁed NS iteration uses only one variable,\nwe note that it is an equivalent representation with the\nprevious NS iteration. More formally, we have:\nProposition 2. The one-variable NS iteration of [9], [58] is\nequivalent to the two-variable NS iteration of [1], [2], [21].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Proposition 2. The one-variable NS iteration of [9], [58] is\nequivalent to the two-variable NS iteration of [1], [2], [21].\nWe give the proof in the Supplementary Material. The\nmodiﬁed forward NS iteration is compatible with our iter-\native Lyapunov solver. Table 12 compares the performance\nof different methods that use the Lyapunov solver as the\nbackward algorithm. Both the SVD-Lya and NS-Lya achieve\ncompetitive performances.\nTABLE 12: Performance comparison of SVD-Lya and NS-Lya.\nMethods Time (ms)\nResNet-18\nResNet-50\nCIFAR10\nCIFAR100\nCIFAR100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='ResNet-50\nCIFAR10\nCIFAR100\nCIFAR100\nmean±std min mean±std min mean±std min\nSVD-Lya\n4.47\n4.45±0.16 4.20 21.24±0.24 21.02 19.41±0.11 19.26\nNS-Lya\n2.88\n4.51±0.14 4.34 21.16±0.17 20.94 19.65±0.35 19.39\nMPA-Lya\n2.61', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='MPA-Lya\n2.61\n4.39±0.09 4.25 21.11±0.12 20.95 19.55±0.20 19.24\nMTP-Lya\n2.46\n4.49±0.13 4.31 21.42±0.21 21.24 20.55±0.37 20.12\n6\nCONCLUSION\nIn this paper, we propose two fast methods to compute\nthe differentiable matrix square root and the inverse square\nroot. In the forward pass, the MTP and MPA are applied\nto approximate the matrix square root, while an iterative', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='root. In the forward pass, the MTP and MPA are applied\nto approximate the matrix square root, while an iterative\nLyapunov solver is proposed to solve the gradient function\nfor back-propagation. A number of numerical tests and com-\nputer vision applications demonstrate that our methods can\nachieve both the fast speed and competitive performances.\nREFERENCES\n[1]\nT.-Y. Lin and S. Maji, “Improved bilinear pooling with cnns,” BMVC,\n2017.\n[2]\nP. Li, J. Xie, Q. Wang, and W. Zuo, “Is second-order information\nhelpful for large-scale visual recognition?” in ICCV, 2017.\n[3]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='helpful for large-scale visual recognition?” in ICCV, 2017.\n[3]\nP. Li, J. Xie, Q. Wang, and Z. Gao, “Towards faster training of\nglobal covariance pooling networks by iterative matrix square root\nnormalization,” in CVPR, 2018.\n[4]\nY. Song, N. Sebe, and W. Wang, “Why approximate matrix square\nroot outperforms accurate svd in global covariance pooling?” in\nICCV, 2021.\n[5]\nJ. Xie, R. Zeng, Q. Wang, Z. Zhou, and P. Li, “So-vit: Mind visual', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[5]\nJ. Xie, R. Zeng, Q. Wang, Z. Zhou, and P. Li, “So-vit: Mind visual\ntokens for vision transformer,” arXiv preprint arXiv:2104.10935, 2021.\n[6]\nZ. Gao, Q. Wang, B. Zhang, Q. Hu, and P. Li, “Temporal-attentive\ncovariance pooling networks for video recognition,” in NeurIPS,\n2021.\n[7]\nY. Song, N. Sebe, and W. Wang, “On the eigenvalues of global\ncovariance pooling for ﬁne-grained visual recognition,” IEEE\nTPAMI, 2022.\n[8]', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='covariance pooling for ﬁne-grained visual recognition,” IEEE\nTPAMI, 2022.\n[8]\nL. Huang, D. Yang, B. Lang, and J. Deng, “Decorrelated batch\nnormalization,” in CVPR, 2018.\n[9]\nL. Huang, Y. Zhou, F. Zhu, L. Liu, and L. Shao, “Iterative normal-\nization: Beyond standardization towards efﬁcient whitening,” in\nCVPR, 2019.\n[10] L. Huang, L. Zhao, Y. Zhou, F. Zhu, L. Liu, and L. Shao, “An\ninvestigation into the stochasticity of batch whitening,” in CVPR,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='investigation into the stochasticity of batch whitening,” in CVPR,\n2020.\n[11] A. Siarohin, E. Sangineto, and N. Sebe, “Whitening and coloring\nbatch transform for gans,” in ICLR, 2018.\n[12] A. Ermolov, A. Siarohin, E. Sangineto, and N. Sebe, “Whitening for\nself-supervised representation learning,” in ICML, 2021.\n[13] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang, “Universal\nstyle transfer via feature transforms,” in NeurIPS, 2017.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='style transfer via feature transforms,” in NeurIPS, 2017.\n[14] W. Cho, S. Choi, D. K. Park, I. Shin, and J. Choo, “Image-to-\nimage translation via group-wise deep whitening-and-coloring\ntransformation,” in CVPR, 2019.\n[15] S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, and J. Choo, “Robustnet:\nImproving domain generalization in urban-scene segmentation via\ninstance selective whitening,” in CVPR, 2021.\n[16] C. Ionescu, O. Vantzos, and C. Sminchisescu, “Training deep\nnetworks with structured layers by matrix backpropagation,” arXiv', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='networks with structured layers by matrix backpropagation,” arXiv\npreprint arXiv:1509.07838, 2015.\n[17] W.\nWang,\nZ.\nDang,\nY.\nHu,\nP.\nFua,\nand\nM.\nSalzmann,\n“Backpropagation-friendly eigendecomposition,” in NeurIPS, 2019.\n[18] ——, “Robust differentiable svd,” TPAMI, 2021.\n[19] S. Lahabar and P. Narayanan, “Singular value decomposition on', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[19] S. Lahabar and P. Narayanan, “Singular value decomposition on\ngpu using cuda,” in 2009 IEEE International Symposium on Parallel\n& Distributed Processing.\nIEEE, 2009, pp. 1–10.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n13\n[20] G. Schulz, “Iterative berechung der reziproken matrix,” ZAMM-\nJournal of Applied Mathematics and Mechanics/Zeitschrift f¨ur Ange-\nwandte Mathematik und Mechanik, vol. 13, no. 1, pp. 57–59, 1933.\n[21] N. J. Higham, Functions of matrices: theory and computation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[21] N. J. Higham, Functions of matrices: theory and computation.\nSIAM,\n2008.\n[22] Y. Song, N. Sebe, and W. Wang, “Fast differentiable matrix square\nroot,” in ICLR, 2022.\n[23] C. Ionescu, O. Vantzos, and C. Sminchisescu, “Matrix backpropa-\ngation for deep networks with structured layers,” in ICCV, 2015.\n[24] Z. Dang, K. M. Yi, Y. Hu, F. Wang, P. Fua, and M. Salzmann,\n“Eigendecomposition-Free Training of Deep Networks with Zero\nEigenvalue-Based Losses,” in ECCV, 2018.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='“Eigendecomposition-Free Training of Deep Networks with Zero\nEigenvalue-Based Losses,” in ECCV, 2018.\n[25] Z. Dang, K. Yi, F. Wang, Y. Hu, P. Fua, and M. Salzmann,\n“Eigendecomposition-Free Training of Deep Networks for Linear\nLeast-Square Problems,” TPAMI, 2020.\n[26] R. H. Bartels and G. W. Stewart, “Solution of the matrix equation\nax+ xb= c [f4],” Communications of the ACM, vol. 15, no. 9, pp.\n820–826, 1972.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='820–826, 1972.\n[27] T.-Y. Lin, A. RoyChowdhury, and S. Maji, “Bilinear cnn models for\nﬁne-grained visual recognition,” in ICCV, 2015.\n[28] Q. Wang, P. Li, Q. Hu, P. Zhu, and W. Zuo, “Deep global generalized\ngaussian networks,” in CVPR, 2019.\n[29] Q. Wang, J. Xie, W. Zuo, L. Zhang, and P. Li, “Deep cnns meet\nglobal covariance pooling: Better representation and generalization,”\nTPAMI, 2020.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='global covariance pooling: Better representation and generalization,”\nTPAMI, 2020.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017.\n[31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al., “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” in ICLR, 2020.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='et al., “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” in ICLR, 2020.\n[32] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in ICML,\n2015.\n[33] X. Pan, X. Zhan, J. Shi, X. Tang, and P. Luo, “Switchable whitening\nfor deep representation learning,” in ICCV, 2019.\n[34] L. Huang, Y. Zhou, L. Liu, F. Zhu, and L. Shao, “Group whitening:\nBalancing learning efﬁciency and representational capacity,” in\nCVPR, 2021.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Balancing learning efﬁciency and representational capacity,” in\nCVPR, 2021.\n[35] S. Zhang, E. Nezhadarya, H. Fashandi, J. Liu, D. Graham, and\nM. Shah, “Stochastic whitening batch normalization,” in CVPR,\n2021.\n[36] Y. Cho, H. Cho, Y. Kim, and J. Kim, “Improving generalization\nof batch whitening by convolutional unit optimization,” in ICCV,\n2021.\n[37] Y. Li, M.-Y. Liu, X. Li, M.-H. Yang, and J. Kautz, “A closed-form\nsolution to photorealistic image stylization,” in ECCV, 2018.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='solution to photorealistic image stylization,” in ECCV, 2018.\n[38] Z. Wang, L. Zhao, H. Chen, L. Qiu, Q. Mo, S. Lin, W. Xing,\nand D. Lu, “Diversiﬁed arbitrary style transfer via deep feature\nperturbation,” in CVPR, 2020.\n[39] A. Abramov, C. Bayer, and C. Heller, “Keep it simple: Im-\nage statistics matching for domain adaptation,” arXiv preprint\narXiv:2005.12551, 2020.\n[40] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Improved texture\nnetworks: Maximizing quality and diversity in feed-forward', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='networks: Maximizing quality and diversity in feed-forward\nstylization and texture synthesis,” in CVPR, 2017.\n[41] Q. Sun, Z. Zhang, and P. Li, “Second-order encoding networks for\nsemantic segmentation,” Neurocomputing, 2021.\n[42] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, “Second-order\nattention network for single image super-resolution,” in CVPR,\n2019.\n[43] W. Van Assche, “Pad´e and hermite-pad´e approximation and\northogonality,” arXiv preprint math/0609094, 2006.\n[44] J. D. Roberts, “Linear model reduction and solution of the algebraic', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='[44] J. D. Roberts, “Linear model reduction and solution of the algebraic\nriccati equation by use of the sign function,” International Journal of\nControl, vol. 32, no. 4, pp. 677–687, 1980.\n[45] C. S. Kenney and A. J. Laub, “The matrix sign function,” IEEE\ntransactions on automatic control, vol. 40, no. 8, pp. 1330–1348, 1995.\n[46] P. Benner, E. S. Quintana-Ort´ı, and G. Quintana-Ort´ı, “Solving\nstable sylvester equations via rational iterative schemes,” Journal of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='stable sylvester equations via rational iterative schemes,” Journal of\nScientiﬁc Computing, vol. 28, no. 1, pp. 51–83, 2006.\n[47] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in CVPR, 2016.\n[48] A. Krizhevsky, “Learning multiple layers of features from tiny\nimages,” Master’s thesis, University of Tront, 2009.\n[49] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in CVPR, 2009.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A large-scale hierarchical image database,” in CVPR, 2009.\n[50] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie,\nand P. Perona, “Caltech-UCSD Birds 200,” California Institute of\nTechnology, Tech. Rep. CNS-TR-2010-001, 2010.\n[51] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi,\n“Fine-grained visual classiﬁcation of aircraft,” arXiv preprint\narXiv:1306.5151, 2013.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='“Fine-grained visual classiﬁcation of aircraft,” arXiv preprint\narXiv:1306.5151, 2013.\n[52] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object represen-\ntations for ﬁne-grained categorization,” in 4th International IEEE\nWorkshop on 3D Representation and Recognition (3dRR-13), Sydney,\nAustralia, 2013.\n[53] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, “HMDB:\na large video database for human motion recognition,” in ICCV,\n2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='a large video database for human motion recognition,” in ICCV,\n2011.\n[54] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101\nhuman actions classes from videos in the wild,” arXiv preprint\narXiv:1212.0402, 2012.\n[55] Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, and L. Wang, “Tea: Temporal\nexcitation and aggregation for action recognition,” in CVPR, 2020.\n[56] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The\nunreasonable effectiveness of deep features as a perceptual metric,”', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='unreasonable effectiveness of deep features as a perceptual metric,”\nin CVPR, 2018.\n[57] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image\ntranslation with conditional adversarial networks,” in CVPR, 2017.\n[58] D. A. Bini, N. J. Higham, and B. Meini, “Algorithms for the matrix\np th root,” Numerical Algorithms, vol. 39, no. 4, pp. 349–378, 2005.\n[59] G. A. Baker and J. L. Gammel, The Pad´e approximant in theoretical\nphysics.\nAcademic Press, 1970.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='physics.\nAcademic Press, 1970.\n[60] H. Stahl, “Spurious poles in pad´e approximation,” Journal of\ncomputational and applied mathematics, vol. 99, no. 1-2, pp. 511–527,\n1998.\n[61] G. A. Baker, “Defects and the convergence of pad´e approximants,”\nActa Applicandae Mathematica, vol. 61, no. 1, pp. 37–52, 2000.\nYue Song received the B.Sc. cum laude from KU\nLeuven, Belgium and the joint M.Sc. summa cum\nlaude from the University of Trento, Italy and KTH\nRoyal Institute of Technology, Sweden. Currently,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='laude from the University of Trento, Italy and KTH\nRoyal Institute of Technology, Sweden. Currently,\nhe is a Ph.D. student with the Multimedia and\nHuman Understanding Group (MHUG) at the Uni-\nversity of Trento, Italy. His research interests are\ncomputer vision, deep learning, and numerical\nanalysis and optimization.\nNicu Sebe is Professor with the University of\nTrento, Italy, leading the research in the areas\nof multimedia information retrieval and human\nbehavior understanding. He was the General\nCo- Chair of ACM Multimedia 2013, and the\nProgram Chair of ACM Multimedia 2007 and\n2011, ECCV 2016, ICCV 2017 and ICPR 2020.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2011, ECCV 2016, ICCV 2017 and ICPR 2020.\nHe is a fellow of the International Association for\nPattern Recognition.\nWei Wang is an Assistant Professor of Computer\nScience at University of Trento, Italy. Previously,\nafter obtaining his PhD from University of Trento\nin 2018, he became a Postdoc at EPFL, Switzer-\nland. His research interests include machine\nlearning and its application to computer vision\nand multimedia analysis.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n14\nAPPENDIX A\nSUMMARY OF ALGORITHM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='14\nAPPENDIX A\nSUMMARY OF ALGORITHM\nAlgorithm. 1 and Algorithm. 2 summarize the forward pass\n(FP) and the backward pass (BP) of our proposed methods,\nrespectively. The hyper-parameter K in Algorithm. 1 means\nthe degrees of power series, and T in Algorithm. 2 denotes\nthe iteration times.\nAlgorithm 1: FP of our MTP and MPA for the matrix\nsquare root and the inverse square root.\nInput: A and K\nOutput: A\n1\n2 or A− 1\n2\nif MTP then\n// FP method is MTP\nif Matrix Square Root then', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\nif MTP then\n// FP method is MTP\nif Matrix Square Root then\nA\n1\n2 ←I− PK\nk=1\n1\n2\nk\n! (I −\nA\n||A||F )k;\nelse\nA− 1\n2 ←I+ P∞\nk=1\n− 1\n2\nk\n! (I −\nA\n||A||F )k\nend\nelse\n// FP method is MPA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n||A||F )k\nend\nelse\n// FP method is MPA\nM← K−1\n2\n, N← K−1\n2\n;\nPM←I− PM\nm=1 pm(I −\nA\n||A||F )m;\nQN←I− PN\nn=1 qn(I −\nA\n||A||F )n;\nif Matrix Square Root then\nA\n1\n2 ←Q−1\nN PM;\nelse\nA− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 ←Q−1\nN PM;\nelse\nA− 1\n2 ←P−1\nM QN;\nend\nend\nif Matrix Square Root then\nPost-compensate A\n1\n2 ←\np\n||A||F · A\n1\n2\nelse\nPost-compensate A− 1\n2 ←\n1\n√\n||A||F · A− 1\n2\nend\nAPPENDIX B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='√\n||A||F · A− 1\n2\nend\nAPPENDIX B\nTHEORETICAL DERIVATION AND PROOF\nB.1\nIterative Lyapunov Function Solver\nLemma 1 (Matrix Sign Function [21]). For a given matrix\nH with no eigenvalues on the imaginary axis, its sign function\nhas the following properties: 1) sign(H)2 = I; 2) if H has the\nJordan decomposition H=TMT−1, then its sign function satisﬁes\nsign(H)=Tsign(M)T−1.\nProof. The ﬁrst property is easy to prove. Consider the SVD\nof USVT = H. As the sign depends on the positiveness of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Proof. The ﬁrst property is easy to prove. Consider the SVD\nof USVT = H. As the sign depends on the positiveness of\nthe eigenvale, the square of sign function is computed as:\nsign(H)2 = sign(S)2\n(42)\nSince all eigenvalues are real, we have sign(S)2=I, and the\nﬁrst property is proved. The alternative deﬁnition of matrix\nsign function is given by:\nsign(H) = H(H2)− 1\n2\n(43)\nAlgorithm 2: BP of our Lyapunov solver for the\nmatrix square root and the inverse square root.\nInput:\n∂l\n∂A', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='matrix square root and the inverse square root.\nInput:\n∂l\n∂A\n1\n2 or\n∂l\n∂A− 1\n2 , A\n1\n2 or A− 1\n2 , and T\nOutput:\n∂l\n∂A\nif Matrix Square Root then\nB0←A\n1\n2 , C0←\n∂l\n∂A\n1\n2 , i←0 ;\nelse\nB0←A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='∂A\n1\n2 , i←0 ;\nelse\nB0←A− 1\n2 , C0← − A−1\n∂l\n∂A− 1\n2 A−1, i←0;\nend\nNormalize B0←\nB0\n||B0||F , C0←\nC0\n||B0||F ;\nwhile i < T do\n// Coupled iteration\nBk+1← 1\n2Bk(3I − B2\nk) ;\nCk+1← 1\n2\n\x10', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k) ;\nCk+1← 1\n2\n\x10\n− B2\nkCk + BkCkBk + Ck(3I − B2\nk)\n\x11\n;\ni←i + 1;\nend\n∂l\n∂A← 1\n2Ck ;\nInjecting sign(H)=Tsign(M)T−1 into the above equation\nleads to\nsign(H) = TMT−1(TM2T)− 1\n2\n= TMT−1Tsign(M)M−1T−1\n= Tsign(M)T−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2\n= TMT−1Tsign(M)M−1T−1\n= Tsign(M)T−1\n(44)\nThe second property gets proved.\nNow we switch how to derive the iterative solver for\nmatrix sign function in detail. Lemma 1.1 shows that sign(H)\nis the matrix square root of the identity matrix. We use the\nNewton-Schulz iteration to compute sign(H) as:\nHk+1= = 1\n2Hk(3I − H2\nk)\n=1\n2\n\x14Bk(3I−B2\nk)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='k)\n=1\n2\n\x14Bk(3I−B2\nk)\n3Ck − Bk(BkCk−CkBk)−CkB2\nk\n0\n−Bk(3I−B2\nk)\n\x15\n(45)\nLemma 1.2 indicates an alternative approach to compute the\nsign function as:\nsign(H) = sign\n\x10 \x14B\nC\n0\n−B\n\x15 \x11\n=\n\x14I\nX\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='−B\n\x15 \x11\n=\n\x14I\nX\n0\nI\n\x15\nsign\n\x10 \x14B\n0\n0\n−B\n\x15 \x11 \x14I\nX\n0\nI\n\x15−1\n=\n\x14I\nX\n0\nI\n\x15 \x14I\n0\n0\n−I\n\x15 \x14I\n−X\n0', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0\n0\n−I\n\x15 \x14I\n−X\n0\nI\n\x15\n=\n\x14I\n2X\n0\n−I\n\x15\n(46)\nThe above two equations deﬁne the coupled iterations and\nthe convergence.\nB.2\nEquivalence of two sets of MPA\nProposition 1. The diagonal MPA\n1\n√\n||A||F S−1\nN RM is equivalent\nto the diagonal MPA\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='√\n||A||F S−1\nN RM is equivalent\nto the diagonal MPA\n1\n√\n||A||F P−1\nM QN, and the relation pm=−sn\nand qn= − rm hold for any m=n.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n15\nProof. Though Pad´e approximants are derived out of a ﬁnite\nTaylor series, they are asymptotic to their inﬁnite Taylor\nseries [43]. Let f(z)=(1 − z)\n1\n2 and f(z)−1=(1 − z)− 1\n2 . We', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 and f(z)−1=(1 − z)− 1\n2 . We\nhave the relation:\n1 + PM\nm=1 rmzm\n1 + PN\nn=1 snzn = f(z)−1 + R(zM+N+1)\n1 − PM\nm=1 pmzm\n1 − PN\nn=1 qnzn = f(z) + R(zM+N+1)\n(47)\nwhere R(zM+N+1) is the discarded higher-order term. Since\nf(z) =\n1\nf(z)−1 , we have:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='f(z) =\n1\nf(z)−1 , we have:\n1 + PM\nm=1 rmzm\n1 + PN\nn=1 snzn = 1 − PN\nn=1 qnzn\n1 − PM\nm=1 pmzm .\n(48)\nNow we have two sets of Pad´e approximants at both sides.\nSince the numerator and denominator of Pad´e approximants\nare relatively prime to each other by deﬁnition [59], the two\nsets of Pad´e approximants are equivalent and we have:\npm = −sn, qn = −rm\n(49)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='sets of Pad´e approximants are equivalent and we have:\npm = −sn, qn = −rm\n(49)\nGeneralized to the matrix case, this leads to:\nPM = SN, QN = RM.\n(50)\nTherefore, we also have S−1\nN RM=P−1\nM QN. The two sets of\nMPA are actually the same representation when m=n.\nB.3\nEquivalence of Newton-Schulz Iteration\nProposition 2. The one-variable NS iteration of [9], [58] is\nequivalent to the two-variable NS iteration of [1], [2], [21].\nProof. For the two-variable NS iteration, the coupled iteration\nis computed as:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Proof. For the two-variable NS iteration, the coupled iteration\nis computed as:\nYk+1 = 1\n2Yk(3I − ZkYk), Zk+1 = 1\n2(3I − ZkYk)Zk (51)\nwhere Yk and Zk converge to A\n1\n2 and A− 1\n2 , respectively.\nThe two variables are initialized as Y0=\nA\n||A||F and Z0=I.\nSince the two variables have the relation Z−1\nk Yk=\nA\n||A||F ,\nwe can replace Yk in eq. (51) with Zk\nA', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='A\n||A||F ,\nwe can replace Yk in eq. (51) with Zk\nA\n||A||F :\nZk+1 = 1\n2(3I − Z2\nk\nA\n||A||F\n)Zk\n(52)\nNotice that A and Zk have the same eigenspace and their\nmatrix product commutes, i.e., AZk=ZkA. Therefore, the\nabove equation can be further simpliﬁed as:\nZk+1 = 1\n2(3Zk − Z3\nk\nA\n||A||F\n)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2(3Zk − Z3\nk\nA\n||A||F\n)\n(53)\nAs indicated above, the two seemingly different NS iterations\nare in essence equivalent.\nAPPENDIX C\nBASELINES\nIn the experiment section, we compare our proposed two\nmethods with the following baselines:\nPower Iteration (PI). It is suggested in the original So-ViT\nto compute only the dominant eigenpair.\nSVD-PI [17] that uses PI to compute the gradients of SVD.\nSVD-Taylor [4], [18] that applies the Taylor polynomial to\napproximate the gradients.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='SVD-Taylor [4], [18] that applies the Taylor polynomial to\napproximate the gradients.\nSVD-Pad´e [4] that proposes to closely approximate the\nSVD gradients using Pad´e approximants. Notice that our\nMTP/MPA used in the FP is fundamentally different from\nthe Taylor polynomial or Pad´e approximants used in the\nBP of SVD-Pad´e. For our method, we use Matrix Taylor\nPolynomial (MTP) and Matrix Pad´e Approximants (MPA)\nto derive the matrix square root in the FP. For the SVD-\nPad´e, they use scalar Taylor polynomial and scalar Pad´e\napproximants to approximate the gradient\n1\nλi−λj in the BP.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='approximants to approximate the gradient\n1\nλi−λj in the BP.\nThat is to say, their aim is to use the technique to compute\nthe gradient and this will not involve the back-propagation\nof Taylor polynomial or Pad´e approximants.\nNS iteration [20], [21] that uses the Newton-Schulz iteration\nto compute the matrix square root. It has been widely\napplied in different tasks, including covariance pooling [3]\nand ZCA whitening [8]. We note that although [9] and [21]\nuse different forms of NS iteration, the two representations\nare equivalent to each other (see the proof in the paper).\nThe modiﬁed NS iteration in [9] just replaces Yk with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='are equivalent to each other (see the proof in the paper).\nThe modiﬁed NS iteration in [9] just replaces Yk with\nZkA and re-formulates the iteration using one variable.\nThe computation complexity is still the same.\nAs the ordinary differentiable SVD suffers from the\ngradient explosion issue and easily causes the program to\nfail, we do not take it into account for comparison.\nUnlike previous methods such as SVD and NS iteration,\nour MPA-Lya/MTP-Lya does not have a consistent FP and\nBP algorithm. However, we do not think it will bring any\ncaveat to the stability or performance. Our MTP and MPA\ndo not need coupled iteration in the FP and always have\ngradient back-propagating through A\n1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='do not need coupled iteration in the FP and always have\ngradient back-propagating through A\n1\n2 or A− 1\n2 in the BP,\nwhich could guarantee the training stability. Moreover, our\nablation study implies that our BP Lyapunov solver ap-\nproximates the real gradient very well (i.e., ||Bk−I||F<3e−7\nand ||0.5Ck−X||F<7e−6). Also, our extensive experiments\ndemonstrate the superior performances. In light of these\nexperimental results, we argue that as long as the BP\nalgorithm is accurate enough, the inconsistency between\nthe BP and FP is not an issue.\nAPPENDIX D\nEXPERIMENTAL SETTINGS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the BP and FP is not an issue.\nAPPENDIX D\nEXPERIMENTAL SETTINGS\nAll the source codes are implemented in Pytorch. For the\nSVD methods, the forward eigendecomposition is performed\non the CPU using the ofﬁcial Pytorch function TORCH.SVD,\nwhich calls the LAPACK’s routine gesdd that uses the\nDivide-and-Conquer algorithm for the fast calculation. All\nthe numerical tests are conducted on a single workstation\nequipped with a Tesla K40 GPU and a 6-core Intel(R) Xeon(R)\nGPU @ 2.20GHz.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n16', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n16\nFor our method throughout all the experiments, in the\nforward pass, we match the MTP to the power series of\ndegree 11 and set the degree for both numerator and\ndenominator of our MPA as 5. We keep iterating 8 times\nfor our backward Lyapunov solver.\nNow we turn to the implementation details for each\nexperiment in the paper.\nD.1\nDecorrelated Batch Normalization\nFig. 13: The architecture changes of ResNet models in\nthe experiment of ZCA whitening. The decorrelated batch\nnormalization layer is inserted after the ﬁrst convolutional', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the experiment of ZCA whitening. The decorrelated batch\nnormalization layer is inserted after the ﬁrst convolutional\nlayer. The kernel sizes, the stride of the ﬁrst convolution\nlayer, and the stride of the ﬁrst ResNet block are changed\ncorrespondingly.\nFig. 13 displays the detailed architecture changes of\nResNet. Suggested by [29], we truncate the Taylor polynomial\nto degree 20 for SVD-Taylor. To make Pad´e approximant\nmatch the same degree with Taylor polynomial, we set the\ndegree of both numerator and denominator to 10 for SVD-\nPad´e. For SVD-PI, the iteration times are also set as 20. For\nthe NS iteration, according to the setting in [3], [8], we set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Pad´e. For SVD-PI, the iteration times are also set as 20. For\nthe NS iteration, according to the setting in [3], [8], we set\nthe iteration times to 5. The other experimental settings\nfollow the implementation in [18]. We use the workstation\nequipped with a Tesla K40 GPU and a 6-core Intel(R) Xeon(R)\nGPU @ 2.20GHz for training. Notice that in our previous\nconference paper, we ﬁrst calculate the matrix square root\nA\n1\n2 and then compute Xwhitend by solving the linear system\nA\n1\n2 Xwhitend=X. Thanks to the algorithm extension to the\ninverse square root, we can directly computes A− 1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='1\n2 Xwhitend=X. Thanks to the algorithm extension to the\ninverse square root, we can directly computes A− 1\n2 in this\npaper.\nD.2\nSecond-order Vision Transformer\nWe use 8 Tesla G40 GPUs for distributed training and the\nNVIDIA Apex mixed-precision trainer is used. Except that\nthe spectral layer uses the single-precision (i.e., ﬂoat32), other\nlayers use the half-precision (i.e., ﬂoat16) to accelerate the\ntraining. Other implementation details follow the experimen-\ntal setting of the original So-ViT [5]. Following the experiment\nof covariance pooling for CNNs [4], the degrees of Taylor', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='tal setting of the original So-ViT [5]. Following the experiment\nof covariance pooling for CNNs [4], the degrees of Taylor\npolynomial are truncated to 100 for SVD-Taylor, and the\ndegree of both the numerator and denominator of Pad´e\napproximants are set to 50 for SVD-Pad´e. The iteration times\nof SVD-PI are set to 100. In the experiment of covariance\npooling, more terms of the Taylor series are used because\nthe covariance pooling meta-layer requires more accurate\ngradient estimation [4].\nFor the SVD-based methods, usually the double-precision\nis required to ensure an effective numerical representation\nof the eigenvalues. Using a lower precision would make the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='For the SVD-based methods, usually the double-precision\nis required to ensure an effective numerical representation\nof the eigenvalues. Using a lower precision would make the\nmodel fail to converge at the beginning of the training [4].\nThis is particularly severe for vision transformers which are\nknown slow and hard to converge in the early training stage.\nOne may consider to cast the tensor into double-precision (64\nbits) to alleviate this issue. However, this will trigger much\nlarger gradient and introduce round-off errors when the\ngradient is passed to previous layer in half-precision (16 bits).\nTo avoid this caveat, we ﬁrst apply the NS iteration to train\nthe network for 50 epochs, then switch to the corresponding\nSVD method and continue the training till the end. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the network for 50 epochs, then switch to the corresponding\nSVD method and continue the training till the end. This\nhybrid approach can avoid the non-convergence of the SVD\nmethods at the beginning of the training phase.\nD.3\nGlobal Covariance Pooling\nFor the experiment on large-scale and ﬁne-grained image\nrecognition, we refer to [4] for all the experimental settings.\nIn the video action recognition experiment [6], the iteration\ntime for NS iteration is set as 5. Othe implementation details\nare unchanged.\nD.4\nNeural Style Transfer\nFor the loss functions, we follow the settings in [14] and\nuse the cycle-consistent reconstruction loss in both the latent', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Neural Style Transfer\nFor the loss functions, we follow the settings in [14] and\nuse the cycle-consistent reconstruction loss in both the latent\nand the pixel space. The image is resized to the resolution\nof 216×216 before passing to the network, and the model is\ntrained for 100, 000 iterations. The batch size is set to 4.\nTable 13 and Fig. 14 present the detailed quantitative\nevaluation and more visual comparison, respectively. As sug-\ngested in [13], [38], we use the LPIPS [56] score and the user\npreference as the evaluation metrics. For the LPIPS metric,\nwe compute the score between each pair of transferred image\nand the content image. A higher LPIPS score implies that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='preference as the evaluation metrics. For the LPIPS metric,\nwe compute the score between each pair of transferred image\nand the content image. A higher LPIPS score implies that\nthe image carries less content information but more style\ninformation. For the user study, we randomly select 100\nimages from each dataset and ask 20 volunteers to vote for\nthe image that characterizes more the style information. In\nsome cases where the volunteer thinks none of the images\ncorrectly carries the style, he/she can abstain and does not\nvote for any one.\nAPPENDIX E\nCOMPARISON OF LYAPUNOV SOLVER AGAINST IM-\nPLICIT FUNCTION AND AUTOMATIC DIFFERENTIA-\nTION', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='PLICIT FUNCTION AND AUTOMATIC DIFFERENTIA-\nTION\nBesides our proposed custom Lyapunov gradient solver,\none may consider alternative gradient computation schemes,\nsuch as reverse-mode automatic differentiation (RMAD)\nand implicit function (IF). For the RMAD, the backward\npass indeed takes roughly the same operation costs as the\nforward pass. Considering that our MPA uses two sets of\nmatrix power polynomials and one matrix inverse, using\nRMAD for the gradient computation would be less efﬁcient\nthan the Lyapunov solver which only involves matrix\nmultiplications. Moreover, the gradient of some intermediate\nvariables of MPA would be calculated in the RMAD, which\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='variables of MPA would be calculated in the RMAD, which\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n17\nTABLE 13: The detailed LPIPS [56] score and user preference (%) on each subset of Artworks dataset.\nMethods\nLPIPS [56] Score (↑)\nUser Preference (↑)\nCezanne\nMonet\nVangogh\nUkiyoe\nAverage\nCezanne\nMonet\nVangogh\nUkiyoe\nAverage\nSVD-Taylor\n0.4937\n0.4820', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Average\nSVD-Taylor\n0.4937\n0.4820\n0.6074\n0.5274\n0.5276\n15\n16\n25\n9\n16.25\nSVD-Pad´e\n0.6179\n0.4783\n0.5307\n0.5419\n0.5422\n28\n13\n15\n21', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0.5422\n28\n13\n15\n21\n19.25\nNS iteration\n0.5328\n0.5329\n0.5386\n0.6270\n0.5578\n11\n18\n21\n18\n17.00\nOur MPA-Lya\n0.6332\n0.5291\n0.4511\n0.6325', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='0.6332\n0.5291\n0.4511\n0.6325\n0.5615\n25\n29\n18\n27\n24.75\nOur MTP-Lya\n0.6080\n0.4826\n0.4796\n0.6253\n0.5489\n17\n21\n17\n19\n18.50', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='17\n21\n17\n19\n18.50\nFig. 14: More exemplary visualizations on Artworks [57] dataset. Our methods generate sharper images with more coherent\nstyle and better visual appeal. The red rectangular indicates regions with subtle details.\nwould further increase unnecessary memory costs. For the\nIF, the function for matrix square root can be deﬁned as\nf(A, A\n1\n2 ) = (A\n1\n2 )2 − A where A\n1\n2 can be regarded as\na function of A. Performing implicit differentiation and\nmultiplying both sides with\n∂l', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='2 can be regarded as\na function of A. Performing implicit differentiation and\nmultiplying both sides with\n∂l\n∂A\n1\n2 would lead to the gradient\nequation\n∂l\n∂A = −( ∂f\n∂A\n1\n2 )−1 ∂f\n∂A\n∂l\n∂A\n1\n2 . The memory usage of IF\nshould be small since only the gradient of f is introduced in\nthe computation. However, the time cost can be high due to\nthe function gradient evaluation ∂f\n∂A and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='the computation. However, the time cost can be high due to\nthe function gradient evaluation ∂f\n∂A and\n∂f\n∂A\n1\n2 as well as the\nmatrix inverse computation.\nTABLE 14: Backward time and speed comparison for batched\nmatrices of size 64×64×64. We use MPA for forward pass,\nand the evaluation is averaged on 1, 000 randomly generated\nmatrices.\nMethod\nSpeed (ms)\nMemory (MB)\nLyapunov\n2.19\n1.99\nRMAD\n5.69', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Lyapunov\n2.19\n1.99\nRMAD\n5.69\n3.08\nIF\n4.71\n2.03\nTable 14 compares the speed and memory consumption.\nOur Lyapunov solver outperforms both schemes in terms of\nspeed and memory. The memory usage of IF is competitive,\nwhich also meets our expectation. In general, our Lyapunov-\nbased solver can be viewed as a well-optimized RMAD\ncompiler with the least memory and time consumption.\nAPPENDIX F\nSTABILITY OF PAD´E APPROXIMANTS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='compiler with the least memory and time consumption.\nAPPENDIX F\nSTABILITY OF PAD´E APPROXIMANTS\nWhen there is the presence of spurious poles [60], [61], the\nPad´e approximants are very likely to suffer from the well-\nknown defects of instability. The spurious poles mean that\nwhen the approximated function has very close poles and\nzeros, the corresponding Pad´e approximants will also have\nclose poles and zeros. Consequently, the Pad´e approximants\nwill become very unstable in the region of defects (i.e.,\nwhen the input is in the neighborhood of poles and zeros).\nGeneralized to the matrix case, the spurious poles can happen\nwhen the determinant of the matrix denominator is zero (i.e.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Generalized to the matrix case, the spurious poles can happen\nwhen the determinant of the matrix denominator is zero (i.e.\ndet (QN) = 0).\nHowever, in our case, the approximated function for\nmatrix square root is (1−z)\n1\n2 for |z| < 1, which only has one\nzero at z = 1 and does not have any poles. For the inverse\nsquare root, the approximated function (1 − z)− 1\n2 has one\npole but does not have an zeros. Therefore, the spurious pole\ndoes not exist in our approximation and there are no defects\nof our Pad´e approximants.\nNow we brieﬂy prove this claim for the matrix square', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='does not exist in our approximation and there are no defects\nof our Pad´e approximants.\nNow we brieﬂy prove this claim for the matrix square\nroot. The proof for the inverse square root can be given\nsimilarly, and we omit it here for conciseness. Consider the\ndenominator of our Pad´e approximants:\nQN = I −\nN\nX\nn=1\nqn(I −\nA\n||A||F\n)n\n(54)\nIts determinant is calculated as:\ndet (QN) =\nY\ni=1\n(1 −\nN\nX', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='det (QN) =\nY\ni=1\n(1 −\nN\nX\nn=1\nqn(1 −\nλi\nqP\ni λ2\ni\n)n)\n(55)\nThe\ncoefﬁcients\nqn\nof\nour\n[5, 5]\nPad´e\napproximant\nare\npre-computed\nas', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='Pad´e\napproximant\nare\npre-computed\nas\n[2.25, −1.75, 0.54675, −0.05859375, 0.0009765625].\nLet\nxi denotes (1 −\nλi\n√P\ni λ2\ni ). Then xi is in the range of [0, 1],\nand we have:\nf(xi) = 1 − 2.25xi + 1.75x2\ni − 0.54675x3\ni +\n+0.05859375x4', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='i − 0.54675x3\ni +\n+0.05859375x4\ni − 0.0009765625x5\ni ;\ndet (QN) =\nY\ni=1\n(f(xi)).\n(56)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n18\nThe polynomial f(xi) does not have any zero in the range of\nx∈[0, 1]. The minimal is 0.0108672 when x = 1. This implies\nthat det (QN) ̸= 0 always holds for any QN and our Pad´e', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'}), Document(page_content='that det (QN) ̸= 0 always holds for any QN and our Pad´e\napproximants do not have any pole. Accordingly, there will\nbe no spurious poles and defects. Hence, our MPA is deemed\nstable. Throughout our experiments, we do not encounter\nany instability issue of our MPA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8q9w298o/Fast Differentiable Matrix Square Root and Inverse Square Root.pdf'})]
cuda:2
[UploadFile(filename='9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt', size=5421, headers=Headers({'content-disposition': 'form-data; name="files"; filename="9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpff7ojprh, tmpff7ojprh
File: 9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt, msg: 成功上传文件 9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt, docs: [Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.\nViTBIS: Vision Transformer for Biomedical Image Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='ViTBIS: Vision Transformer for Biomedical Image Segmentation\nIn this paper, we propose a novel network named Vision Transformer forBiomedical Image Segmentation (ViTBIS). Our network splits the input featuremaps into three parts with $1\\times 1$, $3\\times 3$ and $5\\times 5$convolutions in both encoder and decoder. Concat operator is used to merge thefeatures before being fed to three consecutive transformer blocks withattention mechanism embedded inside it. Skip connections are used to connectencoder and decoder transformer blocks. Similarly, transformer blocks and multiscale architecture is used in decoder before being linearly projected toproduce the output segmentation map. We test the performance of our networkusing Synapse multi-organ segmentation dataset, Automated cardiac diagnosischallenge dataset, Brain tumour MRI segmentation dataset and Spleen CTsegmentation dataset. Without bells and whistles, our network outperforms mostof the previous state of the art CNN and transformer based models using Dicescore and the Hausdorff distance as the evaluation metrics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='SelfAugment: Automatic Augmentation Policies for Self-Supervised  Learning\nA common practice in unsupervised representation learning is to use labeleddata to evaluate the quality of the learned representations. This supervisedevaluation is then used to guide critical aspects of the training process suchas selecting the data augmentation policy. However, guiding an unsupervisedtraining process through supervised evaluations is not possible for real-worlddata that does not actually contain labels (which may be the case, for example,in privacy sensitive fields such as medical imaging). Therefore, in this workwe show that evaluating the learned representations with a self-supervisedimage rotation task is highly correlated with a standard set of supervisedevaluations (rank correlation $> 0.94$). We establish this correlation acrosshundreds of augmentation policies, training settings, and network architecturesand provide an algorithm (SelfAugment) to automatically and efficiently selectaugmentation policies without using supervised evaluations. Despite not usingany labeled data, the learned augmentation policies perform comparably withaugmentation policies that were determined using exhaustive supervisedevaluations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='COSA: Concatenated Sample Pretrained Vision-Language Foundation Model', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Due to the limited scale and quality of video-text training corpus, mostvision-language foundation models employ image-text datasets for pretrainingand primarily focus on modeling visually semantic representations whiledisregarding temporal semantic representations and correlations. To addressthis issue, we propose COSA, a COncatenated SAmple pretrained vision-languagefoundation model. COSA jointly models visual contents and event-level temporalcues using only image-text corpora. We achieve this by sequentiallyconcatenating multiple image-text pairs as inputs for pretraining. Thistransformation effectively converts existing image-text corpora into a pseudolong-form video-paragraph corpus, enabling richer scene transformations andexplicit event-description correspondence. Extensive experiments demonstratethat COSA consistently improves performance across a broad range of downstreamtasks, including long-form/short-form video-text tasks and image-text taskssuch as retrieval, captioning, and question answering.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Notably, COSA achievesstate-of-the-art results on various competitive benchmarks. Code and model arereleased at https://github.com/TXH-mercury/COSA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Medical Image Generation using Generative Adversarial Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Generative adversarial networks (GANs) are unsupervised Deep Learningapproach in the computer vision community which has gained significantattention from the last few years in identifying the internal structure ofmultimodal medical imaging data. The adversarial network simultaneouslygenerates realistic medical images and corresponding annotations, which provento be useful in many cases such as image augmentation, image registration,medical image generation, image reconstruction, and image-to-image translation.These properties bring the attention of the researcher in the field of medicalimage analysis and we are witness of rapid adaption in many novel andtraditional applications. This chapter provides state-of-the-art progress inGANs-based clinical application in medical image generation, and cross-modalitysynthesis.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='This chapter provides state-of-the-art progress inGANs-based clinical application in medical image generation, and cross-modalitysynthesis. The various framework of GANs which gained popularity in theinterpretation of medical images, such as Deep Convolutional GAN (DCGAN),Laplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-imagetranslation model (UNIT), continue to improve their performance byincorporating additional hybrid architecture, has been discussed. Further, someof the recent applications of these frameworks for image reconstruction, andsynthesis, and future research directions in the area have been covered.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa56f4e10> 111
cuda:2
[Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.\nViTBIS: Vision Transformer for Biomedical Image Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='ViTBIS: Vision Transformer for Biomedical Image Segmentation\nIn this paper, we propose a novel network named Vision Transformer forBiomedical Image Segmentation (ViTBIS). Our network splits the input featuremaps into three parts with $1\\times 1$, $3\\times 3$ and $5\\times 5$convolutions in both encoder and decoder. Concat operator is used to merge thefeatures before being fed to three consecutive transformer blocks withattention mechanism embedded inside it. Skip connections are used to connectencoder and decoder transformer blocks. Similarly, transformer blocks and multiscale architecture is used in decoder before being linearly projected toproduce the output segmentation map. We test the performance of our networkusing Synapse multi-organ segmentation dataset, Automated cardiac diagnosischallenge dataset, Brain tumour MRI segmentation dataset and Spleen CTsegmentation dataset. Without bells and whistles, our network outperforms mostof the previous state of the art CNN and transformer based models using Dicescore and the Hausdorff distance as the evaluation metrics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='SelfAugment: Automatic Augmentation Policies for Self-Supervised  Learning\nA common practice in unsupervised representation learning is to use labeleddata to evaluate the quality of the learned representations. This supervisedevaluation is then used to guide critical aspects of the training process suchas selecting the data augmentation policy. However, guiding an unsupervisedtraining process through supervised evaluations is not possible for real-worlddata that does not actually contain labels (which may be the case, for example,in privacy sensitive fields such as medical imaging). Therefore, in this workwe show that evaluating the learned representations with a self-supervisedimage rotation task is highly correlated with a standard set of supervisedevaluations (rank correlation $> 0.94$). We establish this correlation acrosshundreds of augmentation policies, training settings, and network architecturesand provide an algorithm (SelfAugment) to automatically and efficiently selectaugmentation policies without using supervised evaluations. Despite not usingany labeled data, the learned augmentation policies perform comparably withaugmentation policies that were determined using exhaustive supervisedevaluations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='COSA: Concatenated Sample Pretrained Vision-Language Foundation Model', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Due to the limited scale and quality of video-text training corpus, mostvision-language foundation models employ image-text datasets for pretrainingand primarily focus on modeling visually semantic representations whiledisregarding temporal semantic representations and correlations. To addressthis issue, we propose COSA, a COncatenated SAmple pretrained vision-languagefoundation model. COSA jointly models visual contents and event-level temporalcues using only image-text corpora. We achieve this by sequentiallyconcatenating multiple image-text pairs as inputs for pretraining. Thistransformation effectively converts existing image-text corpora into a pseudolong-form video-paragraph corpus, enabling richer scene transformations andexplicit event-description correspondence. Extensive experiments demonstratethat COSA consistently improves performance across a broad range of downstreamtasks, including long-form/short-form video-text tasks and image-text taskssuch as retrieval, captioning, and question answering.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Notably, COSA achievesstate-of-the-art results on various competitive benchmarks. Code and model arereleased at https://github.com/TXH-mercury/COSA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Medical Image Generation using Generative Adversarial Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='Generative adversarial networks (GANs) are unsupervised Deep Learningapproach in the computer vision community which has gained significantattention from the last few years in identifying the internal structure ofmultimodal medical imaging data. The adversarial network simultaneouslygenerates realistic medical images and corresponding annotations, which provento be useful in many cases such as image augmentation, image registration,medical image generation, image reconstruction, and image-to-image translation.These properties bring the attention of the researcher in the field of medicalimage analysis and we are witness of rapid adaption in many novel andtraditional applications. This chapter provides state-of-the-art progress inGANs-based clinical application in medical image generation, and cross-modalitysynthesis.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'}), Document(page_content='This chapter provides state-of-the-art progress inGANs-based clinical application in medical image generation, and cross-modalitysynthesis. The various framework of GANs which gained popularity in theinterpretation of medical images, such as Deep Convolutional GAN (DCGAN),Laplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-imagetranslation model (UNIT), continue to improve their performance byincorporating additional hybrid architecture, has been discussed. Further, someof the recent applications of these frameworks for image reconstruction, andsynthesis, and future research directions in the area have been covered.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpff7ojprh/9bbaf96e-9e7e-45fd-a502-813b4bd150f0.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp8hl0x7dg, tmp8hl0x7dg
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc750cd0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp8hl0x7dg/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='05473f62-adf9-4adf-9196-f589d9c96780.txt', size=5283, headers=Headers({'content-disposition': 'form-data; name="files"; filename="05473f62-adf9-4adf-9196-f589d9c96780.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpnnlv138a, tmpnnlv138a
File: 05473f62-adf9-4adf-9196-f589d9c96780.txt, msg: 成功上传文件 05473f62-adf9-4adf-9196-f589d9c96780.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc2403d0> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='DDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Superpixel Segmentation with Fully Convolutional Networks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='In computer vision, superpixels have been widely used as an effective way toreduce the number of image primitives for subsequent processing. But only a fewattempts have been made to incorporate them into deep neural networks. One mainreason is that the standard convolution operation is defined on regular gridsand becomes inefficient when applied to superpixels. Inspired by aninitialization strategy commonly adopted by traditional superpixel algorithms,we present a novel method that employs a simple fully convolutional network topredict superpixels on a regular image grid. Experimental results on benchmarkdatasets show that our method achieves state-of-the-art superpixel segmentationperformance while running at about 50fps. Based on the predicted superpixels,we further develop a downsampling/upsampling scheme for deep networks with thegoal of generating high-resolution outputs for dense prediction tasks.Specifically, we modify a popular network architecture for stereo matching tosimultaneously predict superpixels and disparities. We show that improveddisparity estimation accuracy can be obtained on public datasets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='VideoSSL: Semi-Supervised Learning for Video Classification\nWe propose a semi-supervised learning approach for video classification,VideoSSL, using convolutional neural networks (CNN). Like other computer visiontasks, existing supervised video classification methods demand a large amountof labeled data to attain good performance. However, annotation of a largedataset is expensive and time consuming. To minimize the dependence on a largeannotated dataset, our proposed semi-supervised method trains from a smallnumber of labeled examples and exploits two regulatory signals from unlabeleddata. The first signal is the pseudo-labels of unlabeled examples computed fromthe confidences of the CNN being trained. The other is the normalizedprobabilities, as predicted by an image classifier CNN, that captures theinformation about appearances of the interesting objects in the video. We showthat, under the supervision of these guiding signals from unlabeled examples, avideo classification CNN can achieve impressive performances utilizing a smallfraction of annotated examples on three publicly available datasets: UCF101,HMDB51 and Kinetics.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'}), Document(page_content='Phasic dopamine release identification using ensemble of AlexNet\nDopamine (DA) is an organic chemical that influences several parts ofbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is atechnique used for in vivo phasic dopamine release measurements. The analysisof such measurements, though, requires notable effort. In this paper, wepresent the use of convolutional neural networks (CNNs) for the identificationof phasic dopamine releases.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnnlv138a/05473f62-adf9-4adf-9196-f589d9c96780.txt'})]
cuda:2
[UploadFile(filename='路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf', size=664985, headers=Headers({'content-disposition': 'form-data; name="files"; filename="è·¯ç\x94± - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmplwsfm8bq, tmplwsfm8bq
File: 路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf, msg: 成功上传文件 路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf, docs: [Document(page_content='Lightweight Source Authentication and Path Validation\nTiffany Hyun-Jin Kim\nCyLab, CMU\nhyunjin@cmu.edu\nCristina Basescu\nETH Zürich\ncba@inf.eth.ch\nLimin Jia\nCyLab, CMU\nliminjia@cmu.edu\nSoo Bum Lee\nQualcomm\nsoobuml@qti.qualcomm.com\nYih-Chun Hu\nUIUC\nyihchun@uiuc.edu\nAdrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Adrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT\nIn-network source authentication and path validation are funda-\nmental primitives to construct higher-level security mechanisms\nsuch as DDoS mitigation, path compliance, packet attribution, or\nprotection against ﬂow redirection. Unfortunately, currently pro-\nposed solutions either fall short of addressing important security\nconcerns or require a substantial amount of router overhead. In this\npaper, we propose lightweight, scalable, and secure protocols for\nshared key setup, source authentication, and path validation. Our\nprototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors\nC.2.0 [Computer-Communication Networks]: Security and pro-\ntection; C.2.1 [Network Architecture and Design]: Circuit-switch-\ning networks, Packet-switching networks\nKeywords\nSource Authentication, Path Validation, Retroactive Key Setup\n1.\nINTRODUCTION\nSource authentication and path validation are useful primitives\nto help mitigate various network-based attacks, such as DDoS, ad-\ndress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='dress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according\nto the policies of ISPs, enterprises, and datacenters. Endhosts and\nISPs desire to validate service level agreement compliance regard-\ning data delivery in the network: Did the packet truly originate from\nthe claimed client? Did the client select a path that complies with\nthe service provider’s policy? Did the packet indeed travel through\nthe path selected by the client?\nUnfortunately, the current Internet provides almost no means for\nsource authentication and path validation by routers or endhosts,\nopening up numerous attack surfaces. For example, a malicious\nISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for proﬁt or commercial advantage and that copies bear\nthis notice and the full citation on the ﬁrst page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior speciﬁc permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='permissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.\nCopyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00.\nhttp://dx.doi.org/10.1145/2619239.2626323.\ntively, a malicious router may inject packets with a spoofed source\naddress to incriminate a victim source node into having sent an ex-\ncessive number of packets. A malicious router may simply alter the\ncontents of received packets as well. The inability to detect such\nattacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-\nvert trafﬁc to traverse a point of eavesdropping it controls—a seri-\nous issue in particular for sensitive information.\nEnd-to-end encryption and authentication mechanisms, such as\nTLS, do not solve any of the above issues, since they are agnos-\ntic to which path the packet takes. A stronger approach is needed,\nwhich enables routers and destinations to perform source authenti-\ncation and path validation. As we discuss in the related work, ex-\nisting solutions either require extensive overhead, or only partially\naddress fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='address fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses\nboth source authentication and path validation, but it requires each\nintermediate router on a path to store and look up keys shared with\nother routers; ICING requires 42 bytes per verifying router in the\npacket header. Furthermore, ICING requires each router to calcu-\nlate a Message Authentication Code (MACs) for all other routers\non the path. In contrast, our protocol does not require any per-\nclient state on routers; it requires only 16 bytes per hop (which can\nbe reduced to 2 bytes for a lower level of security), and only a sin-\ngle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='gle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-\nvents against coward attacks [20], where an adversary only attacks\nwhen it knows that the attack will not be detected. Our protocol,\nhowever, offers reduced security in the case of a malicious sender\ncolluding with a malicious router on the path, which we describe\nin detail in the related work section. Since in the common case,\nsender and receiver trust each other, the performance gain of O(1)\nMAC operation per router instead of O(n) is worth the tradeoff.\nContributions. In this paper, we present Dynamically Recreatable\nKey (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Key (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of\nDRKey on routers prevents state exhaustion DoS attacks and sim-\npliﬁes router architecture. We further enrich DRKey with a new\nnotion called retroactive key setup that provides the following de-\nsirable properties: (1) in contrast to previous protocols, source and\ndestination can start the communication without needing to wait\nfor the expensive key setup to complete, providing efﬁciency; (2)\nif misbehavior is suspected, endhosts set up keys retroactively to\nverify previous packets, defending against coward attacks.\nBased on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Based on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,\nscalable, and secure protocols for source authentication and path\nvalidation. We introduce an extension called Retroactive-PathTrace\nthat supports the destination to perform path validation with retroac-\n271\ntive key setup and to detect coward attackers with small, constant\noverhead in the packet header. Our OPT protocols enable imple-\nmentation on SW routers with minimal performance impact.\n2.\nPROBLEM DEFINITION\n2.1\nDesired Security Properties\nSource authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Source authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether\nthe packet content has not been altered en route. In this paper,\nsource authentication includes data authentication.\nPath validation. The source, intermediate routers, and the desti-\nnation should be able to validate that the packet indeed traversed\nthe path known to (or selected by) the source. Successful path val-\nidation ensures that the packet traversed each honest router on the\npath in the correct order. Unfortunately, no scheme can provide\nany guarantees for malicious routers: if malicious router Rm pub-\nlishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='lishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2\nElided Security Properties\nNo packet delivery guarantee. Routers generally have the free-\ndom to decide whether or not to forward packets. Hence, it is not\nthe purpose of path validation to guarantee that packets will be de-\nlivered to the speciﬁed destination.\nNo detection of packet siphoning. Misbehaving router Rm on the\nsource-selected path can siphon packets and send them over a sep-\narate channel to a remote entity. Since Rm still forwards the packet\nto Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with\nthe packet.\nNo locating of packet altering and dropping routers. Locating\nrouters that alter or drop packets is the goal of fault-localization\nmechanisms—another challenging problem especially in inter-domain\nsettings [39]. Since path validation is a simpler problem, the goal\nis to achieve a more efﬁcient protocol than heavy-weight fault lo-\ncalization.\n2.3\nAdversary Model\nWe consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.\nPacket alteration. A malicious router alters any part of the packet,\nsuch as source address, header information, or payload data.\nPacket injection. A malicious router fabricates a packet and sends\nit towards a destination of its choice. A packet replay attack is a\nspecial case of packet injection.\nPath deviation. A malicious router may perform path deviation\nattacks, which cause packets to be forwarded along a path other\nthan the path previously selected by the source. We subdivide this\nattack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns\nto the correct downstream router Rm+1 to resume traversal of all\nrouters on the intended path.\nRouter skipping: A malicious router redirects the packet and\nskips other router(s) on the path. Thus, some routers on the in-\ntended path does not forward the packet.\nOut-of-order traversal: An adversary causes path deviations\nsuch that routers on the intended path are not traversed in the\nright order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='right order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For\nexample, an attacker diverts trafﬁc only when the protocol is inac-\ntive (e.g., required keys for validation have not been established).\nDenial-of-Service (DoS). As part of DoS attacks, we consider mem-\nory and computation exhaustion attacks on routers performing source\nauthentication and path validation.\nCollusion. Protocol participants may collude to carry out any of\nthe attacks listed above. For example, two or more intermediate\nrouters may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router\nto spoof authenticators for its downstream routers if the destina-\ntion prefers/trusts skipped routers. Also, both the source and the\ndestination could collude with some intermediate routers to frame\nanother router on the path by not forwarding packets to it.\nIn Section 6, we explore potential attacks against our protocols\nthat violate the desired properties and discuss how OPT defends\nagainst these attacks.\n3.\nOPT DESIGN OVERVIEW\nWe consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and\nRi’s as tracing entities. At a very high level, the main insights for\nachieving source authentication and path validation without requir-\ning routers to maintain per-source or per-path-length state are as\nfollows: (1) In the packet header, source S includes H(P), which is\nthe hash of the packet payload to help receiving entities identify the\npacket while avoiding expensive hash computation at each router;\n(2) On demand, each router Ri generates key Ki using a symmet-\nric cryptographic operation, and requires only router’s local secret\nSVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='SVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless\nand faster than storing or retrieving secrets. (3) Each router per-\nforms source authentication using a MAC computed over H(P); (4)\nEach router Ri extends a special authentication ﬁeld called PVF by\nperforming a MAC operation. Hence, path validation is achieved\nthrough a chain consisting of nested MACs.\n3.1\nAssumptions\nFor the communication properties of the network, we assume\nthat the source knows the path that the packet will traverse at the\nAS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the\nvalidations. This information can stem from (1) the BGP proto-\ncol where the source can learn the AS path that the packet is ex-\npected to traverse, (2) Pathlet routing [11] or SCION [40] where\nthe source can specify the path in the packet header, or (3) i3 [33]\nor Platypus [29] where the source can deﬁne a sequence of servers\nto traverse. Alternatively, an ISP may provide the premium path\ninformation to clients as an extra service (e.g., transatlantic cable\nfor ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='for ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.\nFor the cryptographic key setup, the source and the destination\nneed to be able to authenticate the router’s cryptographic materi-\nals (i.e., validate a signature that binds an entity to some crypto-\ngraphic materials). In the case of AS-level tracing, the AS needs to\nbe authenticated, and such authentication can be achieved through\nRPKI [4], which is already operational. RPKI provides a PKI that\nenables authentication of AS certiﬁcates, each of which binds an\nAS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn\ncreates certiﬁcates for each router using the AS’s private key—\nenabling the tracing entity to verify via the AS certiﬁcate using\nRPKI.1\n1Alternatively, OPT can authenticate entities based on mechanisms\n272\nTable 1: Notation.\n(PKE,PK−1\nE )\nEntity E’s public-private key pair\nCertPKE\nEntity E’s public-key Certiﬁcate\nˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions\nKE\nSymmetric key among S, D, and entity E for a single session\nKE1E2\nSymmetric key between entities E1 and E2 for a single session\nKE1E2σ\nSymmetric key for E1 and E2 in session σ for E1-initiated\npackets\nSVE\nEntity E’s local secret value\nP\nNetwork packet payload\n(PKσ,PK−1\nσ )\nPublic-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Public-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ\nTime when S initiates session σ\nSESSIONID\nHash of session σ’s public key, path, session initiation time\nAUTHσ\nAuthenticated and encrypted SESSIONID and private key for\nsession σ\nSignKEYPK−1\nE ,σ\nSignature on a symmetric key for session σ using entity E’s\nprivate key\nEncKEYK,σ\nEncryption of a symmetric key for session σ using key K\nKEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH\nHash of the packet’s payload\nPVF\nField enabling D to verify the path\nPVFS\nField enabling Ri and D to verify the path\nPVFD\nField enabling D to conﬁrm the actual path\nOVi\nField enabling Ri to validate the packet sender\nOPVi\nField enabling Ri to verify both the packet sender and path\nSignPK−1\nE (·)\nSignature using entity E’s private key\nCheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='CheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key\nEncK(·), DecK(·)\nEncryption, decryption using key K\nAuthEncK(·)\nAuthenticated encryption using key K\nAuthDecK(·)\nAuthenticated decryption using key K\nFK(·)\nPseudo-random function using key K\nMACK(·)\nMessage Authentication Code using key K\nH(·)\nCryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Cryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret\nkey between each other. In case the tracing entities are AS infras-\ntructure hosts such as edge routers, ﬁrewalls, or a middlebox at a\nservice provider, either RPKI can be used as described above or an\nadministrator can set up trusted public keys between entities that\nneed path veriﬁcation. If endhosts perform tracing, then a shared\nkey can be set up through SSL or TLS if one of the endhosts is a\nHTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='HTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted\nkeys, TOFU (Trust On First Use) in SSH, TOFU with Perspec-\ntives [35], RPKI with domain-certiﬁed host keys, self-certifying\nIDs as public keys [1,23,24,34], or self-validation using an anony-\nmous service [10]. We assume that one of these approaches is used\nto set up symmetric key ˆKSD between source S and destination D.\n3.2\nMain Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Main Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each\nrouter to maintain a secret key for each ﬂow, our design enables\nrouters to derive the secret keys on the ﬂy using only local se-\ncrets stored at the routers and an efﬁcient pseudo-random function.\nThus, we avoid storing all the keys.\nMore precisely, OPT runs in sessions. In each session σ, source\nS sends packets to destination D on path PATHσ. S and D leverage\nlong-term symmetric key ˆKSD to set up keys with each router in\nPATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,\nthe source prepares and inserts a special ﬁeld in the packet header\ncalled SESSIONID such that intermediate routers Ri on PATHσ dy-\nnamically compute the shared symmetric key with S and D (Ri only\nneeds to look up its local secret SVRi for computation).\nthat use self-certifying IDs as public keys [1,23,24,34] as assumed\nin ICING. However, such mechanisms have issues with key revo-\ncations. Hence, we prefer to use RPKI.\nOur key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Our key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS\nattacks based on state exhaustion. Moreover, computing pseudo-\nrandom function (PRF) F is faster than performing a cache access;\nfor instance, a key derivation using AESni takes 32 cycles, whereas\na L3 cache read operation requires approximately 40 cycles (on In-\ntel “Sandy-Bridge”-based Xeon architecture).\nOPT includes the hash of the packet payload H(P) in the header,\nwhich enables an important optimization: routers can either par-\nallelize the computations of MAC and the hash of the packet, or\nprobabilistically validate H(P).\n3.3\nOPT Protocol Overview', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='probabilistically validate H(P).\n3.3\nOPT Protocol Overview\nDRKey for path selection and key setup. When source S initiates\nsession σ at time Tσ, S selects path PATHσ to destination D, gener-\nates asymmetric public/private key pair (PKσ,PK−1\nσ ), and creates a\nsession identiﬁer, where SESSIONID = H(PKσ∥PATHσ∥Tσ). Af-\nter preparing some values that support source authentication and\npath validation for other entities on PATHσ, S forwards the OPT\npacket to its downstream router on PATHσ. If Tσ is recent (i.e.,\nwithin some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='within some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and\nSESSIONID. Detailed DRKey protocols are explained in Section 4.\nGeneration of veriﬁcation ﬁelds. S uses the path information to\npre-compute veriﬁcation ﬁelds, one for each router Ri on PATHσ,\nand a special ﬁeld called PVF such that routers can perform source\nauthentication and path validation.\nVeriﬁcation and update by intermediate routers. Upon receiv-\ning a packet, Ri ﬁrst regenerates the shared symmetric key Ki and\nrecomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='recomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it\nsuccessfully authenticates the source and the content of the packet,\nand validates the traversed path. Ri then updates PVF, by applying\na MAC operation using Ki to the ﬁeld. This process helps down-\nstream routers and the destination to validate that each router on the\npath has indeed seen the packet.\nVeriﬁcation by destination. The destination ﬁnally recomputes\nthe veriﬁcation ﬁelds using all the symmetric keys shared with\nother entities on the path. Successful veriﬁcation indicates source\nand packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS\nThis section introduces the DRKey protocols that enable routers\nto set up shared keys with source S and destination D. Section 4.1\ndescribes the case when both S and D trust each other. Section 4.2\nrelaxes this assumption and describes the case when S and D do not\ntrust each other. Section 4.3 describes how S and D retroactively set\nup shared keys with intermediate routers to enable path validation\nof prior packets.\n4.1\nDRKey for Benign Source and Destination\nWhen both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='When both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is\nshared with both S and D. Figure 1 shows the key setup steps and\nthe associated cryptographic operations.\nS creates a fresh public/private key pair (PKσ,PK−1\nσ ) for each\nsession such that routers encrypt session symmetric key Ki’s. Since\nS and D trust each other, they share private key PK−1\nσ , the en-\ncrypted and authenticated value of which is sent to D. The public\nkey is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='key is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path\nand Tσ is when S initiates session σ. Note that Tσ prevents re-\nplay attacks since routers can drop expired packets based on loose\ntime synchronization.\n273\nInitialization by Source S\n0.\nAssume long-term symmetric key ˆKSD shared with D\n(Optional) Assume public/private key pair (PKS,PK−1\nS ), and CertPKS\n1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONID)\n5.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1\nσ )\nS → R1\n7.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID) }\nPairwise Key Derivation by R1\n8.\nCompute K1 = FSVR1 (SESSIONID)\n9.\nEncrypt K1: EncKEYR1,σ = EncPKσ (K1)\nSign: SignKEYR1,σ = SignPK−1\nR1 (K1∥PKσ)\nR1 → R2\n10.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R1 (K1∥PKσ)\nR1 → R2\n10.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ }\nPairwise Key Derivation by R2\n11.\nComputes K2 = FSVR2 (SESSIONID)\n12.\nEncrypt K2: EncKEYR2,σ = EncPKσ (K2)\nSign: SignKEYR2,σ = SignPK−1\nR2 (K2∥PKσ)\nR2 → D\n13.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (K2∥PKσ)\nR2 → D\n13.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPKS(SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ, EncKEYR2,σ, SignKEYR2,σ}\nKey Retrieval by Destination D\n14.\nAlready has ˆKSD, which is the long-term shared symmetric key with S\n15.\nCheck that D is the last entity on PATHσ\n16.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ) and check the integrity\n17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n18.\nDecrypt AUTHσ and authenticate PK−1\nσ : AuthDecKSDσ (AUTHσ)\n19.\nDecrypt K1 and K2 and check their signatures:\nDecPK−1\nσ (EncKEYR1,σ),CheckSigPKR1 (SignKEYR1,σ)\nDecPK−1\nσ (EncKEYR2,σ),CheckSigPKR2 (SignKEYR2,σ)\nK1 and K2 become shared symmetric keys between each router and D\n20.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='K1 and K2 become shared symmetric keys between each router and D\n20.\nCompute KD = FSVD(SESSIONID)\nD → S\n21.\nForward authenticated and encrypted shared keys:\nKEYSσ = AuthEncKDSσ (K1∥K2∥KD∥AUTHσ)\nKey Retrieval by Source S\n22.\nDecrypt and authenticate the keys received from D: AuthDecKDSσ (KEYSσ)\nK1, K2 and KD become shared keys between S and R1, R2, and D\nFigure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-\nﬁcient PRF keyed with a secret SVi only known to Ri. The PRF\ntakes SESSIONID as an input. For high efﬁciency, we compute\nour PRF from a pseudo-random permutation using AES. The over-\nhead of the key setup is negligible to affect the on-going trafﬁc (see\nSection 8). Resulting key Ki is encrypted with public key PKσ,\nand digitally signed to enable veriﬁcation that (encrypted) Ki in-\ndeed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='deed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).\nTo prevent reﬂection attacks (i.e., replaying message in the op-\nposite order of communication), communication between S and D\nuses different symmetric keys for each direction: KSDσ and KDSσ\nfor S-initiated and D-initiated packets, respectively.\nThe optional operations in Figure 1 are used only if the router\nalso needs to authenticate S, in which case S also signs the SESSIONID,\nand certiﬁcates needed for routers to verify S’s public key are in-\ncluded in the message.\n4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust\neach other and that they share the same public-private key pair for\nthe session, then each intermediate router needs to set up only one\nshared key with both S and D. However, S and D may not neces-\nPath Agreement and Key Setup Initialization by Source S\n1.\nInitiate new session σand pick random session key (PKSσ,PK−1\nSσ )\nRi uses this key to authenticate S’s packets\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Obtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONIDS = H(PKSσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONIDS)\nS → D\n5.\nForward {PKSσ,PATHσ,Tσ,SignPK−1\nS (PKSσ∥PATHσ∥Tσ)}\nPath Agreement and Key Setup Initialization by Destination D\n6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.\nCompute SESSIONIDD = H(PKDσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nD (SESSIONIDD)\nD → S\n8.\nForward {PKDσ,PATHσ,Tσ,SignPK−1\nD (PKSσ∥PKDσ∥PATHσ),\nSESSIONIDD, (optional) SignPK−1\nD (SESSIONIDD) }\nInitialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Initialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD) }\nPairwise Key Derivation by R1\n10.\nCompute KS1 = FSVR1S(SESSIONIDS))\nKD1 = FSVR1D(SESSIONIDD))\n11.\nEncrypt KS1: EncKEYR1S,σ = EncPKSσ (KS1)\nKS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.\nSign: SignKEYR1S,σ = SignPK−1\nR1 (KS1∥PKSσ∥S)\nSignKEYR1D,σ = SignPK−1\nR1 (KD1∥PKDσ∥D)\nR1 → R2\n13.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ}\nPairwise Key Derivation by R2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Pairwise Key Derivation by R2\n14.\nCompute KS2 = FSVR2S(SESSIONIDS))\nKD2 = FSVR2D(SESSIONIDD))\n15.\nEncrypt KS2: EncKEYR2S,σ = EncPKSσ (KS2)\nKS2: EncKEYR2D,σ = EncPKDσ (KD2)\n16.\nSign: SignKEYR2S,σ = SignPK−1\nR2 (KS2∥PKSσ∥S)\nSignKEYR2D,σ = SignPK−1\nR2 (KD2∥PKDσ∥D)\nR2 → D\n17.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (KD2∥PKDσ∥D)\nR2 → D\n17.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ,\nEncKEYR2S,σ,SignKEYR2S,σ,EncKEYR2D,σ,SignKEYR2D,σ}\nKey Retrieval by D\n18.\nCheck that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Check that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures\nDecPK−1\nDσ (EncKEYR1D,σ),CheckSigPKR1 (SignKEYR1S,σ)\nDecPK−1\nDσ (EncKEYR2D,σ),CheckSigPKR2 (SignKEYR2S,σ)\nKD1 and KD2 become shared symmetric key between each router and D\n20.\nCompute KD = FSVD(SESSIONIDS))\n21.\nEncrypt KD: EncKEYD,σ = EncPKSσ (KD)\n22.\nSign: SignKEYD,σ = SignPK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='22.\nSign: SignKEYD,σ = SignPK−1\nD (KD∥PKSσ∥S)\nD → S\n23.\nForward {EncKEYR1S,σ,SignKEYR1S,σ,EncKEYR2S,σ,SignKEYR2S,σ,\nEncKEYD,σ,SignKEYD,σ}\nKey Retrieval by Source S\n24.\nDecrypt and authenticate keys received from D\nKS1, KS2 and KD become shared keys between S and R1, R2, and D.\nFigure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.\nsarily trust each other, or they may collude. To strengthen the secu-\nrity guarantees under such circumstances, we introduce Extended-\nDRKey , which requires each intermediate router to set up two keys,\nKSi and KDi, where KSi is the shared symmetric key between S and\nRi and KDi is the shared symmetric key between Ri and D. Figure 2\ndescribes the Extended-DRKey protocol and its cryptographic op-\nerations.\nUnlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Unlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating\nshared keys (i.e., KSi and KDi), Ri and D use distinct local secrets\nto encode the directionality of the keys.\n274\n4.3\nRetroactive-DRKey\nThe key setup protocols as presented in Figures 1 and 2 run once\nbefore the session starts. However, the key setup process incurs\nthe following extra latency and computational overhead: (1) Key\nsetup itself requires an extra round trip between the source and the\ndestination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='destination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-\ndent key setup protocol a priori allows routers to launch a coward\nattack, since the key setup protocol warns possibly misbehaving\nrouters to start behaving correctly and to avoid detection. Conse-\nquently, achieving path validation without the apparent key setup\nprocess is desirable.\nWe introduce Retroactive-DRKey that enables entities to set up\nshared keys at any time after the ﬁrst packet in a session reaches\nthe destination. Note that Retroactive-OPT is invoked only if the\nsource or the destination wishes to perform source authentication\nor path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='or path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).\nTo support such a feature, we still assume that source S and des-\ntination D establish a shared symmetric key ˆKSD in advance, and\nS derives a session key pair (PKσ,PK−1\nσ ) before the session starts.\nUnlike DRKey or Extended-DRKey, Retroactive-DRKey utilizes\nthat S creates KD—a shared symmetric key with D for the ses-\nsion (i.e. KD = FSVS(SESSIONID))—and includes encrypted and\nauthenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='authenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some\nﬁelds such that D can check. When a forwarding protocol is used\nwith Retroactive-DRKey, the routers use some keys for OPT during\na session, and only reveal them at a later time (Section 5.2.1).\nRetroactive-DRKey is very similar to the key setup protocol in\nFigure 1. The only difference is that D does not derive KD, be-\ncause it is already included in each forwarded packet. Retroactive-\nDRKey runs at most once during or after a session ends. We ob-\nserve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='serve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.\n5.\nOPT PROTOCOL DESCRIPTION\nThe DRKey protocols described in Section 4 and the techniques\nwe introduce in this section span a protocol family of source au-\nthentication and path validation with varying assumptions and prop-\nerties. Unfortunately, exploring the entire design space is out of\nscope for this paper, and we will present several protocol instantia-\ntions: (1) OriginValidation for source authentication (S and D trust\neach other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-\ntion and path validation (S and D may not trust each other).\n5.1\nOriginValidation for Source Authentication\nOriginValidation enables each intermediate router and the des-\ntination to perform source authentication using MACs computed\nover the hash of the packet. For efﬁcient authentication, the source\nincludes the following ﬁelds in the packet header:\nDATAHASH: Hash of the packet’s payload H(P);\nSESSIONID: Hash of the session public key, path, and session\ninitiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='initiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.\nOVi is a Message Authenti-\ncation Code computed over DATAHASH using key Ki that Ri\nshares with S (i.e., OVi = MACKi(H(P))). Similarly, OVD =\nMACKD(H(P)). The source creates an OV ﬁeld for each inter-\nmediate router and the destination.\nOriginValidation provides efﬁcient MAC veriﬁcation using the\nDATAHASH ﬁeld without requiring each intermediate router to com-\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nPVF (128 bits)\nOV1 (128 bits)\nOV2 (128 bits)\nOVD (128 bits)\nIP Header\nOriginValidation/PathTrace Header\nTCP Header\nFigure 3: The packet header format for OriginValidation and\nPathTrace. DATAHASH, SESSIONID, and OVs help intermedi-\nate routers and the destination authenticate the source (Orig-\ninValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='inValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).\npute the hash over the entire packet. Figure 3 represents the packet\nheader, and only DATAHASH, SESSIONID and OV ﬁelds are needed\nfor OriginValidation.\nWhen intermediate router R1 receives a packet from the source\nS, R1 computes the symmetric key (K1) it shares with S using R1’s\nlocal secret and SESSIONID from the packet header. Then R1 gen-\nerates a MAC as follows: OV′\n1 = MACK1(DATAHASH). If OV′\n1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards\nthe packet to R2. R2 and D perform the same operations as R1.\nAlthough we present the protocol with OV ﬁelds of size 128,\nthe size can be altered to reﬂect the desired level of security. In\ngeneral, assuming a secure MAC function, the success probability\nof a forged n-bit MAC is 2−n, which already results in a low rate at\nn = 16. Thus, for many applications, 2 byte long OV ﬁelds sufﬁce,\nas a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='as a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace\nPathTrace is to help the source and destination validate that a re-\nceived packet traversed the source-selected path. This main objec-\ntive is achieved by Path Validation Field (PVF), which is a nested\nMAC that intermediate routers update in the packet header as they\nforward the packet. In Figure 3, only DATAHASH, SESSIONID,\nand PVF ﬁelds are used for PathTrace, thus, the packet overhead\nis irrespective of the path length. Next we describe how PathTrace\nsupports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='supports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-\nidate the path, the source generates PVF0—the initial PVF value\nwhich is a MAC of DATAHASH using the shared symmetric key\nbetween the source and the destination. Then the source initializes\nthe PVF ﬁeld in the header with PVF0:\nPVF ← PVF0 = MACKD(DATAHASH).\n(1)\nAny intermediate router Ri on the path generates PVFi and updates\nthe PVF ﬁeld in the header as follows:\nPVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and\nthe destination according to the key setup protocol in Section 4.1.\nHence, upon receiving a packet, the destination ﬁrst re-creates the\nnested MACs (here shown for a path of 2 routers):\nPVF′ = MACK2(DATAHASH∥\nMACK1(DATAHASH∥MACKD(DATAHASH))).\n(3)\nIf PVF′ is the same as PVF in the packet header, the destination is\nassured that the packet was indeed delivered on the source-selected\npath. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its\npacket is delivered to the intended destination using the source-\nselected path, the destination forwards the PVF from the received\n275\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nTIMESTAMP (32 bits)\nPVF (128 bits)\nOPV1 (128 bits)\nOPV2 (128 bits)\nOPVD (128 bits)\nIP Header\nOPT Header\nTCP Header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='OPVD (128 bits)\nIP Header\nOPT Header\nTCP Header\nFigure 4: OPT header. The source S initializes all the ﬁelds.\nIntermediate routers only update the PVF ﬁeld.\npacket header back to the source as follows:\nD → S : EncKD(PVF∥DATAHASH).\n(4)\nUpon receiving this information, the source ﬁrst decrypts the\nmessage using KD and then performs the validation by re-constructing\nthe nested MACs using DATAHASH as shown in Eq. (3) and com-\nparing it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='paring it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1\nRetroactive-PathTrace\nRetroactive-PathTrace supports path validation without the ap-\nparent key setup process in advance. Instead, it utilizes Retroactive-\nDRKey that runs after the session ends. Unlike PathTrace, in Retro-\nactive-PathTrace the source cannot pre-compute the OPVi ﬁelds;\nhence no OPV ﬁelds can be used in the packet header. Instead,\nunder the assumption that the source and the destination trust each\nother, Retroactive-PathTrace requires that the source creates KD—\nthe session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the\npacket header.\nThe source uses KD to compute PVF0 and the\nrouters derive their shared key and update the PVF ﬁeld accord-\ningly.\nRetroactive-PathTrace requires the destination to store per-packet\ninformation for later checking. However, the beneﬁt of defending\nagainst coward attacks overcomes such a disadvantage. Namely,\nthe destination stores for each packet the tuple (SESSIONID, DATA-\nHASH,PVF). When the destination wants to validate the path, it re-\nquests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.\nThen the destination can check the PVF ﬁelds and detect coward\nattacks. The source can independently initiate the retroactive pro-\ncess as well.\n5.3\nOPT: Origin and Path Trace\nIn this section, we introduce OPT that combines OriginValida-\ntion and PathTrace such that all entities (including intermediate\nrouters) on the path can perform both source authentication and\npath validation when they trust the source. We assume that all the\nrouters in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2\nFigure 4 illustrates the OPT header format. In addition to DATA-\nHASH, SESSIONID, and PVF, an OPT header includes the follow-\ning additional ﬁelds to enable each intermediate router to perform\npath validation.\nTIMESTAMP: Time when S creates the OPT packet to mitigate\ntiming-based attacks, such as replay attacks.\nOPVi: Origin and Path Veriﬁcation ﬁeld. OPVi is a MAC that\nenables all entities on the path to perform path validation.\n2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.\nAn arrow represents the header ﬁeld initialization.\n1: function SOURCE INITIALIZATION\nRequire: Ki and KD that Ri’s and D share with S, respectively after running\nkey setup protocol in Figure 1\n2:\nDATAHASH ← H(P)\n3:\nSESSIONID ← H(PKσ∥PATHσ∥Tσ)\n4:\nPVF ← PVF0 = MACKD(DATAHASH)\n5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do\n7:\nPVFi = MACKi(PVFi−1)\n8:\nOPVi ← MACKi(PVFi−1∥DATAHASH∥Ri−1∥TIMESTAMP)\n9:\nend for\n10:\nfor destination D do\n11:\nOPVD ← MACKD(PVFl−1∥DATAHASH∥Rl−1∥TIMESTAMP)\n12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function\n15: function VALIDATION AND UPDATE BY Ri\n16:\n(Note PVF in OPT header = PVFi−1)\n17:\nCompute OPV′\ni = MACKi(PVFi−1∥DATAHASH∥ Ri−1∥TIMESTAMP)\n18:\nif OPV′\ni == OPVi then\n19:\nPVF ← PVFi = MACKi(PVFi−1)\n20:\nForward the packet to Ri+1\n21:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='20:\nForward the packet to Ri+1\n21:\nelse\n22:\nDrop the packet\n23:\nend if\n24: end function\n25: function DESTINATION VALIDATION\n26:\n(Note PVF in OPT header = PVFl−1)\n27:\nl = source-selected path length\n28:\nCompute PVF′ = MACKl−1(...(MACK1(MACKD(DATAHASH))))\n29:\nCompute OPV′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='29:\nCompute OPV′\nD = MACKD(PVFl−1∥DATAHASH∥ Rl−1∥TIMESTAMP)\n30:\nif (PVF′ == PVF) && (OPV′\nD == OPVD) then\n31:\nValidation succeeds\n32:\nPrepare packet using Eq. (4) and forward to source\n33:\nelse\n34:\nDrop the packet\n35:\nend if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='end if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-\nscribes how the source initializes the OPT header ﬁelds. Each OPV\nﬁeld includes the following as inputs.\nPrevious PVF: Including PVFi−1 in the OPVi computation sup-\nports the detection of a malicious intermediate router that forwards\nthe packet to a benign router, which is not speciﬁed by the source\nbut follows the protocol.\nPrevious router address: PVF by itself cannot support entities to\ndetect the packet injection attack. Hence, we include the address of\nthe previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.\nConsider an example where packet Pcrt is expected to be sent along\nthe source-selected path PATHcrt, the source previously sent packet\nPold on PATHold, and Pcrt and Pold have the same payload. Con-\nsider router Rbad that is in both PATHcrt and PATHold such that\nPATHcrt = {R1,R2,...,Rbad,Rbad+1,...,Rn} and PATHold = {R′\n1,\nR′\n2,...,Rbad,R′\nbad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='bad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′\nbad+1,...,Rm} in PATHcrt and all the cor-\nresponding ﬁelds in the Pcrt header with those in Pold. Therefore,\nwithout TIMESTAMP, the destination cannot detect the misbehavior\nand ends up validating path {R1,R2,..., Rbad, R′\nbad+1,...,Rm} for\nPcrt. By setting the TIMESTAMP ﬁeld when the source sends out a\npacket, authenticator cloning attacks are mitigated with loose time\nsynchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='synchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-\nTION functions in Algorithm 1 describe the OPT procedure that\nintermediate router Ri and the destination performs, respectively.\n276\n5.3.1\nDistrusting source and destination\nThe previous protocols assumes that the source and the destina-\ntion are honest and trust each other. We now relax this assumption\nand present an extension that handles distrusting entities. In OPT,\nthe source can generate all PVFs by itself since it knows all Ki’s.\nConsequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Consequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D\nwithout going through R1 in Figure 1.\nTo prevent such an attack and address the problem of a distrust-\ning source and destination, we use the key setup protocol in Sec-\ntion 4.2 such that intermediate routers generate two separate shared\nkeys for the source and the destination. Unlike OPT, the Extended-\nOPT header requires two PVF ﬁelds: PVFS that enables interme-\ndiate routers and the destination to validate the source, and PVFD\nthat enables the destination to conﬁrm the actual path, even if the\nsource is malicious and colludes with (at least) one intermediate\nrouter.\n6.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source is malicious and colludes with (at least) one intermediate\nrouter.\n6.\nSECURITY ANALYSIS\nWe prove that OPT has origin authenticity and path validation\nproperties when both the source and the destination are trusted.\nThis property holds on any network conﬁguration, including ones\nthat have malicious routers. Extended-OPT offers stronger prop-\nerties: the router’s origin and path validation property assumes\nthat only the source is honest; and the destination’s path validation\nproperty does not assume the source is honest.\nWe describe how OPT and its variants defend against the adver-\nsary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='sary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;\ndetails can be found in our technical report [17].\nPacket alteration. Without the secret keys (KD and Ki), a mali-\ncious router cannot compute valid PVFi and OPVi. Consequently,\nin OPT, a successful veriﬁcation of PVFi−1 (PVFn) based on OPVi\n(OPVD) implies that there can be no packet alteration attacks to\nrouter Ri (the destination), provided that the source and destination\nare trusted. A malicious destination can carry out the packet al-\nteration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='teration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary\nrouters except that only the source needs to be trusted.\nPacket injection attack. OPT routers can check that an incoming\npacket does come from an intended AS, as such information is in-\ncluded in OPV. Therefore, a malicious router A can only inject\npacket to a router B if A is B’s neighbor and the link AB is on the\nintended path. We will revisit this attack when discussing collusion\nattacks.\nIn order to inject a packet with a valid header, an attacker can\nreplay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='replay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks\ncan be mitigated by including a timestamp in the packet. OPT is\nalso vulnerable to packet injection when the destination colludes\nwith the injecting router.\nPath deviation attack. OPT ensures that a successful veriﬁcation\nof PVFi−1 (PVFn) against OPVi (OPVD) implies that the payload\nRi (the destination) received has traversed all the honest routers in\nthe source-intended path in the correct order, assuming that both\nsource and destination are honest. OPT provides this guarantee\nbecause the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='because the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or\nOPVi. As a result, malicious routers cannot mount router skipping\nor out-of-order traversal attacks.\nThis indicates that if a malicious router selects a path not in-\ntended by the source, an honest intermediary router will reject the\npacket. However, a malicious router can mount a path detour attack\nand send the payload to other routers that are not on the intended\npath.\nIn Extended-OPT, even if the destination is malicious, it cannot\nselect the unauthorized path that drops or reorders honest routers.\nExtended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Extended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops\nor reorders honest routers on the source-intended path.\nCoward attack. Retroactive-OPT can mitigate coward attacks by\nrequiring all forwarding routers to compute relevant PVF and OPV\nﬁelds for probabilistic auditing. As a router cannot reliably guess\nwhen audits will happen, it does not know when to carry out an\nattack. We are unaware of any other path validation protocols that\ncan defend against the coward attack, including ICING.\nDoS attack. We consider attacks aiming to exhaust memory and\ncomputational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='computational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources\nsending trafﬁc and the number of ﬂows transiting the router. For\nthis reason, memory exhaustion attacks are not possible under OPT.\nOPT routers perform very few symmetric cryptographic operations\nper packet during forwarding, which run at line speed (Section 7).\nTherefore, OPT is more resilient to computation resource exhaus-\ntion attacks than existing schemes such as ICING that provide sim-\nilar security guarantees.\nCollusion. The path and source validation are conditioned upon\nwhether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='whether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended\npath before Ri, malicious routers can redirect the packet to any\nrouters as it chooses. However, all preceding links on the desired\npath are still traversed in the correct sequence for this packet to be\naccepted by Ri. Similarly, a malicious router could replace the path\nin the packet and trick its neighbor into forwarding the packet to a\nrouter outside the intended path. Again, this packet will be dropped\nwhen it reaches an honest router.\nWhen there are multiple adjacent malicious routers on the in-\ntended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='tended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the\nhole via R j1 and exited the hole from R jn, but has no knowledge as\nto where the packet has been to in between these two points. In\nparticular, when the source colludes with Ri, Ri+1 can be tricked\ninto accepting and forwarding any packet.\n7.\nIMPLEMENTATION AND EVALUATION\nWe implemented OPT in Section 5.3 with DRKey as a user-level\napplication that performs source authentication and path validation.\nThe cryptographic operations performed by a router during packet\nforwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='forwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic\nfunctions used in the implementation. To compute MACs, we used\nCBC-MAC based on AES, since it requires a single AES opera-\ntion to authenticate a 128-bit value. For computing the PRF, we\nalso use the same CBC-MAC. We implement AES using AESni, a\nnew CPU instruction set provided by recent Intel and AMD CPUs\nto speed up AES operations. AESni is fast: According to Intel,\nexecuting an encryption using AES-128 in CBC mode takes 4.15\ncycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be\nincreased, as we can process 4 blocks in parallel on a single core\nin AES-128 in CBC mode, resulting in 1.33 cycles per byte. We\nimplemented authenticated encryption using Galois/Counter Mode\n(GCM) with AES.\nWe use SHA-3 for computing hashes on long strings, such as\nthe hash of the payload DATAHASH. We truncate the hash from\n277\nTable 2: Per-session storage (σ), long-term storage (LT) related\nto the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter\nSource\nDestination\nStorage\nDRKey (σ)\n0\nn+2\nn+2\n[#items]\nICING (σ)\n2\nn+1\n2∗n+1\nDRKey (LT)\n1\n1\n2\nICING (LT)\n≤ 400,000\n0\n0\nKey setup\nDRKey', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='≤ 400,000\n0\n0\nKey setup\nDRKey\n2\n[#packets]\nICING\n4∗n+4\na 256-bit value to a 128-bit value. For computing hashes on short\nstrings, such as H(PKσ), we use the Merkle-Damgard construction\nwith a Matyas-Meyer-Oseas AES-based compression function that\nmakes use of the fast hardware AESni instructions. We choose a\nsingle-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='single-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header\nsize. Nevertheless, this decision does not pose security concerns:\nan adversary besides the source needs to perform a second-pre-\nimage collision attack, which is still in the order of O(2128) for\nSHA-3 and close to O(2128) for Matyas-Meyer-Oseas.\nFor signatures, we use Ed25519 [6], providing high efﬁciency\nand security, and small signatures. For a security level of 2128 oper-\nations, Ed25519 signature generation and veriﬁcation on a 3.4GHz', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ations, Ed25519 signature generation and veriﬁcation on a 3.4GHz\nCore i7 takes 20us and 60us, respectively. Public keys and signa-\ntures are only 32 and 64 bytes, respectively. Certiﬁcates can thus be\nas small as 128 bytes, enabling routers to add their certiﬁcate to the\nkey setup message. As explained in Section 3, router certiﬁcates\ncan be generated by an AS and signed with the AS’s private key.\nThe AS’s public key can be obtained and veriﬁed through RPKI.\nFor encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='For encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-\ntude slower, it is performed by the endhost which is less perfor-\nmance critical.\nICING implementation and conﬁguration. We compare OPT\nwith ICING, the code of which we obtained from their website3.\nTo ensure the fairness of our comparison, we implemented ICING\nthat also uses AESni. In ICING, the source obtains a proof of con-\nsent (PoC) from the consent server of each node on the path. A\nPoC certiﬁes that the node consents to the full path. Furthermore,\neach node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-\ntion for computing the tag keys needed for the PoCs [25], which\ncan diminish the number of required PRF rounds to 0. In our IC-\nING implementation, we favor ICING and consider that computing\nthese keys has no computational or memory lookup overhead.\nAnother important concept in ICING is the proofs of provenance\n(PoPs)—proofs to the nodes that the packet originates from the\nsender. Computing PoPs requires shared symmetric keys between\neach pair of ICING nodes on the path. These keys can be either de-\nrived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='rived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive\nand impractical on the fast path, PoP keys are always retrieved from\nthe cache in our ICING implementation.\n7.1\nDRKey Evaluation\nDRKey enables the design of low-overhead protocols in terms\nof router resources, such as OPT. In OPT, we use DRKey for key\nsetup, which is executed once per session. Thus, the key setup cost\nis amortized over an OPT session.\nTable 2 provides an analysis of storage overhead at the source,\n3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity\nPath length\nLatency\nRouter\nIrrelevant\n381 µs\nSource\n2\n621 µs\n4\n609 µs\n8\n628 µs\nDestination\n2\n3820 µs\n4\n5520 µs\n8\n14814 µs', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4\n5520 µs\n8\n14814 µs\ndestination, and intermediate routers for DRKey and ICING. We\nalso compare the communication overhead for setting up keys.\nGiven a path of length n (excluding the source and the destina-\ntion), DRKey requires the source and the destination to store, per\nsession, n+1 symmetric keys and a public-private session key pair.\nThe long-term storage of the source and the destination, which out-\nlives multiple sessions, consists of their shared symmetric key and\nthe destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.\nIn contrast, ICING’s source and destination need to store n + 1\nsymmetric keys. Each ICING router needs to store pairwise keys\nwith every router in the Internet, which, according to the ICING au-\nthors, is within 400,000. The source also stores PoCs of all entities\nin the path.\nRegarding the communication overhead of the key setup in DRKey,\nthe source and the destination send one message each, resulting in\n2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the\npath (routers and destination), which leads to 2∗(n+1) messages\nfor a path of length n. The source also sets up pairwise shared keys\nwith each entity on the path, requiring at least a round trip (2 mes-\nsages) per entity, resulting in at least 2 ∗ (n + 1) messages. We do\nnot count the messages that are necessary to set up pairwise shared\nkeys between ICING routers, because these keys are set up once\nbetween all entities in the Internet and then stored at each entity.\nTo prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='To prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate\nrouters. Our experiment measures the latency of key setup packets\nwhile they transit the network entities. For the experiment, we use\na trafﬁc generator that initiates key setup operations and connects\nto a server that performs the key setup operations of the source,\nrouter, or destination. After the key setup, the server forwards the\npackets back to the trafﬁc generator, which measures the receive\nrate.\nTable 3 presents the latency of DRKey packets at the source,\nrouters, and the destination.\nThe results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.\nFor the source and the destination, a longer path increases the\namount of computation. In the case of the source, this is hardly no-\nticeable, because the source does not perform public-key cryptogra-\nphy operations that depend on the path length. In contrast, the des-\ntination performs per-hop public-key decryption using RSA-2048\nto obtain the shared keys, which is expensive and considerably af-\nfects the latency. Nevertheless, the results satisfy our objectives:\nsince the source and the destination have a signiﬁcantly lower traf-\nﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.\n7.2\nOPT Evaluation\nWe evaluate OPT with respect to the desired performance proper-\n278\n0\n200\n400\n600\n800\n1000\n1200\n1400\n20\n30\n40\nPacket Size (B)\nThroughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Throughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments\nuse I/O Engine for trafﬁc delivery, I/O engine represents our\nbaseline.\nties for source and path validation, namely efﬁcient forwarding and\nscalable state, and the cost associated with meeting them. Specif-\nically, we examine (1) OPT’s overhead in terms of per-packet pro-\ncessing by measuring both the throughput (the bandwidth utilized\nby whole packets including the Ethernet header)4 and goodput (the\nbandwidth used to transmit the payload of the packets, excluding\nthe OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that\nvalidates the source and the path of the incoming packets before\nforwarding them. Our comparison is with ICING [26], which we\ndiscuss in more detail in Section 9 since both provide similar se-\ncurity guarantees. We experiment with OPT and ICING to per-\nform source and path validations and we use PacketShader’s I/O\nEngine [15] to send/receive packets to/from the NICs.\nA central aspect of our work is the forwarding speed of a router.\nSince OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Since OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-\nber of sources sending packets or the number of ﬂows transiting\na router. In contrast, we analyze the impact of (1) cryptographic\noperations and (2) memory lookup of cryptographic keys, because\nthe forwarding overhead of path validation protocols that use cryp-\ntography depends on these metrics.\nEvaluation system. Our testbed consists of two routers A and B.\nBoth are equipped with two Intel Ethernet Server Adapter X520-\nT2 NICs, and they both run Ubuntu Linux Kernel version 3.2.0-3.\nSystem A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='System A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM\nand two Xeon L5640 2.26GHz (6 cores) processors. System B\nruns our software router code featuring 16 x 4GB DDR3 RAM and\ntwo Intel Xeon E5-2680 2.70GHz (8 cores) processors. The trafﬁc\ngenerator generates trafﬁc at a rate of 40Gbps, which is processed\non the software router and sent back for measurement.\n7.3\nExperiment setup\nWe ﬁrst describe how packets are forwarded from one router to\nthe other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.\nWe use PacketShader’s I/O Engine [15], a high-speed open source\nimplementation to send/receive packets to/from the NICs. On the\nsending router, I/O Engine takes the packets generated by the user-\nlevel trafﬁc generator and sends them to the NIC. When the packets\narrive at the second router’s NIC, I/O Engine takes the packets from\nthe NIC and delivers them to the user-level application, where they\nare processed according to the protocol (OPT or ICING). The last\nstep is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='step is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at\nthe second router from the moment I/O Engine delivers the packets\nto the user-level application and until the packets are ready to be\nsent back.\nExperiments. We measure the forwarding speed of the software\nrouter for OPT and ICING. Our experiments consider AS-level\npath validation scenarios, where path validation is performed at a\n4We add 20B Ethernet overhead in computing the throughput.\nsingle router (e.g., ingress router) within an AS. Since an AS is ad-\nministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen\ninside an AS in practice. Consequently, we perform tests with a\nmaximum path length of 10 hops (without counting the source).\nThe minimum path length is 2 hops, corresponding to the case of a\nsource, an intermediate hop, and a destination.\nTo measure the forwarding overhead at a router, which includes\nthe memory overhead for storing keys and for retrieving them, we\nconsider a network where each node has α neighbors. The param-\neter α is important only if the router performs work that depends\non the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='on the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to\nverify its OPV ﬁeld, then computes the hash of the payload and\nﬁnally updates the PVF ﬁeld. These operations do not depend on\nthe number of neighbors the router has nor on the path length.\nHowever, in ICING the router has to look up the shared sym-\nmetric keys with each node on the path in a table that contains the\nkeys of all the nodes the router had previously seen on a path. For\na path of maximum length n, these nodes are located within n-hop\ndistance away from the router. Our choice for the parameter α = 3\nand the maximum path length of 10 hops gives (311−1)\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the maximum path length of 10 hops gives (311−1)\n2\n= 9841 as\na maximum key table size, which is within ICING’s maximum key\ntable size of 400,000 [25].\nThe router receives packets at a line rate of 40Gbps. To quantify\nthe throughput and goodput with respect to the overhead of OPT\nand ICING, we perform tests with payload sizes of 20B, 256B,\n576B, 768B, and 1024B. We add to these values the OPT and IC-\nING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond\nto the TCP header size to simulate TCP/ICING and TCP/OPT. We\nnote that TCP/OPT includes the IP header since OPT runs over the\nIP network; whereas TCP/ICING does not include the IP header\nsince ICING is designed to replace IP. Hence, the goodput compu-\ntation favors ICING.\nThe biggest payload size is computed by subtracting from the\nMTU the header size of ICING for the longest tested path (10\nhops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the\nsource identiﬁer and 13B of common ﬁelds, which gives a header\nlength of 457B for a 10-hop path. The computation is 1500B −\n457B = 1043B, which explains our choice of 1024B of the maxi-\nmum payload. In case of OPT, 52B common ﬁelds, 16B per-hop\noverhead and 40B TCP/IP header result in 252B. As a result, the\nmaximum payload size is dictated by the size of the ICING header\nfor the longest path considered.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='maximum payload size is dictated by the size of the ICING header\nfor the longest path considered.\nMethod. We generate trafﬁc for 10 seconds at a rate of 40Gbps,\nwhich is forwarded to the router running the protocol for source\nauthentication and path validation, and then forwarded back to the\nsource. To measure the throughput, we employ I/O Engine’s scripts,\nwhich operate as follows. These scripts read the RX and TX counter\nvalues of the NIC at the beginning of the experiment and then read\nthe values again every second to compute the number of packets\nsent and received. Consequently, we obtain the throughput values\nevery second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='every second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING\nheader, respectively, from the packet size.\n7.4\nForwarding Overhead\nWe evaluate the most computationally intensive protocol version\nof OPT described in Section 5.3. The evaluation results show that\nOPT outperforms ICING by a signiﬁcant margin. Since the other\nversions of OPT feature smaller packet headers and less computa-\n279\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(a) 2-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(b) 4-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(c) 8-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(d) 10-hop path\nFigure 6: Throughput and goodput (i.e., throughput obtained\nonly for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='only for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths\nvarying from 2 to 10 hops.\ntional overhead, they would outperform ICING with an even larger\nmargin.\nFigure 6 depicts the results for throughput and goodput for OPT\nand ICING. We performed experiments for different path lengths\nand packet sizes described in Section 7.3, and the numbers we ob-\ntained show consistent results over all experiments, as explained in\nthe next paragraphs.\nA ﬁrst observation is that throughput registers higher values than\ngoodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='goodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-\nsuring goodput. We also notice that, for all path lengths, OPT’s\nthroughput is close to 40Gbps except for the smallest packet size\n(i.e., 20B). We note that OPT’s throughput for small packets is\nmainly limited by I/O Engine’s throughput as shown in Figure 5.\nAs the path length grows, I/O Engine’s bottleneck becomes re-\nleased because of the reduced number of packet copies between the\nNIC and the user-level packet processing engine5; and as a conse-\nquence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.\nFor each path length, the goodput of both OPT and ICING in-\ncrease as the packet size increases, because the protocol header rep-\nresents a smaller fraction of the total packet size as the payload size\nincreases. Even though ICING’s throughput also increases with\nthe packet size, its value is much smaller than OPT’s throughput.\nGiven the choices for our ICING implementation, as explained ear-\nlier, this result is mainly due to the key table lookup for the PoP\nkeys. Instead, OPT uses AESni operations to derive the keys shared\nwith the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='with the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT\nas much as 10Gbps in comparison to ICING.\n7.5\nPath Length Scalability\nIn order to analyze the protocols’ scalability with respect to the\npath length, we depict in Figure 7 the ratio between the goodput and\nthe throughput (named goodput ratio) for 256B and 1024B packets.\nWe vary the path length from 2 to 10 hops.\nThe goodput ratios of 256B packets are lower those of than 1024B\npackets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='packets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet\nheader, resulting in goodput degradation for both OPT and ICING.\n5The increased header size reduces the number of packets needed\nto saturate the link bandwidth.\n2\n4\n6\n8\n10\n0\n20\n40\n60\n80\n100\nPath Length (Hops)\nGoodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Goodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B\nICING−1024B\nFigure 7: The goodput ratio (i.e., goodput/throughput) of OPT\nand ICING for small and large packets, in the context of path\nlengths varying from 2 to 10 hops.\nThe ﬁgure shows that OPT has better path length scalability than\nICING since the goodput ratio of OPT decreases slower than that of\nICING as the path length increases. Speciﬁcally, when the 2-hop\npath is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)\n64.7·8\n= 3.03%, while IC-\nING’s is (64.5−34.9)\n64.5·8\n= 5.74%; for 1024B packets, OPT’s goodput\ndegradation ratio per hop is (88.1−79.3)\n88.1·8\n= 1.25%, while ICING’s is\n(87.9−68.2)\n87.9·8\n= 2.80%.\n8.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='(87.9−68.2)\n87.9·8\n= 2.80%.\n8.\nDISCUSSION\nKey lifetime. The keys associated with a session σ are valid as long\nas (1) PATHσ between S and D in the session σ does not change,\nand (2) S or D do not terminate the session due to application-driven\nsession lifetime requirement.\nAccording to the ﬁrst point, the maximum key lifetime is deter-\nmined by route stability. Recent end-to-end route stability anal-\nyses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='yses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-\nniques [9, 18, 22]. Although many routes are still short-lived, en-\ntities send packets over long-lived routes (longer than 6 hours) for\n96% of the times [9]. In particular, considering the fact that the\nload balancing within an ISP causes most route variations (i.e., up\nto 82%), OPT running at AS-level uses more stable routes than\nrouter-level OPT. Furthermore, when routers perform per-ﬂow or\nper-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='per-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-\nstrated by numerous incidents such as Man-in-the-Middle BGP route\nhijacking [8]. In this case, the network routes could ﬂap as the\nattacker wishes. Yet, some future Internet architecture proposals\n(Nebula [2], Pathlets [12], SCION [40], XIA [14]) relieve this pain\npoint by having packets carry forwarding information in the packet\nheader, so that the source is always aware of the path and would set\nup a new session if the path changes.\nWe expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the\nearliest at a granularity of tens of minutes. Thus, the key setup\noverhead represents a tiny fraction of the total computation and\ncommunication overhead of a long-lived high-bandwidth connec-\ntion.\nEfﬁcient packet content authentication. As described in Sec-\ntion 5.3, each intermediate router uses the DATAHASH ﬁeld in the\nOPT header when it veriﬁes its OPV ﬁeld. Such a veriﬁcation does\nnot authenticate the packet content since a malicious intermediate\nrouter could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='router could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in\nparallel while the packet is being scheduled for transmission.\nAs an alternative, probabilistic veriﬁcation schemes [16] can be\n280\napplied such that every router decreases the veriﬁcation probabil-\nity if the DATAHASH veriﬁcation succeeds. However, if a router\ndetects a packet with a bogus hash value, the probability to run\nhash veriﬁcation increases. Furthermore, as soon as a router re-\nceives multiple mismatching hash values, it immediately performs\nhash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets\nincoming on interfaces arriving from the malicious router. With\nsuch a probabilistic veriﬁcation approach, we can further improve\nthe efﬁciency and practicality while providing data authentication.\nOPT in the current Internet. OPT could be incrementally de-\nployed in the current Internet. An AS could announce its OPT\nfunctionality within BGP update messages (as a transitive attribute)\nor as extension to RPKI certiﬁcates, enabling the selection and con-\nstruction of end-to-end OPT paths at source ASes. Endhosts could\nobtain OPT path information from a local route server which col-\nlects BGP and RPKI information.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='obtain OPT path information from a local route server which col-\nlects BGP and RPKI information.\nTo carry OPT-based information in packets, the simplest ap-\nproach would be an IPv6 extension header. In IPv4, spare IP header\nbits would need to be used to indicate the presence of an extra\nOPT header or trailer, but an extra header after the IP header may\ndisrupt processing at legacy ﬁrewalls or other middleboxes. With\nthe increasing support for IPv6, we prefer incremental deployment\nvia the IPv6 extension header. Since the DRKey information is\nlarger than the 256 bytes that ﬁt into an IPv6 extension header, we\npropose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='propose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,\nASes could specify addresses of DRKey servers that would handle\nDRKey packets to set up the keys for the routers and thus would\nshare the secret keys KRi of routers. An endhost could then place\na sequence of DRKey server addresses (similar to a loose source\nrouting option in IPv4) into the DRKey packet, which would be\nsequentially processed and forwarded until the destination. This\nlatter approach avoids routers from analyzing the IP protocol ﬁeld.\n9.\nRELATED WORK\nThe most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source\npre-computes a veriﬁer MAC (Vi) for each intermediate router Ri\nusing the respective shared secret key as well as the hash value of\nthe path and the static content in the header. For each packet, Ri ﬁrst\nreconstructs and XORs the MAC for the source and each upstream\nrouter, and veriﬁes if the XORed MACs are equivalent to what is\nstored in Vi. Then Ri (1) computes a MAC for each downstream\nrouter on the path using the shared secret key, the hash of the path,\nand the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).\nICING is more heavy-weight than OPT. ICING requires each\nRi to derive a Difﬁe-Hellman (DH) key with each router R j on\nthe path, which requires routers to cache keys to avoid the heavy-\nweight DH computation during packet forwarding. For the case\nthe keys are not cached any more, ICING suggests adding the 20-\nbyte public key of each router into each packet, resulting in a high\nper-packet overhead. Also, ICING requires each node to insert a\nMAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='MAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require\nrouters to store keys shared with sources or other routers, nor per-\nform a MAC computation for each router on the path. In terms of\nsecurity, even ICING intermediate routers can detect colluding path\ndeviation attacks mounted by the source and a malicious router to\nanother intermediary router. In contrast, Extended-OPT supports\nthe destination to detect such attacks. More speciﬁcally, the ori-\ngin and path validation property of the routers still depends on the\ncorrect behavior of the source, but not on the destination. Conse-\nquently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,\nan illegitimate packet travels longer in Extended-OPT than ICING,\nbut will be rejected by the destination. Beside the malicious source\ncollusion attack, OPT and ICING provide the same kind of source\nand path authenticity properties for the destination. On the other\nhand, retroactive key setup OPT with path tracing (which only en-\nables the destination to verify the path) can mitigate the coward\nattack, which ICING fails to mitigate.\nLiu et al. propose Passport for intermediate routers to perform\nsource authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any\ntwo ASes to compute a shared secret based on Difﬁe-Hellman key\nexchange. Using the respective shared secret key with each down-\nstream AS, the source AS computes a MAC for each AS on the\npath, and inserts it in the Passport header. Each intermediate AS\nauthenticates the source AS by recomputing the MAC using the\nshared key and conﬁrming that it matches the MAC in the Passport\nheader. Similar to Passport, the accountability service by Ben-\nder et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='der et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers\nthrough a key derivation mechanism that is similar to DRKey (al-\nthough it cannot provide retroactive key setup) and uses these keys\nfor embedded source authenticators. Passport and the accountabil-\nity service provide weaker security guarantees than OPT, as they\nprovide only source AS authentication, and fail to defend against\nsource and data spooﬁng, as well as path deviation attacks. While\nSNAPP does prevent against source and data spooﬁng, it does not\nprevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet\nstate [19, 31], perform packet marking [30, 32], or active path in-\nterrogation [27]. Pi suggests a path identiﬁer to detect source IP\naddress spooﬁng [36]. Unfortunately, these schemes are suscepti-\nble to attacks listed in Section 2.3, because they were designed for\na different purpose. Similarly, network capability mechanisms [3,\n29, 37, 38] cannot provide source authentication or path validation\nas the capability can be easily copied and inserted by the last AS.\n10.\nCONCLUSION\nDespite the importance of network-based source authentication', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='10.\nCONCLUSION\nDespite the importance of network-based source authentication\nand path validation, these primitives have not been implemented\nso far, perhaps because of the lack of an efﬁcient protocol that does\nnot burden the router. This paper introduces (1) DRKeys as efﬁcient\nand dynamically recreatable key setup protocols, and (2) OPT as an\nextremely lightweight, scalable, and secure protocol that provides\nsource authentication and path validation. Compared with currently\nproposed solutions, OPT achieves performance improvements with\nminimal latency and computational overhead on routers regardless\nof the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='of the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-\nity. We also introduce a retroactive key setup process that protects\nagainst coward attacks, as routers cannot know in advance which\npaths are being monitored subsequently. We anticipate that OPT’s\nsecurity and performance properties will bring source authentica-\ntion and path validation into the realm of practicality.\n11.\nACKNOWLEDGMENTS\nWe thank George Danezis, Yue-Hsun Lin, Raphael Reischuk,\nmembers of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='members of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281\nWe gratefully acknowledge funding support for this research from\nCyLab at Carnegie Mellon, NSF under award CNS-1040801, Euro-\npean Research Council under the European Union’s Seventh Frame-\nwork Programme (FP7/2007-2013) / ERC grant agreement 617605,\nand a gift from KDDI.\n12.\nREFERENCES\n[1] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen,\nD. Moon, and S. Shenker. Accountable Internet Protocol', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='D. Moon, and S. Shenker. Accountable Internet Protocol\n(AIP). In Proceedings of ACM SIGCOMM, 2008.\n[2] T. Anderson, K. Birman, R. Broberg, M. Caesar, D. Comer,\nC. Cotton, M. Freedman, A. Haeberlen, Z. Ives,\nA. Krishnamurthy, W. Lehr, B. Loo, D. Mazières,\nA. Nicolosi, J. Smith, I. Stoica, R. van Renesse, M. Walﬁsh,\nH. Weatherspoon, and C. Yoo. The nebula future internet\narchitecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='architecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer\nScience, pages 16–26. Springer Berlin Heidelberg, 2013.\n[3] T. Anderson, T. Roscoe, and D. Wetherall. Preventing\nInternet Denial-of-Service with Capabilities. In Proceedings\nof Hotnets-II, 2003.\n[4] ARIN. Resource Public Key Infrastructure (RPKI).\nhttps://www.arin.net/resources/rpki/.\n[5] A. Bender, N. Spring, D. Levin, and B. Bhattacharjee.\nAccountability as a Service. In Proc. of USENIX SRUTI,\n2007.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Accountability as a Service. In Proc. of USENIX SRUTI,\n2007.\n[6] D. J. Bernstein, N. Duif, T. Lange, P. Schwabe, and B.-Y.\nYang. High-speed high-security signatures. In Proc. of\nCHES, 2011.\n[7] J. Cowie. The new threat: Targeted internet trafﬁc\nmisdirection. http://www.renesys.com/2013/11/mitm-\ninternet-hijacking/, Nov. 2013.\n[8] J. Cowie. The New Threat: Targeted Internet Trafﬁc\nMisdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Misdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-\nhijacking/, Nov 2013.\n[9] I. Cunha, R. Teixeira, and C. Diot. Measuring and\ncharacterizing end-to-end route dynamics in the presence of\nload balancing. In Proc. of PAM’11, 2011.\n[10] Y. Gilad and A. Herzberg. Plug-and-Play IP Security:\nAnonymity Infrastructure Instead of PKI. In Proceedings of\nESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In\nProc. of SIGCOMM, 2009.\n[12] P. B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica. Pathlet\nrouting. In Proceedings of the ACM SIGCOMM 2009\nConference on Data Communication, 2009.\n[13] S. Gueron. Intel Advanced Encryption Standard (AES) New\nInstructions Set, Mar. 2010. white paper 323641-001,\nRevision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Revision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,\nA. Mukundan, W. Wu, A. Akella, D. G. Andersen, J. W.\nByers, S. Seshan, and P. Steenkiste. XIA: Efﬁcient support\nfor evolvable internetworking. In Proceedings of USENIX\nConference on Networked Systems Design and\nImplementation, 2012.\n[15] S. Han, K. Jang, K. Park, and S. Moon. PacketShader: a\nGPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='GPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.\n[16] H.-C. Hsiao, A. Studer, C. Chen, A. Perrig, F. Bai, B. Bellur,\nand A. Iyer. Flooding-Resilient Broadcast Authentication for\nVANETs. In Proc. of MobiCom, 2011.\n[17] L. Jia, C. Basescu, T. H.-J. Kim, A. Perrig, Y.-C. Hu, and\nF. Zhang. Mechanized network origin and path authenticity\nproofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='proofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.\n[18] M. S. Kang, S. B. Lee, and V. D. Gligor. The Crossﬁre\nAttack. In Proc. of IEEE Security and Privacy, 2013.\n[19] J. Li, M. Sung, J. Xu, and L. Li. Large-Scale IP Traceback in\nHigh-Speed Internet: Practical Techniques and Theoretical\nFoundation. In Proc. of IEEE Security and Privacy, 2004.\n[20] B. Liu, J. T. Chiang, J. J. Haas, and Y.-C. Hu. Coward\nAttacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Attacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.\n[21] X. Liu, A. Li, X. Yang, and D. Wetherall. Passport: Secure\nand Adoptable Source Authentication. In Proc. of NSDI,\n2008.\n[22] H. V. Madhyastha, E. Katz-Bassett, T. Anderson,\nA. Krishnamurthy, and A. Venkataramani. iPlane Nano: Path\nPrediction for Peer-to-peer Applications. In Proc. of NSDI,\n2009.\n[23] D. Mazieres, M. Kaminsky, M. F. Kaashoek, and E. Witchel.\nSeparating Key Mangement from File System Security. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Separating Key Mangement from File System Security. In\nProceedings of SOSP, 1999.\n[24] R. Moskowitz and P. Nikander. Host Identity Protocol (HIP)\nArchitecture, May 2006.\nhttp://tools.ietf.org/html/rfc4423.\n[25] J. Naous. Path-policy Compliant Networking and a Platform\nfor Heterogeneous IAAS management. In PhD thesis, 2011.\n[26] J. Naous, M. Walﬁsh, A. Nicolosi, D. Mazieres, M. Miller,\nand A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.\n[27] V. N. Padmanabhan and D. R. Simon. Secure Traceroute to\nDetect Faulty or Malicious Routing. ACM SIGCOMM\nComputer Communications Review, January 2003.\n[28] J. Pappalardo. New Transatlantic Cable Built to Shave 5\nMiliseconds off Stock Trades. Popular Mechanics,\nhttp://www.popularmechanics.com/technology/\nengineering/infrastructure/a-transatlantic-\ncable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.\n[29] R. Raghavan and A. C. Snoeren. A system for authenticated\npolicy-compliant routing. In Proc. of ACM SIGCOMM,\n2004.\n[30] S. Savage, D. Wetherall, A. Karlin, and T. Anderson.\nPractical Network Support for IP Traceback. In Proc. of\nSIGCOMM, 2000.\n[31] A. C. Snoeren, C. Partridge, L. A. Galindo, C. E. Jones,\nF. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='F. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP\nTraceback. In Proceedings of ACM SIGCOMM, 2001.\n[32] D. X. Song and A. Perrig. Advanced and Authenticated\nMarking Schemes for IP Traceback. In Proceedings of IEEE\nINFOCOM, 2001.\n[33] I. Stoica, D. Adkins, S. Zhaung, S. Shenker, and S. Surana.\nInternet indirection infrastructure. In Proc. of SIGCOMM,\n2002.\n[34] M. Walﬁsh, J. Stribling, M. Krohn, H. Balakrishnan,\nR. Morris, and S. Shenker. Middleboxes No Longer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R. Morris, and S. Shenker. Middleboxes No Longer\nConsidered Harmful. In Proceedings of OSDI, 2004.\n[35] D. Wendlandt, D. G. Andersen, and A. Perrig. Perspectives:\nImproving SSH-style host authentication with multi-path\nprobing. In Proceedings of USENIX Annual Technical\nConference, June 2008.\n[36] A. Yaar, A. Perrig, and D. Song. Pi: A Path Identiﬁcation\nMechanism to Defend against DDoS Attacks. In Proc. of\nIEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='IEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability\nMechanism to Mitigate DDoS Flooding Attacks. In Proc. of\nthe IEEE Security and Privacy, May 2004.\n[38] X. Yang, D. Wetherall, and T. Anderson. A DoS-limiting\nNetwork Architecture. In Proc. of SIGCOMM, 2005.\n[39] X. Zhang. Secure and Efﬁcient Network Fault Localization.\nPhD thesis, Carnegie Mellon University, 2012.\n[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and\nD. G. Andersen. SCION: Scalability, control, and isolation\non next-generation networks. In Proceedings of the IEEE\nSymposium on Security and Privacy (Oakland), May 2011.\n282', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc605f50> 111
cuda:2
[Document(page_content='Lightweight Source Authentication and Path Validation\nTiffany Hyun-Jin Kim\nCyLab, CMU\nhyunjin@cmu.edu\nCristina Basescu\nETH Zürich\ncba@inf.eth.ch\nLimin Jia\nCyLab, CMU\nliminjia@cmu.edu\nSoo Bum Lee\nQualcomm\nsoobuml@qti.qualcomm.com\nYih-Chun Hu\nUIUC\nyihchun@uiuc.edu\nAdrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Adrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT\nIn-network source authentication and path validation are funda-\nmental primitives to construct higher-level security mechanisms\nsuch as DDoS mitigation, path compliance, packet attribution, or\nprotection against ﬂow redirection. Unfortunately, currently pro-\nposed solutions either fall short of addressing important security\nconcerns or require a substantial amount of router overhead. In this\npaper, we propose lightweight, scalable, and secure protocols for\nshared key setup, source authentication, and path validation. Our\nprototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors\nC.2.0 [Computer-Communication Networks]: Security and pro-\ntection; C.2.1 [Network Architecture and Design]: Circuit-switch-\ning networks, Packet-switching networks\nKeywords\nSource Authentication, Path Validation, Retroactive Key Setup\n1.\nINTRODUCTION\nSource authentication and path validation are useful primitives\nto help mitigate various network-based attacks, such as DDoS, ad-\ndress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='dress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according\nto the policies of ISPs, enterprises, and datacenters. Endhosts and\nISPs desire to validate service level agreement compliance regard-\ning data delivery in the network: Did the packet truly originate from\nthe claimed client? Did the client select a path that complies with\nthe service provider’s policy? Did the packet indeed travel through\nthe path selected by the client?\nUnfortunately, the current Internet provides almost no means for\nsource authentication and path validation by routers or endhosts,\nopening up numerous attack surfaces. For example, a malicious\nISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for proﬁt or commercial advantage and that copies bear\nthis notice and the full citation on the ﬁrst page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior speciﬁc permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='permissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.\nCopyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00.\nhttp://dx.doi.org/10.1145/2619239.2626323.\ntively, a malicious router may inject packets with a spoofed source\naddress to incriminate a victim source node into having sent an ex-\ncessive number of packets. A malicious router may simply alter the\ncontents of received packets as well. The inability to detect such\nattacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-\nvert trafﬁc to traverse a point of eavesdropping it controls—a seri-\nous issue in particular for sensitive information.\nEnd-to-end encryption and authentication mechanisms, such as\nTLS, do not solve any of the above issues, since they are agnos-\ntic to which path the packet takes. A stronger approach is needed,\nwhich enables routers and destinations to perform source authenti-\ncation and path validation. As we discuss in the related work, ex-\nisting solutions either require extensive overhead, or only partially\naddress fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='address fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses\nboth source authentication and path validation, but it requires each\nintermediate router on a path to store and look up keys shared with\nother routers; ICING requires 42 bytes per verifying router in the\npacket header. Furthermore, ICING requires each router to calcu-\nlate a Message Authentication Code (MACs) for all other routers\non the path. In contrast, our protocol does not require any per-\nclient state on routers; it requires only 16 bytes per hop (which can\nbe reduced to 2 bytes for a lower level of security), and only a sin-\ngle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='gle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-\nvents against coward attacks [20], where an adversary only attacks\nwhen it knows that the attack will not be detected. Our protocol,\nhowever, offers reduced security in the case of a malicious sender\ncolluding with a malicious router on the path, which we describe\nin detail in the related work section. Since in the common case,\nsender and receiver trust each other, the performance gain of O(1)\nMAC operation per router instead of O(n) is worth the tradeoff.\nContributions. In this paper, we present Dynamically Recreatable\nKey (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Key (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of\nDRKey on routers prevents state exhaustion DoS attacks and sim-\npliﬁes router architecture. We further enrich DRKey with a new\nnotion called retroactive key setup that provides the following de-\nsirable properties: (1) in contrast to previous protocols, source and\ndestination can start the communication without needing to wait\nfor the expensive key setup to complete, providing efﬁciency; (2)\nif misbehavior is suspected, endhosts set up keys retroactively to\nverify previous packets, defending against coward attacks.\nBased on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Based on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,\nscalable, and secure protocols for source authentication and path\nvalidation. We introduce an extension called Retroactive-PathTrace\nthat supports the destination to perform path validation with retroac-\n271\ntive key setup and to detect coward attackers with small, constant\noverhead in the packet header. Our OPT protocols enable imple-\nmentation on SW routers with minimal performance impact.\n2.\nPROBLEM DEFINITION\n2.1\nDesired Security Properties\nSource authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Source authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether\nthe packet content has not been altered en route. In this paper,\nsource authentication includes data authentication.\nPath validation. The source, intermediate routers, and the desti-\nnation should be able to validate that the packet indeed traversed\nthe path known to (or selected by) the source. Successful path val-\nidation ensures that the packet traversed each honest router on the\npath in the correct order. Unfortunately, no scheme can provide\nany guarantees for malicious routers: if malicious router Rm pub-\nlishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='lishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2\nElided Security Properties\nNo packet delivery guarantee. Routers generally have the free-\ndom to decide whether or not to forward packets. Hence, it is not\nthe purpose of path validation to guarantee that packets will be de-\nlivered to the speciﬁed destination.\nNo detection of packet siphoning. Misbehaving router Rm on the\nsource-selected path can siphon packets and send them over a sep-\narate channel to a remote entity. Since Rm still forwards the packet\nto Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with\nthe packet.\nNo locating of packet altering and dropping routers. Locating\nrouters that alter or drop packets is the goal of fault-localization\nmechanisms—another challenging problem especially in inter-domain\nsettings [39]. Since path validation is a simpler problem, the goal\nis to achieve a more efﬁcient protocol than heavy-weight fault lo-\ncalization.\n2.3\nAdversary Model\nWe consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.\nPacket alteration. A malicious router alters any part of the packet,\nsuch as source address, header information, or payload data.\nPacket injection. A malicious router fabricates a packet and sends\nit towards a destination of its choice. A packet replay attack is a\nspecial case of packet injection.\nPath deviation. A malicious router may perform path deviation\nattacks, which cause packets to be forwarded along a path other\nthan the path previously selected by the source. We subdivide this\nattack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns\nto the correct downstream router Rm+1 to resume traversal of all\nrouters on the intended path.\nRouter skipping: A malicious router redirects the packet and\nskips other router(s) on the path. Thus, some routers on the in-\ntended path does not forward the packet.\nOut-of-order traversal: An adversary causes path deviations\nsuch that routers on the intended path are not traversed in the\nright order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='right order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For\nexample, an attacker diverts trafﬁc only when the protocol is inac-\ntive (e.g., required keys for validation have not been established).\nDenial-of-Service (DoS). As part of DoS attacks, we consider mem-\nory and computation exhaustion attacks on routers performing source\nauthentication and path validation.\nCollusion. Protocol participants may collude to carry out any of\nthe attacks listed above. For example, two or more intermediate\nrouters may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router\nto spoof authenticators for its downstream routers if the destina-\ntion prefers/trusts skipped routers. Also, both the source and the\ndestination could collude with some intermediate routers to frame\nanother router on the path by not forwarding packets to it.\nIn Section 6, we explore potential attacks against our protocols\nthat violate the desired properties and discuss how OPT defends\nagainst these attacks.\n3.\nOPT DESIGN OVERVIEW\nWe consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and\nRi’s as tracing entities. At a very high level, the main insights for\nachieving source authentication and path validation without requir-\ning routers to maintain per-source or per-path-length state are as\nfollows: (1) In the packet header, source S includes H(P), which is\nthe hash of the packet payload to help receiving entities identify the\npacket while avoiding expensive hash computation at each router;\n(2) On demand, each router Ri generates key Ki using a symmet-\nric cryptographic operation, and requires only router’s local secret\nSVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='SVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless\nand faster than storing or retrieving secrets. (3) Each router per-\nforms source authentication using a MAC computed over H(P); (4)\nEach router Ri extends a special authentication ﬁeld called PVF by\nperforming a MAC operation. Hence, path validation is achieved\nthrough a chain consisting of nested MACs.\n3.1\nAssumptions\nFor the communication properties of the network, we assume\nthat the source knows the path that the packet will traverse at the\nAS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the\nvalidations. This information can stem from (1) the BGP proto-\ncol where the source can learn the AS path that the packet is ex-\npected to traverse, (2) Pathlet routing [11] or SCION [40] where\nthe source can specify the path in the packet header, or (3) i3 [33]\nor Platypus [29] where the source can deﬁne a sequence of servers\nto traverse. Alternatively, an ISP may provide the premium path\ninformation to clients as an extra service (e.g., transatlantic cable\nfor ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='for ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.\nFor the cryptographic key setup, the source and the destination\nneed to be able to authenticate the router’s cryptographic materi-\nals (i.e., validate a signature that binds an entity to some crypto-\ngraphic materials). In the case of AS-level tracing, the AS needs to\nbe authenticated, and such authentication can be achieved through\nRPKI [4], which is already operational. RPKI provides a PKI that\nenables authentication of AS certiﬁcates, each of which binds an\nAS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn\ncreates certiﬁcates for each router using the AS’s private key—\nenabling the tracing entity to verify via the AS certiﬁcate using\nRPKI.1\n1Alternatively, OPT can authenticate entities based on mechanisms\n272\nTable 1: Notation.\n(PKE,PK−1\nE )\nEntity E’s public-private key pair\nCertPKE\nEntity E’s public-key Certiﬁcate\nˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions\nKE\nSymmetric key among S, D, and entity E for a single session\nKE1E2\nSymmetric key between entities E1 and E2 for a single session\nKE1E2σ\nSymmetric key for E1 and E2 in session σ for E1-initiated\npackets\nSVE\nEntity E’s local secret value\nP\nNetwork packet payload\n(PKσ,PK−1\nσ )\nPublic-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Public-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ\nTime when S initiates session σ\nSESSIONID\nHash of session σ’s public key, path, session initiation time\nAUTHσ\nAuthenticated and encrypted SESSIONID and private key for\nsession σ\nSignKEYPK−1\nE ,σ\nSignature on a symmetric key for session σ using entity E’s\nprivate key\nEncKEYK,σ\nEncryption of a symmetric key for session σ using key K\nKEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH\nHash of the packet’s payload\nPVF\nField enabling D to verify the path\nPVFS\nField enabling Ri and D to verify the path\nPVFD\nField enabling D to conﬁrm the actual path\nOVi\nField enabling Ri to validate the packet sender\nOPVi\nField enabling Ri to verify both the packet sender and path\nSignPK−1\nE (·)\nSignature using entity E’s private key\nCheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='CheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key\nEncK(·), DecK(·)\nEncryption, decryption using key K\nAuthEncK(·)\nAuthenticated encryption using key K\nAuthDecK(·)\nAuthenticated decryption using key K\nFK(·)\nPseudo-random function using key K\nMACK(·)\nMessage Authentication Code using key K\nH(·)\nCryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Cryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret\nkey between each other. In case the tracing entities are AS infras-\ntructure hosts such as edge routers, ﬁrewalls, or a middlebox at a\nservice provider, either RPKI can be used as described above or an\nadministrator can set up trusted public keys between entities that\nneed path veriﬁcation. If endhosts perform tracing, then a shared\nkey can be set up through SSL or TLS if one of the endhosts is a\nHTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='HTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted\nkeys, TOFU (Trust On First Use) in SSH, TOFU with Perspec-\ntives [35], RPKI with domain-certiﬁed host keys, self-certifying\nIDs as public keys [1,23,24,34], or self-validation using an anony-\nmous service [10]. We assume that one of these approaches is used\nto set up symmetric key ˆKSD between source S and destination D.\n3.2\nMain Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Main Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each\nrouter to maintain a secret key for each ﬂow, our design enables\nrouters to derive the secret keys on the ﬂy using only local se-\ncrets stored at the routers and an efﬁcient pseudo-random function.\nThus, we avoid storing all the keys.\nMore precisely, OPT runs in sessions. In each session σ, source\nS sends packets to destination D on path PATHσ. S and D leverage\nlong-term symmetric key ˆKSD to set up keys with each router in\nPATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,\nthe source prepares and inserts a special ﬁeld in the packet header\ncalled SESSIONID such that intermediate routers Ri on PATHσ dy-\nnamically compute the shared symmetric key with S and D (Ri only\nneeds to look up its local secret SVRi for computation).\nthat use self-certifying IDs as public keys [1,23,24,34] as assumed\nin ICING. However, such mechanisms have issues with key revo-\ncations. Hence, we prefer to use RPKI.\nOur key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Our key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS\nattacks based on state exhaustion. Moreover, computing pseudo-\nrandom function (PRF) F is faster than performing a cache access;\nfor instance, a key derivation using AESni takes 32 cycles, whereas\na L3 cache read operation requires approximately 40 cycles (on In-\ntel “Sandy-Bridge”-based Xeon architecture).\nOPT includes the hash of the packet payload H(P) in the header,\nwhich enables an important optimization: routers can either par-\nallelize the computations of MAC and the hash of the packet, or\nprobabilistically validate H(P).\n3.3\nOPT Protocol Overview', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='probabilistically validate H(P).\n3.3\nOPT Protocol Overview\nDRKey for path selection and key setup. When source S initiates\nsession σ at time Tσ, S selects path PATHσ to destination D, gener-\nates asymmetric public/private key pair (PKσ,PK−1\nσ ), and creates a\nsession identiﬁer, where SESSIONID = H(PKσ∥PATHσ∥Tσ). Af-\nter preparing some values that support source authentication and\npath validation for other entities on PATHσ, S forwards the OPT\npacket to its downstream router on PATHσ. If Tσ is recent (i.e.,\nwithin some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='within some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and\nSESSIONID. Detailed DRKey protocols are explained in Section 4.\nGeneration of veriﬁcation ﬁelds. S uses the path information to\npre-compute veriﬁcation ﬁelds, one for each router Ri on PATHσ,\nand a special ﬁeld called PVF such that routers can perform source\nauthentication and path validation.\nVeriﬁcation and update by intermediate routers. Upon receiv-\ning a packet, Ri ﬁrst regenerates the shared symmetric key Ki and\nrecomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='recomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it\nsuccessfully authenticates the source and the content of the packet,\nand validates the traversed path. Ri then updates PVF, by applying\na MAC operation using Ki to the ﬁeld. This process helps down-\nstream routers and the destination to validate that each router on the\npath has indeed seen the packet.\nVeriﬁcation by destination. The destination ﬁnally recomputes\nthe veriﬁcation ﬁelds using all the symmetric keys shared with\nother entities on the path. Successful veriﬁcation indicates source\nand packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS\nThis section introduces the DRKey protocols that enable routers\nto set up shared keys with source S and destination D. Section 4.1\ndescribes the case when both S and D trust each other. Section 4.2\nrelaxes this assumption and describes the case when S and D do not\ntrust each other. Section 4.3 describes how S and D retroactively set\nup shared keys with intermediate routers to enable path validation\nof prior packets.\n4.1\nDRKey for Benign Source and Destination\nWhen both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='When both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is\nshared with both S and D. Figure 1 shows the key setup steps and\nthe associated cryptographic operations.\nS creates a fresh public/private key pair (PKσ,PK−1\nσ ) for each\nsession such that routers encrypt session symmetric key Ki’s. Since\nS and D trust each other, they share private key PK−1\nσ , the en-\ncrypted and authenticated value of which is sent to D. The public\nkey is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='key is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path\nand Tσ is when S initiates session σ. Note that Tσ prevents re-\nplay attacks since routers can drop expired packets based on loose\ntime synchronization.\n273\nInitialization by Source S\n0.\nAssume long-term symmetric key ˆKSD shared with D\n(Optional) Assume public/private key pair (PKS,PK−1\nS ), and CertPKS\n1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONID)\n5.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1\nσ )\nS → R1\n7.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID) }\nPairwise Key Derivation by R1\n8.\nCompute K1 = FSVR1 (SESSIONID)\n9.\nEncrypt K1: EncKEYR1,σ = EncPKσ (K1)\nSign: SignKEYR1,σ = SignPK−1\nR1 (K1∥PKσ)\nR1 → R2\n10.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R1 (K1∥PKσ)\nR1 → R2\n10.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ }\nPairwise Key Derivation by R2\n11.\nComputes K2 = FSVR2 (SESSIONID)\n12.\nEncrypt K2: EncKEYR2,σ = EncPKσ (K2)\nSign: SignKEYR2,σ = SignPK−1\nR2 (K2∥PKσ)\nR2 → D\n13.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (K2∥PKσ)\nR2 → D\n13.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPKS(SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ, EncKEYR2,σ, SignKEYR2,σ}\nKey Retrieval by Destination D\n14.\nAlready has ˆKSD, which is the long-term shared symmetric key with S\n15.\nCheck that D is the last entity on PATHσ\n16.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ) and check the integrity\n17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n18.\nDecrypt AUTHσ and authenticate PK−1\nσ : AuthDecKSDσ (AUTHσ)\n19.\nDecrypt K1 and K2 and check their signatures:\nDecPK−1\nσ (EncKEYR1,σ),CheckSigPKR1 (SignKEYR1,σ)\nDecPK−1\nσ (EncKEYR2,σ),CheckSigPKR2 (SignKEYR2,σ)\nK1 and K2 become shared symmetric keys between each router and D\n20.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='K1 and K2 become shared symmetric keys between each router and D\n20.\nCompute KD = FSVD(SESSIONID)\nD → S\n21.\nForward authenticated and encrypted shared keys:\nKEYSσ = AuthEncKDSσ (K1∥K2∥KD∥AUTHσ)\nKey Retrieval by Source S\n22.\nDecrypt and authenticate the keys received from D: AuthDecKDSσ (KEYSσ)\nK1, K2 and KD become shared keys between S and R1, R2, and D\nFigure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-\nﬁcient PRF keyed with a secret SVi only known to Ri. The PRF\ntakes SESSIONID as an input. For high efﬁciency, we compute\nour PRF from a pseudo-random permutation using AES. The over-\nhead of the key setup is negligible to affect the on-going trafﬁc (see\nSection 8). Resulting key Ki is encrypted with public key PKσ,\nand digitally signed to enable veriﬁcation that (encrypted) Ki in-\ndeed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='deed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).\nTo prevent reﬂection attacks (i.e., replaying message in the op-\nposite order of communication), communication between S and D\nuses different symmetric keys for each direction: KSDσ and KDSσ\nfor S-initiated and D-initiated packets, respectively.\nThe optional operations in Figure 1 are used only if the router\nalso needs to authenticate S, in which case S also signs the SESSIONID,\nand certiﬁcates needed for routers to verify S’s public key are in-\ncluded in the message.\n4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust\neach other and that they share the same public-private key pair for\nthe session, then each intermediate router needs to set up only one\nshared key with both S and D. However, S and D may not neces-\nPath Agreement and Key Setup Initialization by Source S\n1.\nInitiate new session σand pick random session key (PKSσ,PK−1\nSσ )\nRi uses this key to authenticate S’s packets\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Obtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONIDS = H(PKSσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONIDS)\nS → D\n5.\nForward {PKSσ,PATHσ,Tσ,SignPK−1\nS (PKSσ∥PATHσ∥Tσ)}\nPath Agreement and Key Setup Initialization by Destination D\n6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.\nCompute SESSIONIDD = H(PKDσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nD (SESSIONIDD)\nD → S\n8.\nForward {PKDσ,PATHσ,Tσ,SignPK−1\nD (PKSσ∥PKDσ∥PATHσ),\nSESSIONIDD, (optional) SignPK−1\nD (SESSIONIDD) }\nInitialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Initialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD) }\nPairwise Key Derivation by R1\n10.\nCompute KS1 = FSVR1S(SESSIONIDS))\nKD1 = FSVR1D(SESSIONIDD))\n11.\nEncrypt KS1: EncKEYR1S,σ = EncPKSσ (KS1)\nKS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.\nSign: SignKEYR1S,σ = SignPK−1\nR1 (KS1∥PKSσ∥S)\nSignKEYR1D,σ = SignPK−1\nR1 (KD1∥PKDσ∥D)\nR1 → R2\n13.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ}\nPairwise Key Derivation by R2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Pairwise Key Derivation by R2\n14.\nCompute KS2 = FSVR2S(SESSIONIDS))\nKD2 = FSVR2D(SESSIONIDD))\n15.\nEncrypt KS2: EncKEYR2S,σ = EncPKSσ (KS2)\nKS2: EncKEYR2D,σ = EncPKDσ (KD2)\n16.\nSign: SignKEYR2S,σ = SignPK−1\nR2 (KS2∥PKSσ∥S)\nSignKEYR2D,σ = SignPK−1\nR2 (KD2∥PKDσ∥D)\nR2 → D\n17.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (KD2∥PKDσ∥D)\nR2 → D\n17.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ,\nEncKEYR2S,σ,SignKEYR2S,σ,EncKEYR2D,σ,SignKEYR2D,σ}\nKey Retrieval by D\n18.\nCheck that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Check that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures\nDecPK−1\nDσ (EncKEYR1D,σ),CheckSigPKR1 (SignKEYR1S,σ)\nDecPK−1\nDσ (EncKEYR2D,σ),CheckSigPKR2 (SignKEYR2S,σ)\nKD1 and KD2 become shared symmetric key between each router and D\n20.\nCompute KD = FSVD(SESSIONIDS))\n21.\nEncrypt KD: EncKEYD,σ = EncPKSσ (KD)\n22.\nSign: SignKEYD,σ = SignPK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='22.\nSign: SignKEYD,σ = SignPK−1\nD (KD∥PKSσ∥S)\nD → S\n23.\nForward {EncKEYR1S,σ,SignKEYR1S,σ,EncKEYR2S,σ,SignKEYR2S,σ,\nEncKEYD,σ,SignKEYD,σ}\nKey Retrieval by Source S\n24.\nDecrypt and authenticate keys received from D\nKS1, KS2 and KD become shared keys between S and R1, R2, and D.\nFigure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.\nsarily trust each other, or they may collude. To strengthen the secu-\nrity guarantees under such circumstances, we introduce Extended-\nDRKey , which requires each intermediate router to set up two keys,\nKSi and KDi, where KSi is the shared symmetric key between S and\nRi and KDi is the shared symmetric key between Ri and D. Figure 2\ndescribes the Extended-DRKey protocol and its cryptographic op-\nerations.\nUnlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Unlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating\nshared keys (i.e., KSi and KDi), Ri and D use distinct local secrets\nto encode the directionality of the keys.\n274\n4.3\nRetroactive-DRKey\nThe key setup protocols as presented in Figures 1 and 2 run once\nbefore the session starts. However, the key setup process incurs\nthe following extra latency and computational overhead: (1) Key\nsetup itself requires an extra round trip between the source and the\ndestination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='destination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-\ndent key setup protocol a priori allows routers to launch a coward\nattack, since the key setup protocol warns possibly misbehaving\nrouters to start behaving correctly and to avoid detection. Conse-\nquently, achieving path validation without the apparent key setup\nprocess is desirable.\nWe introduce Retroactive-DRKey that enables entities to set up\nshared keys at any time after the ﬁrst packet in a session reaches\nthe destination. Note that Retroactive-OPT is invoked only if the\nsource or the destination wishes to perform source authentication\nor path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='or path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).\nTo support such a feature, we still assume that source S and des-\ntination D establish a shared symmetric key ˆKSD in advance, and\nS derives a session key pair (PKσ,PK−1\nσ ) before the session starts.\nUnlike DRKey or Extended-DRKey, Retroactive-DRKey utilizes\nthat S creates KD—a shared symmetric key with D for the ses-\nsion (i.e. KD = FSVS(SESSIONID))—and includes encrypted and\nauthenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='authenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some\nﬁelds such that D can check. When a forwarding protocol is used\nwith Retroactive-DRKey, the routers use some keys for OPT during\na session, and only reveal them at a later time (Section 5.2.1).\nRetroactive-DRKey is very similar to the key setup protocol in\nFigure 1. The only difference is that D does not derive KD, be-\ncause it is already included in each forwarded packet. Retroactive-\nDRKey runs at most once during or after a session ends. We ob-\nserve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='serve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.\n5.\nOPT PROTOCOL DESCRIPTION\nThe DRKey protocols described in Section 4 and the techniques\nwe introduce in this section span a protocol family of source au-\nthentication and path validation with varying assumptions and prop-\nerties. Unfortunately, exploring the entire design space is out of\nscope for this paper, and we will present several protocol instantia-\ntions: (1) OriginValidation for source authentication (S and D trust\neach other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-\ntion and path validation (S and D may not trust each other).\n5.1\nOriginValidation for Source Authentication\nOriginValidation enables each intermediate router and the des-\ntination to perform source authentication using MACs computed\nover the hash of the packet. For efﬁcient authentication, the source\nincludes the following ﬁelds in the packet header:\nDATAHASH: Hash of the packet’s payload H(P);\nSESSIONID: Hash of the session public key, path, and session\ninitiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='initiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.\nOVi is a Message Authenti-\ncation Code computed over DATAHASH using key Ki that Ri\nshares with S (i.e., OVi = MACKi(H(P))). Similarly, OVD =\nMACKD(H(P)). The source creates an OV ﬁeld for each inter-\nmediate router and the destination.\nOriginValidation provides efﬁcient MAC veriﬁcation using the\nDATAHASH ﬁeld without requiring each intermediate router to com-\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nPVF (128 bits)\nOV1 (128 bits)\nOV2 (128 bits)\nOVD (128 bits)\nIP Header\nOriginValidation/PathTrace Header\nTCP Header\nFigure 3: The packet header format for OriginValidation and\nPathTrace. DATAHASH, SESSIONID, and OVs help intermedi-\nate routers and the destination authenticate the source (Orig-\ninValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='inValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).\npute the hash over the entire packet. Figure 3 represents the packet\nheader, and only DATAHASH, SESSIONID and OV ﬁelds are needed\nfor OriginValidation.\nWhen intermediate router R1 receives a packet from the source\nS, R1 computes the symmetric key (K1) it shares with S using R1’s\nlocal secret and SESSIONID from the packet header. Then R1 gen-\nerates a MAC as follows: OV′\n1 = MACK1(DATAHASH). If OV′\n1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards\nthe packet to R2. R2 and D perform the same operations as R1.\nAlthough we present the protocol with OV ﬁelds of size 128,\nthe size can be altered to reﬂect the desired level of security. In\ngeneral, assuming a secure MAC function, the success probability\nof a forged n-bit MAC is 2−n, which already results in a low rate at\nn = 16. Thus, for many applications, 2 byte long OV ﬁelds sufﬁce,\nas a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='as a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace\nPathTrace is to help the source and destination validate that a re-\nceived packet traversed the source-selected path. This main objec-\ntive is achieved by Path Validation Field (PVF), which is a nested\nMAC that intermediate routers update in the packet header as they\nforward the packet. In Figure 3, only DATAHASH, SESSIONID,\nand PVF ﬁelds are used for PathTrace, thus, the packet overhead\nis irrespective of the path length. Next we describe how PathTrace\nsupports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='supports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-\nidate the path, the source generates PVF0—the initial PVF value\nwhich is a MAC of DATAHASH using the shared symmetric key\nbetween the source and the destination. Then the source initializes\nthe PVF ﬁeld in the header with PVF0:\nPVF ← PVF0 = MACKD(DATAHASH).\n(1)\nAny intermediate router Ri on the path generates PVFi and updates\nthe PVF ﬁeld in the header as follows:\nPVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and\nthe destination according to the key setup protocol in Section 4.1.\nHence, upon receiving a packet, the destination ﬁrst re-creates the\nnested MACs (here shown for a path of 2 routers):\nPVF′ = MACK2(DATAHASH∥\nMACK1(DATAHASH∥MACKD(DATAHASH))).\n(3)\nIf PVF′ is the same as PVF in the packet header, the destination is\nassured that the packet was indeed delivered on the source-selected\npath. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its\npacket is delivered to the intended destination using the source-\nselected path, the destination forwards the PVF from the received\n275\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nTIMESTAMP (32 bits)\nPVF (128 bits)\nOPV1 (128 bits)\nOPV2 (128 bits)\nOPVD (128 bits)\nIP Header\nOPT Header\nTCP Header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='OPVD (128 bits)\nIP Header\nOPT Header\nTCP Header\nFigure 4: OPT header. The source S initializes all the ﬁelds.\nIntermediate routers only update the PVF ﬁeld.\npacket header back to the source as follows:\nD → S : EncKD(PVF∥DATAHASH).\n(4)\nUpon receiving this information, the source ﬁrst decrypts the\nmessage using KD and then performs the validation by re-constructing\nthe nested MACs using DATAHASH as shown in Eq. (3) and com-\nparing it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='paring it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1\nRetroactive-PathTrace\nRetroactive-PathTrace supports path validation without the ap-\nparent key setup process in advance. Instead, it utilizes Retroactive-\nDRKey that runs after the session ends. Unlike PathTrace, in Retro-\nactive-PathTrace the source cannot pre-compute the OPVi ﬁelds;\nhence no OPV ﬁelds can be used in the packet header. Instead,\nunder the assumption that the source and the destination trust each\nother, Retroactive-PathTrace requires that the source creates KD—\nthe session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the\npacket header.\nThe source uses KD to compute PVF0 and the\nrouters derive their shared key and update the PVF ﬁeld accord-\ningly.\nRetroactive-PathTrace requires the destination to store per-packet\ninformation for later checking. However, the beneﬁt of defending\nagainst coward attacks overcomes such a disadvantage. Namely,\nthe destination stores for each packet the tuple (SESSIONID, DATA-\nHASH,PVF). When the destination wants to validate the path, it re-\nquests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.\nThen the destination can check the PVF ﬁelds and detect coward\nattacks. The source can independently initiate the retroactive pro-\ncess as well.\n5.3\nOPT: Origin and Path Trace\nIn this section, we introduce OPT that combines OriginValida-\ntion and PathTrace such that all entities (including intermediate\nrouters) on the path can perform both source authentication and\npath validation when they trust the source. We assume that all the\nrouters in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2\nFigure 4 illustrates the OPT header format. In addition to DATA-\nHASH, SESSIONID, and PVF, an OPT header includes the follow-\ning additional ﬁelds to enable each intermediate router to perform\npath validation.\nTIMESTAMP: Time when S creates the OPT packet to mitigate\ntiming-based attacks, such as replay attacks.\nOPVi: Origin and Path Veriﬁcation ﬁeld. OPVi is a MAC that\nenables all entities on the path to perform path validation.\n2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.\nAn arrow represents the header ﬁeld initialization.\n1: function SOURCE INITIALIZATION\nRequire: Ki and KD that Ri’s and D share with S, respectively after running\nkey setup protocol in Figure 1\n2:\nDATAHASH ← H(P)\n3:\nSESSIONID ← H(PKσ∥PATHσ∥Tσ)\n4:\nPVF ← PVF0 = MACKD(DATAHASH)\n5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do\n7:\nPVFi = MACKi(PVFi−1)\n8:\nOPVi ← MACKi(PVFi−1∥DATAHASH∥Ri−1∥TIMESTAMP)\n9:\nend for\n10:\nfor destination D do\n11:\nOPVD ← MACKD(PVFl−1∥DATAHASH∥Rl−1∥TIMESTAMP)\n12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function\n15: function VALIDATION AND UPDATE BY Ri\n16:\n(Note PVF in OPT header = PVFi−1)\n17:\nCompute OPV′\ni = MACKi(PVFi−1∥DATAHASH∥ Ri−1∥TIMESTAMP)\n18:\nif OPV′\ni == OPVi then\n19:\nPVF ← PVFi = MACKi(PVFi−1)\n20:\nForward the packet to Ri+1\n21:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='20:\nForward the packet to Ri+1\n21:\nelse\n22:\nDrop the packet\n23:\nend if\n24: end function\n25: function DESTINATION VALIDATION\n26:\n(Note PVF in OPT header = PVFl−1)\n27:\nl = source-selected path length\n28:\nCompute PVF′ = MACKl−1(...(MACK1(MACKD(DATAHASH))))\n29:\nCompute OPV′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='29:\nCompute OPV′\nD = MACKD(PVFl−1∥DATAHASH∥ Rl−1∥TIMESTAMP)\n30:\nif (PVF′ == PVF) && (OPV′\nD == OPVD) then\n31:\nValidation succeeds\n32:\nPrepare packet using Eq. (4) and forward to source\n33:\nelse\n34:\nDrop the packet\n35:\nend if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='end if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-\nscribes how the source initializes the OPT header ﬁelds. Each OPV\nﬁeld includes the following as inputs.\nPrevious PVF: Including PVFi−1 in the OPVi computation sup-\nports the detection of a malicious intermediate router that forwards\nthe packet to a benign router, which is not speciﬁed by the source\nbut follows the protocol.\nPrevious router address: PVF by itself cannot support entities to\ndetect the packet injection attack. Hence, we include the address of\nthe previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.\nConsider an example where packet Pcrt is expected to be sent along\nthe source-selected path PATHcrt, the source previously sent packet\nPold on PATHold, and Pcrt and Pold have the same payload. Con-\nsider router Rbad that is in both PATHcrt and PATHold such that\nPATHcrt = {R1,R2,...,Rbad,Rbad+1,...,Rn} and PATHold = {R′\n1,\nR′\n2,...,Rbad,R′\nbad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='bad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′\nbad+1,...,Rm} in PATHcrt and all the cor-\nresponding ﬁelds in the Pcrt header with those in Pold. Therefore,\nwithout TIMESTAMP, the destination cannot detect the misbehavior\nand ends up validating path {R1,R2,..., Rbad, R′\nbad+1,...,Rm} for\nPcrt. By setting the TIMESTAMP ﬁeld when the source sends out a\npacket, authenticator cloning attacks are mitigated with loose time\nsynchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='synchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-\nTION functions in Algorithm 1 describe the OPT procedure that\nintermediate router Ri and the destination performs, respectively.\n276\n5.3.1\nDistrusting source and destination\nThe previous protocols assumes that the source and the destina-\ntion are honest and trust each other. We now relax this assumption\nand present an extension that handles distrusting entities. In OPT,\nthe source can generate all PVFs by itself since it knows all Ki’s.\nConsequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Consequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D\nwithout going through R1 in Figure 1.\nTo prevent such an attack and address the problem of a distrust-\ning source and destination, we use the key setup protocol in Sec-\ntion 4.2 such that intermediate routers generate two separate shared\nkeys for the source and the destination. Unlike OPT, the Extended-\nOPT header requires two PVF ﬁelds: PVFS that enables interme-\ndiate routers and the destination to validate the source, and PVFD\nthat enables the destination to conﬁrm the actual path, even if the\nsource is malicious and colludes with (at least) one intermediate\nrouter.\n6.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source is malicious and colludes with (at least) one intermediate\nrouter.\n6.\nSECURITY ANALYSIS\nWe prove that OPT has origin authenticity and path validation\nproperties when both the source and the destination are trusted.\nThis property holds on any network conﬁguration, including ones\nthat have malicious routers. Extended-OPT offers stronger prop-\nerties: the router’s origin and path validation property assumes\nthat only the source is honest; and the destination’s path validation\nproperty does not assume the source is honest.\nWe describe how OPT and its variants defend against the adver-\nsary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='sary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;\ndetails can be found in our technical report [17].\nPacket alteration. Without the secret keys (KD and Ki), a mali-\ncious router cannot compute valid PVFi and OPVi. Consequently,\nin OPT, a successful veriﬁcation of PVFi−1 (PVFn) based on OPVi\n(OPVD) implies that there can be no packet alteration attacks to\nrouter Ri (the destination), provided that the source and destination\nare trusted. A malicious destination can carry out the packet al-\nteration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='teration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary\nrouters except that only the source needs to be trusted.\nPacket injection attack. OPT routers can check that an incoming\npacket does come from an intended AS, as such information is in-\ncluded in OPV. Therefore, a malicious router A can only inject\npacket to a router B if A is B’s neighbor and the link AB is on the\nintended path. We will revisit this attack when discussing collusion\nattacks.\nIn order to inject a packet with a valid header, an attacker can\nreplay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='replay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks\ncan be mitigated by including a timestamp in the packet. OPT is\nalso vulnerable to packet injection when the destination colludes\nwith the injecting router.\nPath deviation attack. OPT ensures that a successful veriﬁcation\nof PVFi−1 (PVFn) against OPVi (OPVD) implies that the payload\nRi (the destination) received has traversed all the honest routers in\nthe source-intended path in the correct order, assuming that both\nsource and destination are honest. OPT provides this guarantee\nbecause the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='because the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or\nOPVi. As a result, malicious routers cannot mount router skipping\nor out-of-order traversal attacks.\nThis indicates that if a malicious router selects a path not in-\ntended by the source, an honest intermediary router will reject the\npacket. However, a malicious router can mount a path detour attack\nand send the payload to other routers that are not on the intended\npath.\nIn Extended-OPT, even if the destination is malicious, it cannot\nselect the unauthorized path that drops or reorders honest routers.\nExtended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Extended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops\nor reorders honest routers on the source-intended path.\nCoward attack. Retroactive-OPT can mitigate coward attacks by\nrequiring all forwarding routers to compute relevant PVF and OPV\nﬁelds for probabilistic auditing. As a router cannot reliably guess\nwhen audits will happen, it does not know when to carry out an\nattack. We are unaware of any other path validation protocols that\ncan defend against the coward attack, including ICING.\nDoS attack. We consider attacks aiming to exhaust memory and\ncomputational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='computational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources\nsending trafﬁc and the number of ﬂows transiting the router. For\nthis reason, memory exhaustion attacks are not possible under OPT.\nOPT routers perform very few symmetric cryptographic operations\nper packet during forwarding, which run at line speed (Section 7).\nTherefore, OPT is more resilient to computation resource exhaus-\ntion attacks than existing schemes such as ICING that provide sim-\nilar security guarantees.\nCollusion. The path and source validation are conditioned upon\nwhether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='whether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended\npath before Ri, malicious routers can redirect the packet to any\nrouters as it chooses. However, all preceding links on the desired\npath are still traversed in the correct sequence for this packet to be\naccepted by Ri. Similarly, a malicious router could replace the path\nin the packet and trick its neighbor into forwarding the packet to a\nrouter outside the intended path. Again, this packet will be dropped\nwhen it reaches an honest router.\nWhen there are multiple adjacent malicious routers on the in-\ntended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='tended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the\nhole via R j1 and exited the hole from R jn, but has no knowledge as\nto where the packet has been to in between these two points. In\nparticular, when the source colludes with Ri, Ri+1 can be tricked\ninto accepting and forwarding any packet.\n7.\nIMPLEMENTATION AND EVALUATION\nWe implemented OPT in Section 5.3 with DRKey as a user-level\napplication that performs source authentication and path validation.\nThe cryptographic operations performed by a router during packet\nforwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='forwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic\nfunctions used in the implementation. To compute MACs, we used\nCBC-MAC based on AES, since it requires a single AES opera-\ntion to authenticate a 128-bit value. For computing the PRF, we\nalso use the same CBC-MAC. We implement AES using AESni, a\nnew CPU instruction set provided by recent Intel and AMD CPUs\nto speed up AES operations. AESni is fast: According to Intel,\nexecuting an encryption using AES-128 in CBC mode takes 4.15\ncycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be\nincreased, as we can process 4 blocks in parallel on a single core\nin AES-128 in CBC mode, resulting in 1.33 cycles per byte. We\nimplemented authenticated encryption using Galois/Counter Mode\n(GCM) with AES.\nWe use SHA-3 for computing hashes on long strings, such as\nthe hash of the payload DATAHASH. We truncate the hash from\n277\nTable 2: Per-session storage (σ), long-term storage (LT) related\nto the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter\nSource\nDestination\nStorage\nDRKey (σ)\n0\nn+2\nn+2\n[#items]\nICING (σ)\n2\nn+1\n2∗n+1\nDRKey (LT)\n1\n1\n2\nICING (LT)\n≤ 400,000\n0\n0\nKey setup\nDRKey', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='≤ 400,000\n0\n0\nKey setup\nDRKey\n2\n[#packets]\nICING\n4∗n+4\na 256-bit value to a 128-bit value. For computing hashes on short\nstrings, such as H(PKσ), we use the Merkle-Damgard construction\nwith a Matyas-Meyer-Oseas AES-based compression function that\nmakes use of the fast hardware AESni instructions. We choose a\nsingle-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='single-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header\nsize. Nevertheless, this decision does not pose security concerns:\nan adversary besides the source needs to perform a second-pre-\nimage collision attack, which is still in the order of O(2128) for\nSHA-3 and close to O(2128) for Matyas-Meyer-Oseas.\nFor signatures, we use Ed25519 [6], providing high efﬁciency\nand security, and small signatures. For a security level of 2128 oper-\nations, Ed25519 signature generation and veriﬁcation on a 3.4GHz', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ations, Ed25519 signature generation and veriﬁcation on a 3.4GHz\nCore i7 takes 20us and 60us, respectively. Public keys and signa-\ntures are only 32 and 64 bytes, respectively. Certiﬁcates can thus be\nas small as 128 bytes, enabling routers to add their certiﬁcate to the\nkey setup message. As explained in Section 3, router certiﬁcates\ncan be generated by an AS and signed with the AS’s private key.\nThe AS’s public key can be obtained and veriﬁed through RPKI.\nFor encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='For encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-\ntude slower, it is performed by the endhost which is less perfor-\nmance critical.\nICING implementation and conﬁguration. We compare OPT\nwith ICING, the code of which we obtained from their website3.\nTo ensure the fairness of our comparison, we implemented ICING\nthat also uses AESni. In ICING, the source obtains a proof of con-\nsent (PoC) from the consent server of each node on the path. A\nPoC certiﬁes that the node consents to the full path. Furthermore,\neach node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-\ntion for computing the tag keys needed for the PoCs [25], which\ncan diminish the number of required PRF rounds to 0. In our IC-\nING implementation, we favor ICING and consider that computing\nthese keys has no computational or memory lookup overhead.\nAnother important concept in ICING is the proofs of provenance\n(PoPs)—proofs to the nodes that the packet originates from the\nsender. Computing PoPs requires shared symmetric keys between\neach pair of ICING nodes on the path. These keys can be either de-\nrived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='rived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive\nand impractical on the fast path, PoP keys are always retrieved from\nthe cache in our ICING implementation.\n7.1\nDRKey Evaluation\nDRKey enables the design of low-overhead protocols in terms\nof router resources, such as OPT. In OPT, we use DRKey for key\nsetup, which is executed once per session. Thus, the key setup cost\nis amortized over an OPT session.\nTable 2 provides an analysis of storage overhead at the source,\n3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity\nPath length\nLatency\nRouter\nIrrelevant\n381 µs\nSource\n2\n621 µs\n4\n609 µs\n8\n628 µs\nDestination\n2\n3820 µs\n4\n5520 µs\n8\n14814 µs', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4\n5520 µs\n8\n14814 µs\ndestination, and intermediate routers for DRKey and ICING. We\nalso compare the communication overhead for setting up keys.\nGiven a path of length n (excluding the source and the destina-\ntion), DRKey requires the source and the destination to store, per\nsession, n+1 symmetric keys and a public-private session key pair.\nThe long-term storage of the source and the destination, which out-\nlives multiple sessions, consists of their shared symmetric key and\nthe destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.\nIn contrast, ICING’s source and destination need to store n + 1\nsymmetric keys. Each ICING router needs to store pairwise keys\nwith every router in the Internet, which, according to the ICING au-\nthors, is within 400,000. The source also stores PoCs of all entities\nin the path.\nRegarding the communication overhead of the key setup in DRKey,\nthe source and the destination send one message each, resulting in\n2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the\npath (routers and destination), which leads to 2∗(n+1) messages\nfor a path of length n. The source also sets up pairwise shared keys\nwith each entity on the path, requiring at least a round trip (2 mes-\nsages) per entity, resulting in at least 2 ∗ (n + 1) messages. We do\nnot count the messages that are necessary to set up pairwise shared\nkeys between ICING routers, because these keys are set up once\nbetween all entities in the Internet and then stored at each entity.\nTo prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='To prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate\nrouters. Our experiment measures the latency of key setup packets\nwhile they transit the network entities. For the experiment, we use\na trafﬁc generator that initiates key setup operations and connects\nto a server that performs the key setup operations of the source,\nrouter, or destination. After the key setup, the server forwards the\npackets back to the trafﬁc generator, which measures the receive\nrate.\nTable 3 presents the latency of DRKey packets at the source,\nrouters, and the destination.\nThe results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.\nFor the source and the destination, a longer path increases the\namount of computation. In the case of the source, this is hardly no-\nticeable, because the source does not perform public-key cryptogra-\nphy operations that depend on the path length. In contrast, the des-\ntination performs per-hop public-key decryption using RSA-2048\nto obtain the shared keys, which is expensive and considerably af-\nfects the latency. Nevertheless, the results satisfy our objectives:\nsince the source and the destination have a signiﬁcantly lower traf-\nﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.\n7.2\nOPT Evaluation\nWe evaluate OPT with respect to the desired performance proper-\n278\n0\n200\n400\n600\n800\n1000\n1200\n1400\n20\n30\n40\nPacket Size (B)\nThroughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Throughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments\nuse I/O Engine for trafﬁc delivery, I/O engine represents our\nbaseline.\nties for source and path validation, namely efﬁcient forwarding and\nscalable state, and the cost associated with meeting them. Specif-\nically, we examine (1) OPT’s overhead in terms of per-packet pro-\ncessing by measuring both the throughput (the bandwidth utilized\nby whole packets including the Ethernet header)4 and goodput (the\nbandwidth used to transmit the payload of the packets, excluding\nthe OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that\nvalidates the source and the path of the incoming packets before\nforwarding them. Our comparison is with ICING [26], which we\ndiscuss in more detail in Section 9 since both provide similar se-\ncurity guarantees. We experiment with OPT and ICING to per-\nform source and path validations and we use PacketShader’s I/O\nEngine [15] to send/receive packets to/from the NICs.\nA central aspect of our work is the forwarding speed of a router.\nSince OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Since OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-\nber of sources sending packets or the number of ﬂows transiting\na router. In contrast, we analyze the impact of (1) cryptographic\noperations and (2) memory lookup of cryptographic keys, because\nthe forwarding overhead of path validation protocols that use cryp-\ntography depends on these metrics.\nEvaluation system. Our testbed consists of two routers A and B.\nBoth are equipped with two Intel Ethernet Server Adapter X520-\nT2 NICs, and they both run Ubuntu Linux Kernel version 3.2.0-3.\nSystem A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='System A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM\nand two Xeon L5640 2.26GHz (6 cores) processors. System B\nruns our software router code featuring 16 x 4GB DDR3 RAM and\ntwo Intel Xeon E5-2680 2.70GHz (8 cores) processors. The trafﬁc\ngenerator generates trafﬁc at a rate of 40Gbps, which is processed\non the software router and sent back for measurement.\n7.3\nExperiment setup\nWe ﬁrst describe how packets are forwarded from one router to\nthe other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.\nWe use PacketShader’s I/O Engine [15], a high-speed open source\nimplementation to send/receive packets to/from the NICs. On the\nsending router, I/O Engine takes the packets generated by the user-\nlevel trafﬁc generator and sends them to the NIC. When the packets\narrive at the second router’s NIC, I/O Engine takes the packets from\nthe NIC and delivers them to the user-level application, where they\nare processed according to the protocol (OPT or ICING). The last\nstep is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='step is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at\nthe second router from the moment I/O Engine delivers the packets\nto the user-level application and until the packets are ready to be\nsent back.\nExperiments. We measure the forwarding speed of the software\nrouter for OPT and ICING. Our experiments consider AS-level\npath validation scenarios, where path validation is performed at a\n4We add 20B Ethernet overhead in computing the throughput.\nsingle router (e.g., ingress router) within an AS. Since an AS is ad-\nministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen\ninside an AS in practice. Consequently, we perform tests with a\nmaximum path length of 10 hops (without counting the source).\nThe minimum path length is 2 hops, corresponding to the case of a\nsource, an intermediate hop, and a destination.\nTo measure the forwarding overhead at a router, which includes\nthe memory overhead for storing keys and for retrieving them, we\nconsider a network where each node has α neighbors. The param-\neter α is important only if the router performs work that depends\non the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='on the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to\nverify its OPV ﬁeld, then computes the hash of the payload and\nﬁnally updates the PVF ﬁeld. These operations do not depend on\nthe number of neighbors the router has nor on the path length.\nHowever, in ICING the router has to look up the shared sym-\nmetric keys with each node on the path in a table that contains the\nkeys of all the nodes the router had previously seen on a path. For\na path of maximum length n, these nodes are located within n-hop\ndistance away from the router. Our choice for the parameter α = 3\nand the maximum path length of 10 hops gives (311−1)\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the maximum path length of 10 hops gives (311−1)\n2\n= 9841 as\na maximum key table size, which is within ICING’s maximum key\ntable size of 400,000 [25].\nThe router receives packets at a line rate of 40Gbps. To quantify\nthe throughput and goodput with respect to the overhead of OPT\nand ICING, we perform tests with payload sizes of 20B, 256B,\n576B, 768B, and 1024B. We add to these values the OPT and IC-\nING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond\nto the TCP header size to simulate TCP/ICING and TCP/OPT. We\nnote that TCP/OPT includes the IP header since OPT runs over the\nIP network; whereas TCP/ICING does not include the IP header\nsince ICING is designed to replace IP. Hence, the goodput compu-\ntation favors ICING.\nThe biggest payload size is computed by subtracting from the\nMTU the header size of ICING for the longest tested path (10\nhops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the\nsource identiﬁer and 13B of common ﬁelds, which gives a header\nlength of 457B for a 10-hop path. The computation is 1500B −\n457B = 1043B, which explains our choice of 1024B of the maxi-\nmum payload. In case of OPT, 52B common ﬁelds, 16B per-hop\noverhead and 40B TCP/IP header result in 252B. As a result, the\nmaximum payload size is dictated by the size of the ICING header\nfor the longest path considered.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='maximum payload size is dictated by the size of the ICING header\nfor the longest path considered.\nMethod. We generate trafﬁc for 10 seconds at a rate of 40Gbps,\nwhich is forwarded to the router running the protocol for source\nauthentication and path validation, and then forwarded back to the\nsource. To measure the throughput, we employ I/O Engine’s scripts,\nwhich operate as follows. These scripts read the RX and TX counter\nvalues of the NIC at the beginning of the experiment and then read\nthe values again every second to compute the number of packets\nsent and received. Consequently, we obtain the throughput values\nevery second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='every second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING\nheader, respectively, from the packet size.\n7.4\nForwarding Overhead\nWe evaluate the most computationally intensive protocol version\nof OPT described in Section 5.3. The evaluation results show that\nOPT outperforms ICING by a signiﬁcant margin. Since the other\nversions of OPT feature smaller packet headers and less computa-\n279\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(a) 2-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(b) 4-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(c) 8-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(d) 10-hop path\nFigure 6: Throughput and goodput (i.e., throughput obtained\nonly for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='only for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths\nvarying from 2 to 10 hops.\ntional overhead, they would outperform ICING with an even larger\nmargin.\nFigure 6 depicts the results for throughput and goodput for OPT\nand ICING. We performed experiments for different path lengths\nand packet sizes described in Section 7.3, and the numbers we ob-\ntained show consistent results over all experiments, as explained in\nthe next paragraphs.\nA ﬁrst observation is that throughput registers higher values than\ngoodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='goodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-\nsuring goodput. We also notice that, for all path lengths, OPT’s\nthroughput is close to 40Gbps except for the smallest packet size\n(i.e., 20B). We note that OPT’s throughput for small packets is\nmainly limited by I/O Engine’s throughput as shown in Figure 5.\nAs the path length grows, I/O Engine’s bottleneck becomes re-\nleased because of the reduced number of packet copies between the\nNIC and the user-level packet processing engine5; and as a conse-\nquence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.\nFor each path length, the goodput of both OPT and ICING in-\ncrease as the packet size increases, because the protocol header rep-\nresents a smaller fraction of the total packet size as the payload size\nincreases. Even though ICING’s throughput also increases with\nthe packet size, its value is much smaller than OPT’s throughput.\nGiven the choices for our ICING implementation, as explained ear-\nlier, this result is mainly due to the key table lookup for the PoP\nkeys. Instead, OPT uses AESni operations to derive the keys shared\nwith the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='with the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT\nas much as 10Gbps in comparison to ICING.\n7.5\nPath Length Scalability\nIn order to analyze the protocols’ scalability with respect to the\npath length, we depict in Figure 7 the ratio between the goodput and\nthe throughput (named goodput ratio) for 256B and 1024B packets.\nWe vary the path length from 2 to 10 hops.\nThe goodput ratios of 256B packets are lower those of than 1024B\npackets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='packets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet\nheader, resulting in goodput degradation for both OPT and ICING.\n5The increased header size reduces the number of packets needed\nto saturate the link bandwidth.\n2\n4\n6\n8\n10\n0\n20\n40\n60\n80\n100\nPath Length (Hops)\nGoodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Goodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B\nICING−1024B\nFigure 7: The goodput ratio (i.e., goodput/throughput) of OPT\nand ICING for small and large packets, in the context of path\nlengths varying from 2 to 10 hops.\nThe ﬁgure shows that OPT has better path length scalability than\nICING since the goodput ratio of OPT decreases slower than that of\nICING as the path length increases. Speciﬁcally, when the 2-hop\npath is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)\n64.7·8\n= 3.03%, while IC-\nING’s is (64.5−34.9)\n64.5·8\n= 5.74%; for 1024B packets, OPT’s goodput\ndegradation ratio per hop is (88.1−79.3)\n88.1·8\n= 1.25%, while ICING’s is\n(87.9−68.2)\n87.9·8\n= 2.80%.\n8.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='(87.9−68.2)\n87.9·8\n= 2.80%.\n8.\nDISCUSSION\nKey lifetime. The keys associated with a session σ are valid as long\nas (1) PATHσ between S and D in the session σ does not change,\nand (2) S or D do not terminate the session due to application-driven\nsession lifetime requirement.\nAccording to the ﬁrst point, the maximum key lifetime is deter-\nmined by route stability. Recent end-to-end route stability anal-\nyses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='yses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-\nniques [9, 18, 22]. Although many routes are still short-lived, en-\ntities send packets over long-lived routes (longer than 6 hours) for\n96% of the times [9]. In particular, considering the fact that the\nload balancing within an ISP causes most route variations (i.e., up\nto 82%), OPT running at AS-level uses more stable routes than\nrouter-level OPT. Furthermore, when routers perform per-ﬂow or\nper-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='per-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-\nstrated by numerous incidents such as Man-in-the-Middle BGP route\nhijacking [8]. In this case, the network routes could ﬂap as the\nattacker wishes. Yet, some future Internet architecture proposals\n(Nebula [2], Pathlets [12], SCION [40], XIA [14]) relieve this pain\npoint by having packets carry forwarding information in the packet\nheader, so that the source is always aware of the path and would set\nup a new session if the path changes.\nWe expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the\nearliest at a granularity of tens of minutes. Thus, the key setup\noverhead represents a tiny fraction of the total computation and\ncommunication overhead of a long-lived high-bandwidth connec-\ntion.\nEfﬁcient packet content authentication. As described in Sec-\ntion 5.3, each intermediate router uses the DATAHASH ﬁeld in the\nOPT header when it veriﬁes its OPV ﬁeld. Such a veriﬁcation does\nnot authenticate the packet content since a malicious intermediate\nrouter could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='router could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in\nparallel while the packet is being scheduled for transmission.\nAs an alternative, probabilistic veriﬁcation schemes [16] can be\n280\napplied such that every router decreases the veriﬁcation probabil-\nity if the DATAHASH veriﬁcation succeeds. However, if a router\ndetects a packet with a bogus hash value, the probability to run\nhash veriﬁcation increases. Furthermore, as soon as a router re-\nceives multiple mismatching hash values, it immediately performs\nhash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets\nincoming on interfaces arriving from the malicious router. With\nsuch a probabilistic veriﬁcation approach, we can further improve\nthe efﬁciency and practicality while providing data authentication.\nOPT in the current Internet. OPT could be incrementally de-\nployed in the current Internet. An AS could announce its OPT\nfunctionality within BGP update messages (as a transitive attribute)\nor as extension to RPKI certiﬁcates, enabling the selection and con-\nstruction of end-to-end OPT paths at source ASes. Endhosts could\nobtain OPT path information from a local route server which col-\nlects BGP and RPKI information.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='obtain OPT path information from a local route server which col-\nlects BGP and RPKI information.\nTo carry OPT-based information in packets, the simplest ap-\nproach would be an IPv6 extension header. In IPv4, spare IP header\nbits would need to be used to indicate the presence of an extra\nOPT header or trailer, but an extra header after the IP header may\ndisrupt processing at legacy ﬁrewalls or other middleboxes. With\nthe increasing support for IPv6, we prefer incremental deployment\nvia the IPv6 extension header. Since the DRKey information is\nlarger than the 256 bytes that ﬁt into an IPv6 extension header, we\npropose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='propose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,\nASes could specify addresses of DRKey servers that would handle\nDRKey packets to set up the keys for the routers and thus would\nshare the secret keys KRi of routers. An endhost could then place\na sequence of DRKey server addresses (similar to a loose source\nrouting option in IPv4) into the DRKey packet, which would be\nsequentially processed and forwarded until the destination. This\nlatter approach avoids routers from analyzing the IP protocol ﬁeld.\n9.\nRELATED WORK\nThe most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source\npre-computes a veriﬁer MAC (Vi) for each intermediate router Ri\nusing the respective shared secret key as well as the hash value of\nthe path and the static content in the header. For each packet, Ri ﬁrst\nreconstructs and XORs the MAC for the source and each upstream\nrouter, and veriﬁes if the XORed MACs are equivalent to what is\nstored in Vi. Then Ri (1) computes a MAC for each downstream\nrouter on the path using the shared secret key, the hash of the path,\nand the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).\nICING is more heavy-weight than OPT. ICING requires each\nRi to derive a Difﬁe-Hellman (DH) key with each router R j on\nthe path, which requires routers to cache keys to avoid the heavy-\nweight DH computation during packet forwarding. For the case\nthe keys are not cached any more, ICING suggests adding the 20-\nbyte public key of each router into each packet, resulting in a high\nper-packet overhead. Also, ICING requires each node to insert a\nMAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='MAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require\nrouters to store keys shared with sources or other routers, nor per-\nform a MAC computation for each router on the path. In terms of\nsecurity, even ICING intermediate routers can detect colluding path\ndeviation attacks mounted by the source and a malicious router to\nanother intermediary router. In contrast, Extended-OPT supports\nthe destination to detect such attacks. More speciﬁcally, the ori-\ngin and path validation property of the routers still depends on the\ncorrect behavior of the source, but not on the destination. Conse-\nquently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,\nan illegitimate packet travels longer in Extended-OPT than ICING,\nbut will be rejected by the destination. Beside the malicious source\ncollusion attack, OPT and ICING provide the same kind of source\nand path authenticity properties for the destination. On the other\nhand, retroactive key setup OPT with path tracing (which only en-\nables the destination to verify the path) can mitigate the coward\nattack, which ICING fails to mitigate.\nLiu et al. propose Passport for intermediate routers to perform\nsource authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any\ntwo ASes to compute a shared secret based on Difﬁe-Hellman key\nexchange. Using the respective shared secret key with each down-\nstream AS, the source AS computes a MAC for each AS on the\npath, and inserts it in the Passport header. Each intermediate AS\nauthenticates the source AS by recomputing the MAC using the\nshared key and conﬁrming that it matches the MAC in the Passport\nheader. Similar to Passport, the accountability service by Ben-\nder et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='der et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers\nthrough a key derivation mechanism that is similar to DRKey (al-\nthough it cannot provide retroactive key setup) and uses these keys\nfor embedded source authenticators. Passport and the accountabil-\nity service provide weaker security guarantees than OPT, as they\nprovide only source AS authentication, and fail to defend against\nsource and data spooﬁng, as well as path deviation attacks. While\nSNAPP does prevent against source and data spooﬁng, it does not\nprevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet\nstate [19, 31], perform packet marking [30, 32], or active path in-\nterrogation [27]. Pi suggests a path identiﬁer to detect source IP\naddress spooﬁng [36]. Unfortunately, these schemes are suscepti-\nble to attacks listed in Section 2.3, because they were designed for\na different purpose. Similarly, network capability mechanisms [3,\n29, 37, 38] cannot provide source authentication or path validation\nas the capability can be easily copied and inserted by the last AS.\n10.\nCONCLUSION\nDespite the importance of network-based source authentication', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='10.\nCONCLUSION\nDespite the importance of network-based source authentication\nand path validation, these primitives have not been implemented\nso far, perhaps because of the lack of an efﬁcient protocol that does\nnot burden the router. This paper introduces (1) DRKeys as efﬁcient\nand dynamically recreatable key setup protocols, and (2) OPT as an\nextremely lightweight, scalable, and secure protocol that provides\nsource authentication and path validation. Compared with currently\nproposed solutions, OPT achieves performance improvements with\nminimal latency and computational overhead on routers regardless\nof the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='of the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-\nity. We also introduce a retroactive key setup process that protects\nagainst coward attacks, as routers cannot know in advance which\npaths are being monitored subsequently. We anticipate that OPT’s\nsecurity and performance properties will bring source authentica-\ntion and path validation into the realm of practicality.\n11.\nACKNOWLEDGMENTS\nWe thank George Danezis, Yue-Hsun Lin, Raphael Reischuk,\nmembers of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='members of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281\nWe gratefully acknowledge funding support for this research from\nCyLab at Carnegie Mellon, NSF under award CNS-1040801, Euro-\npean Research Council under the European Union’s Seventh Frame-\nwork Programme (FP7/2007-2013) / ERC grant agreement 617605,\nand a gift from KDDI.\n12.\nREFERENCES\n[1] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen,\nD. Moon, and S. Shenker. Accountable Internet Protocol', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='D. Moon, and S. Shenker. Accountable Internet Protocol\n(AIP). In Proceedings of ACM SIGCOMM, 2008.\n[2] T. Anderson, K. Birman, R. Broberg, M. Caesar, D. Comer,\nC. Cotton, M. Freedman, A. Haeberlen, Z. Ives,\nA. Krishnamurthy, W. Lehr, B. Loo, D. Mazières,\nA. Nicolosi, J. Smith, I. Stoica, R. van Renesse, M. Walﬁsh,\nH. Weatherspoon, and C. Yoo. The nebula future internet\narchitecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='architecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer\nScience, pages 16–26. Springer Berlin Heidelberg, 2013.\n[3] T. Anderson, T. Roscoe, and D. Wetherall. Preventing\nInternet Denial-of-Service with Capabilities. In Proceedings\nof Hotnets-II, 2003.\n[4] ARIN. Resource Public Key Infrastructure (RPKI).\nhttps://www.arin.net/resources/rpki/.\n[5] A. Bender, N. Spring, D. Levin, and B. Bhattacharjee.\nAccountability as a Service. In Proc. of USENIX SRUTI,\n2007.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Accountability as a Service. In Proc. of USENIX SRUTI,\n2007.\n[6] D. J. Bernstein, N. Duif, T. Lange, P. Schwabe, and B.-Y.\nYang. High-speed high-security signatures. In Proc. of\nCHES, 2011.\n[7] J. Cowie. The new threat: Targeted internet trafﬁc\nmisdirection. http://www.renesys.com/2013/11/mitm-\ninternet-hijacking/, Nov. 2013.\n[8] J. Cowie. The New Threat: Targeted Internet Trafﬁc\nMisdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Misdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-\nhijacking/, Nov 2013.\n[9] I. Cunha, R. Teixeira, and C. Diot. Measuring and\ncharacterizing end-to-end route dynamics in the presence of\nload balancing. In Proc. of PAM’11, 2011.\n[10] Y. Gilad and A. Herzberg. Plug-and-Play IP Security:\nAnonymity Infrastructure Instead of PKI. In Proceedings of\nESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In\nProc. of SIGCOMM, 2009.\n[12] P. B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica. Pathlet\nrouting. In Proceedings of the ACM SIGCOMM 2009\nConference on Data Communication, 2009.\n[13] S. Gueron. Intel Advanced Encryption Standard (AES) New\nInstructions Set, Mar. 2010. white paper 323641-001,\nRevision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Revision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,\nA. Mukundan, W. Wu, A. Akella, D. G. Andersen, J. W.\nByers, S. Seshan, and P. Steenkiste. XIA: Efﬁcient support\nfor evolvable internetworking. In Proceedings of USENIX\nConference on Networked Systems Design and\nImplementation, 2012.\n[15] S. Han, K. Jang, K. Park, and S. Moon. PacketShader: a\nGPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='GPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.\n[16] H.-C. Hsiao, A. Studer, C. Chen, A. Perrig, F. Bai, B. Bellur,\nand A. Iyer. Flooding-Resilient Broadcast Authentication for\nVANETs. In Proc. of MobiCom, 2011.\n[17] L. Jia, C. Basescu, T. H.-J. Kim, A. Perrig, Y.-C. Hu, and\nF. Zhang. Mechanized network origin and path authenticity\nproofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='proofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.\n[18] M. S. Kang, S. B. Lee, and V. D. Gligor. The Crossﬁre\nAttack. In Proc. of IEEE Security and Privacy, 2013.\n[19] J. Li, M. Sung, J. Xu, and L. Li. Large-Scale IP Traceback in\nHigh-Speed Internet: Practical Techniques and Theoretical\nFoundation. In Proc. of IEEE Security and Privacy, 2004.\n[20] B. Liu, J. T. Chiang, J. J. Haas, and Y.-C. Hu. Coward\nAttacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Attacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.\n[21] X. Liu, A. Li, X. Yang, and D. Wetherall. Passport: Secure\nand Adoptable Source Authentication. In Proc. of NSDI,\n2008.\n[22] H. V. Madhyastha, E. Katz-Bassett, T. Anderson,\nA. Krishnamurthy, and A. Venkataramani. iPlane Nano: Path\nPrediction for Peer-to-peer Applications. In Proc. of NSDI,\n2009.\n[23] D. Mazieres, M. Kaminsky, M. F. Kaashoek, and E. Witchel.\nSeparating Key Mangement from File System Security. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Separating Key Mangement from File System Security. In\nProceedings of SOSP, 1999.\n[24] R. Moskowitz and P. Nikander. Host Identity Protocol (HIP)\nArchitecture, May 2006.\nhttp://tools.ietf.org/html/rfc4423.\n[25] J. Naous. Path-policy Compliant Networking and a Platform\nfor Heterogeneous IAAS management. In PhD thesis, 2011.\n[26] J. Naous, M. Walﬁsh, A. Nicolosi, D. Mazieres, M. Miller,\nand A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.\n[27] V. N. Padmanabhan and D. R. Simon. Secure Traceroute to\nDetect Faulty or Malicious Routing. ACM SIGCOMM\nComputer Communications Review, January 2003.\n[28] J. Pappalardo. New Transatlantic Cable Built to Shave 5\nMiliseconds off Stock Trades. Popular Mechanics,\nhttp://www.popularmechanics.com/technology/\nengineering/infrastructure/a-transatlantic-\ncable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.\n[29] R. Raghavan and A. C. Snoeren. A system for authenticated\npolicy-compliant routing. In Proc. of ACM SIGCOMM,\n2004.\n[30] S. Savage, D. Wetherall, A. Karlin, and T. Anderson.\nPractical Network Support for IP Traceback. In Proc. of\nSIGCOMM, 2000.\n[31] A. C. Snoeren, C. Partridge, L. A. Galindo, C. E. Jones,\nF. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='F. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP\nTraceback. In Proceedings of ACM SIGCOMM, 2001.\n[32] D. X. Song and A. Perrig. Advanced and Authenticated\nMarking Schemes for IP Traceback. In Proceedings of IEEE\nINFOCOM, 2001.\n[33] I. Stoica, D. Adkins, S. Zhaung, S. Shenker, and S. Surana.\nInternet indirection infrastructure. In Proc. of SIGCOMM,\n2002.\n[34] M. Walﬁsh, J. Stribling, M. Krohn, H. Balakrishnan,\nR. Morris, and S. Shenker. Middleboxes No Longer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R. Morris, and S. Shenker. Middleboxes No Longer\nConsidered Harmful. In Proceedings of OSDI, 2004.\n[35] D. Wendlandt, D. G. Andersen, and A. Perrig. Perspectives:\nImproving SSH-style host authentication with multi-path\nprobing. In Proceedings of USENIX Annual Technical\nConference, June 2008.\n[36] A. Yaar, A. Perrig, and D. Song. Pi: A Path Identiﬁcation\nMechanism to Defend against DDoS Attacks. In Proc. of\nIEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='IEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability\nMechanism to Mitigate DDoS Flooding Attacks. In Proc. of\nthe IEEE Security and Privacy, May 2004.\n[38] X. Yang, D. Wetherall, and T. Anderson. A DoS-limiting\nNetwork Architecture. In Proc. of SIGCOMM, 2005.\n[39] X. Zhang. Secure and Efﬁcient Network Fault Localization.\nPhD thesis, Carnegie Mellon University, 2012.\n[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and\nD. G. Andersen. SCION: Scalability, control, and isolation\non next-generation networks. In Proceedings of the IEEE\nSymposium on Security and Privacy (Oakland), May 2011.\n282', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmplwsfm8bq/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'})]
cuda:2
这篇文章讲了什么 tmplwsfm8bq
[]
cuda:2
[1.2571448, 1.2652377, 1.2719353, 1.2741276, 1.2787124, 1.2823689]
这篇文章讲了什么 tmplwsfm8bq
[]
cuda:2
[1.2571448, 1.2652377, 1.2719353, 1.2741276, 1.2787124, 1.2823689]
6 tmplwsfm8bq
[History(role='user', content='这篇文章讲了什么'), History(role='assistant', content='这篇文章讨论了互联网安全中的攻击和防御问题。作者研究了网络中的路由器认证和路径验证，提出了一种名为“ accountable internet protocol”（AIP）的方法，以解决现有互联网协议中的一些安全问题。此外，文章还讨论了一些现有的解决方案，以及这些解决方案的局限性。')]
cuda:2
[1.1647792, 1.214886, 1.2444748, 1.2463716, 1.2466165, 1.2468548]
6 tmplwsfm8bq
[History(role='user', content='这篇文章讲了什么'), History(role='assistant', content='这篇文章讨论了互联网安全中的攻击和防御问题。作者研究了网络中的路由器认证和路径验证，提出了一种名为“ accountable internet protocol”（AIP）的方法，以解决现有互联网协议中的一些安全问题。此外，文章还讨论了一些现有的解决方案，以及这些解决方案的局限性。')]
cuda:2
[1.1647792, 1.214886, 1.2444748, 1.2463716, 1.2466165, 1.2468548]
[UploadFile(filename='高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf', size=1920751, headers=Headers({'content-disposition': 'form-data; name="files"; filename="é«\x98æ\x80§è\x83½è®¡ç®\x97 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpsaqud_36, tmpsaqud_36
File: 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, msg: 成功上传文件 高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf, docs: [Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc4ec790> 111
cuda:2
[Document(page_content='Energy-Eficient 360-Degree Video Rendering on FPGA via\nAlgorithm-Architecture Co-Design\nQiuyue Sun\nqsun15@u.rochester.edu\nAmir Taherin\nataherin@ur.rochester.edu\nYawo Siatitse\nasiatits@u.rochester.edu\nYuhao Zhu\nyzhu@rochester.edu\nUniversity of Rochester\nhttp://horizon-lab.org\nAbstract\n360° panoramic video provides an immersive Virtual Reality expe-\nrience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='rience. However, rendering 360° videos consumes excessive energy\non client devices. FPGA is an ideal ofoading target to improve the\nenergy-efciency. However, a naive implementation of the process-\ning algorithm would lead to an excessive memory footprint that\nofsets the energy beneft. In this paper, we propose an algorithm-\narchitecture co-designed system that dramatically reduces the on-\nchip memory requirement of VR video processing to enable FPGA\nofoading. Evaluation shows that our system is able to achieve\nsignifcant energy reduction with no loss of performance compared\nto today’s of-the-shelf VR video rendering system.\nCCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='CCS Concepts\nHardware → Hardware accelerators; • Computing method-\nologies → Virtual reality; • Computer systems organization\n→ Reconfgurable computing.\nKeywords\nVirtual Reality; Panoramic Video; Perspective Projection; Locality\nACM Reference Format:\nQiuyue Sun, Amir Taherin, Yawo Siatitse, and Yuhao Zhu. 2020. Energy-\nEfcient 360-Degree Video Rendering on FPGA via Algorithm-Architecture\nCo-Design. In Proceedings of the 2020 ACM/SIGDA International Symposium\non Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='on Field-Programmable Gate Arrays (FPGA ’20), February 23–25, 2020, Sea-\nside, CA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/\n3373087.3375317\n1\nIntroduction\nWith the rapid development of panoramic cameras and popularity of\nVirtual Reality technologies, there is an explosion of 360° content in\nrecent years. Content sharing websites such as YouTube, Facebook,\nand Instagram support editing, streaming, and sharing this new\nform of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='form of content, further accelerate the penetration of 360° videos.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proft or commercial advantage and that copies bear this notice and the full citation\non the frst page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specifc permission and/or a\nfee. Request permissions from permissions@acm.org.\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='FPGA ’20, February 23–25, 2020, Seaside, CA, USA\n© 2020 Association for Computing Machinery.\nACM ISBN 978-1-4503-7099-8/20/02...$15.00\nhttps://doi.org/10.1145/3373087.3375317\nFueled by next-generation cellular technologies such as millimeter\nwave that promise orders of magnitude higher bandwidth and lower\nlatency, users soon will be able to create, share, and watch 360°\nvideos just like any other media.\nAlthough with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Although with huge opportunities, rendering 360° content is\npower-hungry. Previous work has shown that rendering a 720p 360°\nvideo in 30 frames per second (FPS) consumes over 4 W power [18,\n19], exceeding the Thermal Design Point (TDP) of typical mobile\ndevices [14]. The reason that rendering 360° is power-hungry is\nthat today’s rendering software translates 360° video rendering\nto a texture mapping problem that gets ofoaded to the GPU [9].\nAlthough using the GPU in of-the-shelf Systems-on-a-chip (SoCs)\naccelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accelerates the adoption of 360° content, GPUs are power hungry.\nWe expect that next-generation mobile SoCs will soon integrate\n360° content-specifc Intellectual Property (IP) blocks to improve\nthe rendering energy-efciency. To facilitate this trend, we propose\na new 360° video rendering accelerator. We choose to base the\naccelerator design on FPGA, which not only allows us to exploit\nthe fne-grained, pixel-level parallelisms exist in the 360° content\nrendering algorithms, but also to retain fexibility to accommodate\nfuture developments in the rendering algorithms.\nThe key challenge of accelerating 360° rendering is the render-\ning algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='ing algorithm’s large memory footprint and irregular data access\npattern, which not only introduce a high memory footprint that\nexceeds the on-chip memory of a mobile SoC and but also are\nnot amenable to conventional memory optimizations such as line-\nbufering and prefetching. As a result, frequent, and random, DRAM\naccesses would have to be made, ofsetting the energy beneft of\nhardware acceleration.\nThis paper proposes an algorithm-architecture co-designed sys-\ntem for efcient 360° content rendering. Our key observation is that\nthe irregular memory accesses in today’s rendering algorithm are\nfundamentally caused by the algorithm’s data-fow that leads to\narbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='arbitrary indexing of the input frame pixels. To tame the memory-\ninefciencies, we propose a new rendering algorithm that, by design,\nenforces a diferent data-fow that guarantees a streaming memory\naccess pattern. As a result, the rendering computation becomes a\nstream of stencil operations, each operating on a fxed-size window\nof pixels in a raster order.\nThe new rendering algorithm uniquely enables us to design\na simple, yet efcient, hardware accelerator. The accelerator ar-\nchitecture exploits the pixel-level parallelisms by pipelining the\nrendering of diferent pixels, and hides the memory transfer la-\ntency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='tency with rendering computations. We judiciously apply a series\nof energy-oriented optimizations including trading-of the pipeline\ndepth for the overall latency and tuning pixel data representations.\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n97\nPlayback\n(Mobile device, focus of this paper)\nCapture\n(Special capture device)\nHead Motion\nSpherical\nPanoramic\nPlanar\nEncode\nDecode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Decode\nFOV Planar\nRendering\nFig. 1: A typical 360° content delivery pipeline. This paper focuses\non the client rendering on mobile devices.\nWe implement our algorithm-architecture co-designed system on\nthe Xilinx Zynq Ultrascale+ ZCU104 FPGA development board [7].\nComparing against an of-the-shelf baseline running on the Nvidia\nJetson TX2 development board utilizing its mobile Pascal GPU [2],\nour system achieves 55% energy savings at the same frame rate.\nIn summary, this paper makes the following contributions:\nWe provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='We provide a detailed analysis of the memory access patterns\nof today’s 360° content rendering algorithm, and demonstrate\nthe inefciencies of conventional memory optimizations such\nas line-bufering. (§ 3).\nWe propose a new 360° content rendering algorithm that im-\nproves the data locality of 360° content rendering, and thus\nenables efcient hardware acceleration (§ 4.1).\nWe propose an accelerator architecture that is co-designed with\nthe new algorithm to maximize its efciency (§ 4.2).\nWe prototype the co-designed system on an embedded FPGA\nand demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and demonstrate signifcant energy savings (§ 5).\nThe rest of the paper is organized as follows. We frst provide\nthe background and related work of 360° content rendering (§ 2).\nWe then analyze today’s rendering algorithm (§ 3), focusing on its\nirregular memory accesses. We then introduce our new rendering\nalgorithm and the co-designed hardware architecture (§ 4). We\nevaluate the our system (§ 5), followed by conclusion (§ 6).\n2\nBackground\nThis section briefy introduces the necessary background and ter-\nminologies that are used throughout this paper. We refer interested\nreaders to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='readers to El-Ganainy and Hefeeda [12] for a comprehensive survey.\nPanoramic Content 360° video is a form of Virtual Reality that\nhas seen wide adoption recently in many areas such as news, movie,\nsports, and medical industry [21]. Fig. 1 shows an end-to-end 360°\ncontent delivery pipeline. It mainly consists of a capturing (cre-\nation) phase and a playback phase. 360° videos are typically created\nusing special capture devices (e.g., an omnidirectional camera [8])\nthat capture every direction of the scene, which later are stitched\ntogether to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='together to form panoramic frames that present a 360° view of the\nscene to users. After creation, 360° videos are streamed to client VR\ndevice for playback, which is the focus of this paper.\nRendering Algorithm Once on the VR device, the playback\nsoftware renders diferent regions of the frame according to the\nuser’s head movement. In the context of 360° video only rotational\nmotion is captured, but not translational motion. The head motion\ncan be characterized by the polar and the azimuthal angles in a\nspherical coordination system. The size of the displayed region\ndepends on the feld-of-view (FOV) of a particular device, which\ncharacterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='characterizes the vertical and horizontal angles of the viewing area.\nAlgorithm 1: Classic 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWo,Ho = IOut .res ;\n// Output resolution\n// iterate over all coordinates in output frame\nfor i = 0; i < Ho; i = i + 1 do\nfor j = 0; j < Wo; j = j + 1 do\n// rotation\n<x, y, z> = R(i, j, α, β, θ, λ);\n// projection\n<u, v> = P(x, y, z);\n// filtering', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='// projection\n<u, v> = P(x, y, z);\n// filtering\nif u and v are integer coordinates then\nIout (i, j) = Iin(u,v);\nelse\nIout (i, j) = F (Iin,u,v);\nend\nend\nend\nConventional video frames, once decoded, can be directly ren-\ndered on the display. However for 360° videos, the client rendering\nsoftware converts an input panoramic frame (decoded from the\nvideo streamed from the cloud) to a frame that contains only the\nuser’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='user’s viewing area based on the user’s viewing angle and device’s\nFOV. Prior work shows that the rendering algorithm contribute\nabout 40% of the device power consumption [19].\nHardware Architecture Today’s of-the-shelf mobile SoCs di-\nrectly support rendering 360° videos. In particular, the video codec\nfrst decodes 360° videos into a set of panoramic planar frames;\nthe Graphics Processing Unit (GPU) is then used to execute the\nrendering algorithm that converts the panoramic planar frames to\nFOV frames that are then sent to the display processor [9].\nThe reason that the GPU is tasked with the rendering algorithm\nis that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is that the latter can be viewed as a texture mapping problem [15],\nwhere the planar panoramic frame is treated as a texture map that\nis mapped to a particular region on a sphere. The spherical region’s\nsize is the same as the device’s FOV and its location is determined by\nthe user’s viewing angle. Modern GPUs can efectively execute tex-\nture mapping through the specialized Texture Mapping Unit (TMU)\nand the texture cache [13]. The TMU accelerates the computation\nof texture mapping, and the texture cache captures the irregular\ndata access pattern to the texture map, i.e., the input panoramic\nframe in the case of 360° video rendering.\n3\nRendering Algorithm Analysis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame in the case of 360° video rendering.\n3\nRendering Algorithm Analysis\nThis section frst presents the algorithm used in today’s 360° render-\ning software and discusses its computation pattern that is suitable\nfor hardware acceleration (§ 3.1). We then particularly focus on\nthe irregular memory access patterns of the algorithm (§ 3.2), from\nwhich we motivate the need for a new algorithm-architecture co-\ndesigned strategy.\n2\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n98\n(a) Head orientation at (45°, 90°).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='98\n(a) Head orientation at (45°, 90°).\n(b) Head orientation at (45°, 45°).\nFig. 2: 360° frame rendering at two diferent head orientations. The\nleft fgures are the input panoramic frame, and the right fgures are\nthe output FOV frame1. The black pixels in the left fgures are the\npixels that are referenced during the rendering process.\n3.1\nAlgorithm and Its Computation Patterns\nThe goal of the rendering algorithm is to generate an output frame\nIout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Iout from the input panoramic frame Iin. Algo. 1 shows the pseu-\ndocode. Specifcally, the rendering algorithm calculates each output\nframe pixel (<i, j>) by mapping it to a pixel in the input frame\n(<u,v>), efectively sampling the input frame. The mapping is done\nby raster scanning all the points in the output frame and iteratively\napplying two operations, rotation (R) and projection (P), on each\n<i, j> point to obtain its corresponding <u,v> coordinates. R and\nP are matrix multiplications and cartesian-spherical conversions\nto support perspective rotation and projection [20]. The renderer\nthen uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='then uses the <u,v> coordinates to look up the input frame, and\nreturns the corresponding input pixel as Iout <i, j>. If <u,v> are\nnot integer coordinates, the renderer applies a so-called fltering\nfunction (F ), such as nearest neighbor or bilinear fltering [15], to\nreturn a “best approximation” of the pixel value at Iout <i, j>.\nThe rendering algorithm is highly parallel. In particular, the ren-\ndering of every output pixel is completely independent of each\nother. Under a particular head orientation, an output pixel’s value\nIout (i, j) depends only on its coordinates <i, j>. In addition, the\ncomputation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation involved in R, P, and F are mostly afne transfor-\nmations that are suitable for efcient hardware implementations.\nOverall, the computation patten is ideal for hardware acceleration.\n3.2\nMemory Access Patterns\nIn stark contrast to the computation pattern, the memory access\npattern of the rendering algorithm is far from ideal for an efcient\naccelerator design, especially on FPGAs.\nLarge Footprint The rendering algorithm accesses the memory\nin the fltering step, which uses the <u, v> coordinates generated\nfrom the projection step to index into the input frame (Iin), and\nsequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='sequentially writes to the output frame (Iout ) in the raster order. The\noutput frame is small in size; its accesses are sequential, and thus\n1All fgures here are down-sampled to reduce their sizes.\n300\n240\n180\n120\n60\n0\ny-coordinate\n2500\n2000\n1500\n1000\n500\n0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='0\nx-coordinate\nSeq. 1\nSeq. 2\nSeq. 3\n(a) Input accesses.\n400\n300\n200\n100\n0\ny-coordinate\n100 200 300 400\n0\nx-coordinate\n(b) Output accesses.\nFig. 3: Memory access sequences of input frame (left) and output\nframe (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='frame (right) as scatter plots. Each marker <x, y> indicates that the\npixel at position <x, y> is referenced. The three sequences corre-\nspond to when the rendering algorithm scans three diferent rows\n(i in Algo. 1) of the output frame. Diferent markers in the same se-\nquence correspond to diferent columns (j in Algo. 1) on the same\nrow. The head orientation is (45°, 90°), same as in Fig. 2a.\ncould be bufered on-chip and efciently streamed to the DRAM in\nthe end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the end [22]. However, the input frame can not be fully captured\nby a typical on-chip memory. For instance, a 1080 and 4K frame\nwould require over 5.9 MB and 23.7 MB memory, respectively.\nIrregularity The pixel access pattern of the input frame is non-\nsequential, which severely hurts the efciency of hardware acceler-\nation. Fig. 2a shows a rendering example where the input frame on\nthe left is rendered to the output frame on the right. In this example,\nthe FOV size is 110° × 110°, and the head orientation is (45°, 90°).\nThe black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The black pixels at the top of the input frame refer to all the pixels\nthat are accessed by the rendering algorithm. To better illustrate\nthe memory access pattern, Fig. 3a plots the input pixels that are ac-\ncessed as the rendering algorithm iterates over three output frame\nrows, which are shown in Fig. 3b. Each <x, y> marker in the fgures\nindicates that the pixel at position <x, y> is referenced.\nAlthough the output frame pixels are accessed sequentially in a\nstreaming fashion, the input frame pixels are referenced irregularly\nas is evident in the three access traces in Fig. 3a. Irregular memory\naccesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='accesses are known to hurt efciency for two reasons. First, random\nDRAM accesses consume much more energy than sequential DRAM\naccesses [6, 17]. Second, irregular memory accesses require explicit\ncontrol logic to manually coordinate the trafc between DRAM\nand on-chip memory [10]. This is particularly an issue for FPGAs,\nwhich implement control fow logic rather inefciently. Ideally,\nFPGAs prefer streaming data accesses, which exhibit strong locality\nand can be efectively captured by memory optimizations such as\nline-bufering [16]. The non-raster access order of the input pixels\nindicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='indicates that line-bufer would be inefcient.\nTo quantify the inefectiveness of using a line-bufer, Fig. 4 shows\nhow the line-bufer efciency (left y-axis) and hit rate (right y-\naxis) vary with the line-bufer capacity when rendering the frame\nin Fig. 2a. The efciency is defned as the percentage of pixels that\nare brought into the line-bufer and that are actually referenced;\nthe miss rate is defned as the number of memory references that\nare not found in the line-bufer and thereby cause pipeline stall;\nthe line-bufer capacity is characterized by the number of lines in\nthe input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame the line-bufer can hold. The rendering algorithm\nrequires about 512 lines, which roughly equate almost 4 MB of\n3\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n99\n50\n40\n30\n20\n10\n0\nEfficiency (%)\n1\n4\n16\n64\n256\n1024\nCapacity of LB (# of frame lines)\n100', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='256\n1024\nCapacity of LB (# of frame lines)\n100\n80\n60\n40\n20\n0\nHit Rate (%)\nFig. 4: The efciency (left) and miss rate (right) of using a line-bufer\nvary with the line-bufer capacity.\nline-bufer size, to reach 100% hit rate. Even under such a large line-\nbufer size, over 50% the fetched pixels would never be referenced,\nleading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='leading to signifcant bandwidth waste and energy-efciencies.\nVariation The irregular memory access pattern varies both spa-\ntially and temporally, making static optimizations inefective.\nOn one hand, the rendering algorithm exhibits diferent input\naccess patterns when iterating over diferent output rows as shown\nin Fig. 3a, exhibiting spatial variance. On the other hand, although\nthe input access pattern is deterministic given a particular head\norientation, the access pattern changes across diferent head ori-\nentations as users move, exhibiting temporal variance. Fig. 2b il-\nlustrates the memory accesses for a diferent head orientation at\n(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='(45°, 90°), which has a diferent pattern from that of (45°, 45°) shown\nin Fig. 2a. Since the head orientation is not known until runtime,\npre-computing and memoizing the access streams for all possible\nhead orientations would lead to prohibitive memory overhead.\n4\nAlgorithm-Hardware Co-Design\nWe propose a new 360° content rendering algorithm, which stream-\nlines the memory accesses while retaining the pixel-level paral-\nlelism, enabling practical FPGA acceleration. We frst describe the\nalgorithm (§ 4.1), and then describe the co-designed the hardware\narchitecture and the implementation details (§ 4.2).\n4.1\nAlgorithm', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture and the implementation details (§ 4.2).\n4.1\nAlgorithm\nOverview Fundamentally, the root-cause of the irregular memory\naccesses in the original rendering algorithm is inherent in the algo-\nrithm’s data-fow. In particular, the rendering algorithm calculates\neach output frame pixel by mapping it to a pixel in the input frame.\nSince the input pixel indexing is arbitrary, memory accesses to\nthe input frame are irregular. Our idea is to invert the rendering\nalgorithm such that it scans the input frame in the raster order, and\nmaps each input pixel to one pixel in the output frame. In this way,\nthe input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the input frame is accessed in a streaming fashion, enabling ef-\ncient line-bufer optimizations. The trade-of is that output frame is\nnow accessed in an arbitrary order. However, this is an acceptable\ntrade-of because the output frame is small in size and could be\nbufered on-chip before streaming out.\nInverting the original algorithm is possible because the rotation\nfunction (R) and projection function (P) are unique and thus are\nnaturally invertible. The fltering function (F ) is not invertible\nbecause it is not unique. Consider the simple nearest-neighbor fl-\ntering; multiple input points could be mapped to the same output\nAlgorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algorithm 2: Proposed 360° video rendering algorithm.\nInput: Input panoramic frame Iin; FOV size α, β; Head\norientation θ, λ.\nResult: Output FOV frame Iout .\nWi,Hi = IIn.res ;\n// Input resolution\n/* iterate over all output boundary coordinates\n/\nfor <i, j> coordinates on the Iout boundary do\n<u, v> = P(R (i, j, α, β, θ, λ));\nAdd <u, v> to B ; // B is the input boundary set\nend\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='end\n/* iterate over all input pixels\n/\nfor u = 0; u < Hi; u = u + 1 do\nfor v = 0; v < Wi; v = v + 1 do\nif <u, v> within the boundary B then\n<i, j> = R−1(P−1 (u, v), α, β, θ, λ)\nIout (i, j) = Iin(u,v);\nend\nend\nend\n/* apply filtering to all output pixels\n/\nforeach <i, j> in Iout do\nF\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='F\n′(<i, j>);\nend\npoint that is the nearest neighbor to both input points. However,\nsince fltering is inherently an approximation, we could approx-\nimate the fltering step without loss of visual quality as we will\ndemonstrate later.\nReduce Redundancies Naively inverting the rendering algo-\nrithm, however, introduces lots of redundant computation. This\nis because only a small fraction of the pixels in the input frames\nis actually referenced during the rendering process. For instance,\nonly 17.1% and 16.5% of the input pixels are referenced in Fig. 2a\nand Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Fig. 2b, respectively. In other words, the vast majority of the\ninput pixels will not be needed to generate any output frame pixels,\nand therefore inversely mapping them would waste computation.\nTo reduce the redundant computations, our idea is to determine\nthe boundary of the input region that contains the pixels that are\nneeded for rendering. In this way, we are able to apply the inverse\nmapping only to the input pixels that are within the boundary.\nInput boundary calculation can be easily achieved by applying the\noriginal rendering algorithm to the output boundary coordinates.\nSince boundary pixels are only a very small portion of the entire\nframe, boundary calculation has low overhead as we show later.\nAlgo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Algo. 2 shows the pseudocode of the new algorithm. It frst\napplies the original rotation and projection functions R and P to\ngenerate a boundary set B. It then iterates over all the input pixels,\nand apply inverse functions R−1 and P−1 to pixels that are within\nthe boundary delineated by B. In the end, it applies a fltering step\nof the entire output image. Note that this fltering function F ′ is\nnot, and needs not to be, the same as the original fltering function\nF or its inverse F −1 due to the approximate nature of fltering.\nOutput Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Output Quality The output of our new rendering algorithm is\nnot pixel-accurate compared to the original algorithm because the\n4\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n100\nfltering function is non-invertible. We verify that the diference is\nacceptable, both objectively and subjectively.\nObjectively, we use two metrics to quantify the diference be-\ntween the outputs generated by our rendering algorithm and the\noriginal algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='original algorithm: the Peak Signal to Noise Ratio (PSNR) and the\nNormalized Root Mean Square Error (NRMSE). The PSNRs across\nthree representative viewing angles, (0°, 0°), (45°, 45°), and (45°, 90°),\nare 40.4, 57.1, and 42.6, respectively, and the NRMSE is below 0.01,\nconfrming the high precision of the new algorithm. Subjectively,\nwe also conduct subjective user study and fnd that the diference\nis visually indistinguishable.\n4.2\nArchitecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Architecture Co-design\nWe co-design the hardware architecture to maximize the efciency\nof the proposed rendering algorithm. Fig. 5a shows the overall\nexecution model. The boundary calculation is serialized with the\nrest of the processing because it provides the boundary set for\ntesting input pixels. The input frame is streamed from the DRAM,\nwhich is overlapped with pixel rendering. We exploit the pixel-level\nparallelism of the new algorithm by pipelining the processing of\ndiferent pixels. The pipeline has an initiation internal of one. That\nis, a new pixel starts execution every cycle. During pixel rendering,\nthe output frame is bufered on-chip and is streamed out in the end.\nThe architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='The architecture block diagram is illustrated in Fig. 5b. The\nboundary calculation and the rendering module both use a set\nof multiply-accumulate (MAC) units and trigonometric function\nhardware to support the perspective rotation, cartesian-spherical\nconversion, and fltering operations. To support the streaming I/O,\nwe use the simple AXI4-Stream interconnect design, which has\nefcient IP implementation on FPGA [22].\nOptimizations We apply a series of optimizations to improve\nthe performance and reduce resource utilization. First, the boundary\ntest is on the critical path and thus impacts the overall performance.\nTesting precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Testing precisely whether a pixel is within the boundary requires\nstoring all the boundary pixels and many comparisons. To reduce\nthe boundary test, we approximate the boundary by its smallest\nbounding box (a rectangle), which requires us to store only four\nparameters and only four comparisons for boundary test. The trade-\nof is that the rendering algorithm now has to process more pixels\nthat are not in the boundary. We fnd that this is a desirable trade-of\nbecause the benefts of reducing the per-pixel latency out-weights\nthe overhead of pipelining more pixels.\nIn addition, we choose to use a fxed-point representation for\ncomputation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='computation to improve the resource utilization and speed. We em-\npirically fnd that a 28-bit representation with 14 bits for the integer\npart is sufcient to guarantee negligible loss of visual quality.\n5\nEvaluation Results\nThis section frst introduces the experimental setup (§ 5.1). We\nthen evaluate on a set of micro-benchmarks using diferent reso-\nlutions and head orientations to understand the efciency of the\nco-designed system (§ 5.2). Finally, we present the evaluation results\non a 360° dataset using real user head orientations (§ 5.3).\nBC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='BC\nStreaming Input Frame\nPipelined Rendering\nStreaming\nOutput Frame\nTime\nI/O\nCompute\nBT\nBT\nR-1\nF’\nP-1\nBT\nR-1\nF’\nP-1\n……\nPixel 1\nPixel 2\nPixel N\n……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='……\nSkipped if BT fails\n(a) The execution model. “BC” stands for “Boundary Calculation”, i.e., the frst\nloop in Algo. 2; “Pipelined Rendering” is the second and third loop in Algo. 2;\n“BT” stands for “Boundary Test”, i.e., the test condition of the second loop\nin Algo. 2. Execution times are not to scale.\nOutput Buffer\nRendering\nInput FIFO\nAXI4-Stream Interconnect\nBlock 1\n……\nMAC\nArray\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Array\nTrigonometric\nfunctions\nBlock N\nBoundary Calculation\nMAC\nArray\nTrigonometric\nfunctions\nto Memory\n(b) Architecture block diagram.\nFig. 5: Architectural support for the proposed rendering algorithm.\n5.1\nExperimental Setup\nWe implement our architecture on the Xilinx Zynq UltraScale+ MP-\nSoC ZCU104 development board [7], which is specifcally designed\nfor embedded visual applications such as Augmented Reality and\nVirtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Virtual Reality. It has a programmable logic with 1.38 MB on-chip\nBRAM. We synthesize, place, and route the design using the Xilinx\nVivado tool chain, and obtain the post-layout power consumption.\nThe design is clocked at 100 MHz, which meets the 30 FPS real-time\ntarget for all resolutions. The ZCU104 development board uses a 2\nGB, 64-bit wide DDR4 memory system [4]. We estimate the DRAM\npower using the Micron DDR4 power calculator [1, 5] based on the\napplication’s memory access traces.\nBaselines We compare against two baselines. First, we compare\nagainst a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='against a baseline that implements the original rendering algorithm\n(Algo. 1) on the mobile Pascal GPU available on the Nvidia Jetson\nTX2 development board [2]. TX2 is used in many of-the-shelf VR\ndevices such as the ones from Magic Leap [3]. This baseline is\nrepresentative of how 360° video rendering is performed in today’s\nVR devices as discussed in § 2. GPU power is measured using TX2’s\nbuilt-in TI INA 3221 voltage monitor IC, from which we retrieve\npower consumptions through the I2C interface.\nSecond, we compare against an FPGA baseline that implements\nthe original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm on the ZCU104 FPGA. Comparing\nagainst this baseline normalizes the efect of FPGA acceleration and\nthus highlights the benefts of algorithm-architecture co-design.\n5\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n101\n60\n40\n20\n0\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Front\nMiddle\nTop\n(a) GPU baseline comparison.\n50\n40\n30\n20\n10\n0\n10\nEnergy Saving (%)\n480p 720p 1080p 2K\nFront\nMiddle\nTop\n(b) FPGA baseline comparison.\nFig. 6: Microbenchmarking energy savings over the baselines across\ndiferent resolutions and head orientations.\n5.2\nMicrobenchmark Results', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='diferent resolutions and head orientations.\n5.2\nMicrobenchmark Results\nWe evaluate four diferent resolutions, including 480p, 720p, 1080p,\nand 2K, to represent diferent rendering requirements. Since difer-\nent head orientations afect how many input pixels are processed,\nwe evaluate three diferent head orientations, (0°, 0°), (45°, 45°), and\n(45°, 90°), which mimic users watching the front (around the Equa-\ntor), middle, and top (around the North Pole) region of a video.\nEnergy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Energy Savings Our co-designed system achieves signifcant\namount of energy savings compared to the GPU baseline. Fig. 6a\nshows the energy savings per frame across diferent resolutions\nunder the three viewing angles. Under the 480p and 2K resolutions\nand the front viewing angle, our system is able to save close to 75%\nand 55% of the energy compared to the GPU baseline, respectively.\nOn average, under a 2K resolution, our system consumes about\n1.4 W of power, of which about 65% is the dynamic power.\nOur co-design system also out-performs the FPGA baseline with\nthe original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='the original rendering algorithm in most cases, as shown in Fig. 6b.\nThe only exception is under the middle viewing angle. This is\ninherent to our new rendering algorithm, which processes more\npixels when the viewing angle is near the (45°, 45°) region of the\nsphere. Recall from § 4.2 that we frst calculate a bounding box and\nthen process all the pixels that are encapsulated by the box. The\nbounding box contains more pixels when the viewing angle is near\nthe middle than near the Equator and the Poles.\nLatency Breakdown We fnd that each frame’s execution time\nis consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='is consistently dominated by pixel rendering. We break down the\nframe latency into three main phases: boundary calculation, pixel\nrendering (which includes input streaming, which overlaps with\npixel rendering), and output streaming. Regardless of the resolution,\nthe pixel rendering time contributes to over 90% of the total frame\nlatency. The boundary calculation has negligible execution time\n(< 0.2%), indicating that although it is on the critical path of frame\nlatency, it is far from being a bottleneck.\nResource Utilization Finally, we show that our proposed sys-\ntem has low resource utilization. Fig. 7a shows the BRAM utilization\nacross diferent resolutions. The BRAM usage increases from 0.1 MB\nat 480p resolution to 0.84 MB at 2K resolution, but is still well under', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='at 480p resolution to 0.84 MB at 2K resolution, but is still well under\nthe budget of the mobile-grade Ultrascale+ FPGA. Fig. 7b shows\nthe utilizations of other FPGA resources including DSP, FF, and\nLUT. Their utilizations do not change across resolutions because\nthe underlying data-path is exactly the same for diferent resolu-\ntions. Although boundary calculation contribute little to the frame\n64\n48\n32\n16\n0\nUtilization (%)\n480p 720p1080p 2K\n1.0\n0.8\n0.5', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='1.0\n0.8\n0.5\n0.2\n0.0\nUtilization (MB)\n(a) BRAM utilization.\n50\n40\n30\n20\n10\n0\nUtilization (%)\n480p\n720p\n1080p\n2K\nBoundary Calc.\nRendering\nOthers\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Others\nDSP\nFF\nLUT\n(b) DSP, FF, LUT utilization.\nFig. 7: Resource utilization at the (0°, 0°) viewing angle.\n60\n40\n20\n0\nEnergy Saving (%)\nRC\nElephant\nNYC\nRhino\nParis\nVenice\nAvg\nSaving over FPGA\nSaving over GPU\n(a) Energy savings.\n100\n80\n60\n40\n20', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='100\n80\n60\n40\n20\n0\nPercentage (%)\n90\n75\n60\n45\n30\n15\n0\nVertical Angle (Degree)\nNYC\nElephant\nParis\nRC\nRhino\nVenice\n(b) CDF of vertical viewing angles.\nFig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Fig. 8: Evaluation results on real user viewing traces.\nlatency, it occupies signifcant amount of FPGA resources. This is a\ntypical resource-performance trade-of that accelerators make.\n5.3\nReal User Trace Evaluation\nWe evaluate on a recently released 360° dataset [11], which includes\nthe per-frame head orientations of 59 real users watching six difer-\nent YouTube 360° videos. Fig. 8a shows the average energy saving\nof our system over the FPGA and the GPU baselines. The error bars\nindicate one standard deviation across all the users. On average,\nwe achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='we achieve 26.4% and 40.0% energy savings over the GPU and the\nFPGA baselines in all fve benchmarked videos across all users.\nWe fnd that when watching 360° videos users tend to focus on\nthe scenes in front of them, under which circumstances our system\nis able to signifcantly out-perform the baselines as quantifed be-\nfore using microbenchmarks (Fig. 6). Fig. 8b shows the cumulative\ndistribution function of the absolute vertical angles of all users\nacross all videos. Each < x,y > point in Fig. 8b reads as: users’\nvertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='vertical viewing angles are between -y° and y° for x% of time. While\nthe vertical viewing angle theoretically span between −90° and 90°,\nover 80% of the time users focus on regions that are between −30°\nand 30° vertically. Users rare look at the 45° vertical angle, in which\ncase our rendering algorithm introduces redundant pixel process-\ning. In essence, our co-design system optimizes for the common\ncase to achieve signifcant overall energy savings.\n6\nConclusions\nWe expect that a signifcant amount of video trafc in the near\nfuture will be 360° panoramic videos. Mobile system designers will\nsoon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='soon face the challenge of guaranteeing desirable user experience\nwhile rendering 360° content under severe energy constraints. This\npaper takes a promising frst step in energy-efcient 360° content\nrendering through a specialized accelerator design. We demon-\nstrate that the key is to tame the irregular memory accesses by\nco-designing the rendering algorithm with the architecture.\n6\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n102\nReferences\n[1] [n.\nd.].\nDDR4\nPower\nCalculator\n4.0\nMicron', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='d.].\nDDR4\nPower\nCalculator\n4.0\nMicron\nTechnology,\nInc.\nhttps://www.micron.com/~/media/documents/products/power-calculator/\nddr4_power_calc.xlsm.\n[2] [n. d.]. Jetson TX2 Module. http://www.nvidia.com/object/embedded-systems-\ndev-kits-modules.html.\n[3] [n. d.]. Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.\nhttps://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='https://www.tomshardware.com/news/magic-leap-tegra-specs-\nrelease,37443.html.\n[4] [n. d.].\nMicron MT40A256M16GE-083E DDR4 datasheet.\nhttps://\nwww.datasheets360.com/pdf/-3972376754899166939.\n[5] [n. d.].\nTN-40-07: Calculating Memory Power for DDR4 SDRAM.\nhttps://www.micron.com/-/media/documents/products/technical-\nnote/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='note/dram/tn4007_ddr4_power_calculation.pdf.\n[6] [n. d.].\nTN-41-01: Calculating Memory System Power for DDR3.\nhttps://www.micron.com/-/media/Documents/Products/Technical%20Note/\nDRAM/TN41_01DDR3_Power.pdf.\n[7] [n. d.]. ZCU104 Evaluation Board User Guide. https://www.xilinx.com/support/\ndocumentation/boards_and_kits/zcu104/ug1267-zcu104-eval-bd.pdf.\n[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[8] Robert Anderson, David Gallup, Jonathan T Barron, Janne Kontkanen, Noah\nSnavely, Carlos Hernández, Sameer Agarwal, and Steven M Seitz. 2016. Jump:\nvirtual reality video. ACM Transactions on Graphics (TOG) 35, 6 (2016), 198.\n[9] Panagiotis Christopoulos Charitos, Hans-Kristian Arntzen, Alberto Duenas, and\nDaniele Di Donato. 2017. 360-Degree Video Rendering: Using Arm Technology\nto Implement 360-Degree Video Efcientl. (2017).\n[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='[10] Tao Chen and G Edward Suh. 2016. Efcient data supply for hardware accelerators\nwith prefetching and access/execute decoupling. In Proc. of MICRO.\n[11] Xavier Corbillon, Francesca De Simone, and Gwendal Simon. 2017. 360-degree\nvideo head movement dataset. In Proc. of MMSys.\n[12] Tarek El-Ganainy and Mohamed Hefeeda. 2016. Streaming virtual reality content.\narXiv preprint arXiv:1612.08350 (2016).\n[13] Ziyad S Hakura and Anoop Gupta. 1997. The design and analysis of a cache\narchitecture for texture mapping. In Proc. of ISCA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='architecture for texture mapping. In Proc. of ISCA.\n[14] Matthew Halpern, Yuhao Zhu, and Vijay Janapa Reddi. 2016. Mobile CPU’s Rise\nto Power: Quantifying the Impact of Generational Mobile CPU Design Trends\non Performance, Energy, and User Satisfaction. In Proc. of HPCA.\n[15] Paul S Heckbert. 1986. Survey of texture mapping. IEEE computer graphics and\napplications 6, 11 (1986), 56–67.\n[16] James Hegarty, John Brunhaver, Zachary DeVito, Jonathan Ragan-Kelley, Noy Co-\nhen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='hen, Steven Bell, Artem Vasilyev, Mark Horowitz, and Pat Hanrahan. 2014. Dark-\nroom: Compiling High-Level Image Processing Code into Hardware Pipelines. In\nProc. of SIGGRAPH.\n[17] Bruce Jacob, Spencer Ng, and David Wang. 2010. Memory systems: cache, DRAM,\ndisk. Morgan Kaufmann.\n[18] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2018.\nSemantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='Semantic-Aware Virtual Reality Video Streaming. In Proceedings of the 9th Asia-\nPacifc Workshop on Systems. ACM, 21.\n[19] Yue Leng, Chi-Chun Chen, Qiuyue Sun, Jian Huang, and Yuhao Zhu. 2019. Energy-\nEfcient Video Processing for Virtual Reality. In ISCA.\n[20] William M Newman and Robert F Sproull. 1979. Principles of interactive computer\ngraphics. McGraw-Hill, Inc.\n[21] Benjamin O’Sullivan, Fahad Alam, and Clyde Matava. 2018. Creating Low-Cost\n360-Degree Virtual Reality Videos for Hospitals: A Technical Paper on the Dos\nand Don’ts. Journal of medical Internet research 20, 7 (2018).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'}), Document(page_content='and Don’ts. Journal of medical Internet research 20, 7 (2018).\n[22] Xilinx. 2017. AXI4-Stream Interconnect v1.1 LogiCORE IP Product Guide (PG035).\n(2017).\n7\nSession: Applications I\nFPGA ’20, February 23–25, 2020, Seaside, CA, USA\n103', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpsaqud_36/高性能计算 - FPGA - Energy-Efficient 360-Degree Video Rendering on FPGA via Algorithm-Architecture Co-Design.pdf'})]
cuda:2
[UploadFile(filename='路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf', size=664985, headers=Headers({'content-disposition': 'form-data; name="files"; filename="è·¯ç\x94± - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpt5nmp1bu, tmpt5nmp1bu
File: 路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf, msg: 成功上传文件 路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf, docs: [Document(page_content='Lightweight Source Authentication and Path Validation\nTiffany Hyun-Jin Kim\nCyLab, CMU\nhyunjin@cmu.edu\nCristina Basescu\nETH Zürich\ncba@inf.eth.ch\nLimin Jia\nCyLab, CMU\nliminjia@cmu.edu\nSoo Bum Lee\nQualcomm\nsoobuml@qti.qualcomm.com\nYih-Chun Hu\nUIUC\nyihchun@uiuc.edu\nAdrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Adrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT\nIn-network source authentication and path validation are funda-\nmental primitives to construct higher-level security mechanisms\nsuch as DDoS mitigation, path compliance, packet attribution, or\nprotection against ﬂow redirection. Unfortunately, currently pro-\nposed solutions either fall short of addressing important security\nconcerns or require a substantial amount of router overhead. In this\npaper, we propose lightweight, scalable, and secure protocols for\nshared key setup, source authentication, and path validation. Our\nprototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors\nC.2.0 [Computer-Communication Networks]: Security and pro-\ntection; C.2.1 [Network Architecture and Design]: Circuit-switch-\ning networks, Packet-switching networks\nKeywords\nSource Authentication, Path Validation, Retroactive Key Setup\n1.\nINTRODUCTION\nSource authentication and path validation are useful primitives\nto help mitigate various network-based attacks, such as DDoS, ad-\ndress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='dress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according\nto the policies of ISPs, enterprises, and datacenters. Endhosts and\nISPs desire to validate service level agreement compliance regard-\ning data delivery in the network: Did the packet truly originate from\nthe claimed client? Did the client select a path that complies with\nthe service provider’s policy? Did the packet indeed travel through\nthe path selected by the client?\nUnfortunately, the current Internet provides almost no means for\nsource authentication and path validation by routers or endhosts,\nopening up numerous attack surfaces. For example, a malicious\nISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for proﬁt or commercial advantage and that copies bear\nthis notice and the full citation on the ﬁrst page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior speciﬁc permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='permissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.\nCopyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00.\nhttp://dx.doi.org/10.1145/2619239.2626323.\ntively, a malicious router may inject packets with a spoofed source\naddress to incriminate a victim source node into having sent an ex-\ncessive number of packets. A malicious router may simply alter the\ncontents of received packets as well. The inability to detect such\nattacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-\nvert trafﬁc to traverse a point of eavesdropping it controls—a seri-\nous issue in particular for sensitive information.\nEnd-to-end encryption and authentication mechanisms, such as\nTLS, do not solve any of the above issues, since they are agnos-\ntic to which path the packet takes. A stronger approach is needed,\nwhich enables routers and destinations to perform source authenti-\ncation and path validation. As we discuss in the related work, ex-\nisting solutions either require extensive overhead, or only partially\naddress fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='address fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses\nboth source authentication and path validation, but it requires each\nintermediate router on a path to store and look up keys shared with\nother routers; ICING requires 42 bytes per verifying router in the\npacket header. Furthermore, ICING requires each router to calcu-\nlate a Message Authentication Code (MACs) for all other routers\non the path. In contrast, our protocol does not require any per-\nclient state on routers; it requires only 16 bytes per hop (which can\nbe reduced to 2 bytes for a lower level of security), and only a sin-\ngle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='gle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-\nvents against coward attacks [20], where an adversary only attacks\nwhen it knows that the attack will not be detected. Our protocol,\nhowever, offers reduced security in the case of a malicious sender\ncolluding with a malicious router on the path, which we describe\nin detail in the related work section. Since in the common case,\nsender and receiver trust each other, the performance gain of O(1)\nMAC operation per router instead of O(n) is worth the tradeoff.\nContributions. In this paper, we present Dynamically Recreatable\nKey (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Key (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of\nDRKey on routers prevents state exhaustion DoS attacks and sim-\npliﬁes router architecture. We further enrich DRKey with a new\nnotion called retroactive key setup that provides the following de-\nsirable properties: (1) in contrast to previous protocols, source and\ndestination can start the communication without needing to wait\nfor the expensive key setup to complete, providing efﬁciency; (2)\nif misbehavior is suspected, endhosts set up keys retroactively to\nverify previous packets, defending against coward attacks.\nBased on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Based on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,\nscalable, and secure protocols for source authentication and path\nvalidation. We introduce an extension called Retroactive-PathTrace\nthat supports the destination to perform path validation with retroac-\n271\ntive key setup and to detect coward attackers with small, constant\noverhead in the packet header. Our OPT protocols enable imple-\nmentation on SW routers with minimal performance impact.\n2.\nPROBLEM DEFINITION\n2.1\nDesired Security Properties\nSource authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Source authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether\nthe packet content has not been altered en route. In this paper,\nsource authentication includes data authentication.\nPath validation. The source, intermediate routers, and the desti-\nnation should be able to validate that the packet indeed traversed\nthe path known to (or selected by) the source. Successful path val-\nidation ensures that the packet traversed each honest router on the\npath in the correct order. Unfortunately, no scheme can provide\nany guarantees for malicious routers: if malicious router Rm pub-\nlishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='lishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2\nElided Security Properties\nNo packet delivery guarantee. Routers generally have the free-\ndom to decide whether or not to forward packets. Hence, it is not\nthe purpose of path validation to guarantee that packets will be de-\nlivered to the speciﬁed destination.\nNo detection of packet siphoning. Misbehaving router Rm on the\nsource-selected path can siphon packets and send them over a sep-\narate channel to a remote entity. Since Rm still forwards the packet\nto Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with\nthe packet.\nNo locating of packet altering and dropping routers. Locating\nrouters that alter or drop packets is the goal of fault-localization\nmechanisms—another challenging problem especially in inter-domain\nsettings [39]. Since path validation is a simpler problem, the goal\nis to achieve a more efﬁcient protocol than heavy-weight fault lo-\ncalization.\n2.3\nAdversary Model\nWe consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.\nPacket alteration. A malicious router alters any part of the packet,\nsuch as source address, header information, or payload data.\nPacket injection. A malicious router fabricates a packet and sends\nit towards a destination of its choice. A packet replay attack is a\nspecial case of packet injection.\nPath deviation. A malicious router may perform path deviation\nattacks, which cause packets to be forwarded along a path other\nthan the path previously selected by the source. We subdivide this\nattack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns\nto the correct downstream router Rm+1 to resume traversal of all\nrouters on the intended path.\nRouter skipping: A malicious router redirects the packet and\nskips other router(s) on the path. Thus, some routers on the in-\ntended path does not forward the packet.\nOut-of-order traversal: An adversary causes path deviations\nsuch that routers on the intended path are not traversed in the\nright order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='right order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For\nexample, an attacker diverts trafﬁc only when the protocol is inac-\ntive (e.g., required keys for validation have not been established).\nDenial-of-Service (DoS). As part of DoS attacks, we consider mem-\nory and computation exhaustion attacks on routers performing source\nauthentication and path validation.\nCollusion. Protocol participants may collude to carry out any of\nthe attacks listed above. For example, two or more intermediate\nrouters may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router\nto spoof authenticators for its downstream routers if the destina-\ntion prefers/trusts skipped routers. Also, both the source and the\ndestination could collude with some intermediate routers to frame\nanother router on the path by not forwarding packets to it.\nIn Section 6, we explore potential attacks against our protocols\nthat violate the desired properties and discuss how OPT defends\nagainst these attacks.\n3.\nOPT DESIGN OVERVIEW\nWe consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and\nRi’s as tracing entities. At a very high level, the main insights for\nachieving source authentication and path validation without requir-\ning routers to maintain per-source or per-path-length state are as\nfollows: (1) In the packet header, source S includes H(P), which is\nthe hash of the packet payload to help receiving entities identify the\npacket while avoiding expensive hash computation at each router;\n(2) On demand, each router Ri generates key Ki using a symmet-\nric cryptographic operation, and requires only router’s local secret\nSVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='SVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless\nand faster than storing or retrieving secrets. (3) Each router per-\nforms source authentication using a MAC computed over H(P); (4)\nEach router Ri extends a special authentication ﬁeld called PVF by\nperforming a MAC operation. Hence, path validation is achieved\nthrough a chain consisting of nested MACs.\n3.1\nAssumptions\nFor the communication properties of the network, we assume\nthat the source knows the path that the packet will traverse at the\nAS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the\nvalidations. This information can stem from (1) the BGP proto-\ncol where the source can learn the AS path that the packet is ex-\npected to traverse, (2) Pathlet routing [11] or SCION [40] where\nthe source can specify the path in the packet header, or (3) i3 [33]\nor Platypus [29] where the source can deﬁne a sequence of servers\nto traverse. Alternatively, an ISP may provide the premium path\ninformation to clients as an extra service (e.g., transatlantic cable\nfor ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='for ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.\nFor the cryptographic key setup, the source and the destination\nneed to be able to authenticate the router’s cryptographic materi-\nals (i.e., validate a signature that binds an entity to some crypto-\ngraphic materials). In the case of AS-level tracing, the AS needs to\nbe authenticated, and such authentication can be achieved through\nRPKI [4], which is already operational. RPKI provides a PKI that\nenables authentication of AS certiﬁcates, each of which binds an\nAS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn\ncreates certiﬁcates for each router using the AS’s private key—\nenabling the tracing entity to verify via the AS certiﬁcate using\nRPKI.1\n1Alternatively, OPT can authenticate entities based on mechanisms\n272\nTable 1: Notation.\n(PKE,PK−1\nE )\nEntity E’s public-private key pair\nCertPKE\nEntity E’s public-key Certiﬁcate\nˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions\nKE\nSymmetric key among S, D, and entity E for a single session\nKE1E2\nSymmetric key between entities E1 and E2 for a single session\nKE1E2σ\nSymmetric key for E1 and E2 in session σ for E1-initiated\npackets\nSVE\nEntity E’s local secret value\nP\nNetwork packet payload\n(PKσ,PK−1\nσ )\nPublic-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Public-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ\nTime when S initiates session σ\nSESSIONID\nHash of session σ’s public key, path, session initiation time\nAUTHσ\nAuthenticated and encrypted SESSIONID and private key for\nsession σ\nSignKEYPK−1\nE ,σ\nSignature on a symmetric key for session σ using entity E’s\nprivate key\nEncKEYK,σ\nEncryption of a symmetric key for session σ using key K\nKEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH\nHash of the packet’s payload\nPVF\nField enabling D to verify the path\nPVFS\nField enabling Ri and D to verify the path\nPVFD\nField enabling D to conﬁrm the actual path\nOVi\nField enabling Ri to validate the packet sender\nOPVi\nField enabling Ri to verify both the packet sender and path\nSignPK−1\nE (·)\nSignature using entity E’s private key\nCheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='CheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key\nEncK(·), DecK(·)\nEncryption, decryption using key K\nAuthEncK(·)\nAuthenticated encryption using key K\nAuthDecK(·)\nAuthenticated decryption using key K\nFK(·)\nPseudo-random function using key K\nMACK(·)\nMessage Authentication Code using key K\nH(·)\nCryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Cryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret\nkey between each other. In case the tracing entities are AS infras-\ntructure hosts such as edge routers, ﬁrewalls, or a middlebox at a\nservice provider, either RPKI can be used as described above or an\nadministrator can set up trusted public keys between entities that\nneed path veriﬁcation. If endhosts perform tracing, then a shared\nkey can be set up through SSL or TLS if one of the endhosts is a\nHTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='HTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted\nkeys, TOFU (Trust On First Use) in SSH, TOFU with Perspec-\ntives [35], RPKI with domain-certiﬁed host keys, self-certifying\nIDs as public keys [1,23,24,34], or self-validation using an anony-\nmous service [10]. We assume that one of these approaches is used\nto set up symmetric key ˆKSD between source S and destination D.\n3.2\nMain Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Main Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each\nrouter to maintain a secret key for each ﬂow, our design enables\nrouters to derive the secret keys on the ﬂy using only local se-\ncrets stored at the routers and an efﬁcient pseudo-random function.\nThus, we avoid storing all the keys.\nMore precisely, OPT runs in sessions. In each session σ, source\nS sends packets to destination D on path PATHσ. S and D leverage\nlong-term symmetric key ˆKSD to set up keys with each router in\nPATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,\nthe source prepares and inserts a special ﬁeld in the packet header\ncalled SESSIONID such that intermediate routers Ri on PATHσ dy-\nnamically compute the shared symmetric key with S and D (Ri only\nneeds to look up its local secret SVRi for computation).\nthat use self-certifying IDs as public keys [1,23,24,34] as assumed\nin ICING. However, such mechanisms have issues with key revo-\ncations. Hence, we prefer to use RPKI.\nOur key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Our key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS\nattacks based on state exhaustion. Moreover, computing pseudo-\nrandom function (PRF) F is faster than performing a cache access;\nfor instance, a key derivation using AESni takes 32 cycles, whereas\na L3 cache read operation requires approximately 40 cycles (on In-\ntel “Sandy-Bridge”-based Xeon architecture).\nOPT includes the hash of the packet payload H(P) in the header,\nwhich enables an important optimization: routers can either par-\nallelize the computations of MAC and the hash of the packet, or\nprobabilistically validate H(P).\n3.3\nOPT Protocol Overview', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='probabilistically validate H(P).\n3.3\nOPT Protocol Overview\nDRKey for path selection and key setup. When source S initiates\nsession σ at time Tσ, S selects path PATHσ to destination D, gener-\nates asymmetric public/private key pair (PKσ,PK−1\nσ ), and creates a\nsession identiﬁer, where SESSIONID = H(PKσ∥PATHσ∥Tσ). Af-\nter preparing some values that support source authentication and\npath validation for other entities on PATHσ, S forwards the OPT\npacket to its downstream router on PATHσ. If Tσ is recent (i.e.,\nwithin some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='within some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and\nSESSIONID. Detailed DRKey protocols are explained in Section 4.\nGeneration of veriﬁcation ﬁelds. S uses the path information to\npre-compute veriﬁcation ﬁelds, one for each router Ri on PATHσ,\nand a special ﬁeld called PVF such that routers can perform source\nauthentication and path validation.\nVeriﬁcation and update by intermediate routers. Upon receiv-\ning a packet, Ri ﬁrst regenerates the shared symmetric key Ki and\nrecomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='recomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it\nsuccessfully authenticates the source and the content of the packet,\nand validates the traversed path. Ri then updates PVF, by applying\na MAC operation using Ki to the ﬁeld. This process helps down-\nstream routers and the destination to validate that each router on the\npath has indeed seen the packet.\nVeriﬁcation by destination. The destination ﬁnally recomputes\nthe veriﬁcation ﬁelds using all the symmetric keys shared with\nother entities on the path. Successful veriﬁcation indicates source\nand packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS\nThis section introduces the DRKey protocols that enable routers\nto set up shared keys with source S and destination D. Section 4.1\ndescribes the case when both S and D trust each other. Section 4.2\nrelaxes this assumption and describes the case when S and D do not\ntrust each other. Section 4.3 describes how S and D retroactively set\nup shared keys with intermediate routers to enable path validation\nof prior packets.\n4.1\nDRKey for Benign Source and Destination\nWhen both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='When both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is\nshared with both S and D. Figure 1 shows the key setup steps and\nthe associated cryptographic operations.\nS creates a fresh public/private key pair (PKσ,PK−1\nσ ) for each\nsession such that routers encrypt session symmetric key Ki’s. Since\nS and D trust each other, they share private key PK−1\nσ , the en-\ncrypted and authenticated value of which is sent to D. The public\nkey is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='key is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path\nand Tσ is when S initiates session σ. Note that Tσ prevents re-\nplay attacks since routers can drop expired packets based on loose\ntime synchronization.\n273\nInitialization by Source S\n0.\nAssume long-term symmetric key ˆKSD shared with D\n(Optional) Assume public/private key pair (PKS,PK−1\nS ), and CertPKS\n1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONID)\n5.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1\nσ )\nS → R1\n7.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID) }\nPairwise Key Derivation by R1\n8.\nCompute K1 = FSVR1 (SESSIONID)\n9.\nEncrypt K1: EncKEYR1,σ = EncPKσ (K1)\nSign: SignKEYR1,σ = SignPK−1\nR1 (K1∥PKσ)\nR1 → R2\n10.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R1 (K1∥PKσ)\nR1 → R2\n10.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ }\nPairwise Key Derivation by R2\n11.\nComputes K2 = FSVR2 (SESSIONID)\n12.\nEncrypt K2: EncKEYR2,σ = EncPKσ (K2)\nSign: SignKEYR2,σ = SignPK−1\nR2 (K2∥PKσ)\nR2 → D\n13.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (K2∥PKσ)\nR2 → D\n13.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPKS(SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ, EncKEYR2,σ, SignKEYR2,σ}\nKey Retrieval by Destination D\n14.\nAlready has ˆKSD, which is the long-term shared symmetric key with S\n15.\nCheck that D is the last entity on PATHσ\n16.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ) and check the integrity\n17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n18.\nDecrypt AUTHσ and authenticate PK−1\nσ : AuthDecKSDσ (AUTHσ)\n19.\nDecrypt K1 and K2 and check their signatures:\nDecPK−1\nσ (EncKEYR1,σ),CheckSigPKR1 (SignKEYR1,σ)\nDecPK−1\nσ (EncKEYR2,σ),CheckSigPKR2 (SignKEYR2,σ)\nK1 and K2 become shared symmetric keys between each router and D\n20.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='K1 and K2 become shared symmetric keys between each router and D\n20.\nCompute KD = FSVD(SESSIONID)\nD → S\n21.\nForward authenticated and encrypted shared keys:\nKEYSσ = AuthEncKDSσ (K1∥K2∥KD∥AUTHσ)\nKey Retrieval by Source S\n22.\nDecrypt and authenticate the keys received from D: AuthDecKDSσ (KEYSσ)\nK1, K2 and KD become shared keys between S and R1, R2, and D\nFigure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-\nﬁcient PRF keyed with a secret SVi only known to Ri. The PRF\ntakes SESSIONID as an input. For high efﬁciency, we compute\nour PRF from a pseudo-random permutation using AES. The over-\nhead of the key setup is negligible to affect the on-going trafﬁc (see\nSection 8). Resulting key Ki is encrypted with public key PKσ,\nand digitally signed to enable veriﬁcation that (encrypted) Ki in-\ndeed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='deed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).\nTo prevent reﬂection attacks (i.e., replaying message in the op-\nposite order of communication), communication between S and D\nuses different symmetric keys for each direction: KSDσ and KDSσ\nfor S-initiated and D-initiated packets, respectively.\nThe optional operations in Figure 1 are used only if the router\nalso needs to authenticate S, in which case S also signs the SESSIONID,\nand certiﬁcates needed for routers to verify S’s public key are in-\ncluded in the message.\n4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust\neach other and that they share the same public-private key pair for\nthe session, then each intermediate router needs to set up only one\nshared key with both S and D. However, S and D may not neces-\nPath Agreement and Key Setup Initialization by Source S\n1.\nInitiate new session σand pick random session key (PKSσ,PK−1\nSσ )\nRi uses this key to authenticate S’s packets\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Obtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONIDS = H(PKSσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONIDS)\nS → D\n5.\nForward {PKSσ,PATHσ,Tσ,SignPK−1\nS (PKSσ∥PATHσ∥Tσ)}\nPath Agreement and Key Setup Initialization by Destination D\n6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.\nCompute SESSIONIDD = H(PKDσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nD (SESSIONIDD)\nD → S\n8.\nForward {PKDσ,PATHσ,Tσ,SignPK−1\nD (PKSσ∥PKDσ∥PATHσ),\nSESSIONIDD, (optional) SignPK−1\nD (SESSIONIDD) }\nInitialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Initialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD) }\nPairwise Key Derivation by R1\n10.\nCompute KS1 = FSVR1S(SESSIONIDS))\nKD1 = FSVR1D(SESSIONIDD))\n11.\nEncrypt KS1: EncKEYR1S,σ = EncPKSσ (KS1)\nKS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.\nSign: SignKEYR1S,σ = SignPK−1\nR1 (KS1∥PKSσ∥S)\nSignKEYR1D,σ = SignPK−1\nR1 (KD1∥PKDσ∥D)\nR1 → R2\n13.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ}\nPairwise Key Derivation by R2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Pairwise Key Derivation by R2\n14.\nCompute KS2 = FSVR2S(SESSIONIDS))\nKD2 = FSVR2D(SESSIONIDD))\n15.\nEncrypt KS2: EncKEYR2S,σ = EncPKSσ (KS2)\nKS2: EncKEYR2D,σ = EncPKDσ (KD2)\n16.\nSign: SignKEYR2S,σ = SignPK−1\nR2 (KS2∥PKSσ∥S)\nSignKEYR2D,σ = SignPK−1\nR2 (KD2∥PKDσ∥D)\nR2 → D\n17.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (KD2∥PKDσ∥D)\nR2 → D\n17.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ,\nEncKEYR2S,σ,SignKEYR2S,σ,EncKEYR2D,σ,SignKEYR2D,σ}\nKey Retrieval by D\n18.\nCheck that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Check that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures\nDecPK−1\nDσ (EncKEYR1D,σ),CheckSigPKR1 (SignKEYR1S,σ)\nDecPK−1\nDσ (EncKEYR2D,σ),CheckSigPKR2 (SignKEYR2S,σ)\nKD1 and KD2 become shared symmetric key between each router and D\n20.\nCompute KD = FSVD(SESSIONIDS))\n21.\nEncrypt KD: EncKEYD,σ = EncPKSσ (KD)\n22.\nSign: SignKEYD,σ = SignPK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='22.\nSign: SignKEYD,σ = SignPK−1\nD (KD∥PKSσ∥S)\nD → S\n23.\nForward {EncKEYR1S,σ,SignKEYR1S,σ,EncKEYR2S,σ,SignKEYR2S,σ,\nEncKEYD,σ,SignKEYD,σ}\nKey Retrieval by Source S\n24.\nDecrypt and authenticate keys received from D\nKS1, KS2 and KD become shared keys between S and R1, R2, and D.\nFigure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.\nsarily trust each other, or they may collude. To strengthen the secu-\nrity guarantees under such circumstances, we introduce Extended-\nDRKey , which requires each intermediate router to set up two keys,\nKSi and KDi, where KSi is the shared symmetric key between S and\nRi and KDi is the shared symmetric key between Ri and D. Figure 2\ndescribes the Extended-DRKey protocol and its cryptographic op-\nerations.\nUnlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Unlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating\nshared keys (i.e., KSi and KDi), Ri and D use distinct local secrets\nto encode the directionality of the keys.\n274\n4.3\nRetroactive-DRKey\nThe key setup protocols as presented in Figures 1 and 2 run once\nbefore the session starts. However, the key setup process incurs\nthe following extra latency and computational overhead: (1) Key\nsetup itself requires an extra round trip between the source and the\ndestination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='destination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-\ndent key setup protocol a priori allows routers to launch a coward\nattack, since the key setup protocol warns possibly misbehaving\nrouters to start behaving correctly and to avoid detection. Conse-\nquently, achieving path validation without the apparent key setup\nprocess is desirable.\nWe introduce Retroactive-DRKey that enables entities to set up\nshared keys at any time after the ﬁrst packet in a session reaches\nthe destination. Note that Retroactive-OPT is invoked only if the\nsource or the destination wishes to perform source authentication\nor path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='or path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).\nTo support such a feature, we still assume that source S and des-\ntination D establish a shared symmetric key ˆKSD in advance, and\nS derives a session key pair (PKσ,PK−1\nσ ) before the session starts.\nUnlike DRKey or Extended-DRKey, Retroactive-DRKey utilizes\nthat S creates KD—a shared symmetric key with D for the ses-\nsion (i.e. KD = FSVS(SESSIONID))—and includes encrypted and\nauthenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='authenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some\nﬁelds such that D can check. When a forwarding protocol is used\nwith Retroactive-DRKey, the routers use some keys for OPT during\na session, and only reveal them at a later time (Section 5.2.1).\nRetroactive-DRKey is very similar to the key setup protocol in\nFigure 1. The only difference is that D does not derive KD, be-\ncause it is already included in each forwarded packet. Retroactive-\nDRKey runs at most once during or after a session ends. We ob-\nserve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='serve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.\n5.\nOPT PROTOCOL DESCRIPTION\nThe DRKey protocols described in Section 4 and the techniques\nwe introduce in this section span a protocol family of source au-\nthentication and path validation with varying assumptions and prop-\nerties. Unfortunately, exploring the entire design space is out of\nscope for this paper, and we will present several protocol instantia-\ntions: (1) OriginValidation for source authentication (S and D trust\neach other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-\ntion and path validation (S and D may not trust each other).\n5.1\nOriginValidation for Source Authentication\nOriginValidation enables each intermediate router and the des-\ntination to perform source authentication using MACs computed\nover the hash of the packet. For efﬁcient authentication, the source\nincludes the following ﬁelds in the packet header:\nDATAHASH: Hash of the packet’s payload H(P);\nSESSIONID: Hash of the session public key, path, and session\ninitiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='initiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.\nOVi is a Message Authenti-\ncation Code computed over DATAHASH using key Ki that Ri\nshares with S (i.e., OVi = MACKi(H(P))). Similarly, OVD =\nMACKD(H(P)). The source creates an OV ﬁeld for each inter-\nmediate router and the destination.\nOriginValidation provides efﬁcient MAC veriﬁcation using the\nDATAHASH ﬁeld without requiring each intermediate router to com-\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nPVF (128 bits)\nOV1 (128 bits)\nOV2 (128 bits)\nOVD (128 bits)\nIP Header\nOriginValidation/PathTrace Header\nTCP Header\nFigure 3: The packet header format for OriginValidation and\nPathTrace. DATAHASH, SESSIONID, and OVs help intermedi-\nate routers and the destination authenticate the source (Orig-\ninValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='inValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).\npute the hash over the entire packet. Figure 3 represents the packet\nheader, and only DATAHASH, SESSIONID and OV ﬁelds are needed\nfor OriginValidation.\nWhen intermediate router R1 receives a packet from the source\nS, R1 computes the symmetric key (K1) it shares with S using R1’s\nlocal secret and SESSIONID from the packet header. Then R1 gen-\nerates a MAC as follows: OV′\n1 = MACK1(DATAHASH). If OV′\n1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards\nthe packet to R2. R2 and D perform the same operations as R1.\nAlthough we present the protocol with OV ﬁelds of size 128,\nthe size can be altered to reﬂect the desired level of security. In\ngeneral, assuming a secure MAC function, the success probability\nof a forged n-bit MAC is 2−n, which already results in a low rate at\nn = 16. Thus, for many applications, 2 byte long OV ﬁelds sufﬁce,\nas a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='as a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace\nPathTrace is to help the source and destination validate that a re-\nceived packet traversed the source-selected path. This main objec-\ntive is achieved by Path Validation Field (PVF), which is a nested\nMAC that intermediate routers update in the packet header as they\nforward the packet. In Figure 3, only DATAHASH, SESSIONID,\nand PVF ﬁelds are used for PathTrace, thus, the packet overhead\nis irrespective of the path length. Next we describe how PathTrace\nsupports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='supports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-\nidate the path, the source generates PVF0—the initial PVF value\nwhich is a MAC of DATAHASH using the shared symmetric key\nbetween the source and the destination. Then the source initializes\nthe PVF ﬁeld in the header with PVF0:\nPVF ← PVF0 = MACKD(DATAHASH).\n(1)\nAny intermediate router Ri on the path generates PVFi and updates\nthe PVF ﬁeld in the header as follows:\nPVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and\nthe destination according to the key setup protocol in Section 4.1.\nHence, upon receiving a packet, the destination ﬁrst re-creates the\nnested MACs (here shown for a path of 2 routers):\nPVF′ = MACK2(DATAHASH∥\nMACK1(DATAHASH∥MACKD(DATAHASH))).\n(3)\nIf PVF′ is the same as PVF in the packet header, the destination is\nassured that the packet was indeed delivered on the source-selected\npath. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its\npacket is delivered to the intended destination using the source-\nselected path, the destination forwards the PVF from the received\n275\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nTIMESTAMP (32 bits)\nPVF (128 bits)\nOPV1 (128 bits)\nOPV2 (128 bits)\nOPVD (128 bits)\nIP Header\nOPT Header\nTCP Header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='OPVD (128 bits)\nIP Header\nOPT Header\nTCP Header\nFigure 4: OPT header. The source S initializes all the ﬁelds.\nIntermediate routers only update the PVF ﬁeld.\npacket header back to the source as follows:\nD → S : EncKD(PVF∥DATAHASH).\n(4)\nUpon receiving this information, the source ﬁrst decrypts the\nmessage using KD and then performs the validation by re-constructing\nthe nested MACs using DATAHASH as shown in Eq. (3) and com-\nparing it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='paring it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1\nRetroactive-PathTrace\nRetroactive-PathTrace supports path validation without the ap-\nparent key setup process in advance. Instead, it utilizes Retroactive-\nDRKey that runs after the session ends. Unlike PathTrace, in Retro-\nactive-PathTrace the source cannot pre-compute the OPVi ﬁelds;\nhence no OPV ﬁelds can be used in the packet header. Instead,\nunder the assumption that the source and the destination trust each\nother, Retroactive-PathTrace requires that the source creates KD—\nthe session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the\npacket header.\nThe source uses KD to compute PVF0 and the\nrouters derive their shared key and update the PVF ﬁeld accord-\ningly.\nRetroactive-PathTrace requires the destination to store per-packet\ninformation for later checking. However, the beneﬁt of defending\nagainst coward attacks overcomes such a disadvantage. Namely,\nthe destination stores for each packet the tuple (SESSIONID, DATA-\nHASH,PVF). When the destination wants to validate the path, it re-\nquests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.\nThen the destination can check the PVF ﬁelds and detect coward\nattacks. The source can independently initiate the retroactive pro-\ncess as well.\n5.3\nOPT: Origin and Path Trace\nIn this section, we introduce OPT that combines OriginValida-\ntion and PathTrace such that all entities (including intermediate\nrouters) on the path can perform both source authentication and\npath validation when they trust the source. We assume that all the\nrouters in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2\nFigure 4 illustrates the OPT header format. In addition to DATA-\nHASH, SESSIONID, and PVF, an OPT header includes the follow-\ning additional ﬁelds to enable each intermediate router to perform\npath validation.\nTIMESTAMP: Time when S creates the OPT packet to mitigate\ntiming-based attacks, such as replay attacks.\nOPVi: Origin and Path Veriﬁcation ﬁeld. OPVi is a MAC that\nenables all entities on the path to perform path validation.\n2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.\nAn arrow represents the header ﬁeld initialization.\n1: function SOURCE INITIALIZATION\nRequire: Ki and KD that Ri’s and D share with S, respectively after running\nkey setup protocol in Figure 1\n2:\nDATAHASH ← H(P)\n3:\nSESSIONID ← H(PKσ∥PATHσ∥Tσ)\n4:\nPVF ← PVF0 = MACKD(DATAHASH)\n5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do\n7:\nPVFi = MACKi(PVFi−1)\n8:\nOPVi ← MACKi(PVFi−1∥DATAHASH∥Ri−1∥TIMESTAMP)\n9:\nend for\n10:\nfor destination D do\n11:\nOPVD ← MACKD(PVFl−1∥DATAHASH∥Rl−1∥TIMESTAMP)\n12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function\n15: function VALIDATION AND UPDATE BY Ri\n16:\n(Note PVF in OPT header = PVFi−1)\n17:\nCompute OPV′\ni = MACKi(PVFi−1∥DATAHASH∥ Ri−1∥TIMESTAMP)\n18:\nif OPV′\ni == OPVi then\n19:\nPVF ← PVFi = MACKi(PVFi−1)\n20:\nForward the packet to Ri+1\n21:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='20:\nForward the packet to Ri+1\n21:\nelse\n22:\nDrop the packet\n23:\nend if\n24: end function\n25: function DESTINATION VALIDATION\n26:\n(Note PVF in OPT header = PVFl−1)\n27:\nl = source-selected path length\n28:\nCompute PVF′ = MACKl−1(...(MACK1(MACKD(DATAHASH))))\n29:\nCompute OPV′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='29:\nCompute OPV′\nD = MACKD(PVFl−1∥DATAHASH∥ Rl−1∥TIMESTAMP)\n30:\nif (PVF′ == PVF) && (OPV′\nD == OPVD) then\n31:\nValidation succeeds\n32:\nPrepare packet using Eq. (4) and forward to source\n33:\nelse\n34:\nDrop the packet\n35:\nend if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='end if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-\nscribes how the source initializes the OPT header ﬁelds. Each OPV\nﬁeld includes the following as inputs.\nPrevious PVF: Including PVFi−1 in the OPVi computation sup-\nports the detection of a malicious intermediate router that forwards\nthe packet to a benign router, which is not speciﬁed by the source\nbut follows the protocol.\nPrevious router address: PVF by itself cannot support entities to\ndetect the packet injection attack. Hence, we include the address of\nthe previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.\nConsider an example where packet Pcrt is expected to be sent along\nthe source-selected path PATHcrt, the source previously sent packet\nPold on PATHold, and Pcrt and Pold have the same payload. Con-\nsider router Rbad that is in both PATHcrt and PATHold such that\nPATHcrt = {R1,R2,...,Rbad,Rbad+1,...,Rn} and PATHold = {R′\n1,\nR′\n2,...,Rbad,R′\nbad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='bad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′\nbad+1,...,Rm} in PATHcrt and all the cor-\nresponding ﬁelds in the Pcrt header with those in Pold. Therefore,\nwithout TIMESTAMP, the destination cannot detect the misbehavior\nand ends up validating path {R1,R2,..., Rbad, R′\nbad+1,...,Rm} for\nPcrt. By setting the TIMESTAMP ﬁeld when the source sends out a\npacket, authenticator cloning attacks are mitigated with loose time\nsynchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='synchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-\nTION functions in Algorithm 1 describe the OPT procedure that\nintermediate router Ri and the destination performs, respectively.\n276\n5.3.1\nDistrusting source and destination\nThe previous protocols assumes that the source and the destina-\ntion are honest and trust each other. We now relax this assumption\nand present an extension that handles distrusting entities. In OPT,\nthe source can generate all PVFs by itself since it knows all Ki’s.\nConsequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Consequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D\nwithout going through R1 in Figure 1.\nTo prevent such an attack and address the problem of a distrust-\ning source and destination, we use the key setup protocol in Sec-\ntion 4.2 such that intermediate routers generate two separate shared\nkeys for the source and the destination. Unlike OPT, the Extended-\nOPT header requires two PVF ﬁelds: PVFS that enables interme-\ndiate routers and the destination to validate the source, and PVFD\nthat enables the destination to conﬁrm the actual path, even if the\nsource is malicious and colludes with (at least) one intermediate\nrouter.\n6.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source is malicious and colludes with (at least) one intermediate\nrouter.\n6.\nSECURITY ANALYSIS\nWe prove that OPT has origin authenticity and path validation\nproperties when both the source and the destination are trusted.\nThis property holds on any network conﬁguration, including ones\nthat have malicious routers. Extended-OPT offers stronger prop-\nerties: the router’s origin and path validation property assumes\nthat only the source is honest; and the destination’s path validation\nproperty does not assume the source is honest.\nWe describe how OPT and its variants defend against the adver-\nsary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='sary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;\ndetails can be found in our technical report [17].\nPacket alteration. Without the secret keys (KD and Ki), a mali-\ncious router cannot compute valid PVFi and OPVi. Consequently,\nin OPT, a successful veriﬁcation of PVFi−1 (PVFn) based on OPVi\n(OPVD) implies that there can be no packet alteration attacks to\nrouter Ri (the destination), provided that the source and destination\nare trusted. A malicious destination can carry out the packet al-\nteration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='teration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary\nrouters except that only the source needs to be trusted.\nPacket injection attack. OPT routers can check that an incoming\npacket does come from an intended AS, as such information is in-\ncluded in OPV. Therefore, a malicious router A can only inject\npacket to a router B if A is B’s neighbor and the link AB is on the\nintended path. We will revisit this attack when discussing collusion\nattacks.\nIn order to inject a packet with a valid header, an attacker can\nreplay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='replay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks\ncan be mitigated by including a timestamp in the packet. OPT is\nalso vulnerable to packet injection when the destination colludes\nwith the injecting router.\nPath deviation attack. OPT ensures that a successful veriﬁcation\nof PVFi−1 (PVFn) against OPVi (OPVD) implies that the payload\nRi (the destination) received has traversed all the honest routers in\nthe source-intended path in the correct order, assuming that both\nsource and destination are honest. OPT provides this guarantee\nbecause the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='because the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or\nOPVi. As a result, malicious routers cannot mount router skipping\nor out-of-order traversal attacks.\nThis indicates that if a malicious router selects a path not in-\ntended by the source, an honest intermediary router will reject the\npacket. However, a malicious router can mount a path detour attack\nand send the payload to other routers that are not on the intended\npath.\nIn Extended-OPT, even if the destination is malicious, it cannot\nselect the unauthorized path that drops or reorders honest routers.\nExtended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Extended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops\nor reorders honest routers on the source-intended path.\nCoward attack. Retroactive-OPT can mitigate coward attacks by\nrequiring all forwarding routers to compute relevant PVF and OPV\nﬁelds for probabilistic auditing. As a router cannot reliably guess\nwhen audits will happen, it does not know when to carry out an\nattack. We are unaware of any other path validation protocols that\ncan defend against the coward attack, including ICING.\nDoS attack. We consider attacks aiming to exhaust memory and\ncomputational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='computational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources\nsending trafﬁc and the number of ﬂows transiting the router. For\nthis reason, memory exhaustion attacks are not possible under OPT.\nOPT routers perform very few symmetric cryptographic operations\nper packet during forwarding, which run at line speed (Section 7).\nTherefore, OPT is more resilient to computation resource exhaus-\ntion attacks than existing schemes such as ICING that provide sim-\nilar security guarantees.\nCollusion. The path and source validation are conditioned upon\nwhether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='whether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended\npath before Ri, malicious routers can redirect the packet to any\nrouters as it chooses. However, all preceding links on the desired\npath are still traversed in the correct sequence for this packet to be\naccepted by Ri. Similarly, a malicious router could replace the path\nin the packet and trick its neighbor into forwarding the packet to a\nrouter outside the intended path. Again, this packet will be dropped\nwhen it reaches an honest router.\nWhen there are multiple adjacent malicious routers on the in-\ntended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='tended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the\nhole via R j1 and exited the hole from R jn, but has no knowledge as\nto where the packet has been to in between these two points. In\nparticular, when the source colludes with Ri, Ri+1 can be tricked\ninto accepting and forwarding any packet.\n7.\nIMPLEMENTATION AND EVALUATION\nWe implemented OPT in Section 5.3 with DRKey as a user-level\napplication that performs source authentication and path validation.\nThe cryptographic operations performed by a router during packet\nforwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='forwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic\nfunctions used in the implementation. To compute MACs, we used\nCBC-MAC based on AES, since it requires a single AES opera-\ntion to authenticate a 128-bit value. For computing the PRF, we\nalso use the same CBC-MAC. We implement AES using AESni, a\nnew CPU instruction set provided by recent Intel and AMD CPUs\nto speed up AES operations. AESni is fast: According to Intel,\nexecuting an encryption using AES-128 in CBC mode takes 4.15\ncycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be\nincreased, as we can process 4 blocks in parallel on a single core\nin AES-128 in CBC mode, resulting in 1.33 cycles per byte. We\nimplemented authenticated encryption using Galois/Counter Mode\n(GCM) with AES.\nWe use SHA-3 for computing hashes on long strings, such as\nthe hash of the payload DATAHASH. We truncate the hash from\n277\nTable 2: Per-session storage (σ), long-term storage (LT) related\nto the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter\nSource\nDestination\nStorage\nDRKey (σ)\n0\nn+2\nn+2\n[#items]\nICING (σ)\n2\nn+1\n2∗n+1\nDRKey (LT)\n1\n1\n2\nICING (LT)\n≤ 400,000\n0\n0\nKey setup\nDRKey', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='≤ 400,000\n0\n0\nKey setup\nDRKey\n2\n[#packets]\nICING\n4∗n+4\na 256-bit value to a 128-bit value. For computing hashes on short\nstrings, such as H(PKσ), we use the Merkle-Damgard construction\nwith a Matyas-Meyer-Oseas AES-based compression function that\nmakes use of the fast hardware AESni instructions. We choose a\nsingle-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='single-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header\nsize. Nevertheless, this decision does not pose security concerns:\nan adversary besides the source needs to perform a second-pre-\nimage collision attack, which is still in the order of O(2128) for\nSHA-3 and close to O(2128) for Matyas-Meyer-Oseas.\nFor signatures, we use Ed25519 [6], providing high efﬁciency\nand security, and small signatures. For a security level of 2128 oper-\nations, Ed25519 signature generation and veriﬁcation on a 3.4GHz', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ations, Ed25519 signature generation and veriﬁcation on a 3.4GHz\nCore i7 takes 20us and 60us, respectively. Public keys and signa-\ntures are only 32 and 64 bytes, respectively. Certiﬁcates can thus be\nas small as 128 bytes, enabling routers to add their certiﬁcate to the\nkey setup message. As explained in Section 3, router certiﬁcates\ncan be generated by an AS and signed with the AS’s private key.\nThe AS’s public key can be obtained and veriﬁed through RPKI.\nFor encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='For encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-\ntude slower, it is performed by the endhost which is less perfor-\nmance critical.\nICING implementation and conﬁguration. We compare OPT\nwith ICING, the code of which we obtained from their website3.\nTo ensure the fairness of our comparison, we implemented ICING\nthat also uses AESni. In ICING, the source obtains a proof of con-\nsent (PoC) from the consent server of each node on the path. A\nPoC certiﬁes that the node consents to the full path. Furthermore,\neach node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-\ntion for computing the tag keys needed for the PoCs [25], which\ncan diminish the number of required PRF rounds to 0. In our IC-\nING implementation, we favor ICING and consider that computing\nthese keys has no computational or memory lookup overhead.\nAnother important concept in ICING is the proofs of provenance\n(PoPs)—proofs to the nodes that the packet originates from the\nsender. Computing PoPs requires shared symmetric keys between\neach pair of ICING nodes on the path. These keys can be either de-\nrived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='rived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive\nand impractical on the fast path, PoP keys are always retrieved from\nthe cache in our ICING implementation.\n7.1\nDRKey Evaluation\nDRKey enables the design of low-overhead protocols in terms\nof router resources, such as OPT. In OPT, we use DRKey for key\nsetup, which is executed once per session. Thus, the key setup cost\nis amortized over an OPT session.\nTable 2 provides an analysis of storage overhead at the source,\n3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity\nPath length\nLatency\nRouter\nIrrelevant\n381 µs\nSource\n2\n621 µs\n4\n609 µs\n8\n628 µs\nDestination\n2\n3820 µs\n4\n5520 µs\n8\n14814 µs', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4\n5520 µs\n8\n14814 µs\ndestination, and intermediate routers for DRKey and ICING. We\nalso compare the communication overhead for setting up keys.\nGiven a path of length n (excluding the source and the destina-\ntion), DRKey requires the source and the destination to store, per\nsession, n+1 symmetric keys and a public-private session key pair.\nThe long-term storage of the source and the destination, which out-\nlives multiple sessions, consists of their shared symmetric key and\nthe destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.\nIn contrast, ICING’s source and destination need to store n + 1\nsymmetric keys. Each ICING router needs to store pairwise keys\nwith every router in the Internet, which, according to the ICING au-\nthors, is within 400,000. The source also stores PoCs of all entities\nin the path.\nRegarding the communication overhead of the key setup in DRKey,\nthe source and the destination send one message each, resulting in\n2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the\npath (routers and destination), which leads to 2∗(n+1) messages\nfor a path of length n. The source also sets up pairwise shared keys\nwith each entity on the path, requiring at least a round trip (2 mes-\nsages) per entity, resulting in at least 2 ∗ (n + 1) messages. We do\nnot count the messages that are necessary to set up pairwise shared\nkeys between ICING routers, because these keys are set up once\nbetween all entities in the Internet and then stored at each entity.\nTo prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='To prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate\nrouters. Our experiment measures the latency of key setup packets\nwhile they transit the network entities. For the experiment, we use\na trafﬁc generator that initiates key setup operations and connects\nto a server that performs the key setup operations of the source,\nrouter, or destination. After the key setup, the server forwards the\npackets back to the trafﬁc generator, which measures the receive\nrate.\nTable 3 presents the latency of DRKey packets at the source,\nrouters, and the destination.\nThe results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.\nFor the source and the destination, a longer path increases the\namount of computation. In the case of the source, this is hardly no-\nticeable, because the source does not perform public-key cryptogra-\nphy operations that depend on the path length. In contrast, the des-\ntination performs per-hop public-key decryption using RSA-2048\nto obtain the shared keys, which is expensive and considerably af-\nfects the latency. Nevertheless, the results satisfy our objectives:\nsince the source and the destination have a signiﬁcantly lower traf-\nﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.\n7.2\nOPT Evaluation\nWe evaluate OPT with respect to the desired performance proper-\n278\n0\n200\n400\n600\n800\n1000\n1200\n1400\n20\n30\n40\nPacket Size (B)\nThroughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Throughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments\nuse I/O Engine for trafﬁc delivery, I/O engine represents our\nbaseline.\nties for source and path validation, namely efﬁcient forwarding and\nscalable state, and the cost associated with meeting them. Specif-\nically, we examine (1) OPT’s overhead in terms of per-packet pro-\ncessing by measuring both the throughput (the bandwidth utilized\nby whole packets including the Ethernet header)4 and goodput (the\nbandwidth used to transmit the payload of the packets, excluding\nthe OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that\nvalidates the source and the path of the incoming packets before\nforwarding them. Our comparison is with ICING [26], which we\ndiscuss in more detail in Section 9 since both provide similar se-\ncurity guarantees. We experiment with OPT and ICING to per-\nform source and path validations and we use PacketShader’s I/O\nEngine [15] to send/receive packets to/from the NICs.\nA central aspect of our work is the forwarding speed of a router.\nSince OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Since OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-\nber of sources sending packets or the number of ﬂows transiting\na router. In contrast, we analyze the impact of (1) cryptographic\noperations and (2) memory lookup of cryptographic keys, because\nthe forwarding overhead of path validation protocols that use cryp-\ntography depends on these metrics.\nEvaluation system. Our testbed consists of two routers A and B.\nBoth are equipped with two Intel Ethernet Server Adapter X520-\nT2 NICs, and they both run Ubuntu Linux Kernel version 3.2.0-3.\nSystem A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='System A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM\nand two Xeon L5640 2.26GHz (6 cores) processors. System B\nruns our software router code featuring 16 x 4GB DDR3 RAM and\ntwo Intel Xeon E5-2680 2.70GHz (8 cores) processors. The trafﬁc\ngenerator generates trafﬁc at a rate of 40Gbps, which is processed\non the software router and sent back for measurement.\n7.3\nExperiment setup\nWe ﬁrst describe how packets are forwarded from one router to\nthe other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.\nWe use PacketShader’s I/O Engine [15], a high-speed open source\nimplementation to send/receive packets to/from the NICs. On the\nsending router, I/O Engine takes the packets generated by the user-\nlevel trafﬁc generator and sends them to the NIC. When the packets\narrive at the second router’s NIC, I/O Engine takes the packets from\nthe NIC and delivers them to the user-level application, where they\nare processed according to the protocol (OPT or ICING). The last\nstep is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='step is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at\nthe second router from the moment I/O Engine delivers the packets\nto the user-level application and until the packets are ready to be\nsent back.\nExperiments. We measure the forwarding speed of the software\nrouter for OPT and ICING. Our experiments consider AS-level\npath validation scenarios, where path validation is performed at a\n4We add 20B Ethernet overhead in computing the throughput.\nsingle router (e.g., ingress router) within an AS. Since an AS is ad-\nministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen\ninside an AS in practice. Consequently, we perform tests with a\nmaximum path length of 10 hops (without counting the source).\nThe minimum path length is 2 hops, corresponding to the case of a\nsource, an intermediate hop, and a destination.\nTo measure the forwarding overhead at a router, which includes\nthe memory overhead for storing keys and for retrieving them, we\nconsider a network where each node has α neighbors. The param-\neter α is important only if the router performs work that depends\non the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='on the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to\nverify its OPV ﬁeld, then computes the hash of the payload and\nﬁnally updates the PVF ﬁeld. These operations do not depend on\nthe number of neighbors the router has nor on the path length.\nHowever, in ICING the router has to look up the shared sym-\nmetric keys with each node on the path in a table that contains the\nkeys of all the nodes the router had previously seen on a path. For\na path of maximum length n, these nodes are located within n-hop\ndistance away from the router. Our choice for the parameter α = 3\nand the maximum path length of 10 hops gives (311−1)\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the maximum path length of 10 hops gives (311−1)\n2\n= 9841 as\na maximum key table size, which is within ICING’s maximum key\ntable size of 400,000 [25].\nThe router receives packets at a line rate of 40Gbps. To quantify\nthe throughput and goodput with respect to the overhead of OPT\nand ICING, we perform tests with payload sizes of 20B, 256B,\n576B, 768B, and 1024B. We add to these values the OPT and IC-\nING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond\nto the TCP header size to simulate TCP/ICING and TCP/OPT. We\nnote that TCP/OPT includes the IP header since OPT runs over the\nIP network; whereas TCP/ICING does not include the IP header\nsince ICING is designed to replace IP. Hence, the goodput compu-\ntation favors ICING.\nThe biggest payload size is computed by subtracting from the\nMTU the header size of ICING for the longest tested path (10\nhops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the\nsource identiﬁer and 13B of common ﬁelds, which gives a header\nlength of 457B for a 10-hop path. The computation is 1500B −\n457B = 1043B, which explains our choice of 1024B of the maxi-\nmum payload. In case of OPT, 52B common ﬁelds, 16B per-hop\noverhead and 40B TCP/IP header result in 252B. As a result, the\nmaximum payload size is dictated by the size of the ICING header\nfor the longest path considered.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='maximum payload size is dictated by the size of the ICING header\nfor the longest path considered.\nMethod. We generate trafﬁc for 10 seconds at a rate of 40Gbps,\nwhich is forwarded to the router running the protocol for source\nauthentication and path validation, and then forwarded back to the\nsource. To measure the throughput, we employ I/O Engine’s scripts,\nwhich operate as follows. These scripts read the RX and TX counter\nvalues of the NIC at the beginning of the experiment and then read\nthe values again every second to compute the number of packets\nsent and received. Consequently, we obtain the throughput values\nevery second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='every second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING\nheader, respectively, from the packet size.\n7.4\nForwarding Overhead\nWe evaluate the most computationally intensive protocol version\nof OPT described in Section 5.3. The evaluation results show that\nOPT outperforms ICING by a signiﬁcant margin. Since the other\nversions of OPT feature smaller packet headers and less computa-\n279\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(a) 2-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(b) 4-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(c) 8-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(d) 10-hop path\nFigure 6: Throughput and goodput (i.e., throughput obtained\nonly for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='only for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths\nvarying from 2 to 10 hops.\ntional overhead, they would outperform ICING with an even larger\nmargin.\nFigure 6 depicts the results for throughput and goodput for OPT\nand ICING. We performed experiments for different path lengths\nand packet sizes described in Section 7.3, and the numbers we ob-\ntained show consistent results over all experiments, as explained in\nthe next paragraphs.\nA ﬁrst observation is that throughput registers higher values than\ngoodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='goodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-\nsuring goodput. We also notice that, for all path lengths, OPT’s\nthroughput is close to 40Gbps except for the smallest packet size\n(i.e., 20B). We note that OPT’s throughput for small packets is\nmainly limited by I/O Engine’s throughput as shown in Figure 5.\nAs the path length grows, I/O Engine’s bottleneck becomes re-\nleased because of the reduced number of packet copies between the\nNIC and the user-level packet processing engine5; and as a conse-\nquence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.\nFor each path length, the goodput of both OPT and ICING in-\ncrease as the packet size increases, because the protocol header rep-\nresents a smaller fraction of the total packet size as the payload size\nincreases. Even though ICING’s throughput also increases with\nthe packet size, its value is much smaller than OPT’s throughput.\nGiven the choices for our ICING implementation, as explained ear-\nlier, this result is mainly due to the key table lookup for the PoP\nkeys. Instead, OPT uses AESni operations to derive the keys shared\nwith the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='with the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT\nas much as 10Gbps in comparison to ICING.\n7.5\nPath Length Scalability\nIn order to analyze the protocols’ scalability with respect to the\npath length, we depict in Figure 7 the ratio between the goodput and\nthe throughput (named goodput ratio) for 256B and 1024B packets.\nWe vary the path length from 2 to 10 hops.\nThe goodput ratios of 256B packets are lower those of than 1024B\npackets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='packets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet\nheader, resulting in goodput degradation for both OPT and ICING.\n5The increased header size reduces the number of packets needed\nto saturate the link bandwidth.\n2\n4\n6\n8\n10\n0\n20\n40\n60\n80\n100\nPath Length (Hops)\nGoodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Goodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B\nICING−1024B\nFigure 7: The goodput ratio (i.e., goodput/throughput) of OPT\nand ICING for small and large packets, in the context of path\nlengths varying from 2 to 10 hops.\nThe ﬁgure shows that OPT has better path length scalability than\nICING since the goodput ratio of OPT decreases slower than that of\nICING as the path length increases. Speciﬁcally, when the 2-hop\npath is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)\n64.7·8\n= 3.03%, while IC-\nING’s is (64.5−34.9)\n64.5·8\n= 5.74%; for 1024B packets, OPT’s goodput\ndegradation ratio per hop is (88.1−79.3)\n88.1·8\n= 1.25%, while ICING’s is\n(87.9−68.2)\n87.9·8\n= 2.80%.\n8.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='(87.9−68.2)\n87.9·8\n= 2.80%.\n8.\nDISCUSSION\nKey lifetime. The keys associated with a session σ are valid as long\nas (1) PATHσ between S and D in the session σ does not change,\nand (2) S or D do not terminate the session due to application-driven\nsession lifetime requirement.\nAccording to the ﬁrst point, the maximum key lifetime is deter-\nmined by route stability. Recent end-to-end route stability anal-\nyses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='yses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-\nniques [9, 18, 22]. Although many routes are still short-lived, en-\ntities send packets over long-lived routes (longer than 6 hours) for\n96% of the times [9]. In particular, considering the fact that the\nload balancing within an ISP causes most route variations (i.e., up\nto 82%), OPT running at AS-level uses more stable routes than\nrouter-level OPT. Furthermore, when routers perform per-ﬂow or\nper-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='per-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-\nstrated by numerous incidents such as Man-in-the-Middle BGP route\nhijacking [8]. In this case, the network routes could ﬂap as the\nattacker wishes. Yet, some future Internet architecture proposals\n(Nebula [2], Pathlets [12], SCION [40], XIA [14]) relieve this pain\npoint by having packets carry forwarding information in the packet\nheader, so that the source is always aware of the path and would set\nup a new session if the path changes.\nWe expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the\nearliest at a granularity of tens of minutes. Thus, the key setup\noverhead represents a tiny fraction of the total computation and\ncommunication overhead of a long-lived high-bandwidth connec-\ntion.\nEfﬁcient packet content authentication. As described in Sec-\ntion 5.3, each intermediate router uses the DATAHASH ﬁeld in the\nOPT header when it veriﬁes its OPV ﬁeld. Such a veriﬁcation does\nnot authenticate the packet content since a malicious intermediate\nrouter could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='router could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in\nparallel while the packet is being scheduled for transmission.\nAs an alternative, probabilistic veriﬁcation schemes [16] can be\n280\napplied such that every router decreases the veriﬁcation probabil-\nity if the DATAHASH veriﬁcation succeeds. However, if a router\ndetects a packet with a bogus hash value, the probability to run\nhash veriﬁcation increases. Furthermore, as soon as a router re-\nceives multiple mismatching hash values, it immediately performs\nhash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets\nincoming on interfaces arriving from the malicious router. With\nsuch a probabilistic veriﬁcation approach, we can further improve\nthe efﬁciency and practicality while providing data authentication.\nOPT in the current Internet. OPT could be incrementally de-\nployed in the current Internet. An AS could announce its OPT\nfunctionality within BGP update messages (as a transitive attribute)\nor as extension to RPKI certiﬁcates, enabling the selection and con-\nstruction of end-to-end OPT paths at source ASes. Endhosts could\nobtain OPT path information from a local route server which col-\nlects BGP and RPKI information.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='obtain OPT path information from a local route server which col-\nlects BGP and RPKI information.\nTo carry OPT-based information in packets, the simplest ap-\nproach would be an IPv6 extension header. In IPv4, spare IP header\nbits would need to be used to indicate the presence of an extra\nOPT header or trailer, but an extra header after the IP header may\ndisrupt processing at legacy ﬁrewalls or other middleboxes. With\nthe increasing support for IPv6, we prefer incremental deployment\nvia the IPv6 extension header. Since the DRKey information is\nlarger than the 256 bytes that ﬁt into an IPv6 extension header, we\npropose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='propose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,\nASes could specify addresses of DRKey servers that would handle\nDRKey packets to set up the keys for the routers and thus would\nshare the secret keys KRi of routers. An endhost could then place\na sequence of DRKey server addresses (similar to a loose source\nrouting option in IPv4) into the DRKey packet, which would be\nsequentially processed and forwarded until the destination. This\nlatter approach avoids routers from analyzing the IP protocol ﬁeld.\n9.\nRELATED WORK\nThe most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source\npre-computes a veriﬁer MAC (Vi) for each intermediate router Ri\nusing the respective shared secret key as well as the hash value of\nthe path and the static content in the header. For each packet, Ri ﬁrst\nreconstructs and XORs the MAC for the source and each upstream\nrouter, and veriﬁes if the XORed MACs are equivalent to what is\nstored in Vi. Then Ri (1) computes a MAC for each downstream\nrouter on the path using the shared secret key, the hash of the path,\nand the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).\nICING is more heavy-weight than OPT. ICING requires each\nRi to derive a Difﬁe-Hellman (DH) key with each router R j on\nthe path, which requires routers to cache keys to avoid the heavy-\nweight DH computation during packet forwarding. For the case\nthe keys are not cached any more, ICING suggests adding the 20-\nbyte public key of each router into each packet, resulting in a high\nper-packet overhead. Also, ICING requires each node to insert a\nMAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='MAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require\nrouters to store keys shared with sources or other routers, nor per-\nform a MAC computation for each router on the path. In terms of\nsecurity, even ICING intermediate routers can detect colluding path\ndeviation attacks mounted by the source and a malicious router to\nanother intermediary router. In contrast, Extended-OPT supports\nthe destination to detect such attacks. More speciﬁcally, the ori-\ngin and path validation property of the routers still depends on the\ncorrect behavior of the source, but not on the destination. Conse-\nquently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,\nan illegitimate packet travels longer in Extended-OPT than ICING,\nbut will be rejected by the destination. Beside the malicious source\ncollusion attack, OPT and ICING provide the same kind of source\nand path authenticity properties for the destination. On the other\nhand, retroactive key setup OPT with path tracing (which only en-\nables the destination to verify the path) can mitigate the coward\nattack, which ICING fails to mitigate.\nLiu et al. propose Passport for intermediate routers to perform\nsource authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any\ntwo ASes to compute a shared secret based on Difﬁe-Hellman key\nexchange. Using the respective shared secret key with each down-\nstream AS, the source AS computes a MAC for each AS on the\npath, and inserts it in the Passport header. Each intermediate AS\nauthenticates the source AS by recomputing the MAC using the\nshared key and conﬁrming that it matches the MAC in the Passport\nheader. Similar to Passport, the accountability service by Ben-\nder et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='der et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers\nthrough a key derivation mechanism that is similar to DRKey (al-\nthough it cannot provide retroactive key setup) and uses these keys\nfor embedded source authenticators. Passport and the accountabil-\nity service provide weaker security guarantees than OPT, as they\nprovide only source AS authentication, and fail to defend against\nsource and data spooﬁng, as well as path deviation attacks. While\nSNAPP does prevent against source and data spooﬁng, it does not\nprevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet\nstate [19, 31], perform packet marking [30, 32], or active path in-\nterrogation [27]. Pi suggests a path identiﬁer to detect source IP\naddress spooﬁng [36]. Unfortunately, these schemes are suscepti-\nble to attacks listed in Section 2.3, because they were designed for\na different purpose. Similarly, network capability mechanisms [3,\n29, 37, 38] cannot provide source authentication or path validation\nas the capability can be easily copied and inserted by the last AS.\n10.\nCONCLUSION\nDespite the importance of network-based source authentication', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='10.\nCONCLUSION\nDespite the importance of network-based source authentication\nand path validation, these primitives have not been implemented\nso far, perhaps because of the lack of an efﬁcient protocol that does\nnot burden the router. This paper introduces (1) DRKeys as efﬁcient\nand dynamically recreatable key setup protocols, and (2) OPT as an\nextremely lightweight, scalable, and secure protocol that provides\nsource authentication and path validation. Compared with currently\nproposed solutions, OPT achieves performance improvements with\nminimal latency and computational overhead on routers regardless\nof the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='of the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-\nity. We also introduce a retroactive key setup process that protects\nagainst coward attacks, as routers cannot know in advance which\npaths are being monitored subsequently. We anticipate that OPT’s\nsecurity and performance properties will bring source authentica-\ntion and path validation into the realm of practicality.\n11.\nACKNOWLEDGMENTS\nWe thank George Danezis, Yue-Hsun Lin, Raphael Reischuk,\nmembers of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='members of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281\nWe gratefully acknowledge funding support for this research from\nCyLab at Carnegie Mellon, NSF under award CNS-1040801, Euro-\npean Research Council under the European Union’s Seventh Frame-\nwork Programme (FP7/2007-2013) / ERC grant agreement 617605,\nand a gift from KDDI.\n12.\nREFERENCES\n[1] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen,\nD. Moon, and S. Shenker. Accountable Internet Protocol', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='D. Moon, and S. Shenker. Accountable Internet Protocol\n(AIP). In Proceedings of ACM SIGCOMM, 2008.\n[2] T. Anderson, K. Birman, R. Broberg, M. Caesar, D. Comer,\nC. Cotton, M. Freedman, A. Haeberlen, Z. Ives,\nA. Krishnamurthy, W. Lehr, B. Loo, D. Mazières,\nA. Nicolosi, J. Smith, I. Stoica, R. van Renesse, M. Walﬁsh,\nH. Weatherspoon, and C. Yoo. The nebula future internet\narchitecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='architecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer\nScience, pages 16–26. Springer Berlin Heidelberg, 2013.\n[3] T. Anderson, T. Roscoe, and D. Wetherall. Preventing\nInternet Denial-of-Service with Capabilities. In Proceedings\nof Hotnets-II, 2003.\n[4] ARIN. Resource Public Key Infrastructure (RPKI).\nhttps://www.arin.net/resources/rpki/.\n[5] A. Bender, N. Spring, D. Levin, and B. Bhattacharjee.\nAccountability as a Service. In Proc. of USENIX SRUTI,\n2007.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Accountability as a Service. In Proc. of USENIX SRUTI,\n2007.\n[6] D. J. Bernstein, N. Duif, T. Lange, P. Schwabe, and B.-Y.\nYang. High-speed high-security signatures. In Proc. of\nCHES, 2011.\n[7] J. Cowie. The new threat: Targeted internet trafﬁc\nmisdirection. http://www.renesys.com/2013/11/mitm-\ninternet-hijacking/, Nov. 2013.\n[8] J. Cowie. The New Threat: Targeted Internet Trafﬁc\nMisdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Misdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-\nhijacking/, Nov 2013.\n[9] I. Cunha, R. Teixeira, and C. Diot. Measuring and\ncharacterizing end-to-end route dynamics in the presence of\nload balancing. In Proc. of PAM’11, 2011.\n[10] Y. Gilad and A. Herzberg. Plug-and-Play IP Security:\nAnonymity Infrastructure Instead of PKI. In Proceedings of\nESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In\nProc. of SIGCOMM, 2009.\n[12] P. B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica. Pathlet\nrouting. In Proceedings of the ACM SIGCOMM 2009\nConference on Data Communication, 2009.\n[13] S. Gueron. Intel Advanced Encryption Standard (AES) New\nInstructions Set, Mar. 2010. white paper 323641-001,\nRevision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Revision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,\nA. Mukundan, W. Wu, A. Akella, D. G. Andersen, J. W.\nByers, S. Seshan, and P. Steenkiste. XIA: Efﬁcient support\nfor evolvable internetworking. In Proceedings of USENIX\nConference on Networked Systems Design and\nImplementation, 2012.\n[15] S. Han, K. Jang, K. Park, and S. Moon. PacketShader: a\nGPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='GPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.\n[16] H.-C. Hsiao, A. Studer, C. Chen, A. Perrig, F. Bai, B. Bellur,\nand A. Iyer. Flooding-Resilient Broadcast Authentication for\nVANETs. In Proc. of MobiCom, 2011.\n[17] L. Jia, C. Basescu, T. H.-J. Kim, A. Perrig, Y.-C. Hu, and\nF. Zhang. Mechanized network origin and path authenticity\nproofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='proofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.\n[18] M. S. Kang, S. B. Lee, and V. D. Gligor. The Crossﬁre\nAttack. In Proc. of IEEE Security and Privacy, 2013.\n[19] J. Li, M. Sung, J. Xu, and L. Li. Large-Scale IP Traceback in\nHigh-Speed Internet: Practical Techniques and Theoretical\nFoundation. In Proc. of IEEE Security and Privacy, 2004.\n[20] B. Liu, J. T. Chiang, J. J. Haas, and Y.-C. Hu. Coward\nAttacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Attacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.\n[21] X. Liu, A. Li, X. Yang, and D. Wetherall. Passport: Secure\nand Adoptable Source Authentication. In Proc. of NSDI,\n2008.\n[22] H. V. Madhyastha, E. Katz-Bassett, T. Anderson,\nA. Krishnamurthy, and A. Venkataramani. iPlane Nano: Path\nPrediction for Peer-to-peer Applications. In Proc. of NSDI,\n2009.\n[23] D. Mazieres, M. Kaminsky, M. F. Kaashoek, and E. Witchel.\nSeparating Key Mangement from File System Security. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Separating Key Mangement from File System Security. In\nProceedings of SOSP, 1999.\n[24] R. Moskowitz and P. Nikander. Host Identity Protocol (HIP)\nArchitecture, May 2006.\nhttp://tools.ietf.org/html/rfc4423.\n[25] J. Naous. Path-policy Compliant Networking and a Platform\nfor Heterogeneous IAAS management. In PhD thesis, 2011.\n[26] J. Naous, M. Walﬁsh, A. Nicolosi, D. Mazieres, M. Miller,\nand A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.\n[27] V. N. Padmanabhan and D. R. Simon. Secure Traceroute to\nDetect Faulty or Malicious Routing. ACM SIGCOMM\nComputer Communications Review, January 2003.\n[28] J. Pappalardo. New Transatlantic Cable Built to Shave 5\nMiliseconds off Stock Trades. Popular Mechanics,\nhttp://www.popularmechanics.com/technology/\nengineering/infrastructure/a-transatlantic-\ncable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.\n[29] R. Raghavan and A. C. Snoeren. A system for authenticated\npolicy-compliant routing. In Proc. of ACM SIGCOMM,\n2004.\n[30] S. Savage, D. Wetherall, A. Karlin, and T. Anderson.\nPractical Network Support for IP Traceback. In Proc. of\nSIGCOMM, 2000.\n[31] A. C. Snoeren, C. Partridge, L. A. Galindo, C. E. Jones,\nF. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='F. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP\nTraceback. In Proceedings of ACM SIGCOMM, 2001.\n[32] D. X. Song and A. Perrig. Advanced and Authenticated\nMarking Schemes for IP Traceback. In Proceedings of IEEE\nINFOCOM, 2001.\n[33] I. Stoica, D. Adkins, S. Zhaung, S. Shenker, and S. Surana.\nInternet indirection infrastructure. In Proc. of SIGCOMM,\n2002.\n[34] M. Walﬁsh, J. Stribling, M. Krohn, H. Balakrishnan,\nR. Morris, and S. Shenker. Middleboxes No Longer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R. Morris, and S. Shenker. Middleboxes No Longer\nConsidered Harmful. In Proceedings of OSDI, 2004.\n[35] D. Wendlandt, D. G. Andersen, and A. Perrig. Perspectives:\nImproving SSH-style host authentication with multi-path\nprobing. In Proceedings of USENIX Annual Technical\nConference, June 2008.\n[36] A. Yaar, A. Perrig, and D. Song. Pi: A Path Identiﬁcation\nMechanism to Defend against DDoS Attacks. In Proc. of\nIEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='IEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability\nMechanism to Mitigate DDoS Flooding Attacks. In Proc. of\nthe IEEE Security and Privacy, May 2004.\n[38] X. Yang, D. Wetherall, and T. Anderson. A DoS-limiting\nNetwork Architecture. In Proc. of SIGCOMM, 2005.\n[39] X. Zhang. Secure and Efﬁcient Network Fault Localization.\nPhD thesis, Carnegie Mellon University, 2012.\n[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and\nD. G. Andersen. SCION: Scalability, control, and isolation\non next-generation networks. In Proceedings of the IEEE\nSymposium on Security and Privacy (Oakland), May 2011.\n282', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fa5514790> 111
cuda:2
[Document(page_content='Lightweight Source Authentication and Path Validation\nTiffany Hyun-Jin Kim\nCyLab, CMU\nhyunjin@cmu.edu\nCristina Basescu\nETH Zürich\ncba@inf.eth.ch\nLimin Jia\nCyLab, CMU\nliminjia@cmu.edu\nSoo Bum Lee\nQualcomm\nsoobuml@qti.qualcomm.com\nYih-Chun Hu\nUIUC\nyihchun@uiuc.edu\nAdrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Adrian Perrig\nETH Zürich\nperrig@inf.eth.ch\nABSTRACT\nIn-network source authentication and path validation are funda-\nmental primitives to construct higher-level security mechanisms\nsuch as DDoS mitigation, path compliance, packet attribution, or\nprotection against ﬂow redirection. Unfortunately, currently pro-\nposed solutions either fall short of addressing important security\nconcerns or require a substantial amount of router overhead. In this\npaper, we propose lightweight, scalable, and secure protocols for\nshared key setup, source authentication, and path validation. Our\nprototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prototype implementation demonstrates the efﬁciency and scalabil-\nity of the protocols, especially for software-based implementations.\nCategories and Subject Descriptors\nC.2.0 [Computer-Communication Networks]: Security and pro-\ntection; C.2.1 [Network Architecture and Design]: Circuit-switch-\ning networks, Packet-switching networks\nKeywords\nSource Authentication, Path Validation, Retroactive Key Setup\n1.\nINTRODUCTION\nSource authentication and path validation are useful primitives\nto help mitigate various network-based attacks, such as DDoS, ad-\ndress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='dress spooﬁng, and ﬂow redirection attacks [7]. Path validation,\nin particular, provides a way to enforce path compliance according\nto the policies of ISPs, enterprises, and datacenters. Endhosts and\nISPs desire to validate service level agreement compliance regard-\ning data delivery in the network: Did the packet truly originate from\nthe claimed client? Did the client select a path that complies with\nthe service provider’s policy? Did the packet indeed travel through\nthe path selected by the client?\nUnfortunately, the current Internet provides almost no means for\nsource authentication and path validation by routers or endhosts,\nopening up numerous attack surfaces. For example, a malicious\nISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ISP may forward a packet on an inferior path while claiming to its\nclient that it forwarded the packet on the premium path. Alterna-\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for proﬁt or commercial advantage and that copies bear\nthis notice and the full citation on the ﬁrst page. Copyrights for components\nof this work owned by others than ACM must be honored. Abstracting with\ncredit is permitted. To copy otherwise, or republish, to post on servers or to\nredistribute to lists, requires prior speciﬁc permission and/or a fee. Request\npermissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='permissions from permissions@acm.org.\nSIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.\nCopyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00.\nhttp://dx.doi.org/10.1145/2619239.2626323.\ntively, a malicious router may inject packets with a spoofed source\naddress to incriminate a victim source node into having sent an ex-\ncessive number of packets. A malicious router may simply alter the\ncontents of received packets as well. The inability to detect such\nattacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attacks near the point of deviation wastes downstream resources.\nFurthermore, an adversary may exploit the routing protocol to di-\nvert trafﬁc to traverse a point of eavesdropping it controls—a seri-\nous issue in particular for sensitive information.\nEnd-to-end encryption and authentication mechanisms, such as\nTLS, do not solve any of the above issues, since they are agnos-\ntic to which path the packet takes. A stronger approach is needed,\nwhich enables routers and destinations to perform source authenti-\ncation and path validation. As we discuss in the related work, ex-\nisting solutions either require extensive overhead, or only partially\naddress fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='address fundamental problems, affecting both feasibility and prac-\nticality in the existing network. For example, ICING [26] addresses\nboth source authentication and path validation, but it requires each\nintermediate router on a path to store and look up keys shared with\nother routers; ICING requires 42 bytes per verifying router in the\npacket header. Furthermore, ICING requires each router to calcu-\nlate a Message Authentication Code (MACs) for all other routers\non the path. In contrast, our protocol does not require any per-\nclient state on routers; it requires only 16 bytes per hop (which can\nbe reduced to 2 bytes for a lower level of security), and only a sin-\ngle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='gle MAC and a single PRF computation per router irrespectively of\nthe path length. Moreover, one of our protocol instantiations pre-\nvents against coward attacks [20], where an adversary only attacks\nwhen it knows that the attack will not be detected. Our protocol,\nhowever, offers reduced security in the case of a malicious sender\ncolluding with a malicious router on the path, which we describe\nin detail in the related work section. Since in the common case,\nsender and receiver trust each other, the performance gain of O(1)\nMAC operation per router instead of O(n) is worth the tradeoff.\nContributions. In this paper, we present Dynamically Recreatable\nKey (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Key (DRKey) protocols that enable routers to (re-)create symmetric\nkeys shared with the endhosts on the ﬂy. The stateless operation of\nDRKey on routers prevents state exhaustion DoS attacks and sim-\npliﬁes router architecture. We further enrich DRKey with a new\nnotion called retroactive key setup that provides the following de-\nsirable properties: (1) in contrast to previous protocols, source and\ndestination can start the communication without needing to wait\nfor the expensive key setup to complete, providing efﬁciency; (2)\nif misbehavior is suspected, endhosts set up keys retroactively to\nverify previous packets, defending against coward attacks.\nBased on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Based on the dynamically (re-)creatable keys through DRKey\nprotocols, we present Origin and Path Trace (OPT)—-lightweight,\nscalable, and secure protocols for source authentication and path\nvalidation. We introduce an extension called Retroactive-PathTrace\nthat supports the destination to perform path validation with retroac-\n271\ntive key setup and to detect coward attackers with small, constant\noverhead in the packet header. Our OPT protocols enable imple-\nmentation on SW routers with minimal performance impact.\n2.\nPROBLEM DEFINITION\n2.1\nDesired Security Properties\nSource authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Source authentication and data authentication. The destination\nand each intermediate router should be able to determine whether\nthe packet indeed originated from the claimed source and whether\nthe packet content has not been altered en route. In this paper,\nsource authentication includes data authentication.\nPath validation. The source, intermediate routers, and the desti-\nnation should be able to validate that the packet indeed traversed\nthe path known to (or selected by) the source. Successful path val-\nidation ensures that the packet traversed each honest router on the\npath in the correct order. Unfortunately, no scheme can provide\nany guarantees for malicious routers: if malicious router Rm pub-\nlishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='lishes its secret keys, another malicious router Rm′ could perform\ncryptographic operations on a packet without traversing Rm.\n2.2\nElided Security Properties\nNo packet delivery guarantee. Routers generally have the free-\ndom to decide whether or not to forward packets. Hence, it is not\nthe purpose of path validation to guarantee that packets will be de-\nlivered to the speciﬁed destination.\nNo detection of packet siphoning. Misbehaving router Rm on the\nsource-selected path can siphon packets and send them over a sep-\narate channel to a remote entity. Since Rm still forwards the packet\nto Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to Rm+1, this attack is not detected. We consider Rm as obeying the\nprotocol as long as it performs protocol-compliant operations with\nthe packet.\nNo locating of packet altering and dropping routers. Locating\nrouters that alter or drop packets is the goal of fault-localization\nmechanisms—another challenging problem especially in inter-domain\nsettings [39]. Since path validation is a simpler problem, the goal\nis to achieve a more efﬁcient protocol than heavy-weight fault lo-\ncalization.\n2.3\nAdversary Model\nWe consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a computationally-bounded network attacker that\ndeviates from the protocol and violates its security goals as we de-\nscribe next.\nPacket alteration. A malicious router alters any part of the packet,\nsuch as source address, header information, or payload data.\nPacket injection. A malicious router fabricates a packet and sends\nit towards a destination of its choice. A packet replay attack is a\nspecial case of packet injection.\nPath deviation. A malicious router may perform path deviation\nattacks, which cause packets to be forwarded along a path other\nthan the path previously selected by the source. We subdivide this\nattack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='attack as follows:\nPath detour: Malicious router Rm causes a packet to deviate\nfrom the intended forwarding path, but later the packet returns\nto the correct downstream router Rm+1 to resume traversal of all\nrouters on the intended path.\nRouter skipping: A malicious router redirects the packet and\nskips other router(s) on the path. Thus, some routers on the in-\ntended path does not forward the packet.\nOut-of-order traversal: An adversary causes path deviations\nsuch that routers on the intended path are not traversed in the\nright order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='right order.\nCoward attack. An adversary launches a coward attack [20] only\nwhen the adversary believes that the attack cannot be detected. For\nexample, an attacker diverts trafﬁc only when the protocol is inac-\ntive (e.g., required keys for validation have not been established).\nDenial-of-Service (DoS). As part of DoS attacks, we consider mem-\nory and computation exhaustion attacks on routers performing source\nauthentication and path validation.\nCollusion. Protocol participants may collude to carry out any of\nthe attacks listed above. For example, two or more intermediate\nrouters may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers may collude to claim the use of an expensive path for mon-\netary proﬁt, or the source may collude with an intermediate router\nto spoof authenticators for its downstream routers if the destina-\ntion prefers/trusts skipped routers. Also, both the source and the\ndestination could collude with some intermediate routers to frame\nanother router on the path by not forwarding packets to it.\nIn Section 6, we explore potential attacks against our protocols\nthat violate the desired properties and discuss how OPT defends\nagainst these attacks.\n3.\nOPT DESIGN OVERVIEW\nWe consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We consider a setting in which source S sends a packet to des-\ntination D along a sequence of routers Ri. We refer to S, D, and\nRi’s as tracing entities. At a very high level, the main insights for\nachieving source authentication and path validation without requir-\ning routers to maintain per-source or per-path-length state are as\nfollows: (1) In the packet header, source S includes H(P), which is\nthe hash of the packet payload to help receiving entities identify the\npacket while avoiding expensive hash computation at each router;\n(2) On demand, each router Ri generates key Ki using a symmet-\nric cryptographic operation, and requires only router’s local secret\nSVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='SVRi and a special value called SESSIONID in the packet header\nas inputs. Consequently, generating deterministic keys is stateless\nand faster than storing or retrieving secrets. (3) Each router per-\nforms source authentication using a MAC computed over H(P); (4)\nEach router Ri extends a special authentication ﬁeld called PVF by\nperforming a MAC operation. Hence, path validation is achieved\nthrough a chain consisting of nested MACs.\n3.1\nAssumptions\nFor the communication properties of the network, we assume\nthat the source knows the path that the packet will traverse at the\nAS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS- or router-level granularity to enable path validation, and that\nthe source knows which entities in that path desire to perform the\nvalidations. This information can stem from (1) the BGP proto-\ncol where the source can learn the AS path that the packet is ex-\npected to traverse, (2) Pathlet routing [11] or SCION [40] where\nthe source can specify the path in the packet header, or (3) i3 [33]\nor Platypus [29] where the source can deﬁne a sequence of servers\nto traverse. Alternatively, an ISP may provide the premium path\ninformation to clients as an extra service (e.g., transatlantic cable\nfor ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='for ﬁnancial transactions [28]), in which case a client can validate\nthat the traversed path is indeed the premium path paid for.\nFor the cryptographic key setup, the source and the destination\nneed to be able to authenticate the router’s cryptographic materi-\nals (i.e., validate a signature that binds an entity to some crypto-\ngraphic materials). In the case of AS-level tracing, the AS needs to\nbe authenticated, and such authentication can be achieved through\nRPKI [4], which is already operational. RPKI provides a PKI that\nenables authentication of AS certiﬁcates, each of which binds an\nAS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='AS number to its public key, given the correct RPKI public root\nkey. In case of router-level tracing, we assume that each AS in turn\ncreates certiﬁcates for each router using the AS’s private key—\nenabling the tracing entity to verify via the AS certiﬁcate using\nRPKI.1\n1Alternatively, OPT can authenticate entities based on mechanisms\n272\nTable 1: Notation.\n(PKE,PK−1\nE )\nEntity E’s public-private key pair\nCertPKE\nEntity E’s public-key Certiﬁcate\nˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ˆKSD\nLong-term symmetric key between S and D, which is valid\nover multiple sessions\nKE\nSymmetric key among S, D, and entity E for a single session\nKE1E2\nSymmetric key between entities E1 and E2 for a single session\nKE1E2σ\nSymmetric key for E1 and E2 in session σ for E1-initiated\npackets\nSVE\nEntity E’s local secret value\nP\nNetwork packet payload\n(PKσ,PK−1\nσ )\nPublic-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Public-private key pair of session σ\nPATHσ\nSession σ’s path information\nTσ\nTime when S initiates session σ\nSESSIONID\nHash of session σ’s public key, path, session initiation time\nAUTHσ\nAuthenticated and encrypted SESSIONID and private key for\nsession σ\nSignKEYPK−1\nE ,σ\nSignature on a symmetric key for session σ using entity E’s\nprivate key\nEncKEYK,σ\nEncryption of a symmetric key for session σ using key K\nKEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KEYSσ\nSet of shared keys between routers and S for session σ\nDATAHASH\nHash of the packet’s payload\nPVF\nField enabling D to verify the path\nPVFS\nField enabling Ri and D to verify the path\nPVFD\nField enabling D to conﬁrm the actual path\nOVi\nField enabling Ri to validate the packet sender\nOPVi\nField enabling Ri to verify both the packet sender and path\nSignPK−1\nE (·)\nSignature using entity E’s private key\nCheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='CheckSigPK−1\nE (·)\nSignature veriﬁcation using entity E’s private key\nEncK(·), DecK(·)\nEncryption, decryption using key K\nAuthEncK(·)\nAuthenticated encryption using key K\nAuthDecK(·)\nAuthenticated decryption using key K\nFK(·)\nPseudo-random function using key K\nMACK(·)\nMessage Authentication Code using key K\nH(·)\nCryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Cryptographic hash operation\nWe also assume that the source and destination entities that per-\nform the tracing of the intermediate routers can establish a secret\nkey between each other. In case the tracing entities are AS infras-\ntructure hosts such as edge routers, ﬁrewalls, or a middlebox at a\nservice provider, either RPKI can be used as described above or an\nadministrator can set up trusted public keys between entities that\nneed path veriﬁcation. If endhosts perform tracing, then a shared\nkey can be set up through SSL or TLS if one of the endhosts is a\nHTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='HTTPS server, through IPsec or SSH. The public keys can be ver-\niﬁed through a regular PKI, administrator-based set up of trusted\nkeys, TOFU (Trust On First Use) in SSH, TOFU with Perspec-\ntives [35], RPKI with domain-certiﬁed host keys, self-certifying\nIDs as public keys [1,23,24,34], or self-validation using an anony-\nmous service [10]. We assume that one of these approaches is used\nto set up symmetric key ˆKSD between source S and destination D.\n3.2\nMain Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Main Insights\nOne of OPT’s crucial insights is our approach to avoid storing\nper-ﬂow state on routers; unlike prior approaches that require each\nrouter to maintain a secret key for each ﬂow, our design enables\nrouters to derive the secret keys on the ﬂy using only local se-\ncrets stored at the routers and an efﬁcient pseudo-random function.\nThus, we avoid storing all the keys.\nMore precisely, OPT runs in sessions. In each session σ, source\nS sends packets to destination D on path PATHσ. S and D leverage\nlong-term symmetric key ˆKSD to set up keys with each router in\nPATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PATHσ. DRKey, the details of which is explained in Section 4, is\ninstrumental in achieving OPT’s efﬁciency properties. In particular,\nthe source prepares and inserts a special ﬁeld in the packet header\ncalled SESSIONID such that intermediate routers Ri on PATHσ dy-\nnamically compute the shared symmetric key with S and D (Ri only\nneeds to look up its local secret SVRi for computation).\nthat use self-certifying IDs as public keys [1,23,24,34] as assumed\nin ICING. However, such mechanisms have issues with key revo-\ncations. Hence, we prefer to use RPKI.\nOur key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Our key establishment mechanism does not require any per-ﬂow\nstate on the routers.\nConsequently, OPT is robust against DoS\nattacks based on state exhaustion. Moreover, computing pseudo-\nrandom function (PRF) F is faster than performing a cache access;\nfor instance, a key derivation using AESni takes 32 cycles, whereas\na L3 cache read operation requires approximately 40 cycles (on In-\ntel “Sandy-Bridge”-based Xeon architecture).\nOPT includes the hash of the packet payload H(P) in the header,\nwhich enables an important optimization: routers can either par-\nallelize the computations of MAC and the hash of the packet, or\nprobabilistically validate H(P).\n3.3\nOPT Protocol Overview', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='probabilistically validate H(P).\n3.3\nOPT Protocol Overview\nDRKey for path selection and key setup. When source S initiates\nsession σ at time Tσ, S selects path PATHσ to destination D, gener-\nates asymmetric public/private key pair (PKσ,PK−1\nσ ), and creates a\nsession identiﬁer, where SESSIONID = H(PKσ∥PATHσ∥Tσ). Af-\nter preparing some values that support source authentication and\npath validation for other entities on PATHσ, S forwards the OPT\npacket to its downstream router on PATHσ. If Tσ is recent (i.e.,\nwithin some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='within some predeﬁned interval by the AS), each intermediate router\nRi sets up shared symmetric key Ki using its local secret SVRi and\nSESSIONID. Detailed DRKey protocols are explained in Section 4.\nGeneration of veriﬁcation ﬁelds. S uses the path information to\npre-compute veriﬁcation ﬁelds, one for each router Ri on PATHσ,\nand a special ﬁeld called PVF such that routers can perform source\nauthentication and path validation.\nVeriﬁcation and update by intermediate routers. Upon receiv-\ning a packet, Ri ﬁrst regenerates the shared symmetric key Ki and\nrecomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='recomputes the veriﬁcation ﬁeld based on PVF. When the com-\nputed value matches what is present in the packet header for Ri, it\nsuccessfully authenticates the source and the content of the packet,\nand validates the traversed path. Ri then updates PVF, by applying\na MAC operation using Ki to the ﬁeld. This process helps down-\nstream routers and the destination to validate that each router on the\npath has indeed seen the packet.\nVeriﬁcation by destination. The destination ﬁnally recomputes\nthe veriﬁcation ﬁelds using all the symmetric keys shared with\nother entities on the path. Successful veriﬁcation indicates source\nand packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and packet content authentication as well as path validation.\n4.\nDYNAMICALLY RECREATABLE KEYS\nThis section introduces the DRKey protocols that enable routers\nto set up shared keys with source S and destination D. Section 4.1\ndescribes the case when both S and D trust each other. Section 4.2\nrelaxes this assumption and describes the case when S and D do not\ntrust each other. Section 4.3 describes how S and D retroactively set\nup shared keys with intermediate routers to enable path validation\nof prior packets.\n4.1\nDRKey for Benign Source and Destination\nWhen both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='When both S and D trust each other, each entity on the source-\nselected path needs to pre-establish only one symmetric key that is\nshared with both S and D. Figure 1 shows the key setup steps and\nthe associated cryptographic operations.\nS creates a fresh public/private key pair (PKσ,PK−1\nσ ) for each\nsession such that routers encrypt session symmetric key Ki’s. Since\nS and D trust each other, they share private key PK−1\nσ , the en-\ncrypted and authenticated value of which is sent to D. The public\nkey is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='key is used to derive the SESSIONID as follows: SESSIONID =\nH(PKσ∥PATHσ∥Tσ), where PATHσ is the source-selected path\nand Tσ is when S initiates session σ. Note that Tσ prevents re-\nplay attacks since routers can drop expired packets based on loose\ntime synchronization.\n273\nInitialization by Source S\n0.\nAssume long-term symmetric key ˆKSD shared with D\n(Optional) Assume public/private key pair (PKS,PK−1\nS ), and CertPKS\n1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1.\nInitiate new session σ and pick random session key (PKσ,PK−1\nσ )\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONID)\n5.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nCompute AUTHσ = AuthEncKSDσ (SESSIONID∥PK−1\nσ )\nS → R1\n7.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID) }\nPairwise Key Derivation by R1\n8.\nCompute K1 = FSVR1 (SESSIONID)\n9.\nEncrypt K1: EncKEYR1,σ = EncPKσ (K1)\nSign: SignKEYR1,σ = SignPK−1\nR1 (K1∥PKσ)\nR1 → R2\n10.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R1 (K1∥PKσ)\nR1 → R2\n10.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPK−1\nS (SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ }\nPairwise Key Derivation by R2\n11.\nComputes K2 = FSVR2 (SESSIONID)\n12.\nEncrypt K2: EncKEYR2,σ = EncPKσ (K2)\nSign: SignKEYR2,σ = SignPK−1\nR2 (K2∥PKσ)\nR2 → D\n13.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (K2∥PKσ)\nR2 → D\n13.\nForward {PKσ,PATHσ,Tσ,AUTHσ, (optional) SignPKS(SESSIONID),\nEncKEYR1,σ, SignKEYR1,σ, EncKEYR2,σ, SignKEYR2,σ}\nKey Retrieval by Destination D\n14.\nAlready has ˆKSD, which is the long-term shared symmetric key with S\n15.\nCheck that D is the last entity on PATHσ\n16.\nCompute SESSIONID = H(PKσ∥PATHσ∥Tσ) and check the integrity\n17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='17.\nCompute KSDσ = FˆKSD(S∥D∥SESSIONID)\nKDSσ = FˆKSD(D∥S∥SESSIONID)\n18.\nDecrypt AUTHσ and authenticate PK−1\nσ : AuthDecKSDσ (AUTHσ)\n19.\nDecrypt K1 and K2 and check their signatures:\nDecPK−1\nσ (EncKEYR1,σ),CheckSigPKR1 (SignKEYR1,σ)\nDecPK−1\nσ (EncKEYR2,σ),CheckSigPKR2 (SignKEYR2,σ)\nK1 and K2 become shared symmetric keys between each router and D\n20.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='K1 and K2 become shared symmetric keys between each router and D\n20.\nCompute KD = FSVD(SESSIONID)\nD → S\n21.\nForward authenticated and encrypted shared keys:\nKEYSσ = AuthEncKDSσ (K1∥K2∥KD∥AUTHσ)\nKey Retrieval by Source S\n22.\nDecrypt and authenticate the keys received from D: AuthDecKDSσ (KEYSσ)\nK1, K2 and KD become shared keys between S and R1, R2, and D\nFigure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 1: A session key setup for path S → R1 → R2 → D.\nEach intermediate router Ri generates shared key Ki using an ef-\nﬁcient PRF keyed with a secret SVi only known to Ri. The PRF\ntakes SESSIONID as an input. For high efﬁciency, we compute\nour PRF from a pseudo-random permutation using AES. The over-\nhead of the key setup is negligible to affect the on-going trafﬁc (see\nSection 8). Resulting key Ki is encrypted with public key PKσ,\nand digitally signed to enable veriﬁcation that (encrypted) Ki in-\ndeed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='deed originates from the correct router (as any entity could have\nperformed the public-key encryption on an arbitrary key).\nTo prevent reﬂection attacks (i.e., replaying message in the op-\nposite order of communication), communication between S and D\nuses different symmetric keys for each direction: KSDσ and KDSσ\nfor S-initiated and D-initiated packets, respectively.\nThe optional operations in Figure 1 are used only if the router\nalso needs to authenticate S, in which case S also signs the SESSIONID,\nand certiﬁcates needed for routers to verify S’s public key are in-\ncluded in the message.\n4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4.2\nExtended-DRKey for Distrusting Source\nand Destination\nWhen we assume that the source S and the destination D trust\neach other and that they share the same public-private key pair for\nthe session, then each intermediate router needs to set up only one\nshared key with both S and D. However, S and D may not neces-\nPath Agreement and Key Setup Initialization by Source S\n1.\nInitiate new session σand pick random session key (PKSσ,PK−1\nSσ )\nRi uses this key to authenticate S’s packets\n2.\nObtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Obtain PATHσ = ⟨S,R1,R2,D⟩\n3.\nMeasure current time Tσ\n4.\nCompute SESSIONIDS = H(PKSσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nS (SESSIONIDS)\nS → D\n5.\nForward {PKSσ,PATHσ,Tσ,SignPK−1\nS (PKSσ∥PATHσ∥Tσ)}\nPath Agreement and Key Setup Initialization by Destination D\n6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='6.\nPick random session key (PKDσ, PK−1\nDσ) that Ri uses with D\n7.\nCompute SESSIONIDD = H(PKDσ∥PATHσ∥Tσ)\n(Optional) Authenticate session: SignPK−1\nD (SESSIONIDD)\nD → S\n8.\nForward {PKDσ,PATHσ,Tσ,SignPK−1\nD (PKSσ∥PKDσ∥PATHσ),\nSESSIONIDD, (optional) SignPK−1\nD (SESSIONIDD) }\nInitialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Initialization by S\nS → R1\n9.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD) }\nPairwise Key Derivation by R1\n10.\nCompute KS1 = FSVR1S(SESSIONIDS))\nKD1 = FSVR1D(SESSIONIDD))\n11.\nEncrypt KS1: EncKEYR1S,σ = EncPKSσ (KS1)\nKS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='KS2: EncKEYR1D,σ = EncPKDσ (KD1)\n12.\nSign: SignKEYR1S,σ = SignPK−1\nR1 (KS1∥PKSσ∥S)\nSignKEYR1D,σ = SignPK−1\nR1 (KD1∥PKDσ∥D)\nR1 → R2\n13.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ}\nPairwise Key Derivation by R2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Pairwise Key Derivation by R2\n14.\nCompute KS2 = FSVR2S(SESSIONIDS))\nKD2 = FSVR2D(SESSIONIDD))\n15.\nEncrypt KS2: EncKEYR2S,σ = EncPKSσ (KS2)\nKS2: EncKEYR2D,σ = EncPKDσ (KD2)\n16.\nSign: SignKEYR2S,σ = SignPK−1\nR2 (KS2∥PKSσ∥S)\nSignKEYR2D,σ = SignPK−1\nR2 (KD2∥PKDσ∥D)\nR2 → D\n17.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R2 (KD2∥PKDσ∥D)\nR2 → D\n17.\nForward {PKSσ,PKDσ,PATHσ,Tσ,\n(optional) SignPK−1\nS (SESSIONIDS),SignPK−1\nD (SESSIONIDD),\nEncKEYR1S,σ,SignKEYR1S,σ,EncKEYR1D,σ,SignKEYR1D,σ,\nEncKEYR2S,σ,SignKEYR2S,σ,EncKEYR2D,σ,SignKEYR2D,σ}\nKey Retrieval by D\n18.\nCheck that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Check that D is the last entry in PATHσ\n19.\nDecrypt KD1 and KD2 and check their signatures\nDecPK−1\nDσ (EncKEYR1D,σ),CheckSigPKR1 (SignKEYR1S,σ)\nDecPK−1\nDσ (EncKEYR2D,σ),CheckSigPKR2 (SignKEYR2S,σ)\nKD1 and KD2 become shared symmetric key between each router and D\n20.\nCompute KD = FSVD(SESSIONIDS))\n21.\nEncrypt KD: EncKEYD,σ = EncPKSσ (KD)\n22.\nSign: SignKEYD,σ = SignPK−1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='22.\nSign: SignKEYD,σ = SignPK−1\nD (KD∥PKSσ∥S)\nD → S\n23.\nForward {EncKEYR1S,σ,SignKEYR1S,σ,EncKEYR2S,σ,SignKEYR2S,σ,\nEncKEYD,σ,SignKEYD,σ}\nKey Retrieval by Source S\n24.\nDecrypt and authenticate keys received from D\nKS1, KS2 and KD become shared keys between S and R1, R2, and D.\nFigure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Figure 2: A modiﬁed session key setup when S and D do not\ntrust each other. The path is: S → R1 → R2 → D.\nsarily trust each other, or they may collude. To strengthen the secu-\nrity guarantees under such circumstances, we introduce Extended-\nDRKey , which requires each intermediate router to set up two keys,\nKSi and KDi, where KSi is the shared symmetric key between S and\nRi and KDi is the shared symmetric key between Ri and D. Figure 2\ndescribes the Extended-DRKey protocol and its cryptographic op-\nerations.\nUnlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Unlike DRKey in Section 4.1, Extended-DRKey does not require\nAUTHσ since Ri shares different keys with S and D. For creating\nshared keys (i.e., KSi and KDi), Ri and D use distinct local secrets\nto encode the directionality of the keys.\n274\n4.3\nRetroactive-DRKey\nThe key setup protocols as presented in Figures 1 and 2 run once\nbefore the session starts. However, the key setup process incurs\nthe following extra latency and computational overhead: (1) Key\nsetup itself requires an extra round trip between the source and the\ndestination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='destination; and (2) Key setup requires each intermediate router to\nencrypt and sign its shared key. Furthermore, running an indepen-\ndent key setup protocol a priori allows routers to launch a coward\nattack, since the key setup protocol warns possibly misbehaving\nrouters to start behaving correctly and to avoid detection. Conse-\nquently, achieving path validation without the apparent key setup\nprocess is desirable.\nWe introduce Retroactive-DRKey that enables entities to set up\nshared keys at any time after the ﬁrst packet in a session reaches\nthe destination. Note that Retroactive-OPT is invoked only if the\nsource or the destination wishes to perform source authentication\nor path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='or path validation (i.e., there could be sessions during which the\nsource or the destination performs no retroactive key setup).\nTo support such a feature, we still assume that source S and des-\ntination D establish a shared symmetric key ˆKSD in advance, and\nS derives a session key pair (PKσ,PK−1\nσ ) before the session starts.\nUnlike DRKey or Extended-DRKey, Retroactive-DRKey utilizes\nthat S creates KD—a shared symmetric key with D for the ses-\nsion (i.e. KD = FSVS(SESSIONID))—and includes encrypted and\nauthenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='authenticated KDin AUTHσ in the data packets of the forwarding\nprotocol header. In this way, S can already use KD to compute some\nﬁelds such that D can check. When a forwarding protocol is used\nwith Retroactive-DRKey, the routers use some keys for OPT during\na session, and only reveal them at a later time (Section 5.2.1).\nRetroactive-DRKey is very similar to the key setup protocol in\nFigure 1. The only difference is that D does not derive KD, be-\ncause it is already included in each forwarded packet. Retroactive-\nDRKey runs at most once during or after a session ends. We ob-\nserve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='serve that S can set Tσ to a future time when S may want to trigger\nsource and path validation if S of D detect an anomaly.\n5.\nOPT PROTOCOL DESCRIPTION\nThe DRKey protocols described in Section 4 and the techniques\nwe introduce in this section span a protocol family of source au-\nthentication and path validation with varying assumptions and prop-\nerties. Unfortunately, exploring the entire design space is out of\nscope for this paper, and we will present several protocol instantia-\ntions: (1) OriginValidation for source authentication (S and D trust\neach other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each other), (2) PathTrace for path validation (S and D trust each\nother), and (3) Origin and Path Trace (OPT) for source authentica-\ntion and path validation (S and D may not trust each other).\n5.1\nOriginValidation for Source Authentication\nOriginValidation enables each intermediate router and the des-\ntination to perform source authentication using MACs computed\nover the hash of the packet. For efﬁcient authentication, the source\nincludes the following ﬁelds in the packet header:\nDATAHASH: Hash of the packet’s payload H(P);\nSESSIONID: Hash of the session public key, path, and session\ninitiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='initiation time H(PKσ∥PATHσ∥Tσ);\nOVi: Origin Veriﬁcation ﬁeld.\nOVi is a Message Authenti-\ncation Code computed over DATAHASH using key Ki that Ri\nshares with S (i.e., OVi = MACKi(H(P))). Similarly, OVD =\nMACKD(H(P)). The source creates an OV ﬁeld for each inter-\nmediate router and the destination.\nOriginValidation provides efﬁcient MAC veriﬁcation using the\nDATAHASH ﬁeld without requiring each intermediate router to com-\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nPVF (128 bits)\nOV1 (128 bits)\nOV2 (128 bits)\nOVD (128 bits)\nIP Header\nOriginValidation/PathTrace Header\nTCP Header\nFigure 3: The packet header format for OriginValidation and\nPathTrace. DATAHASH, SESSIONID, and OVs help intermedi-\nate routers and the destination authenticate the source (Orig-\ninValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='inValidation), and DATAHASH, SESSIONID, and PVF help the\ndestination validate the path (PathTrace).\npute the hash over the entire packet. Figure 3 represents the packet\nheader, and only DATAHASH, SESSIONID and OV ﬁelds are needed\nfor OriginValidation.\nWhen intermediate router R1 receives a packet from the source\nS, R1 computes the symmetric key (K1) it shares with S using R1’s\nlocal secret and SESSIONID from the packet header. Then R1 gen-\nerates a MAC as follows: OV′\n1 = MACK1(DATAHASH). If OV′\n1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='1\nis the same as OV1 in the packet header, R1 is assured that the\npacket indeed originated from the claimed source S, and forwards\nthe packet to R2. R2 and D perform the same operations as R1.\nAlthough we present the protocol with OV ﬁelds of size 128,\nthe size can be altered to reﬂect the desired level of security. In\ngeneral, assuming a secure MAC function, the success probability\nof a forged n-bit MAC is 2−n, which already results in a low rate at\nn = 16. Thus, for many applications, 2 byte long OV ﬁelds sufﬁce,\nas a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='as a router would only let 1 in 65536 packets pass on average.\n5.2\nPathTrace\nPathTrace is to help the source and destination validate that a re-\nceived packet traversed the source-selected path. This main objec-\ntive is achieved by Path Validation Field (PVF), which is a nested\nMAC that intermediate routers update in the packet header as they\nforward the packet. In Figure 3, only DATAHASH, SESSIONID,\nand PVF ﬁelds are used for PathTrace, thus, the packet overhead\nis irrespective of the path length. Next we describe how PathTrace\nsupports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='supports the source and the destination to validate the path.\nPathTrace for destination. To enable only the destination to val-\nidate the path, the source generates PVF0—the initial PVF value\nwhich is a MAC of DATAHASH using the shared symmetric key\nbetween the source and the destination. Then the source initializes\nthe PVF ﬁeld in the header with PVF0:\nPVF ← PVF0 = MACKD(DATAHASH).\n(1)\nAny intermediate router Ri on the path generates PVFi and updates\nthe PVF ﬁeld in the header as follows:\nPVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='PVF ← PVFi = MACKi(DATAHASH∥PVFi−1).\n(2)\nThe shared symmetric key Ki is shared with both the source and\nthe destination according to the key setup protocol in Section 4.1.\nHence, upon receiving a packet, the destination ﬁrst re-creates the\nnested MACs (here shown for a path of 2 routers):\nPVF′ = MACK2(DATAHASH∥\nMACK1(DATAHASH∥MACKD(DATAHASH))).\n(3)\nIf PVF′ is the same as PVF in the packet header, the destination is\nassured that the packet was indeed delivered on the source-selected\npath. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path. Otherwise, the destination drops the packet.\nPathTrace for source.\nTo help the source authenticate that its\npacket is delivered to the intended destination using the source-\nselected path, the destination forwards the PVF from the received\n275\n.\n.\n.\nDATAHASH (128 bits)\nSESSIONID (128 bits)\nTIMESTAMP (32 bits)\nPVF (128 bits)\nOPV1 (128 bits)\nOPV2 (128 bits)\nOPVD (128 bits)\nIP Header\nOPT Header\nTCP Header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='OPVD (128 bits)\nIP Header\nOPT Header\nTCP Header\nFigure 4: OPT header. The source S initializes all the ﬁelds.\nIntermediate routers only update the PVF ﬁeld.\npacket header back to the source as follows:\nD → S : EncKD(PVF∥DATAHASH).\n(4)\nUpon receiving this information, the source ﬁrst decrypts the\nmessage using KD and then performs the validation by re-constructing\nthe nested MACs using DATAHASH as shown in Eq. (3) and com-\nparing it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='paring it with PVF. A successful validation indicates that the packet\nwas indeed delivered on the source-selected path.\n5.2.1\nRetroactive-PathTrace\nRetroactive-PathTrace supports path validation without the ap-\nparent key setup process in advance. Instead, it utilizes Retroactive-\nDRKey that runs after the session ends. Unlike PathTrace, in Retro-\nactive-PathTrace the source cannot pre-compute the OPVi ﬁelds;\nhence no OPV ﬁelds can be used in the packet header. Instead,\nunder the assumption that the source and the destination trust each\nother, Retroactive-PathTrace requires that the source creates KD—\nthe session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the session-speciﬁc shared key with the destination, and the source\nadditionally includes KD and its authenticated encryption in the\npacket header.\nThe source uses KD to compute PVF0 and the\nrouters derive their shared key and update the PVF ﬁeld accord-\ningly.\nRetroactive-PathTrace requires the destination to store per-packet\ninformation for later checking. However, the beneﬁt of defending\nagainst coward attacks overcomes such a disadvantage. Namely,\nthe destination stores for each packet the tuple (SESSIONID, DATA-\nHASH,PVF). When the destination wants to validate the path, it re-\nquests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quests the source to initiate Retroactive-DRKey such that interme-\ndiate routers reveal the key that was used for the received packets.\nThen the destination can check the PVF ﬁelds and detect coward\nattacks. The source can independently initiate the retroactive pro-\ncess as well.\n5.3\nOPT: Origin and Path Trace\nIn this section, we introduce OPT that combines OriginValida-\ntion and PathTrace such that all entities (including intermediate\nrouters) on the path can perform both source authentication and\npath validation when they trust the source. We assume that all the\nrouters in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='routers in a session are loosely time synchronized (within a few\nmilliseconds, e.g., using NTP).2\nFigure 4 illustrates the OPT header format. In addition to DATA-\nHASH, SESSIONID, and PVF, an OPT header includes the follow-\ning additional ﬁelds to enable each intermediate router to perform\npath validation.\nTIMESTAMP: Time when S creates the OPT packet to mitigate\ntiming-based attacks, such as replay attacks.\nOPVi: Origin and Path Veriﬁcation ﬁeld. OPVi is a MAC that\nenables all entities on the path to perform path validation.\n2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2http://www.ntp.org/\nAlgorithm 1 OPT header initialization and validation pseudo code.\nAn arrow represents the header ﬁeld initialization.\n1: function SOURCE INITIALIZATION\nRequire: Ki and KD that Ri’s and D share with S, respectively after running\nkey setup protocol in Figure 1\n2:\nDATAHASH ← H(P)\n3:\nSESSIONID ← H(PKσ∥PATHσ∥Tσ)\n4:\nPVF ← PVF0 = MACKD(DATAHASH)\n5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='5:\nl = source-selected path length\n6:\nfor each intermediate router Ri, where 1 ≤ i < l do\n7:\nPVFi = MACKi(PVFi−1)\n8:\nOPVi ← MACKi(PVFi−1∥DATAHASH∥Ri−1∥TIMESTAMP)\n9:\nend for\n10:\nfor destination D do\n11:\nOPVD ← MACKD(PVFl−1∥DATAHASH∥Rl−1∥TIMESTAMP)\n12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='12:\nend for\n13:\nTIMESTAMP ← current time\n14: end function\n15: function VALIDATION AND UPDATE BY Ri\n16:\n(Note PVF in OPT header = PVFi−1)\n17:\nCompute OPV′\ni = MACKi(PVFi−1∥DATAHASH∥ Ri−1∥TIMESTAMP)\n18:\nif OPV′\ni == OPVi then\n19:\nPVF ← PVFi = MACKi(PVFi−1)\n20:\nForward the packet to Ri+1\n21:', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='20:\nForward the packet to Ri+1\n21:\nelse\n22:\nDrop the packet\n23:\nend if\n24: end function\n25: function DESTINATION VALIDATION\n26:\n(Note PVF in OPT header = PVFl−1)\n27:\nl = source-selected path length\n28:\nCompute PVF′ = MACKl−1(...(MACK1(MACKD(DATAHASH))))\n29:\nCompute OPV′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='29:\nCompute OPV′\nD = MACKD(PVFl−1∥DATAHASH∥ Rl−1∥TIMESTAMP)\n30:\nif (PVF′ == PVF) && (OPV′\nD == OPVD) then\n31:\nValidation succeeds\n32:\nPrepare packet using Eq. (4) and forward to source\n33:\nelse\n34:\nDrop the packet\n35:\nend if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='end if\n36: end function\nThe SOURCE INITIALIZATION pseudo code in Algorithm 1 de-\nscribes how the source initializes the OPT header ﬁelds. Each OPV\nﬁeld includes the following as inputs.\nPrevious PVF: Including PVFi−1 in the OPVi computation sup-\nports the detection of a malicious intermediate router that forwards\nthe packet to a benign router, which is not speciﬁed by the source\nbut follows the protocol.\nPrevious router address: PVF by itself cannot support entities to\ndetect the packet injection attack. Hence, we include the address of\nthe previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the previous router from which each entity receives the packet.\nTIMESTAMP: This ﬁeld mitigates authenticator cloning attacks.\nConsider an example where packet Pcrt is expected to be sent along\nthe source-selected path PATHcrt, the source previously sent packet\nPold on PATHold, and Pcrt and Pold have the same payload. Con-\nsider router Rbad that is in both PATHcrt and PATHold such that\nPATHcrt = {R1,R2,...,Rbad,Rbad+1,...,Rn} and PATHold = {R′\n1,\nR′\n2,...,Rbad,R′\nbad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='bad+1, ...,Rm}. In this scenario, Rbad can replace\n{Rbad+1,...,Rn} with {R′\nbad+1,...,Rm} in PATHcrt and all the cor-\nresponding ﬁelds in the Pcrt header with those in Pold. Therefore,\nwithout TIMESTAMP, the destination cannot detect the misbehavior\nand ends up validating path {R1,R2,..., Rbad, R′\nbad+1,...,Rm} for\nPcrt. By setting the TIMESTAMP ﬁeld when the source sends out a\npacket, authenticator cloning attacks are mitigated with loose time\nsynchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='synchronization between the source and other entities on the path.\nVALIDATION AND UPDATE BY Ri and DESTINATION VALIDA-\nTION functions in Algorithm 1 describe the OPT procedure that\nintermediate router Ri and the destination performs, respectively.\n276\n5.3.1\nDistrusting source and destination\nThe previous protocols assumes that the source and the destina-\ntion are honest and trust each other. We now relax this assumption\nand present an extension that handles distrusting entities. In OPT,\nthe source can generate all PVFs by itself since it knows all Ki’s.\nConsequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Consequently, a malicious source can collude with an intermedi-\nate router (e.g., R2) and forward the packet on a path S → R2 → D\nwithout going through R1 in Figure 1.\nTo prevent such an attack and address the problem of a distrust-\ning source and destination, we use the key setup protocol in Sec-\ntion 4.2 such that intermediate routers generate two separate shared\nkeys for the source and the destination. Unlike OPT, the Extended-\nOPT header requires two PVF ﬁelds: PVFS that enables interme-\ndiate routers and the destination to validate the source, and PVFD\nthat enables the destination to conﬁrm the actual path, even if the\nsource is malicious and colludes with (at least) one intermediate\nrouter.\n6.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source is malicious and colludes with (at least) one intermediate\nrouter.\n6.\nSECURITY ANALYSIS\nWe prove that OPT has origin authenticity and path validation\nproperties when both the source and the destination are trusted.\nThis property holds on any network conﬁguration, including ones\nthat have malicious routers. Extended-OPT offers stronger prop-\nerties: the router’s origin and path validation property assumes\nthat only the source is honest; and the destination’s path validation\nproperty does not assume the source is honest.\nWe describe how OPT and its variants defend against the adver-\nsary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='sary model described in Section 2.3. OPT’s security properties have\nbeen formally veriﬁed using the Coq interactive theorem prover;\ndetails can be found in our technical report [17].\nPacket alteration. Without the secret keys (KD and Ki), a mali-\ncious router cannot compute valid PVFi and OPVi. Consequently,\nin OPT, a successful veriﬁcation of PVFi−1 (PVFn) based on OPVi\n(OPVD) implies that there can be no packet alteration attacks to\nrouter Ri (the destination), provided that the source and destination\nare trusted. A malicious destination can carry out the packet al-\nteration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='teration attack, as it can generate all the necessary PVF and OPV\nﬁelds. Extended-OPT provides similar protections for intermediary\nrouters except that only the source needs to be trusted.\nPacket injection attack. OPT routers can check that an incoming\npacket does come from an intended AS, as such information is in-\ncluded in OPV. Therefore, a malicious router A can only inject\npacket to a router B if A is B’s neighbor and the link AB is on the\nintended path. We will revisit this attack when discussing collusion\nattacks.\nIn order to inject a packet with a valid header, an attacker can\nreplay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='replay a previously seen packet, which is only possible when the\nsame payload has been sent on that path before. Such replay attacks\ncan be mitigated by including a timestamp in the packet. OPT is\nalso vulnerable to packet injection when the destination colludes\nwith the injecting router.\nPath deviation attack. OPT ensures that a successful veriﬁcation\nof PVFi−1 (PVFn) against OPVi (OPVD) implies that the payload\nRi (the destination) received has traversed all the honest routers in\nthe source-intended path in the correct order, assuming that both\nsource and destination are honest. OPT provides this guarantee\nbecause the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='because the attacker does not possess the secret keys required to\ncompute PVFi and OPVi, and thus cannot generate valid PVFi or\nOPVi. As a result, malicious routers cannot mount router skipping\nor out-of-order traversal attacks.\nThis indicates that if a malicious router selects a path not in-\ntended by the source, an honest intermediary router will reject the\npacket. However, a malicious router can mount a path detour attack\nand send the payload to other routers that are not on the intended\npath.\nIn Extended-OPT, even if the destination is malicious, it cannot\nselect the unauthorized path that drops or reorders honest routers.\nExtended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Extended-OPT also assures the destination that neither the mali-\ncious routers nor the malicious source can select a path that drops\nor reorders honest routers on the source-intended path.\nCoward attack. Retroactive-OPT can mitigate coward attacks by\nrequiring all forwarding routers to compute relevant PVF and OPV\nﬁelds for probabilistic auditing. As a router cannot reliably guess\nwhen audits will happen, it does not know when to carry out an\nattack. We are unaware of any other path validation protocols that\ncan defend against the coward attack, including ICING.\nDoS attack. We consider attacks aiming to exhaust memory and\ncomputational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='computational resources on routers (Section 2.3). Each OPT router\nstores only a secret value (SVRi), regardless of the number of sources\nsending trafﬁc and the number of ﬂows transiting the router. For\nthis reason, memory exhaustion attacks are not possible under OPT.\nOPT routers perform very few symmetric cryptographic operations\nper packet during forwarding, which run at line speed (Section 7).\nTherefore, OPT is more resilient to computation resource exhaus-\ntion attacks than existing schemes such as ICING that provide sim-\nilar security guarantees.\nCollusion. The path and source validation are conditioned upon\nwhether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='whether a router is honest, i.e., correctly runs the protocol. When\nno two malicious routers are adjacent to each other on the intended\npath before Ri, malicious routers can redirect the packet to any\nrouters as it chooses. However, all preceding links on the desired\npath are still traversed in the correct sequence for this packet to be\naccepted by Ri. Similarly, a malicious router could replace the path\nin the packet and trick its neighbor into forwarding the packet to a\nrouter outside the intended path. Again, this packet will be dropped\nwhen it reaches an honest router.\nWhen there are multiple adjacent malicious routers on the in-\ntended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='tended path (Rj1 to R jn), a wormhole is present: an honest router\ndown the path can only conclude that the packet has entered into the\nhole via R j1 and exited the hole from R jn, but has no knowledge as\nto where the packet has been to in between these two points. In\nparticular, when the source colludes with Ri, Ri+1 can be tricked\ninto accepting and forwarding any packet.\n7.\nIMPLEMENTATION AND EVALUATION\nWe implemented OPT in Section 5.3 with DRKey as a user-level\napplication that performs source authentication and path validation.\nThe cryptographic operations performed by a router during packet\nforwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='forwarding are purely symmetric cryptographic operations, which\nwe instantiate with the AES block cipher. We list the cryptographic\nfunctions used in the implementation. To compute MACs, we used\nCBC-MAC based on AES, since it requires a single AES opera-\ntion to authenticate a 128-bit value. For computing the PRF, we\nalso use the same CBC-MAC. We implement AES using AESni, a\nnew CPU instruction set provided by recent Intel and AMD CPUs\nto speed up AES operations. AESni is fast: According to Intel,\nexecuting an encryption using AES-128 in CBC mode takes 4.15\ncycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cycles per byte to encrypt a 1 KB buffer [13]. While this number\nbounds the processing latency per byte, router throughput can be\nincreased, as we can process 4 blocks in parallel on a single core\nin AES-128 in CBC mode, resulting in 1.33 cycles per byte. We\nimplemented authenticated encryption using Galois/Counter Mode\n(GCM) with AES.\nWe use SHA-3 for computing hashes on long strings, such as\nthe hash of the payload DATAHASH. We truncate the hash from\n277\nTable 2: Per-session storage (σ), long-term storage (LT) related\nto the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='to the key setup, and communication overhead of DRKey’s and\nICING’s key setup for a path of length n.\nRouter\nSource\nDestination\nStorage\nDRKey (σ)\n0\nn+2\nn+2\n[#items]\nICING (σ)\n2\nn+1\n2∗n+1\nDRKey (LT)\n1\n1\n2\nICING (LT)\n≤ 400,000\n0\n0\nKey setup\nDRKey', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='≤ 400,000\n0\n0\nKey setup\nDRKey\n2\n[#packets]\nICING\n4∗n+4\na 256-bit value to a 128-bit value. For computing hashes on short\nstrings, such as H(PKσ), we use the Merkle-Damgard construction\nwith a Matyas-Meyer-Oseas AES-based compression function that\nmakes use of the fast hardware AESni instructions. We choose a\nsingle-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='single-block-length compression function that outputs 128-bit hash\nvalues. We use 128-bit hash values to obtain a smaller OPT header\nsize. Nevertheless, this decision does not pose security concerns:\nan adversary besides the source needs to perform a second-pre-\nimage collision attack, which is still in the order of O(2128) for\nSHA-3 and close to O(2128) for Matyas-Meyer-Oseas.\nFor signatures, we use Ed25519 [6], providing high efﬁciency\nand security, and small signatures. For a security level of 2128 oper-\nations, Ed25519 signature generation and veriﬁcation on a 3.4GHz', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ations, Ed25519 signature generation and veriﬁcation on a 3.4GHz\nCore i7 takes 20us and 60us, respectively. Public keys and signa-\ntures are only 32 and 64 bytes, respectively. Certiﬁcates can thus be\nas small as 128 bytes, enabling routers to add their certiﬁcate to the\nkey setup message. As explained in Section 3, router certiﬁcates\ncan be generated by an AS and signed with the AS’s private key.\nThe AS’s public key can be obtained and veriﬁed through RPKI.\nFor encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='For encrypting routers’ keys in DRKey, we use RSA-2048, which\noffers fast encryption. Although decryption is an order of magni-\ntude slower, it is performed by the endhost which is less perfor-\nmance critical.\nICING implementation and conﬁguration. We compare OPT\nwith ICING, the code of which we obtained from their website3.\nTo ensure the fairness of our comparison, we implemented ICING\nthat also uses AESni. In ICING, the source obtains a proof of con-\nsent (PoC) from the consent server of each node on the path. A\nPoC certiﬁes that the node consents to the full path. Furthermore,\neach node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='each node has an associated tag that describes a set of actions that\nthe source can take for packets. The authors describe an optimiza-\ntion for computing the tag keys needed for the PoCs [25], which\ncan diminish the number of required PRF rounds to 0. In our IC-\nING implementation, we favor ICING and consider that computing\nthese keys has no computational or memory lookup overhead.\nAnother important concept in ICING is the proofs of provenance\n(PoPs)—proofs to the nodes that the packet originates from the\nsender. Computing PoPs requires shared symmetric keys between\neach pair of ICING nodes on the path. These keys can be either de-\nrived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='rived on demand, using non-interactive Difﬁe-Hellman, or cached\nonce computed. Since non-interactive Difﬁe-Hellman is expensive\nand impractical on the fast path, PoP keys are always retrieved from\nthe cache in our ICING implementation.\n7.1\nDRKey Evaluation\nDRKey enables the design of low-overhead protocols in terms\nof router resources, such as OPT. In OPT, we use DRKey for key\nsetup, which is executed once per session. Thus, the key setup cost\nis amortized over an OPT session.\nTable 2 provides an analysis of storage overhead at the source,\n3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='3http://www.cs.utexas.edu/icing/\nTable 3: Packet latency during DRKey processing.\nEntity\nPath length\nLatency\nRouter\nIrrelevant\n381 µs\nSource\n2\n621 µs\n4\n609 µs\n8\n628 µs\nDestination\n2\n3820 µs\n4\n5520 µs\n8\n14814 µs', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='4\n5520 µs\n8\n14814 µs\ndestination, and intermediate routers for DRKey and ICING. We\nalso compare the communication overhead for setting up keys.\nGiven a path of length n (excluding the source and the destina-\ntion), DRKey requires the source and the destination to store, per\nsession, n+1 symmetric keys and a public-private session key pair.\nThe long-term storage of the source and the destination, which out-\nlives multiple sessions, consists of their shared symmetric key and\nthe destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the destination’s secret value. The OPT routers do not store any\nper-session information. Instead, the routers merely store a single\nlocal secret each.\nIn contrast, ICING’s source and destination need to store n + 1\nsymmetric keys. Each ICING router needs to store pairwise keys\nwith every router in the Internet, which, according to the ICING au-\nthors, is within 400,000. The source also stores PoCs of all entities\nin the path.\nRegarding the communication overhead of the key setup in DRKey,\nthe source and the destination send one message each, resulting in\n2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='2 messages per session regardless of the path length. In ICING,\nhowever, the source contacts the PoC server of every node on the\npath (routers and destination), which leads to 2∗(n+1) messages\nfor a path of length n. The source also sets up pairwise shared keys\nwith each entity on the path, requiring at least a round trip (2 mes-\nsages) per entity, resulting in at least 2 ∗ (n + 1) messages. We do\nnot count the messages that are necessary to set up pairwise shared\nkeys between ICING routers, because these keys are set up once\nbetween all entities in the Internet and then stored at each entity.\nTo prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='To prove the efﬁciency of DRKey, we evaluate the per-packet\ncomputation overhead at the source, destination and intermediate\nrouters. Our experiment measures the latency of key setup packets\nwhile they transit the network entities. For the experiment, we use\na trafﬁc generator that initiates key setup operations and connects\nto a server that performs the key setup operations of the source,\nrouter, or destination. After the key setup, the server forwards the\npackets back to the trafﬁc generator, which measures the receive\nrate.\nTable 3 presents the latency of DRKey packets at the source,\nrouters, and the destination.\nThe results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The results show that our crypto-\ngraphic algorithm choices optimize router performance, which is\nin any case independent of path length.\nFor the source and the destination, a longer path increases the\namount of computation. In the case of the source, this is hardly no-\nticeable, because the source does not perform public-key cryptogra-\nphy operations that depend on the path length. In contrast, the des-\ntination performs per-hop public-key decryption using RSA-2048\nto obtain the shared keys, which is expensive and considerably af-\nfects the latency. Nevertheless, the results satisfy our objectives:\nsince the source and the destination have a signiﬁcantly lower traf-\nﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ﬁc throughput than routers, they can spend more cycles on perform-\ning key setup.\n7.2\nOPT Evaluation\nWe evaluate OPT with respect to the desired performance proper-\n278\n0\n200\n400\n600\n800\n1000\n1200\n1400\n20\n30\n40\nPacket Size (B)\nThroughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Throughput (Gbps)\nBaseline\nFigure 5: Throughput of I/O Engine. Since our experiments\nuse I/O Engine for trafﬁc delivery, I/O engine represents our\nbaseline.\nties for source and path validation, namely efﬁcient forwarding and\nscalable state, and the cost associated with meeting them. Specif-\nically, we examine (1) OPT’s overhead in terms of per-packet pro-\ncessing by measuring both the throughput (the bandwidth utilized\nby whole packets including the Ethernet header)4 and goodput (the\nbandwidth used to transmit the payload of the packets, excluding\nthe OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the OPT protocol header); and (2) scalability with respect to the\npath length. For our evaluation, we set up a software router that\nvalidates the source and the path of the incoming packets before\nforwarding them. Our comparison is with ICING [26], which we\ndiscuss in more detail in Section 9 since both provide similar se-\ncurity guarantees. We experiment with OPT and ICING to per-\nform source and path validations and we use PacketShader’s I/O\nEngine [15] to send/receive packets to/from the NICs.\nA central aspect of our work is the forwarding speed of a router.\nSince OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Since OPT does not keep any per-source or per-ﬂow state, an OPT\nrouter’s forwarding speed is not inﬂuenced by varying the num-\nber of sources sending packets or the number of ﬂows transiting\na router. In contrast, we analyze the impact of (1) cryptographic\noperations and (2) memory lookup of cryptographic keys, because\nthe forwarding overhead of path validation protocols that use cryp-\ntography depends on these metrics.\nEvaluation system. Our testbed consists of two routers A and B.\nBoth are equipped with two Intel Ethernet Server Adapter X520-\nT2 NICs, and they both run Ubuntu Linux Kernel version 3.2.0-3.\nSystem A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='System A runs a trafﬁc generator featuring 6 x 2GB DDR3 RAM\nand two Xeon L5640 2.26GHz (6 cores) processors. System B\nruns our software router code featuring 16 x 4GB DDR3 RAM and\ntwo Intel Xeon E5-2680 2.70GHz (8 cores) processors. The trafﬁc\ngenerator generates trafﬁc at a rate of 40Gbps, which is processed\non the software router and sent back for measurement.\n7.3\nExperiment setup\nWe ﬁrst describe how packets are forwarded from one router to\nthe other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='the other. Then we describe the software router implementation for\nsource authentication and path validation for OPT and ICING.\nWe use PacketShader’s I/O Engine [15], a high-speed open source\nimplementation to send/receive packets to/from the NICs. On the\nsending router, I/O Engine takes the packets generated by the user-\nlevel trafﬁc generator and sends them to the NIC. When the packets\narrive at the second router’s NIC, I/O Engine takes the packets from\nthe NIC and delivers them to the user-level application, where they\nare processed according to the protocol (OPT or ICING). The last\nstep is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='step is to send these packets back to the ﬁrst router. In the remainder\nof this section we consider the packet processing that takes place at\nthe second router from the moment I/O Engine delivers the packets\nto the user-level application and until the packets are ready to be\nsent back.\nExperiments. We measure the forwarding speed of the software\nrouter for OPT and ICING. Our experiments consider AS-level\npath validation scenarios, where path validation is performed at a\n4We add 20B Ethernet overhead in computing the throughput.\nsingle router (e.g., ingress router) within an AS. Since an AS is ad-\nministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ministered by a single authority and its internal routing topology\nis conﬁdential, redundant path veriﬁcations are unlikely to happen\ninside an AS in practice. Consequently, we perform tests with a\nmaximum path length of 10 hops (without counting the source).\nThe minimum path length is 2 hops, corresponding to the case of a\nsource, an intermediate hop, and a destination.\nTo measure the forwarding overhead at a router, which includes\nthe memory overhead for storing keys and for retrieving them, we\nconsider a network where each node has α neighbors. The param-\neter α is important only if the router performs work that depends\non the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='on the number of neighbors. This is not the case for OPT, because\nthe router merely computes the key it shares with the source to\nverify its OPV ﬁeld, then computes the hash of the payload and\nﬁnally updates the PVF ﬁeld. These operations do not depend on\nthe number of neighbors the router has nor on the path length.\nHowever, in ICING the router has to look up the shared sym-\nmetric keys with each node on the path in a table that contains the\nkeys of all the nodes the router had previously seen on a path. For\na path of maximum length n, these nodes are located within n-hop\ndistance away from the router. Our choice for the parameter α = 3\nand the maximum path length of 10 hops gives (311−1)\n2', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the maximum path length of 10 hops gives (311−1)\n2\n= 9841 as\na maximum key table size, which is within ICING’s maximum key\ntable size of 400,000 [25].\nThe router receives packets at a line rate of 40Gbps. To quantify\nthe throughput and goodput with respect to the overhead of OPT\nand ICING, we perform tests with payload sizes of 20B, 256B,\n576B, 768B, and 1024B. We add to these values the OPT and IC-\nING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ING header overhead, respectively, whose size corresponds to the\ntested path length. The 20B of the smallest payload size correspond\nto the TCP header size to simulate TCP/ICING and TCP/OPT. We\nnote that TCP/OPT includes the IP header since OPT runs over the\nIP network; whereas TCP/ICING does not include the IP header\nsince ICING is designed to replace IP. Hence, the goodput compu-\ntation favors ICING.\nThe biggest payload size is computed by subtracting from the\nMTU the header size of ICING for the longest tested path (10\nhops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hops), as ICING has a higher header overhead than OPT. More\nprecisely, ICING has a per-hop overhead of 42B plus 24B for the\nsource identiﬁer and 13B of common ﬁelds, which gives a header\nlength of 457B for a 10-hop path. The computation is 1500B −\n457B = 1043B, which explains our choice of 1024B of the maxi-\nmum payload. In case of OPT, 52B common ﬁelds, 16B per-hop\noverhead and 40B TCP/IP header result in 252B. As a result, the\nmaximum payload size is dictated by the size of the ICING header\nfor the longest path considered.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='maximum payload size is dictated by the size of the ICING header\nfor the longest path considered.\nMethod. We generate trafﬁc for 10 seconds at a rate of 40Gbps,\nwhich is forwarded to the router running the protocol for source\nauthentication and path validation, and then forwarded back to the\nsource. To measure the throughput, we employ I/O Engine’s scripts,\nwhich operate as follows. These scripts read the RX and TX counter\nvalues of the NIC at the beginning of the experiment and then read\nthe values again every second to compute the number of packets\nsent and received. Consequently, we obtain the throughput values\nevery second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='every second, which we then average to compute the ﬁnal through-\nput value.\nFor goodput results we subtract the OPT or ICING\nheader, respectively, from the packet size.\n7.4\nForwarding Overhead\nWe evaluate the most computationally intensive protocol version\nof OPT described in Section 5.3. The evaluation results show that\nOPT outperforms ICING by a signiﬁcant margin. Since the other\nversions of OPT feature smaller packet headers and less computa-\n279\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(a) 2-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(b) 4-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(c) 8-hop path\n0\n200\n400\n600\n800\n1000\n1200', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='400\n600\n800\n1000\n1200\n0\n10\n20\n30\n40\nPacket Size (Payload)\nBandwidth (GBps)\nOPT − Throughput\nICING −Throughput\nOPT − Goodput\nICING − Goodput\n(d) 10-hop path\nFigure 6: Throughput and goodput (i.e., throughput obtained\nonly for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='only for the payload part of the packet) of OPT and ICING\nfor different packet sizes expressed in bytes and path lengths\nvarying from 2 to 10 hops.\ntional overhead, they would outperform ICING with an even larger\nmargin.\nFigure 6 depicts the results for throughput and goodput for OPT\nand ICING. We performed experiments for different path lengths\nand packet sizes described in Section 7.3, and the numbers we ob-\ntained show consistent results over all experiments, as explained in\nthe next paragraphs.\nA ﬁrst observation is that throughput registers higher values than\ngoodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='goodput, as expected, due to the OPT or ICING header overhead\nthat adds to the packet size, yet is not accounted for when mea-\nsuring goodput. We also notice that, for all path lengths, OPT’s\nthroughput is close to 40Gbps except for the smallest packet size\n(i.e., 20B). We note that OPT’s throughput for small packets is\nmainly limited by I/O Engine’s throughput as shown in Figure 5.\nAs the path length grows, I/O Engine’s bottleneck becomes re-\nleased because of the reduced number of packet copies between the\nNIC and the user-level packet processing engine5; and as a conse-\nquence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quence, OPT’s throughput becomes close to 40Gbps. This result is\nconsistent with Figure 5.\nFor each path length, the goodput of both OPT and ICING in-\ncrease as the packet size increases, because the protocol header rep-\nresents a smaller fraction of the total packet size as the payload size\nincreases. Even though ICING’s throughput also increases with\nthe packet size, its value is much smaller than OPT’s throughput.\nGiven the choices for our ICING implementation, as explained ear-\nlier, this result is mainly due to the key table lookup for the PoP\nkeys. Instead, OPT uses AESni operations to derive the keys shared\nwith the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='with the source, on the ﬂy. This choice in protocol design therefore\nproves to be decisive in obtaining a higher forwarding speed in OPT\nas much as 10Gbps in comparison to ICING.\n7.5\nPath Length Scalability\nIn order to analyze the protocols’ scalability with respect to the\npath length, we depict in Figure 7 the ratio between the goodput and\nthe throughput (named goodput ratio) for 256B and 1024B packets.\nWe vary the path length from 2 to 10 hops.\nThe goodput ratios of 256B packets are lower those of than 1024B\npackets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='packets since small packets have higher header overhead (see previ-\nous section). Path length increase adds more overhead to the packet\nheader, resulting in goodput degradation for both OPT and ICING.\n5The increased header size reduces the number of packets needed\nto saturate the link bandwidth.\n2\n4\n6\n8\n10\n0\n20\n40\n60\n80\n100\nPath Length (Hops)\nGoodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Goodput Ratio (%)\nOPT−256B\nICING−256B\nOPT−1024B\nICING−1024B\nFigure 7: The goodput ratio (i.e., goodput/throughput) of OPT\nand ICING for small and large packets, in the context of path\nlengths varying from 2 to 10 hops.\nThe ﬁgure shows that OPT has better path length scalability than\nICING since the goodput ratio of OPT decreases slower than that of\nICING as the path length increases. Speciﬁcally, when the 2-hop\npath is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='path is used as a baseline, for 256B packets, OPT’s average per-\nhop goodput degradation ratio is (64.7−49.0)\n64.7·8\n= 3.03%, while IC-\nING’s is (64.5−34.9)\n64.5·8\n= 5.74%; for 1024B packets, OPT’s goodput\ndegradation ratio per hop is (88.1−79.3)\n88.1·8\n= 1.25%, while ICING’s is\n(87.9−68.2)\n87.9·8\n= 2.80%.\n8.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='(87.9−68.2)\n87.9·8\n= 2.80%.\n8.\nDISCUSSION\nKey lifetime. The keys associated with a session σ are valid as long\nas (1) PATHσ between S and D in the session σ does not change,\nand (2) S or D do not terminate the session due to application-driven\nsession lifetime requirement.\nAccording to the ﬁrst point, the maximum key lifetime is deter-\nmined by route stability. Recent end-to-end route stability anal-\nyses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='yses reveal that most of network routes are stable from tens of\nminutes to days, even when ISPs apply trafﬁc engineering tech-\nniques [9, 18, 22]. Although many routes are still short-lived, en-\ntities send packets over long-lived routes (longer than 6 hours) for\n96% of the times [9]. In particular, considering the fact that the\nload balancing within an ISP causes most route variations (i.e., up\nto 82%), OPT running at AS-level uses more stable routes than\nrouter-level OPT. Furthermore, when routers perform per-ﬂow or\nper-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='per-destination load balancing, paths used in OPT are not affected.\nYet an attacker could cause changes in network routes, as demon-\nstrated by numerous incidents such as Man-in-the-Middle BGP route\nhijacking [8]. In this case, the network routes could ﬂap as the\nattacker wishes. Yet, some future Internet architecture proposals\n(Nebula [2], Pathlets [12], SCION [40], XIA [14]) relieve this pain\npoint by having packets carry forwarding information in the packet\nheader, so that the source is always aware of the path and would set\nup a new session if the path changes.\nWe expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='We expect paths to be stable on the order of at least tens of min-\nutes. Similarly, we expect that applications re-key sessions at the\nearliest at a granularity of tens of minutes. Thus, the key setup\noverhead represents a tiny fraction of the total computation and\ncommunication overhead of a long-lived high-bandwidth connec-\ntion.\nEfﬁcient packet content authentication. As described in Sec-\ntion 5.3, each intermediate router uses the DATAHASH ﬁeld in the\nOPT header when it veriﬁes its OPV ﬁeld. Such a veriﬁcation does\nnot authenticate the packet content since a malicious intermediate\nrouter could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='router could alter the data without changing DATAHASH. To mit-\nigate such an attack, a router can verify the DATAHASH ﬁeld in\nparallel while the packet is being scheduled for transmission.\nAs an alternative, probabilistic veriﬁcation schemes [16] can be\n280\napplied such that every router decreases the veriﬁcation probabil-\nity if the DATAHASH veriﬁcation succeeds. However, if a router\ndetects a packet with a bogus hash value, the probability to run\nhash veriﬁcation increases. Furthermore, as soon as a router re-\nceives multiple mismatching hash values, it immediately performs\nhash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='hash veriﬁcation on all subsequent packets. As a result, the routers\nneighboring a malicious router would verify all hashes of packets\nincoming on interfaces arriving from the malicious router. With\nsuch a probabilistic veriﬁcation approach, we can further improve\nthe efﬁciency and practicality while providing data authentication.\nOPT in the current Internet. OPT could be incrementally de-\nployed in the current Internet. An AS could announce its OPT\nfunctionality within BGP update messages (as a transitive attribute)\nor as extension to RPKI certiﬁcates, enabling the selection and con-\nstruction of end-to-end OPT paths at source ASes. Endhosts could\nobtain OPT path information from a local route server which col-\nlects BGP and RPKI information.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='obtain OPT path information from a local route server which col-\nlects BGP and RPKI information.\nTo carry OPT-based information in packets, the simplest ap-\nproach would be an IPv6 extension header. In IPv4, spare IP header\nbits would need to be used to indicate the presence of an extra\nOPT header or trailer, but an extra header after the IP header may\ndisrupt processing at legacy ﬁrewalls or other middleboxes. With\nthe increasing support for IPv6, we prefer incremental deployment\nvia the IPv6 extension header. Since the DRKey information is\nlarger than the 256 bytes that ﬁt into an IPv6 extension header, we\npropose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='propose to use a new protocol number for DRKey packets, which\nrouters would check for and process in the slow path. Alternatively,\nASes could specify addresses of DRKey servers that would handle\nDRKey packets to set up the keys for the routers and thus would\nshare the secret keys KRi of routers. An endhost could then place\na sequence of DRKey server addresses (similar to a loose source\nrouting option in IPv4) into the DRKey packet, which would be\nsequentially processed and forwarded until the destination. This\nlatter approach avoids routers from analyzing the IP protocol ﬁeld.\n9.\nRELATED WORK\nThe most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='The most closely related work for source authentication and path\nvalidation is ICING by Naous et al. [26]. In a nutshell, the source\npre-computes a veriﬁer MAC (Vi) for each intermediate router Ri\nusing the respective shared secret key as well as the hash value of\nthe path and the static content in the header. For each packet, Ri ﬁrst\nreconstructs and XORs the MAC for the source and each upstream\nrouter, and veriﬁes if the XORed MACs are equivalent to what is\nstored in Vi. Then Ri (1) computes a MAC for each downstream\nrouter on the path using the shared secret key, the hash of the path,\nand the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and the data, and (2) XORs the MAC for each downstream router\nwith each Vj (j ≥ i).\nICING is more heavy-weight than OPT. ICING requires each\nRi to derive a Difﬁe-Hellman (DH) key with each router R j on\nthe path, which requires routers to cache keys to avoid the heavy-\nweight DH computation during packet forwarding. For the case\nthe keys are not cached any more, ICING suggests adding the 20-\nbyte public key of each router into each packet, resulting in a high\nper-packet overhead. Also, ICING requires each node to insert a\nMAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='MAC for each subsequent veriﬁer, requiring each node to compute\nn+1 MAC operations per packet. In contrast, OPT does not require\nrouters to store keys shared with sources or other routers, nor per-\nform a MAC computation for each router on the path. In terms of\nsecurity, even ICING intermediate routers can detect colluding path\ndeviation attacks mounted by the source and a malicious router to\nanother intermediary router. In contrast, Extended-OPT supports\nthe destination to detect such attacks. More speciﬁcally, the ori-\ngin and path validation property of the routers still depends on the\ncorrect behavior of the source, but not on the destination. Conse-\nquently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='quently, Extended-OPT offers the same security guarantee as IC-\nING for the destination. In other words, if the source is malicious,\nan illegitimate packet travels longer in Extended-OPT than ICING,\nbut will be rejected by the destination. Beside the malicious source\ncollusion attack, OPT and ICING provide the same kind of source\nand path authenticity properties for the destination. On the other\nhand, retroactive key setup OPT with path tracing (which only en-\nables the destination to verify the path) can mitigate the coward\nattack, which ICING fails to mitigate.\nLiu et al. propose Passport for intermediate routers to perform\nsource authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='source authentication [21]. Passport proposes that BGP route ad-\nvertisements carry Difﬁe-Hellman public keys, which enables any\ntwo ASes to compute a shared secret based on Difﬁe-Hellman key\nexchange. Using the respective shared secret key with each down-\nstream AS, the source AS computes a MAC for each AS on the\npath, and inserts it in the Passport header. Each intermediate AS\nauthenticates the source AS by recomputing the MAC using the\nshared key and conﬁrming that it matches the MAC in the Passport\nheader. Similar to Passport, the accountability service by Ben-\nder et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='der et al. [5] requires the source AS to embed an authenticator for\nsubsequent ASes. In SNAPP, the sender sets up keys with routers\nthrough a key derivation mechanism that is similar to DRKey (al-\nthough it cannot provide retroactive key setup) and uses these keys\nfor embedded source authenticators. Passport and the accountabil-\nity service provide weaker security guarantees than OPT, as they\nprovide only source AS authentication, and fail to defend against\nsource and data spooﬁng, as well as path deviation attacks. While\nSNAPP does prevent against source and data spooﬁng, it does not\nprevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='prevent path deviation attacks.\nOther source and path validation schemes. In IP traceback, re-\nsearchers proposed mechanisms that require routers carry per-packet\nstate [19, 31], perform packet marking [30, 32], or active path in-\nterrogation [27]. Pi suggests a path identiﬁer to detect source IP\naddress spooﬁng [36]. Unfortunately, these schemes are suscepti-\nble to attacks listed in Section 2.3, because they were designed for\na different purpose. Similarly, network capability mechanisms [3,\n29, 37, 38] cannot provide source authentication or path validation\nas the capability can be easily copied and inserted by the last AS.\n10.\nCONCLUSION\nDespite the importance of network-based source authentication', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='10.\nCONCLUSION\nDespite the importance of network-based source authentication\nand path validation, these primitives have not been implemented\nso far, perhaps because of the lack of an efﬁcient protocol that does\nnot burden the router. This paper introduces (1) DRKeys as efﬁcient\nand dynamically recreatable key setup protocols, and (2) OPT as an\nextremely lightweight, scalable, and secure protocol that provides\nsource authentication and path validation. Compared with currently\nproposed solutions, OPT achieves performance improvements with\nminimal latency and computational overhead on routers regardless\nof the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='of the path length. Moreover, OPT does not require routers to main-\ntain per-source or per-ﬂow state, further improving its practical-\nity. We also introduce a retroactive key setup process that protects\nagainst coward attacks, as routers cannot know in advance which\npaths are being monitored subsequently. We anticipate that OPT’s\nsecurity and performance properties will bring source authentica-\ntion and path validation into the realm of practicality.\n11.\nACKNOWLEDGMENTS\nWe thank George Danezis, Yue-Hsun Lin, Raphael Reischuk,\nmembers of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='members of the XIA team, our shepherd Ratul Mahajan, and the\nanonymous reviewers for their insightful feedback and suggestions.\n281\nWe gratefully acknowledge funding support for this research from\nCyLab at Carnegie Mellon, NSF under award CNS-1040801, Euro-\npean Research Council under the European Union’s Seventh Frame-\nwork Programme (FP7/2007-2013) / ERC grant agreement 617605,\nand a gift from KDDI.\n12.\nREFERENCES\n[1] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen,\nD. Moon, and S. Shenker. Accountable Internet Protocol', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='D. Moon, and S. Shenker. Accountable Internet Protocol\n(AIP). In Proceedings of ACM SIGCOMM, 2008.\n[2] T. Anderson, K. Birman, R. Broberg, M. Caesar, D. Comer,\nC. Cotton, M. Freedman, A. Haeberlen, Z. Ives,\nA. Krishnamurthy, W. Lehr, B. Loo, D. Mazières,\nA. Nicolosi, J. Smith, I. Stoica, R. van Renesse, M. Walﬁsh,\nH. Weatherspoon, and C. Yoo. The nebula future internet\narchitecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='architecture. In A. Galis and A. Gavras, editors, The Future\nInternet, volume 7858 of Lecture Notes in Computer\nScience, pages 16–26. Springer Berlin Heidelberg, 2013.\n[3] T. Anderson, T. Roscoe, and D. Wetherall. Preventing\nInternet Denial-of-Service with Capabilities. In Proceedings\nof Hotnets-II, 2003.\n[4] ARIN. Resource Public Key Infrastructure (RPKI).\nhttps://www.arin.net/resources/rpki/.\n[5] A. Bender, N. Spring, D. Levin, and B. Bhattacharjee.\nAccountability as a Service. In Proc. of USENIX SRUTI,\n2007.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Accountability as a Service. In Proc. of USENIX SRUTI,\n2007.\n[6] D. J. Bernstein, N. Duif, T. Lange, P. Schwabe, and B.-Y.\nYang. High-speed high-security signatures. In Proc. of\nCHES, 2011.\n[7] J. Cowie. The new threat: Targeted internet trafﬁc\nmisdirection. http://www.renesys.com/2013/11/mitm-\ninternet-hijacking/, Nov. 2013.\n[8] J. Cowie. The New Threat: Targeted Internet Trafﬁc\nMisdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Misdirection. Popular Mechanics,\nhttp://www.renesys.com/2013/11/mitm-internet-\nhijacking/, Nov 2013.\n[9] I. Cunha, R. Teixeira, and C. Diot. Measuring and\ncharacterizing end-to-end route dynamics in the presence of\nload balancing. In Proc. of PAM’11, 2011.\n[10] Y. Gilad and A. Herzberg. Plug-and-Play IP Security:\nAnonymity Infrastructure Instead of PKI. In Proceedings of\nESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='ESORICS, 2013.\n[11] B. Godfrey, S. Shenker, and I. Stoica. Pathlet Routing. In\nProc. of SIGCOMM, 2009.\n[12] P. B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica. Pathlet\nrouting. In Proceedings of the ACM SIGCOMM 2009\nConference on Data Communication, 2009.\n[13] S. Gueron. Intel Advanced Encryption Standard (AES) New\nInstructions Set, Mar. 2010. white paper 323641-001,\nRevision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Revision 3.\n[14] D. Han, A. Anand, F. Dogar, B. Li, H. Lim, M. Machado,\nA. Mukundan, W. Wu, A. Akella, D. G. Andersen, J. W.\nByers, S. Seshan, and P. Steenkiste. XIA: Efﬁcient support\nfor evolvable internetworking. In Proceedings of USENIX\nConference on Networked Systems Design and\nImplementation, 2012.\n[15] S. Han, K. Jang, K. Park, and S. Moon. PacketShader: a\nGPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='GPU-accelerated software router. ACM SIGCOMM\nComputer Communication Review, Aug. 2010.\n[16] H.-C. Hsiao, A. Studer, C. Chen, A. Perrig, F. Bai, B. Bellur,\nand A. Iyer. Flooding-Resilient Broadcast Authentication for\nVANETs. In Proc. of MobiCom, 2011.\n[17] L. Jia, C. Basescu, T. H.-J. Kim, A. Perrig, Y.-C. Hu, and\nF. Zhang. Mechanized network origin and path authenticity\nproofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='proofs. Technical Report CMU-CyLab-14-007, Carnegie\nMellon University, 2014.\n[18] M. S. Kang, S. B. Lee, and V. D. Gligor. The Crossﬁre\nAttack. In Proc. of IEEE Security and Privacy, 2013.\n[19] J. Li, M. Sung, J. Xu, and L. Li. Large-Scale IP Traceback in\nHigh-Speed Internet: Practical Techniques and Theoretical\nFoundation. In Proc. of IEEE Security and Privacy, 2004.\n[20] B. Liu, J. T. Chiang, J. J. Haas, and Y.-C. Hu. Coward\nAttacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Attacks in Vehicular Networks. Mobile Computing and\nCommunications Review, 2010.\n[21] X. Liu, A. Li, X. Yang, and D. Wetherall. Passport: Secure\nand Adoptable Source Authentication. In Proc. of NSDI,\n2008.\n[22] H. V. Madhyastha, E. Katz-Bassett, T. Anderson,\nA. Krishnamurthy, and A. Venkataramani. iPlane Nano: Path\nPrediction for Peer-to-peer Applications. In Proc. of NSDI,\n2009.\n[23] D. Mazieres, M. Kaminsky, M. F. Kaashoek, and E. Witchel.\nSeparating Key Mangement from File System Security. In', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='Separating Key Mangement from File System Security. In\nProceedings of SOSP, 1999.\n[24] R. Moskowitz and P. Nikander. Host Identity Protocol (HIP)\nArchitecture, May 2006.\nhttp://tools.ietf.org/html/rfc4423.\n[25] J. Naous. Path-policy Compliant Networking and a Platform\nfor Heterogeneous IAAS management. In PhD thesis, 2011.\n[26] J. Naous, M. Walﬁsh, A. Nicolosi, D. Mazieres, M. Miller,\nand A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='and A. Seehra. Verifying and Enforcing Network Paths with\nICING. In Proceedings of ACM CoNEXT, 2011.\n[27] V. N. Padmanabhan and D. R. Simon. Secure Traceroute to\nDetect Faulty or Malicious Routing. ACM SIGCOMM\nComputer Communications Review, January 2003.\n[28] J. Pappalardo. New Transatlantic Cable Built to Shave 5\nMiliseconds off Stock Trades. Popular Mechanics,\nhttp://www.popularmechanics.com/technology/\nengineering/infrastructure/a-transatlantic-\ncable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='cable-to-shave-5-milliseconds-off-stock-\ntrades, Oct 2011.\n[29] R. Raghavan and A. C. Snoeren. A system for authenticated\npolicy-compliant routing. In Proc. of ACM SIGCOMM,\n2004.\n[30] S. Savage, D. Wetherall, A. Karlin, and T. Anderson.\nPractical Network Support for IP Traceback. In Proc. of\nSIGCOMM, 2000.\n[31] A. C. Snoeren, C. Partridge, L. A. Galindo, C. E. Jones,\nF. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='F. Tchakounito, S. T. Kent, and W. T. Strayer. Hash-Based IP\nTraceback. In Proceedings of ACM SIGCOMM, 2001.\n[32] D. X. Song and A. Perrig. Advanced and Authenticated\nMarking Schemes for IP Traceback. In Proceedings of IEEE\nINFOCOM, 2001.\n[33] I. Stoica, D. Adkins, S. Zhaung, S. Shenker, and S. Surana.\nInternet indirection infrastructure. In Proc. of SIGCOMM,\n2002.\n[34] M. Walﬁsh, J. Stribling, M. Krohn, H. Balakrishnan,\nR. Morris, and S. Shenker. Middleboxes No Longer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='R. Morris, and S. Shenker. Middleboxes No Longer\nConsidered Harmful. In Proceedings of OSDI, 2004.\n[35] D. Wendlandt, D. G. Andersen, and A. Perrig. Perspectives:\nImproving SSH-style host authentication with multi-path\nprobing. In Proceedings of USENIX Annual Technical\nConference, June 2008.\n[36] A. Yaar, A. Perrig, and D. Song. Pi: A Path Identiﬁcation\nMechanism to Defend against DDoS Attacks. In Proc. of\nIEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='IEEE Security and Privacy, 2003.\n[37] A. Yaar, A. Perrig, and D. Song. An Endhost Capability\nMechanism to Mitigate DDoS Flooding Attacks. In Proc. of\nthe IEEE Security and Privacy, May 2004.\n[38] X. Yang, D. Wetherall, and T. Anderson. A DoS-limiting\nNetwork Architecture. In Proc. of SIGCOMM, 2005.\n[39] X. Zhang. Secure and Efﬁcient Network Fault Localization.\nPhD thesis, Carnegie Mellon University, 2012.\n[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'}), Document(page_content='[40] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and\nD. G. Andersen. SCION: Scalability, control, and isolation\non next-generation networks. In Proceedings of the IEEE\nSymposium on Security and Privacy (Oakland), May 2011.\n282', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpt5nmp1bu/路由 - SIGCOMM - Lightweight Source Authentication and Path Validation.pdf'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpnagmndza, tmpnagmndza
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc2bd110> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpnagmndza/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='194a4753-bb9f-4163-84af-b4bd9808f628.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="194a4753-bb9f-4163-84af-b4bd9808f628.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmprchnt5hd, tmprchnt5hd
File: 194a4753-bb9f-4163-84af-b4bd9808f628.txt, msg: 成功上传文件 194a4753-bb9f-4163-84af-b4bd9808f628.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc640750> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmprchnt5hd/194a4753-bb9f-4163-84af-b4bd9808f628.txt'})]
cuda:2
[UploadFile(filename='194a4753-bb9f-4163-84af-b4bd9808f628.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="194a4753-bb9f-4163-84af-b4bd9808f628.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp0d4chywv, tmp0d4chywv
File: 194a4753-bb9f-4163-84af-b4bd9808f628.txt, msg: 成功上传文件 194a4753-bb9f-4163-84af-b4bd9808f628.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc2bc190> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp0d4chywv/194a4753-bb9f-4163-84af-b4bd9808f628.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmppr8hlxfs, tmppr8hlxfs
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc6ad390> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmppr8hlxfs/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
[UploadFile(filename='51be487d-9376-424d-81c3-dde857b1377b.txt', size=5600, headers=Headers({'content-disposition': 'form-data; name="files"; filename="51be487d-9376-424d-81c3-dde857b1377b.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmpraqeafof, tmpraqeafof
File: 51be487d-9376-424d-81c3-dde857b1377b.txt, msg: 成功上传文件 51be487d-9376-424d-81c3-dde857b1377b.txt, docs: [Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc2c6290> 111
cuda:2
[Document(page_content='Quantization of Deep Neural Networks for Accurate Edge Computing', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Automatic Infectious Disease Classification Analysis with Concept  Discovery\nAutomatic infectious disease classification from images can facilitate neededmedical diagnoses. Such an approach can identify diseases, like tuberculosis,which remain under-diagnosed due to resource constraints and also novel andemerging diseases, like monkeypox, which clinicians have little experience oracumen in diagnosing. Avoiding missed or delayed diagnoses would preventfurther transmission and improve clinical outcomes. In order to understand andtrust neural network predictions, analysis of learned representations isnecessary. In this work, we argue that automatic discovery of concepts, i.e.,human interpretable attributes, allows for a deep understanding of learnedinformation in medical image analysis tasks, generalizing beyond the traininglabels or protocols. We provide an overview of existing concept discoveryapproaches in medical image and computer vision communities, and evaluaterepresentative methods on tuberculosis (TB) prediction and monkeypox predictiontasks. Finally, we propose NMFx, a general NMF formulation of interpretabilityby concept discovery that works in a unified way in unsupervised, weaklysupervised, and supervised scenarios.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Deep Learning for Time Series Classification and Extrinsic Regression: A  Current Survey\nTime Series Classification and Extrinsic Regression are important andchallenging machine learning tasks. Deep learning has revolutionized naturallanguage processing and computer vision and holds great promise in other fieldssuch as time series analysis where the relevant features must often beabstracted from the raw data but are not known a priori. This paper surveys thecurrent state of the art in the fast-moving field of deep learning for timeseries classification and extrinsic regression. We review different networkarchitectures and training methods used for these tasks and discuss thechallenges and opportunities when applying deep learning to time series data.We also summarize two critical applications of time series classification andextrinsic regression, human activity recognition and satellite earthobservation.\nWindow Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content='Window Detection In Facade Imagery: A Deep Learning Approach Using Mask  R-CNN\nThe parsing of windows in building facades is a long-desired but challengingtask in computer vision. It is crucial to urban analysis, semanticreconstruction, lifecycle analysis, digital twins, and scene parsing amongstother building-related tasks that require high-quality semantic data. Thisarticle investigates the usage of the mask R-CNN framework to be used forwindow detection of facade imagery input. We utilize transfer learning to trainour proposed method on COCO weights with our own collected dataset of streetview images of facades to produce instance segmentations of our new windowclass. Experimental results show that our suggested approach with a relativelysmall dataset trains the network only with transfer learning and augmentationachieves results on par with prior state-of-the-art window detectionapproaches, even without post-optimization techniques.\nDDANet: Dual Decoder Attention Network for Automatic Polyp Segmentation', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Colonoscopy is the gold standard for examination and detection of colorectalpolyps. Localization and delineation of polyps can play a vital role intreatment (e.g., surgical planning) and prognostic decision making. Polypsegmentation can provide detailed boundary information for clinical analysis.Convolutional neural networks have improved the performance in colonoscopy.However, polyps usually possess various challenges, such as intra-andinter-class variation and noise. While manual labeling for polyp assessmentrequires time from experts and is prone to human error (e.g., missed lesions),an automated, accurate, and fast segmentation can improve the quality ofdelineated lesion boundaries and reduce missed rate. The Endotect challengeprovides an opportunity to benchmark computer vision methods by training on thepublicly available Hyperkvasir and testing on a separate unseen dataset. Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'}), Document(page_content="Inthis paper, we propose a novel architecture called ``DDANet'' based on a dualdecoder attention network. Our experiments demonstrate that the model trainedon the Kvasir-SEG dataset and tested on an unseen dataset achieves a dicecoefficient of 0.7874, mIoU of 0.7010, recall of 0.7987, and a precision of0.8577, demonstrating the generalization ability of our model.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmpraqeafof/51be487d-9376-424d-81c3-dde857b1377b.txt'})]
cuda:2
cuda:2
[UploadFile(filename='None.txt', size=6545, headers=Headers({'content-disposition': 'form-data; name="files"; filename="None.txt"', 'content-type': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}))], /home/se1/se2024/tmp/chatchat/tmp4jvzwcpf, tmp4jvzwcpf
File: None.txt, msg: 成功上传文件 None.txt, docs: [Document(page_content='Convolutional Neural Networks as a Model of the Visual System: Past,  Present, and Future\nConvolutional neural networks (CNNs) were inspired by early findings in thestudy of biological vision. They have since become successful tools in computervision and state-of-the-art models of both neural activity and behavior onvisual tasks. This review highlights what, in the context of CNNs, it means tobe a good model in computational neuroscience and the various ways models canprovide insight. Specifically, it covers the origins of CNNs and the methods bywhich we validate them as models of biological vision. It then goes on toelaborate on what we can learn about biological vision by understanding andexperimenting on CNNs and discusses emerging opportunities for the use of CNNSin vision research beyond basic object recognition.\nCNN-based Local Vision Transformer for COVID-19 Diagnosis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content="CNN-based Local Vision Transformer for COVID-19 Diagnosis\nDeep learning technology can be used as an assistive technology to helpdoctors quickly and accurately identify COVID-19 infections. Recently, VisionTransformer (ViT) has shown great potential towards image classification due toits global receptive field. However, due to the lack of inductive biasesinherent to CNNs, the ViT-based structure leads to limited feature richness anddifficulty in model training. In this paper, we propose a new structure calledTransformer for COVID-19 (COVT) to improve the performance of ViT-basedarchitectures on small COVID-19 datasets. It uses CNN as a feature extractor toeffectively extract local structural information, and introduces averagepooling to ViT's Multilayer Perception(MLP) module for global information.Experiments show the effectiveness of our method on the two COVID-19 datasetsand the ImageNet dataset.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Graph Attention Layer Evolves Semantic Segmentation for Road Pothole  Detection: A Benchmark and Algorithms', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Existing road pothole detection approaches can be classified as computervision-based or machine learning-based. The former approaches typically employ2-D image analysis/understanding or 3-D point cloud modeling and segmentationalgorithms to detect road potholes from vision sensor data. The latterapproaches generally address road pothole detection using convolutional neuralnetworks (CNNs) in an end-to-end manner. However, road potholes are notnecessarily ubiquitous and it is challenging to prepare a large well-annotateddataset for CNN training. In this regard, while computer vision-based methodswere the mainstream research trend in the past decade, machine learning-basedmethods were merely discussed. Recently, we published the first stereovision-based road pothole detection dataset and a novel disparitytransformation algorithm, whereby the damaged and undamaged road areas can behighly distinguished.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Recently, we published the first stereovision-based road pothole detection dataset and a novel disparitytransformation algorithm, whereby the damaged and undamaged road areas can behighly distinguished. However, there are no benchmarks currently available forstate-of-the-art (SoTA) CNNs trained using either disparity images ortransformed disparity images. Therefore, in this paper, we first discuss theSoTA CNNs designed for semantic segmentation and evaluate their performance forroad pothole detection with extensive experiments. Additionally, inspired bygraph neural network (GNN), we propose a novel CNN layer, referred to as graphattention layer (GAL), which can be easily deployed in any existing CNN tooptimize image feature representations for semantic segmentation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Ourexperiments compare GAL-DeepLabv3+, our best-performing implementation, withnine SoTA CNNs on three modalities of training data: RGB images, disparityimages, and transformed disparity images. The experimental results suggest thatour proposed GAL-DeepLabv3+ achieves the best overall pothole detectionaccuracy on all training data modalities.\nConvolutional Embedding Makes Hierarchical Vision Transformer Stronger', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Vision Transformers (ViTs) have recently dominated a range of computer visiontasks, yet it suffers from low training data efficiency and inferior localsemantic representation capability without appropriate inductive bias.Convolutional neural networks (CNNs) inherently capture regional-awaresemantics, inspiring researchers to introduce CNNs back into the architectureof the ViTs to provide desirable inductive bias for ViTs. However, is thelocality achieved by the micro-level CNNs embedded in ViTs good enough? In thispaper, we investigate the problem by profoundly exploring how the macroarchitecture of the hybrid CNNs/ViTs enhances the performances of hierarchicalViTs. Particularly, we study the role of token embedding layers, aliasconvolutional embedding (CE), and systemically reveal how CE injects desirableinductive bias in ViTs. Besides, we apply the optimal CE configuration to 4recently released state-of-the-art ViTs, effectively boosting the correspondingperformances.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Besides, we apply the optimal CE configuration to 4recently released state-of-the-art ViTs, effectively boosting the correspondingperformances. Finally, a family of efficient hybrid CNNs/ViTs, dubbed CETNets,are released, which may serve as generic vision backbones. Specifically,CETNets achieve 84.9% Top-1 accuracy on ImageNet-1K (training from scratch),48.6% box mAP on the COCO benchmark, and 51.6% mIoU on the ADE20K,substantially improving the performances of the corresponding state-of-the-artbaselines.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Ghost-free High Dynamic Range Imaging with Context-aware Transformer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='High dynamic range (HDR) deghosting algorithms aim to generate ghost-free HDRimages with realistic details. Restricted by the locality of the receptivefield, existing CNN-based methods are typically prone to producing ghostingartifacts and intensity distortions in the presence of large motion and severesaturation. In this paper, we propose a novel Context-Aware Vision Transformer(CA-ViT) for ghost-free high dynamic range imaging. The CA-ViT is designed as adual-branch architecture, which can jointly capture both global and localdependencies. Specifically, the global branch employs a window-basedTransformer encoder to model long-range object movements and intensityvariations to solve ghosting. For the local branch, we design a local contextextractor (LCE) to capture short-range image features and use the channelattention mechanism to select informative local details across the extractedfeatures to complement the global branch.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='For the local branch, we design a local contextextractor (LCE) to capture short-range image features and use the channelattention mechanism to select informative local details across the extractedfeatures to complement the global branch. By incorporating the CA-ViT as basiccomponents, we further build the HDR-Transformer, a hierarchical network toreconstruct high-quality ghost-free HDR images. Extensive experiments on threebenchmark datasets show that our approach outperforms state-of-the-art methodsqualitatively and quantitatively with considerably reduced computationalbudgets. Codes are available athttps://github.com/megvii-research/HDR-Transformer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'})]
Embed model: bge-large-zh-v1.5, embed_device: cuda:2
doc: page_content='init', embedding: <server.knowledge_base.kb_service.base.EmbeddingsFunAdapter object at 0x7f8fbc57af90> 111
cuda:2
[Document(page_content='Convolutional Neural Networks as a Model of the Visual System: Past,  Present, and Future\nConvolutional neural networks (CNNs) were inspired by early findings in thestudy of biological vision. They have since become successful tools in computervision and state-of-the-art models of both neural activity and behavior onvisual tasks. This review highlights what, in the context of CNNs, it means tobe a good model in computational neuroscience and the various ways models canprovide insight. Specifically, it covers the origins of CNNs and the methods bywhich we validate them as models of biological vision. It then goes on toelaborate on what we can learn about biological vision by understanding andexperimenting on CNNs and discusses emerging opportunities for the use of CNNSin vision research beyond basic object recognition.\nCNN-based Local Vision Transformer for COVID-19 Diagnosis', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content="CNN-based Local Vision Transformer for COVID-19 Diagnosis\nDeep learning technology can be used as an assistive technology to helpdoctors quickly and accurately identify COVID-19 infections. Recently, VisionTransformer (ViT) has shown great potential towards image classification due toits global receptive field. However, due to the lack of inductive biasesinherent to CNNs, the ViT-based structure leads to limited feature richness anddifficulty in model training. In this paper, we propose a new structure calledTransformer for COVID-19 (COVT) to improve the performance of ViT-basedarchitectures on small COVID-19 datasets. It uses CNN as a feature extractor toeffectively extract local structural information, and introduces averagepooling to ViT's Multilayer Perception(MLP) module for global information.Experiments show the effectiveness of our method on the two COVID-19 datasetsand the ImageNet dataset.", metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Graph Attention Layer Evolves Semantic Segmentation for Road Pothole  Detection: A Benchmark and Algorithms', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Existing road pothole detection approaches can be classified as computervision-based or machine learning-based. The former approaches typically employ2-D image analysis/understanding or 3-D point cloud modeling and segmentationalgorithms to detect road potholes from vision sensor data. The latterapproaches generally address road pothole detection using convolutional neuralnetworks (CNNs) in an end-to-end manner. However, road potholes are notnecessarily ubiquitous and it is challenging to prepare a large well-annotateddataset for CNN training. In this regard, while computer vision-based methodswere the mainstream research trend in the past decade, machine learning-basedmethods were merely discussed. Recently, we published the first stereovision-based road pothole detection dataset and a novel disparitytransformation algorithm, whereby the damaged and undamaged road areas can behighly distinguished.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Recently, we published the first stereovision-based road pothole detection dataset and a novel disparitytransformation algorithm, whereby the damaged and undamaged road areas can behighly distinguished. However, there are no benchmarks currently available forstate-of-the-art (SoTA) CNNs trained using either disparity images ortransformed disparity images. Therefore, in this paper, we first discuss theSoTA CNNs designed for semantic segmentation and evaluate their performance forroad pothole detection with extensive experiments. Additionally, inspired bygraph neural network (GNN), we propose a novel CNN layer, referred to as graphattention layer (GAL), which can be easily deployed in any existing CNN tooptimize image feature representations for semantic segmentation.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Ourexperiments compare GAL-DeepLabv3+, our best-performing implementation, withnine SoTA CNNs on three modalities of training data: RGB images, disparityimages, and transformed disparity images. The experimental results suggest thatour proposed GAL-DeepLabv3+ achieves the best overall pothole detectionaccuracy on all training data modalities.\nConvolutional Embedding Makes Hierarchical Vision Transformer Stronger', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Vision Transformers (ViTs) have recently dominated a range of computer visiontasks, yet it suffers from low training data efficiency and inferior localsemantic representation capability without appropriate inductive bias.Convolutional neural networks (CNNs) inherently capture regional-awaresemantics, inspiring researchers to introduce CNNs back into the architectureof the ViTs to provide desirable inductive bias for ViTs. However, is thelocality achieved by the micro-level CNNs embedded in ViTs good enough? In thispaper, we investigate the problem by profoundly exploring how the macroarchitecture of the hybrid CNNs/ViTs enhances the performances of hierarchicalViTs. Particularly, we study the role of token embedding layers, aliasconvolutional embedding (CE), and systemically reveal how CE injects desirableinductive bias in ViTs. Besides, we apply the optimal CE configuration to 4recently released state-of-the-art ViTs, effectively boosting the correspondingperformances.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Besides, we apply the optimal CE configuration to 4recently released state-of-the-art ViTs, effectively boosting the correspondingperformances. Finally, a family of efficient hybrid CNNs/ViTs, dubbed CETNets,are released, which may serve as generic vision backbones. Specifically,CETNets achieve 84.9% Top-1 accuracy on ImageNet-1K (training from scratch),48.6% box mAP on the COCO benchmark, and 51.6% mIoU on the ADE20K,substantially improving the performances of the corresponding state-of-the-artbaselines.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='Ghost-free High Dynamic Range Imaging with Context-aware Transformer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='High dynamic range (HDR) deghosting algorithms aim to generate ghost-free HDRimages with realistic details. Restricted by the locality of the receptivefield, existing CNN-based methods are typically prone to producing ghostingartifacts and intensity distortions in the presence of large motion and severesaturation. In this paper, we propose a novel Context-Aware Vision Transformer(CA-ViT) for ghost-free high dynamic range imaging. The CA-ViT is designed as adual-branch architecture, which can jointly capture both global and localdependencies. Specifically, the global branch employs a window-basedTransformer encoder to model long-range object movements and intensityvariations to solve ghosting. For the local branch, we design a local contextextractor (LCE) to capture short-range image features and use the channelattention mechanism to select informative local details across the extractedfeatures to complement the global branch.', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'}), Document(page_content='For the local branch, we design a local contextextractor (LCE) to capture short-range image features and use the channelattention mechanism to select informative local details across the extractedfeatures to complement the global branch. By incorporating the CA-ViT as basiccomponents, we further build the HDR-Transformer, a hierarchical network toreconstruct high-quality ghost-free HDR images. Extensive experiments on threebenchmark datasets show that our approach outperforms state-of-the-art methodsqualitatively and quantitatively with considerably reduced computationalbudgets. Codes are available athttps://github.com/megvii-research/HDR-Transformer', metadata={'source': '/home/se1/se2024/tmp/chatchat/tmp4jvzwcpf/None.txt'})]
cuda:2
